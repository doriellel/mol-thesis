{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36da6186-7d29-49c4-9771-9aa97fb13c63",
   "metadata": {},
   "source": [
    "# Various utilities for processing and working with anthroscore files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0883d-4eaa-49aa-8764-1f0020d92314",
   "metadata": {},
   "source": [
    "### Obtain entities for anthroscore evaluation\n",
    "\n",
    "The entities used for the basic anthroscore evaluation (experiment 1) are taken from the mask and then manually revised. This is to make sure that plurals of LLM, LM (LLMs, LMs) are included, as the anthroscore tokenization does not recognize these as plural inflections of the singular form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5297bb9-4b95-4b1a-946e-24d7f0bf74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "    \n",
    "def create_anthroscore_entity_files(filename):\n",
    "    \"\"\"\n",
    "    this function reads the csv files and returns new txt files with the AI entities needed for the AnthroScore evaluation\n",
    "    :param filename: name of the file to be processed\n",
    "    :type sentence: string\n",
    "    \"\"\" \n",
    "    with open(f\"../experiment_1/anthroscore/entities/{filename}_entities.txt\",\"w\") as outfile:\n",
    "\n",
    "        entities = []\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        infile = open(f\"../experiment_1/anthroscore/expectations/{filename}.csv\",\"r\")\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "            AI_entity = normalized(row[3]) # these are the suggested AI masks\n",
    "            entities.append(AI_entity)\n",
    "\n",
    "        for entity in list(set(entities)):\n",
    "            writer.writerow([entity])\n",
    "\n",
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]\n",
    "            \n",
    "for file in files:\n",
    "    print(f\"Creating Anthroscore entity lists for {file}...\")\n",
    "    create_anthroscore_entity_files(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627d659-0304-4a60-aacf-193a6308e376",
   "metadata": {},
   "source": [
    "### Create predictions result from anthroscore output such that all desired information is retained.\n",
    "\n",
    "Anthroscore outputs the results into two types of files: sentence_scores, which are the individual scores per entity, and average scores, which is the final anthroscore for the sentence averaged over the individual entities. We are not interested in the latter, but the former contains limited information. The script below compiles files containing more information - the masked sentence, the AI phrase, mask and entity, as well as the suspected anthropomorphic component (or non-anthropomorphic verb/adjective). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90f9d17-0f2d-4994-bc94-609dbfd8a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "\n",
    "def convert_annotation(score):\n",
    "    \"\"\"\n",
    "     This function converts annotations to numerical values:\n",
    "     negative - 0, positive - 1, inclonclusive - 2\n",
    "    \"\"\" \n",
    "    if score in ['p','p1','p2','p3']:\n",
    "        score = '1'\n",
    "    elif score in ['n1','n2','n3']:\n",
    "        score = '0'\n",
    "    elif score == 'inc':\n",
    "        score = '2'\n",
    "    else:\n",
    "        print(\"score is malformed\")\n",
    "\n",
    "    return score\n",
    "\n",
    "def get_scores_dict(filename):\n",
    "\n",
    "    filedict = {}\n",
    "    \n",
    "    with open(f\"../experiment_2/anthroscore/expectations/csv/{filename}.csv\",\"r\") as csv_file:        \n",
    "        header = csv_file.readline()\n",
    "        reader = csv.reader(csv_file)\n",
    "        for row in reader:\n",
    "            sentence_id = row[0]\n",
    "            sentence_info = row[1:]\n",
    "            filedict[sentence_id] = sentence_info # IDs are unique\n",
    "\n",
    "    return filedict\n",
    "\n",
    "def concat_info(filename,filedict):\n",
    "\n",
    "    with open(f\"../experiment_2/anthroscore/predictions/csv/{filename}.csv\",\"w\") as outfile:\n",
    "\n",
    "        list_check = []\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        # new_header = ['id','sentence','masked_sentence','AI_phrase','suggested_mask','AI_entity',\n",
    "        # 'anthro_component','original_term','original_noun','expectation','anthroscore']\n",
    "        new_header = ['id','sentence','masked_sentence','AI_phrase','mask','AI_entity','component','expectation','prediction']\n",
    "        writer.writerow(new_header)\n",
    "        infile = open(f\"../experiment_2/anthroscore/predictions/anthroscore_output/sentence_scores/{filename}.csv\",\"r\")\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            #print(row)\n",
    "            \n",
    "            sentence_id = normalized(row[3])\n",
    "            sentence = normalized(row[1])\n",
    "            masked = normalized(row[2])\n",
    "            #original_term = normalized(row[6])\n",
    "            #original_noun = normalized(row[7])\n",
    "            #anthroscore = normalized(row[8])\n",
    "            anthroscore = normalized(row[4])            \n",
    "            info = [normalized(x) for x in filedict[sentence_id]]\n",
    "            #orig_score = convert_annotation(info[-1])\n",
    "            orig_score = info[-1]\n",
    "            \n",
    "            #write_to_file = [sentence_id,sentence,masked]+info[1:-1]+[original_term,original_noun,orig_score,anthroscore]\n",
    "            write_to_file = [sentence_id,sentence,masked]+info[-5:-1]+[orig_score,anthroscore]\n",
    "            \n",
    "            writer.writerow(write_to_file)\n",
    "           \n",
    "\n",
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]\n",
    "\n",
    "files_retest = [\"noun_phrases_positive-retest\"]\n",
    "\n",
    "for file in files_retest:\n",
    "    file_dict = get_scores_dict(file)\n",
    "    concat_info(file,file_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37932a-8aba-406e-9f4f-8ea512ddcf2a",
   "metadata": {},
   "source": [
    "### Iterate over removed_sentences file to review information\n",
    "\n",
    "(This was relevant for the manual review of the anthroscore results of experiment 1, which used anthroscore masks. Since many of those masks were irrelevant, they were removed. The script below was used for reviewing the removed sentences and making sure the filtering out of irrelevant masked sentences was consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fad9cfc9-66d1-4123-8af7-8aa09551b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_indices(text, substring):\n",
    "    return [match.start() for match in re.finditer(re.escape(substring), text)]\n",
    "\n",
    "column_names = ['id', 'sentence', 'masked_sentence', 'AI_phrase', 'mask', 'AI_entity', 'anthro_component', \n",
    "                    'anthroscore_entity', 'anthroscore_phrase', 'score', 'anthroscore']\n",
    "df = pd.read_csv(f\"../experiment_1/anthroscore/predictions/txt/removed_sentences.txt\", sep='\\t', header=None, names=column_names,index_col=False)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    masked = row['masked_sentence']\n",
    "    if masked.count('<mask>') > 1:\n",
    "        #print(f\"More than one <mask> found in ID: {row['id']}, Sentence: {row['masked_sentence']}\")\n",
    "        print(row['id'])\n",
    "        print(row['sentence'])\n",
    "        print(row['masked_sentence'])\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
