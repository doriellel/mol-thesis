{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b603e8ee-003a-4823-b485-3e7252161d88",
   "metadata": {},
   "source": [
    "## Get Inter-Annotater Agreement (IAA) rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f1247b91-d679-43c6-9146-b2961d3662c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8487ef2-8b9a-4221-8bc0-37c38604873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee0d18b1-198a-4ceb-9454-9fb0700fc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert txt to csv - fix encoding issue\n",
    "\n",
    "input_path = f\"../data/IAA/IAA_evaluation_set_Jelke_ANNOTATED.txt\"\n",
    "output_path = f\"../data/IAA/IAA_evaluation_set_Jelke_ANNOTATED.csv\"\n",
    "\n",
    "column_names = ['ID', 'Sentence', 'AI entity', 'score']\n",
    "df = pd.read_csv(input_path, sep='\\t', header=None, names=column_names,index_col=False)\n",
    "df.to_csv(output_path, index=False)  # comma is the default delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed18f5a-598d-4ec0-b447-97f454cf39ad",
   "metadata": {},
   "source": [
    "### Functions for obtaining statistics about IAA on both rater sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c4acc50-71a3-4cde-8f04-365a0e1984b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "\n",
    "def convert_annotation(score):\n",
    "    \"\"\"\n",
    "     This function converts annotations to numerical values:\n",
    "     negative - 0, positive - 1, inclonclusive - 2\n",
    "    \"\"\" \n",
    "    if score in ['p','p1','p2','p3']:\n",
    "        score = 1\n",
    "    elif score in ['n1','n2','n3']:\n",
    "        score = 0\n",
    "    elif score == 'inc':\n",
    "        score = 2\n",
    "    else:\n",
    "        print(\"score is malformed\")\n",
    "\n",
    "    return score\n",
    "    \n",
    "def get_rater_dict(rater):\n",
    "    \n",
    "    with open(f\"../data/IAA/IAA_evaluation_set_{rater}_ANNOTATED.csv\",\"r\") as infile:\n",
    "\n",
    "        rater_dict = {}\n",
    "\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "            sentence_id = normalized(row[0])\n",
    "            if sentence_id not in rater_dict:\n",
    "                rater_dict[sentence_id] = {'sentence':normalized(row[1]), 'entity':normalized(row[2]), 'score':int(normalized(row[3]))}\n",
    "            else:\n",
    "                print(\"why is the ID appearing twice?\")\n",
    "\n",
    "        return rater_dict\n",
    "\n",
    "def get_stats(rater_dict,rater,print_res=True):\n",
    "\n",
    "    files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]\n",
    "\n",
    "    annotations = {}\n",
    "\n",
    "    # initiate all counters\n",
    "    orig_pos_annotations = 0\n",
    "    orig_neg_annotations = 0\n",
    "    orig_inc_annotations = 0\n",
    "    rater_pos_annotations = 0\n",
    "    rater_neg_annotations = 0\n",
    "    rater_inc_annotations = 0\n",
    "    agreements = 0\n",
    "    total_disagreements = 0\n",
    "    pos_to_neg = 0\n",
    "    pos_to_inc = 0\n",
    "    neg_to_pos = 0\n",
    "    neg_to_inc = 0\n",
    "    inc_to_pos = 0\n",
    "    inc_to_neg = 0\n",
    "\n",
    "    for file in files:\n",
    "    \n",
    "        with open(f\"../data/evaluation_sentences_csv/{file}.csv\",\"r\") as infile:\n",
    "        \n",
    "            header = infile.readline()\n",
    "            reader = csv.reader(infile)\n",
    "        \n",
    "            for row in reader:\n",
    "                sentence_id = normalized(row[0])\n",
    "                sentence = normalized(row[1])\n",
    "                if sentence_id in rater_dict.keys():\n",
    "                    assert sentence == rater_dict[sentence_id]['sentence']\n",
    "                    rater_annotation = rater_dict[sentence_id]['score']\n",
    "                    AI_entity = rater_dict[sentence_id]['entity']\n",
    "                    sentence_annotation = convert_annotation(normalized(row[-1]))\n",
    "\n",
    "                    # count how many original annotations were of each class\n",
    "                    if sentence_annotation == 1:\n",
    "                        orig_pos_annotations += 1\n",
    "                    elif sentence_annotation == 0:\n",
    "                        orig_neg_annotations += 1\n",
    "                    elif sentence_annotation == 2:\n",
    "                        orig_inc_annotations += 1\n",
    "\n",
    "                    # count how many rater annotations were of each class\n",
    "                    if rater_annotation == 1:\n",
    "                        rater_pos_annotations += 1\n",
    "                    elif rater_annotation == 0:\n",
    "                        rater_neg_annotations += 1\n",
    "                    elif rater_annotation == 2:\n",
    "                        rater_inc_annotations += 1\n",
    "\n",
    "                    # add to annotations dict\n",
    "                    if sentence_id not in annotations:\n",
    "                        annotations[sentence_id] = {'sentence':sentence, 'entity':AI_entity , 'original_annotation':sentence_annotation, rater:rater_annotation}\n",
    "                    else:\n",
    "                        print(\"why is the ID appearing twice?\")\n",
    "\n",
    "                    # count agreements and disagreements of each type\n",
    "                    if sentence_annotation != rater_annotation:\n",
    "                        total_disagreements += 1\n",
    "                        #print(f\"Sentence: {sentence}\")\n",
    "                        #print(f\"Original annotation: {sentence_annotation}\")\n",
    "                        #print(f\"{rater}'s annotation: {rater_annotation}\")\n",
    "                        #print()\n",
    "                        if sentence_annotation == 1 and rater_annotation == 0:\n",
    "                            pos_to_neg += 1\n",
    "                        elif sentence_annotation == 1 and rater_annotation == 2:\n",
    "                            pos_to_inc += 1\n",
    "                        elif sentence_annotation == 0 and rater_annotation == 1:\n",
    "                            neg_to_pos += 1\n",
    "                        elif sentence_annotation == 0 and rater_annotation == 2:\n",
    "                            neg_to_inc += 1\n",
    "                        elif sentence_annotation == 2 and rater_annotation == 1:\n",
    "                            inc_to_pos += 1\n",
    "                        elif sentence_annotation == 2 and rater_annotation == 0:\n",
    "                            inc_to_neg += 1\n",
    "                        else:\n",
    "                            print(\"this case is unexpected.\")\n",
    "                    else:\n",
    "                        agreements += 1 \n",
    "\n",
    "    # assert counts were correct\n",
    "    total_cases = len(annotations.keys())\n",
    "    assert total_cases == len(rater_dict.keys())\n",
    "    assert total_cases == agreements + total_disagreements\n",
    "    \n",
    "    disagreement_on_pos_neg = pos_to_neg + pos_to_inc + neg_to_pos + neg_to_inc\n",
    "    disagreements_on_inc = total_disagreements - disagreement_on_pos_neg\n",
    "    assert disagreements_on_inc == inc_to_pos + inc_to_neg\n",
    "\n",
    "    if pos_to_neg == 1:\n",
    "        be = 'was'\n",
    "        s = ''\n",
    "    else:\n",
    "        be = 'were'\n",
    "        s = 's'\n",
    "    if neg_to_pos == 1:\n",
    "        be2 = 'was'\n",
    "        s2 = ''\n",
    "    else:\n",
    "        be2 = 'were'\n",
    "        s2 = 's'\n",
    "    if pos_to_inc == 1:\n",
    "        s3 = ''\n",
    "    else:\n",
    "        s3 = 's'\n",
    "    if neg_to_inc == 1:\n",
    "        s4 = ''\n",
    "    else:\n",
    "        s4 = 's'\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f\"{rater}'s set had {orig_pos_annotations} positive sentences, {orig_neg_annotations} negative sentences and {orig_inc_annotations} inconclusive cases.\")\n",
    "        print(f\"{rater} labeled {rater_pos_annotations} sentences as positive, {rater_neg_annotations} sentences as negative and {rater_inc_annotations} sentences as inconclusive.\")\n",
    "        print(f\"There are {agreements} matching annotations out of {total_cases} total cases, and {total_disagreements} disagreements.\")\n",
    "        print(f\"There are {disagreement_on_pos_neg} disagreements on positive or negative cases:\")\n",
    "        print(f\"{pos_to_neg} positive sentence{s} {be} labeled negative. {neg_to_pos} negative sentence{s2} {be2} labeled positive.\")\n",
    "        print(f\"{pos_to_inc} positive sentence{s3} and {neg_to_inc} negative sentence{s4} were labeled inconclusive.\")\n",
    "        print(f\"Out of the {disagreements_on_inc} disagreements on inconclusive cases, {inc_to_pos} were labeled as positive by the rater, and {inc_to_neg} were labeled as negative by the rater.\")\n",
    "        print()\n",
    "          \n",
    "    return annotations\n",
    "\n",
    "def print_mismatching_sentences(annotations_dict,rater):\n",
    "\n",
    "    for uniq_id, annotations in annotations_dict.items():\n",
    "\n",
    "        sentence_annotation = annotations['original_annotation']\n",
    "        rater_annotation = annotations[rater]\n",
    "\n",
    "        if sentence_annotation != rater_annotation:\n",
    "            print(f\"Sentence: {annotations['sentence']}\")\n",
    "            print(f\"AI entity: {annotations['entity']}\")\n",
    "            print(f\"Original annotation: {sentence_annotation}\")\n",
    "            print(f\"{rater}'s annotation: {rater_annotation}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1ff6b-47ab-461e-ab19-42c2907a4ccf",
   "metadata": {},
   "source": [
    "### Display overall rater statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4a9a98e-0040-415a-963a-e3c4053bb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pia's set had 15 positive sentences, 14 negative sentences and 13 inconclusive cases.\n",
      "Pia labeled 12 sentences as positive, 23 sentences as negative and 7 sentences as inconclusive.\n",
      "There are 25 matching annotations out of 42 total cases, and 17 disagreements.\n",
      "There are 6 disagreements on positive or negative cases:\n",
      "1 positive sentence was labeled negative. 0 negative sentences were labeled positive.\n",
      "4 positive sentences and 1 negative sentence were labeled inconclusive.\n",
      "Out of the 11 disagreements on inconclusive cases, 2 were labeled as positive by the rater, and 9 were labeled as negative by the rater.\n",
      "\n",
      "Jelke's set had 11 positive sentences, 10 negative sentences and 21 inconclusive cases.\n",
      "Jelke labeled 19 sentences as positive, 20 sentences as negative and 3 sentences as inconclusive.\n",
      "There are 18 matching annotations out of 42 total cases, and 24 disagreements.\n",
      "There are 5 disagreements on positive or negative cases:\n",
      "2 positive sentences were labeled negative. 2 negative sentences were labeled positive.\n",
      "0 positive sentences and 1 negative sentence were labeled inconclusive.\n",
      "Out of the 19 disagreements on inconclusive cases, 8 were labeled as positive by the rater, and 11 were labeled as negative by the rater.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pia_ratings = get_rater_dict('Pia')\n",
    "jelke_ratings = get_rater_dict('Jelke')\n",
    "\n",
    "orig_pia_annotations = get_stats(pia_ratings,'Pia')\n",
    "orig_jelke_annotations = get_stats(jelke_ratings,'Jelke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067ea56-90a3-4c90-adfd-1d62f2ca8d35",
   "metadata": {},
   "source": [
    "### Display mismatching sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f17126fd-8322-4cda-9992-540ea55c8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatching sentences:\n",
      "\n",
      "Sentence: While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue.\n",
      "AI entity: these autonomous and intelligent systems\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: These relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy LLMs.\n",
      "AI entity: untrustworthy LLMs\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment.\n",
      "AI entity: culturally aware LMs\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: To address this blind spot, this study introduces the AI Family Integration Index (AFII), a ten dimensional benchmarking framework that evaluates national preparedness for integrating emotionally intelligent AI into family and caregiving systems.\n",
      "AI entity: emotionally intelligent AI\n",
      "Original annotation: 2\n",
      "Pia's annotation: 1\n",
      "\n",
      "Sentence: While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content.\n",
      "AI entity: Large Language Models (LLMs)\n",
      "Original annotation: 1\n",
      "Pia's annotation: 2\n",
      "\n",
      "Sentence: Examination prioritization was performed by the AI, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.\n",
      "AI entity: the AI\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the model.\n",
      "AI entity: the model\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.\n",
      "AI entity: ChatGPT\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.\n",
      "AI entity: LLMs\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism.\n",
      "AI entity: LLMs\n",
      "Original annotation: 1\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: However their explicitly mention of malicious intent will be easily recognized and defended by LLMs.\n",
      "AI entity: LLMs\n",
      "Original annotation: 1\n",
      "Pia's annotation: 2\n",
      "\n",
      "Sentence: In this work, a dynamic system is controlled by multiple sensor-actuator agents, each of them commanding and observing parts of the system's input and output.\n",
      "AI entity: multiple sensor-actuator agents\n",
      "Original annotation: 1\n",
      "Pia's annotation: 2\n",
      "\n",
      "Sentence: Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights.\n",
      "AI entity: the LLM\n",
      "Original annotation: 1\n",
      "Pia's annotation: 2\n",
      "\n",
      "Sentence: Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena.\n",
      "AI entity: LMs\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: The recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation.\n",
      "AI entity: ChatGPT\n",
      "Original annotation: 2\n",
      "Pia's annotation: 0\n",
      "\n",
      "Sentence: Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence.\n",
      "AI entity: ChatGPT\n",
      "Original annotation: 2\n",
      "Pia's annotation: 1\n",
      "\n",
      "Sentence: In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs.\n",
      "AI entity: LLMs\n",
      "Original annotation: 0\n",
      "Pia's annotation: 2\n",
      "\n",
      "Sentence: This collaborative creative AI presents a new paradigm in AI, one that lets a team of two or more to come together to imagine and envision ideas that synergies well with interests of all members of the team.\n",
      "AI entity: collaborative creative AI\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks.\n",
      "AI entity: the untrustworthy third-party LLMs\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination.\n",
      "AI entity: LLM\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: We show that an “attentive” RNN-LM (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs.\n",
      "AI entity: an \"attentive\" RNN-LM\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: This work seeks to understand the strengths and limitations of editing methods, facilitating practical applications of communicative AI.\n",
      "AI entity: communicative AI\n",
      "Original annotation: 0\n",
      "Jelke's annotation: 2\n",
      "\n",
      "Sentence: We show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.\n",
      "AI entity: the LLM\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.\n",
      "AI entity: a light language model\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected.\n",
      "AI entity: AI\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.\n",
      "AI entity: an agent\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: Specifically, the structural features identified by our algorithm were found to be related to clinical observations of glaucoma.\n",
      "AI entity: our algorithm\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.\n",
      "AI entity: the model\n",
      "Original annotation: 0\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: In the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries.\n",
      "AI entity: the LLMs\n",
      "Original annotation: 1\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: The model does not differentiate between aerosol particles, cloud droplets, drizzle or rain drops.\n",
      "AI entity: The model\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: The results show that ChatGPT made more changes than the average post-editor.\n",
      "AI entity: ChatGPT\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.\n",
      "AI entity: ChatGPT\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.\n",
      "AI entity: GPT-3\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: 4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts.\n",
      "AI entity: LLMs\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: We cross-check with control cases to ensure that the AI is not randomly guessing and is indeed identifying an inherent structure.\n",
      "AI entity: the AI\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses.\n",
      "AI entity: the instruction-tuned model\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: Traditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy.\n",
      "AI entity: AI agents\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: The semantic dependency feature serves as a global signal and helps the model learn simile knowledge that can be applied to unseen domains.\n",
      "AI entity: the model\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.\n",
      "AI entity: larger and more advanced LLMs\n",
      "Original annotation: 2\n",
      "Jelke's annotation: 0\n",
      "\n",
      "Sentence: ChatGPT has learned language patterns and styles from a large dataset of internet texts, allowing it to provide answers that reflect common opinions, ideas, and language patterns found in the community.\n",
      "AI entity: ChatGPT\n",
      "Original annotation: 0\n",
      "Jelke's annotation: 1\n",
      "\n",
      "Sentence: However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize.\n",
      "AI entity: the model\n",
      "Original annotation: 1\n",
      "Jelke's annotation: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Mismatching sentences:\")\n",
    "print()\n",
    "print_mismatching_sentences(orig_pia_annotations,'Pia')\n",
    "print_mismatching_sentences(orig_jelke_annotations,'Jelke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fcd1f-593a-46cc-ad9f-d95c32fce040",
   "metadata": {},
   "source": [
    "### Calculate cohen Kappa score on positive-negative cases alone, and in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fef99853-d99c-4bd3-8d21-78f109eaf61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa score on Pia's set:\n",
      "including inconclusive: 0.39026473099914605\n",
      "without inconclusive: 0.9154929577464789\n",
      "\n",
      "Cohen Kappa score on Jelke's set:\n",
      "including inconclusive: 0.2198142414860681\n",
      "without inconclusive: 0.595959595959596\n"
     ]
    }
   ],
   "source": [
    "def get_cohen_kappa_score(rater,exclude_inconclusive=False):\n",
    "\n",
    "    rater_dict = get_rater_dict(rater)\n",
    "    annotations = get_stats(rater_dict,rater,print_res=False)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for value in annotations.values():\n",
    "        orig_annotation = value['original_annotation']\n",
    "        rater_annotation = value[rater]\n",
    "\n",
    "        if exclude_inconclusive == True:\n",
    "            if orig_annotation != 2 and rater_annotation != 2: # only include positive or negative agreement\n",
    "                y_true.append(orig_annotation)\n",
    "                y_pred.append(rater_annotation)\n",
    "\n",
    "        else:\n",
    "            y_true.append(orig_annotation)\n",
    "            y_pred.append(rater_annotation)\n",
    "\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    return cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "pia_cohen_kappa = get_cohen_kappa_score('Pia')\n",
    "pia_cohen_kappa_excl_inc = get_cohen_kappa_score('Pia',exclude_inconclusive=True)\n",
    "\n",
    "jelke_cohen_kappa = get_cohen_kappa_score('Jelke')\n",
    "jelke_cohen_kappa_excl_inc = get_cohen_kappa_score('Jelke',exclude_inconclusive=True)\n",
    "\n",
    "print(\"Cohen Kappa score on Pia's set:\")\n",
    "print(f\"including inconclusive: {pia_cohen_kappa}\")\n",
    "print(f\"without inconclusive: {pia_cohen_kappa_excl_inc}\")\n",
    "print()\n",
    "print(\"Cohen Kappa score on Jelke's set:\")\n",
    "print(f\"including inconclusive: {jelke_cohen_kappa}\")\n",
    "print(f\"without inconclusive: {jelke_cohen_kappa_excl_inc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d266a630-31da-45ee-9356-609af654dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pia's Cohen's Kappa for class 1: 0.620\n",
      "Pia's Cohen's Kappa for class 0: 0.492\n",
      "Pia's Cohen's Kappa for class 2: -0.021\n",
      "Jelke's Cohen's Kappa for class 1: 0.401\n",
      "Jelke's Cohen's Kappa for class 0: 0.219\n",
      "Jelke's Cohen's Kappa for class 2: 0.048\n"
     ]
    }
   ],
   "source": [
    "def cohen_kappa_per_class(cls,rater):\n",
    "\n",
    "    rater_dict = get_rater_dict(rater)\n",
    "    annotations = get_stats(rater_dict,rater,print_res=False)\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for value in annotations.values():\n",
    "        orig_annotation = value['original_annotation']\n",
    "        rater_annotation = value[rater]\n",
    "        y_true.append(orig_annotation)\n",
    "        y_pred.append(rater_annotation)\n",
    "\n",
    "    # convert to binary: 1 if label == cls, else 0\n",
    "    y_true_binary = [1 if label == cls else 0 for label in y_true]\n",
    "    y_pred_binary = [1 if label == cls else 0 for label in y_pred]\n",
    "    \n",
    "    kappa = cohen_kappa_score(y_true_binary, y_pred_binary)\n",
    "    print(f\"{rater}'s Cohen's Kappa for class {cls}: {kappa:.3f}\")\n",
    "\n",
    "cohen_kappa_per_class(1,'Pia')\n",
    "cohen_kappa_per_class(0,'Pia')\n",
    "cohen_kappa_per_class(2,'Pia')\n",
    "cohen_kappa_per_class(1,'Jelke')\n",
    "cohen_kappa_per_class(0,'Jelke')\n",
    "cohen_kappa_per_class(2,'Jelke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73ed48-1bfe-44f5-a2a5-666bcf126566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
