{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575639f3-fe85-4f8a-8073-933f3920e36f",
   "metadata": {},
   "source": [
    "### Create evaluation sets for AtypicalAnimacy (with masks)\n",
    "\n",
    "This notebook contains code that converts the annotated evaluation sets (in csv format) to AtypicalAnimacy flavored evaluation sets.\n",
    "The AtypicalAnimacy evaluation relies, in addition to the sentence, on a masked sentence, a 3-word context of the original masked phrase, and a 3-word context of the mask. AtypicalAnimacy provides their own masking and context extraction as part of their evaluation pipeline that prepares simultaneously the training sets and the test sets for their own dataset. In order to evaluate their model with the AnthroAI dataset presented in my thesis, I have created my own manual maksing strategy, and rely on that also for context extraction. \n",
    "Chapter 3 of the thesis describes and justifies the strategy according to which elements were kept as part of the context, and which elements were masked alongside the AI entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606305cf-2c1c-4407-a335-bc9ba57bd7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SentenceID', 'currentSentence', 'prevSentence', 'nextSentence', 'Abstract']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def concat_pkl(directory_path):\n",
    "    \"\"\"\n",
    "    this function takes a path to a directory containing .pkl files of dataframes,\n",
    "    and returns a single concatenated dataframe.\n",
    "    The dataframes contain sentences and their unique ID, \n",
    "    as well as the previous and next sentences in the abstract from which the sentence is taken.\n",
    "\n",
    "    :param directory_path: path to a directory containing .pkl files\n",
    "    :type directory_path: string\n",
    "    :return: pd.Dataframe()\n",
    "    \"\"\" \n",
    "    pkl_files = [f for f in os.listdir(directory_path) if f.endswith('.pkl')]\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for file in pkl_files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No pkl files read successfully.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "path = '../data/dataframes'\n",
    "all_sentences_df = concat_pkl(path)\n",
    "print(all_sentences_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd43022-504b-455d-946e-84a14b30626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "\n",
    "def convert_annotation(score):\n",
    "    \"\"\"\n",
    "     This function converts annotations to numerical values:\n",
    "     negative - 0, positive - 1, inclonclusive - 2\n",
    "    \"\"\" \n",
    "    if score in ['p','p1','p2','p3']:\n",
    "        score = '1'\n",
    "    elif score in ['n1','n2','n3']:\n",
    "        score = '0'\n",
    "    elif score == 'inc':\n",
    "        score = '2'\n",
    "    else:\n",
    "        print(\"score is malformed\")\n",
    "\n",
    "    return score\n",
    "\n",
    "def mask_sentence(sentence,position,masked_str):\n",
    "    \"\"\"\n",
    "    this function takes a sentence, an index and a string to be masked,\n",
    "    and returns a masked sentence\n",
    "\n",
    "    :param sentence: sentence from the evaluation set\n",
    "    :type sentence: string\n",
    "    :param position: index of the mask in the sentence\n",
    "    :type position: integer\n",
    "    :param masked_str: phrase in the sentence that should be masked\n",
    "    :type masked_str: string\n",
    "    :return: masked sentence (string)\n",
    "    \"\"\" \n",
    "    masked_sentence = []\n",
    "    len_mask = len(masked_str)\n",
    "    masked_sentence.append(sentence[:position])\n",
    "    masked_sentence.append(\"<mask>\")\n",
    "    position_after_mask = position+len_mask\n",
    "    masked_sentence.append(sentence[position_after_mask:])\n",
    "    masked_sentence = ''.join(masked_sentence)\n",
    "\n",
    "    return masked_sentence\n",
    "\n",
    "def get_context3w(masked_sentence,masked_str):\n",
    "    \"\"\"\n",
    "    this function takes a masked sentence and the string that was masked,\n",
    "    and returns both the 3-word context of the original masked string and a 3-word context of the mask\n",
    "\n",
    "    :param masked_sentence: sentence that has been masked\n",
    "    :type sentence: string\n",
    "    :param masked_str: original phrase in the sentence that was masked\n",
    "    :type masked_str: string\n",
    "    :return: 3-word context and masked 3-word context (tuple of strings)\n",
    "    \"\"\" \n",
    "    masked_sentence_list = masked_sentence.split(' ')\n",
    "    try:\n",
    "        mask_index = masked_sentence_list.index(\"<mask>\")\n",
    "        mask_str = '<mask>'\n",
    "    except ValueError: # the mask is followed by punctuation or a possessive marker\n",
    "        printcheck = True\n",
    "        mask_plus_punct = [x for x in masked_sentence_list if '<mask>' in x][0] # assumes there is exactly one\n",
    "        mask_index = masked_sentence_list.index(mask_plus_punct)\n",
    "        mask_str = mask_plus_punct\n",
    "        masked_str = mask_str.replace('<mask>',masked_str) # restore punctuation and possesive marker to masked string\n",
    "    if mask_index <= 3:\n",
    "        prev_words = masked_sentence_list[:mask_index]\n",
    "    else:\n",
    "        start_index = mask_index - 3\n",
    "        prev_words = masked_sentence_list[start_index:mask_index]\n",
    "    if len(masked_sentence_list) < mask_index + 3:\n",
    "        next_words = masked_sentence_list[mask_index+1:]\n",
    "    else:\n",
    "        end_index = mask_index + 4\n",
    "        next_words = masked_sentence_list[mask_index+1:end_index]\n",
    "    prev_words = ' '.join(prev_words)\n",
    "    next_words = ' '.join(next_words)\n",
    "    context_3w = prev_words + ' ' + masked_str + ' ' + next_words # masked_str is the original text\n",
    "    context_3w_masked = prev_words + ' ' + mask_str + ' ' + next_words # mask_str is <mask> (with or without punct)\n",
    "\n",
    "    return context_3w,context_3w_masked\n",
    "\n",
    "def get_masked_sentence_and_context(sentence,AI_phrase,mask):\n",
    "    \"\"\"\n",
    "    this function takes a sentence and string to be masked, and returns a list of masked versions\n",
    "    as well as 3-word context and 3-word masked context  \n",
    "\n",
    "    :param sentence: sentence from the evaluation set\n",
    "    :type sentence: string\n",
    "    :param AI_phrase: entire AI phrase (including contextual components that should not be masked - \n",
    "    used for identification for when there are multiple occurrences of the mask in the sentence)\n",
    "    :type AI_phrase: string\n",
    "    :param mask: phrase in the sentence that should be masked\n",
    "    :type mask: string\n",
    "    :return: list of tuples containing the sentence, the masked sentence, the 3-word context and the masked 3-word context\n",
    "    \"\"\" \n",
    "    mask_position = [match.start() for match in re.finditer(rf'\\b{re.escape(mask)}\\b', sentence, flags=re.IGNORECASE)]\n",
    "    masked_sentences_and_context = []\n",
    "    \n",
    "    if len(mask_position) == 1: # simple case, only one occurrence of mask\n",
    "        position = mask_position[0]\n",
    "        masked_sentence = mask_sentence(sentence,position,mask)\n",
    "        context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "        context_3w = context_3w_tuple[0]\n",
    "        context_3w_masked = context_3w_tuple[1]\n",
    "        masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "    elif len(mask_position) > 1: # more than one occurrence of the mask\n",
    "        AI_phrase_position = [match.start() for match in re.finditer(rf'\\b{re.escape(AI_phrase)}\\b', sentence,flags=re.IGNORECASE)]\n",
    "        mask_in_phrase_position = [match.start() for match in re.finditer(rf'\\b{re.escape(mask)}\\b', AI_phrase)]\n",
    "        if len(AI_phrase_position) == 1 and len(mask_in_phrase_position) == 1: # found mask by comparing position in AI phrase\n",
    "            position = mask_in_phrase_position[0] + AI_phrase_position[0]\n",
    "            masked_sentence = mask_sentence(sentence,position,mask)\n",
    "            context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "            context_3w = context_3w_tuple[0]\n",
    "            context_3w_masked = context_3w_tuple[1]\n",
    "            masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "        else: # cannot identify, masking all occurrences to be safe\n",
    "            for i,position in enumerate(mask_position):\n",
    "                masked_sentence = mask_sentence(sentence,position,mask)\n",
    "                context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "                context_3w = context_3w_tuple[0]\n",
    "                context_3w_masked = context_3w_tuple[1]\n",
    "                masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "    else: \n",
    "        # brute-force - do not replicate !!! this was done after manual revision and confirmation\n",
    "        masked_sentence = sentence.replace(mask, \"<mask>\")\n",
    "        context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "        context_3w = context_3w_tuple[0]\n",
    "        context_3w_masked = context_3w_tuple[1]\n",
    "        masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "\n",
    "    return masked_sentences_and_context   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b5f81b-1f94-455f-bc56-857fea6483da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def create_AA_evaluation_set(filename):\n",
    "    \"\"\"\n",
    "    this function takes a csv file and returns a new csv file with expanded information used by the AtypicalAnimacy model for evaluation: \n",
    "    in addition to the original information, it adds the previous and next sentence (if applicable), the masked sentence, \n",
    "    the 3-word context of the masked phrase as well as the 3-word context of the mask.\n",
    "\n",
    "    :param filename: name of the file to be processed\n",
    "    :type sentence: string\n",
    "    \"\"\" \n",
    "    with open(f\"../data/AtypicalAnimacy_evaluation/experiment_2/{filename}.csv\",\"w\") as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        AA_header = ['id','Previous Sentence','Current Sentence','Masked Sentence','Next Sentence','AI Phrase','Suggested Mask','AI Entity',\n",
    "                      'Anthropomorphic Component','Target Expression','Animated','context3w','context3wmasked']\n",
    "        writer.writerow(AA_header)\n",
    "        infile = open(f\"../data/evaluation_sentences_csv/{filename}.csv\",\"r\")\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            sentence_id = normalized(row[0])\n",
    "            sentence = normalized(row[1])\n",
    "            orig_sentence_id = '_'.join(sentence_id.split('_')[2:5]) # remove class and dataset prefix added during preprocessing\n",
    "            \n",
    "            # retrieve previous and next sentences from dataframe all sentences dataframe\n",
    "            sentence_info = all_sentences_df[all_sentences_df['SentenceID'] == orig_sentence_id]\n",
    "            if not sentence_info.empty:\n",
    "                current_sentence = sentence_info.iloc[0]['currentSentence']\n",
    "                prev_sent = sentence_info.iloc[0]['prevSentence']\n",
    "                next_sent = sentence_info.iloc[0]['nextSentence']\n",
    "            else: # this is only to capture errors - this does not happen\n",
    "                print(f\"error: the sentence with the id {sentence_id} was not found in the dataframe\")\n",
    "                prev_sent = ''\n",
    "                next_sent = ''\n",
    "                \n",
    "            # get masked sentence, context3w and context3wmasked\n",
    "            AI_phrase = normalized(row[2])\n",
    "            mask = normalized(row[3])\n",
    "            AI_entity = normalized(row[4])\n",
    "            anthro_component = normalized(row[5])\n",
    "            score = convert_annotation(normalized(row[6])) # convert p,n,inc scores to 0,1,2 scores\n",
    "            masked_sentences_and_context = get_masked_sentence_and_context(sentence,AI_phrase,mask)\n",
    "            for m in masked_sentences_and_context:\n",
    "                writer.writerow([sentence_id,prev_sent,m[0],m[1],next_sent,AI_phrase,mask,AI_entity,anthro_component,mask,score,m[2],m[3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee44448-bdc0-4aa5-bdd5-9dd39c3d3024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating AA-flavored evaluation sets for adjective_phrases_inconclusive...\n",
      "Creating AA-flavored evaluation sets for adjective_phrases_negative...\n",
      "Creating AA-flavored evaluation sets for adjective_phrases_positive...\n",
      "Creating AA-flavored evaluation sets for comparisons_inconclusive...\n",
      "Creating AA-flavored evaluation sets for noun_phrases_positive...\n",
      "Creating AA-flavored evaluation sets for possessives_positive...\n",
      "Creating AA-flavored evaluation sets for verb_objects_inconclusive...\n",
      "Creating AA-flavored evaluation sets for verb_objects_negative...\n",
      "Creating AA-flavored evaluation sets for verb_objects_positive...\n",
      "Creating AA-flavored evaluation sets for verb_subjects_inconclusive...\n",
      "Creating AA-flavored evaluation sets for verb_subjects_negative...\n",
      "Creating AA-flavored evaluation sets for verb_subjects_positive...\n"
     ]
    }
   ],
   "source": [
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Creating AA-flavored evaluation sets for {file}...\")\n",
    "    create_AA_evaluation_set(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac47ab-bd83-4997-b439-4907535b462a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
