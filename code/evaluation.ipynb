{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36da6186-7d29-49c4-9771-9aa97fb13c63",
   "metadata": {},
   "source": [
    "# Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31972cd3-bea4-42a5-bdd1-9607d7dc37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582795-5cb6-4cea-a21d-175b255bf265",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48574186-0357-4a2d-adaf-22c0ea0e3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def get_precision_recall_f1_and_accuracy(dataset,experiment,model,print_res=False,map_pred=False,include_inconclusive=False):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}*_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    average_ = 'macro'\n",
    "\n",
    "    if include_inconclusive == True:\n",
    "        df = df[df['expectation'] != '2']\n",
    "        average_ = 'weighted'\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    y_true = df['expectation'].astype(int)\n",
    "    y_pred = df['prediction'].astype(int)\n",
    "    if map_pred == True:\n",
    "        y_pred = [0 if pred == 2 else pred for pred in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    \n",
    "    correct = (y_true == y_pred).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "        print(f\"Correct predictions (accuracy): {correct} / {total}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1 Score:  {f1:.4f}\")\n",
    "        print()\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def get_accuracy(dataset,experiment,model):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}*_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    df['expectation'] = df['expectation'].astype(int)\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "\n",
    "    #print(\"Unique values in expectation:\", df['expectation'].unique())\n",
    "    #print(\"Unique values in prediction:\", df['prediction'].unique())\n",
    "\n",
    "    accuracy = accuracy_score(df['expectation'], df['prediction'])    \n",
    "    correct = (df['expectation'] == df['prediction']).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "    print(f\"Correct predictions: {correct} / {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "\n",
    "def get_inconclusive_trends(dataset,experiment,model):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}_inconclusive_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    y_true = df['expectation'].astype(int)\n",
    "    y_pred = df['prediction'].astype(int)\n",
    "\n",
    "    num_positive_pred = len([pred for pred in y_pred if pred == 1])\n",
    "    num_negative_pred = len([pred for pred in y_pred if pred == 0])\n",
    "    num_inconclusive_pred = len([pred for pred in y_pred if pred == 2])\n",
    "\n",
    "    if model == 'AtypicalAnimacy':\n",
    "        note = \" (AtypicalAnimacy does not have inconclusive predictions)\"\n",
    "    else:\n",
    "        note = \"\"\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}_inconclusive\")\n",
    "    \n",
    "    print(f\"Number of inconclusive cases in the dataset: {len(y_true)}\")\n",
    "    print(f\"Number total predictions: {len(y_pred)}\")\n",
    "    print(f\"Number of positive predictions: {num_positive_pred}\")\n",
    "    print(f\"Number of negative predictions: {num_negative_pred}\")\n",
    "    print(f\"Number of inconlcusive predictions: {num_inconclusive_pred}\", note)\n",
    "    print()\n",
    "\n",
    "def compare_anthroscore_scenarios(dataset):\n",
    "\n",
    "    exp_1_no_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=False,map_pred=False)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 1 no mapping\")\n",
    "    print(f\"Accuracy: {exp_1_no_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_1_no_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_1_no_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_1_no_map[3]:.4f}\")\n",
    "    print()\n",
    "    exp_1_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=False,map_pred=True)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 1 with mapping\")\n",
    "    print(f\"Accuracy: {exp_1_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_1_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_1_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_1_map[3]:.4f}\")\n",
    "    print()\n",
    "    exp_2_no_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=False,map_pred=False)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 2 no mapping\")\n",
    "    print(f\"Accuracy: {exp_2_no_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_2_no_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_2_no_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_2_no_map[3]:.4f}\")\n",
    "    print()\n",
    "    exp_2_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=False,map_pred=True)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 2 with mapping\")\n",
    "    print(f\"Accuracy: {exp_2_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_2_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_2_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_2_map[3]:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcb9e4-ac86-4cee-bf40-ab88f146f6bb",
   "metadata": {},
   "source": [
    "### Comparison between anthroscore and AtypicalAnimacy on the binary cases only\n",
    "\n",
    "Get accuracy, precision, recall and f1-score (for multilabel evaluation sets adjective_phrases, verb_subjects and verb_objects), and accuracy for adjective_phrases, verb_subjects, verb_objects, noun_phrases and possessives.\n",
    "\n",
    "(for the last two accuracy is the same as recall, since there is only 1 expectation - positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8ad7ac4-dc4b-47bb-b630-f131c960dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 50 / 107\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.4877\n",
      "Recall:    0.3763\n",
      "F1 Score:  0.3037\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 62 / 106\n",
      "Accuracy: 0.5849\n",
      "Precision: 0.4481\n",
      "Recall:    0.4625\n",
      "F1 Score:  0.4130\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 39 / 120\n",
      "Accuracy: 0.3250\n",
      "Precision: 0.3338\n",
      "Recall:    0.3063\n",
      "F1 Score:  0.2804\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 71 / 120\n",
      "Accuracy: 0.5917\n",
      "Precision: 0.4481\n",
      "Recall:    0.4865\n",
      "F1 Score:  0.4271\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 75 / 147\n",
      "Accuracy: 0.5102\n",
      "Precision: 0.5186\n",
      "Recall:    0.4608\n",
      "F1 Score:  0.3996\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 94 / 144\n",
      "Accuracy: 0.6528\n",
      "Precision: 0.4355\n",
      "Recall:    0.5356\n",
      "F1 Score:  0.4800\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 56 / 150\n",
      "Accuracy: 0.3733\n",
      "Precision: 0.3700\n",
      "Recall:    0.3608\n",
      "F1 Score:  0.3470\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 99 / 150\n",
      "Accuracy: 0.6600\n",
      "Precision: 0.4392\n",
      "Recall:    0.5355\n",
      "F1 Score:  0.4825\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 68 / 145\n",
      "Accuracy: 0.4690\n",
      "Precision: 0.5415\n",
      "Recall:    0.4419\n",
      "F1 Score:  0.3946\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 84 / 145\n",
      "Accuracy: 0.5793\n",
      "Precision: 0.3870\n",
      "Recall:    0.4985\n",
      "F1 Score:  0.4321\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 63 / 150\n",
      "Accuracy: 0.4200\n",
      "Precision: 0.5109\n",
      "Recall:    0.4001\n",
      "F1 Score:  0.3742\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 101 / 150\n",
      "Accuracy: 0.6733\n",
      "Precision: 0.4501\n",
      "Recall:    0.5734\n",
      "F1 Score:  0.5029\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: noun_phrases\n",
      "Correct predictions: 7 / 66\n",
      "Accuracy: 0.1061\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: possessives\n",
      "Correct predictions: 1 / 57\n",
      "Accuracy: 0.0175\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: noun_phrases\n",
      "Correct predictions: 14 / 70\n",
      "Accuracy: 0.2000\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: possessives\n",
      "Correct predictions: 47 / 60\n",
      "Accuracy: 0.7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"] # recall and precision matters\n",
    "single_label_datasets = ['noun_phrases', 'possessives'] # only recall i.e. accuracy matters\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "    get_precision_recall_f1_and_accuracy(dataset,'experiment_1','AtypicalAnimacy',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "    get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "    get_precision_recall_f1_and_accuracy(dataset,'experiment_2','AtypicalAnimacy',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "\n",
    "for dataset in single_label_datasets:\n",
    "    get_accuracy(dataset,'experiment_1','anthroscore')\n",
    "for dataset in single_label_datasets:\n",
    "    get_accuracy(dataset,'experiment_2','AtypicalAnimacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e483b-752c-4fad-926f-82b22b64d7ab",
   "metadata": {},
   "source": [
    "### compare scenarios for handling anthroscore inconclusive predictions\n",
    "\n",
    "compare accuracy, precision, recall and f1-score for two anthroscore scenarios: \n",
    "\n",
    "1. keeping inconclusive predictions (anthroscore between 1 and -1)\n",
    "2. mapping inconclusive predictions to negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf7c0d2-5884-45c4-8d49-b44d94cc855e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scenario: adjective_phrases anthroscore experiment 1 no mapping\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.4877\n",
      "Recall:    0.3763\n",
      "F1 Score:  0.3037\n",
      "\n",
      "scenario: adjective_phrases anthroscore experiment 1 with mapping\n",
      "Accuracy: 0.4766\n",
      "Precision: 0.4296\n",
      "Recall:    0.3712\n",
      "F1 Score:  0.2753\n",
      "\n",
      "scenario: adjective_phrases anthroscore experiment 2 no mapping\n",
      "Accuracy: 0.3250\n",
      "Precision: 0.3338\n",
      "Recall:    0.3063\n",
      "F1 Score:  0.2804\n",
      "\n",
      "scenario: adjective_phrases anthroscore experiment 2 with mapping\n",
      "Accuracy: 0.4083\n",
      "Precision: 0.2981\n",
      "Recall:    0.3421\n",
      "F1 Score:  0.2594\n",
      "\n",
      "scenario: verb_objects anthroscore experiment 1 no mapping\n",
      "Accuracy: 0.5102\n",
      "Precision: 0.5186\n",
      "Recall:    0.4608\n",
      "F1 Score:  0.3996\n",
      "\n",
      "scenario: verb_objects anthroscore experiment 1 with mapping\n",
      "Accuracy: 0.4626\n",
      "Precision: 0.3818\n",
      "Recall:    0.3743\n",
      "F1 Score:  0.2750\n",
      "\n",
      "scenario: verb_objects anthroscore experiment 2 no mapping\n",
      "Accuracy: 0.3733\n",
      "Precision: 0.3700\n",
      "Recall:    0.3608\n",
      "F1 Score:  0.3470\n",
      "\n",
      "scenario: verb_objects anthroscore experiment 2 with mapping\n",
      "Accuracy: 0.4667\n",
      "Precision: 0.3074\n",
      "Recall:    0.3676\n",
      "F1 Score:  0.3078\n",
      "\n",
      "scenario: verb_subjects anthroscore experiment 1 no mapping\n",
      "Accuracy: 0.4690\n",
      "Precision: 0.5415\n",
      "Recall:    0.4419\n",
      "F1 Score:  0.3946\n",
      "\n",
      "scenario: verb_subjects anthroscore experiment 1 with mapping\n",
      "Accuracy: 0.4483\n",
      "Precision: 0.4074\n",
      "Recall:    0.3818\n",
      "F1 Score:  0.2800\n",
      "\n",
      "scenario: verb_subjects anthroscore experiment 2 no mapping\n",
      "Accuracy: 0.4200\n",
      "Precision: 0.5109\n",
      "Recall:    0.4001\n",
      "F1 Score:  0.3742\n",
      "\n",
      "scenario: verb_subjects anthroscore experiment 2 with mapping\n",
      "Accuracy: 0.4667\n",
      "Precision: 0.4227\n",
      "Recall:    0.3874\n",
      "F1 Score:  0.2990\n",
      "\n",
      "scenario: noun_phrases anthroscore experiment 1 no mapping\n",
      "Accuracy: 0.1061\n",
      "Precision: 0.3333\n",
      "Recall:    0.0354\n",
      "F1 Score:  0.0639\n",
      "\n",
      "scenario: noun_phrases anthroscore experiment 1 with mapping\n",
      "Accuracy: 0.1061\n",
      "Precision: 0.5000\n",
      "Recall:    0.0530\n",
      "F1 Score:  0.0959\n",
      "\n",
      "scenario: noun_phrases anthroscore experiment 2 no mapping\n",
      "Accuracy: 0.0857\n",
      "Precision: 0.3333\n",
      "Recall:    0.0286\n",
      "F1 Score:  0.0526\n",
      "\n",
      "scenario: noun_phrases anthroscore experiment 2 with mapping\n",
      "Accuracy: 0.0857\n",
      "Precision: 0.5000\n",
      "Recall:    0.0429\n",
      "F1 Score:  0.0789\n",
      "\n",
      "scenario: possessives anthroscore experiment 1 no mapping\n",
      "Accuracy: 0.0175\n",
      "Precision: 0.3333\n",
      "Recall:    0.0058\n",
      "F1 Score:  0.0115\n",
      "\n",
      "scenario: possessives anthroscore experiment 1 with mapping\n",
      "Accuracy: 0.0175\n",
      "Precision: 0.5000\n",
      "Recall:    0.0088\n",
      "F1 Score:  0.0172\n",
      "\n",
      "scenario: possessives anthroscore experiment 2 no mapping\n",
      "Accuracy: 0.1167\n",
      "Precision: 0.3333\n",
      "Recall:    0.0389\n",
      "F1 Score:  0.0697\n",
      "\n",
      "scenario: possessives anthroscore experiment 2 with mapping\n",
      "Accuracy: 0.1167\n",
      "Precision: 0.5000\n",
      "Recall:    0.0583\n",
      "F1 Score:  0.1045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"] # recall and precision matters\n",
    "single_label_datasets = ['noun_phrases', 'possessives'] # only recall i.e. accuracy matters\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    compare_anthroscore_scenarios(dataset)\n",
    "for dataset in single_label_datasets:\n",
    "    compare_anthroscore_scenarios(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdace3-2ec9-4599-b98a-5229d57e4069",
   "metadata": {},
   "source": [
    "### Get results for anthroscore including inconclusive expectations\n",
    "\n",
    "Get accuracy, precision, recall and accuracy for anthroscore when the inconclusive cases are not excluded. They are excluded in the comparison between the two models because AtypicalAnimacy only provides positive and negative scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3062430d-bbb0-49ca-8c9b-9bc2251c6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 50 / 107\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.5683\n",
      "Recall:    0.4673\n",
      "F1 Score:  0.3644\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 75 / 147\n",
      "Accuracy: 0.5102\n",
      "Precision: 0.5569\n",
      "Recall:    0.5102\n",
      "F1 Score:  0.4273\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 68 / 145\n",
      "Accuracy: 0.4690\n",
      "Precision: 0.5684\n",
      "Recall:    0.4690\n",
      "F1 Score:  0.4077\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 39 / 120\n",
      "Accuracy: 0.3250\n",
      "Precision: 0.3868\n",
      "Recall:    0.3250\n",
      "F1 Score:  0.3071\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 56 / 150\n",
      "Accuracy: 0.3733\n",
      "Precision: 0.4143\n",
      "Recall:    0.3733\n",
      "F1 Score:  0.3772\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 63 / 150\n",
      "Accuracy: 0.4200\n",
      "Precision: 0.5466\n",
      "Recall:    0.4200\n",
      "F1 Score:  0.3919\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 50 / 107\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.5683\n",
      "Recall:    0.4673\n",
      "F1 Score:  0.3644\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 75 / 147\n",
      "Accuracy: 0.5102\n",
      "Precision: 0.5569\n",
      "Recall:    0.5102\n",
      "F1 Score:  0.4273\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 68 / 145\n",
      "Accuracy: 0.4690\n",
      "Precision: 0.5684\n",
      "Recall:    0.4690\n",
      "F1 Score:  0.4077\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 39 / 120\n",
      "Accuracy: 0.3250\n",
      "Precision: 0.3868\n",
      "Recall:    0.3250\n",
      "F1 Score:  0.3071\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 56 / 150\n",
      "Accuracy: 0.3733\n",
      "Precision: 0.4143\n",
      "Recall:    0.3733\n",
      "F1 Score:  0.3772\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 63 / 150\n",
      "Accuracy: 0.4200\n",
      "Precision: 0.5466\n",
      "Recall:    0.4200\n",
      "F1 Score:  0.3919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get accuracy, precision, recall and accuracy for anthroscore when the inconclusive cases are not excluded \n",
    "# they are excluded in the comparison between the two models because AtypicalAnimacy only provides positive and negative scores\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=True,map_pred=False,include_inconclusive=True)\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=True,map_pred=False,include_inconclusive=True)\n",
    "\n",
    "evaluation_inconclusives()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2427c0-4c9a-4f55-8bf7-32920707dfb7",
   "metadata": {},
   "source": [
    "### Get the trends for the inconclusive sets only\n",
    "\n",
    "Count for each inconclusive set, how many positive, negative and inconclusive (in the case of anthroscore only) predictions were given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35aa72e9-ee58-47d9-925d-b6436a5f75be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 17\n",
      "Number total predictions: 17\n",
      "Number of positive predictions: 1\n",
      "Number of negative predictions: 15\n",
      "Number of inconlcusive predictions: 1 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 21\n",
      "Number total predictions: 21\n",
      "Number of positive predictions: 2\n",
      "Number of negative predictions: 15\n",
      "Number of inconlcusive predictions: 4 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 17\n",
      "Number total predictions: 17\n",
      "Number of positive predictions: 2\n",
      "Number of negative predictions: 15\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 21\n",
      "Number total predictions: 21\n",
      "Number of positive predictions: 4\n",
      "Number of negative predictions: 17\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 29\n",
      "Number total predictions: 29\n",
      "Number of positive predictions: 3\n",
      "Number of negative predictions: 18\n",
      "Number of inconlcusive predictions: 8 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 8\n",
      "Number of negative predictions: 10\n",
      "Number of inconlcusive predictions: 9 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 16\n",
      "Number of negative predictions: 11\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 17\n",
      "Number of negative predictions: 10\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 2\n",
      "Number of negative predictions: 21\n",
      "Number of inconlcusive predictions: 10 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 1\n",
      "Number of negative predictions: 21\n",
      "Number of inconlcusive predictions: 11 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 17\n",
      "Number of negative predictions: 16\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 16\n",
      "Number of negative predictions: 17\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 43\n",
      "Number total predictions: 43\n",
      "Number of positive predictions: 1\n",
      "Number of negative predictions: 39\n",
      "Number of inconlcusive predictions: 3 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 50\n",
      "Number total predictions: 50\n",
      "Number of positive predictions: 3\n",
      "Number of negative predictions: 42\n",
      "Number of inconlcusive predictions: 5 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 42\n",
      "Number total predictions: 42\n",
      "Number of positive predictions: 19\n",
      "Number of negative predictions: 23\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 50\n",
      "Number total predictions: 50\n",
      "Number of positive predictions: 26\n",
      "Number of negative predictions: 24\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inconclusives = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\",\"comparisons\"] \n",
    "\n",
    "for dataset in inconclusives:\n",
    "    get_inconclusive_trends(dataset,'experiment_1','anthroscore')\n",
    "    get_inconclusive_trends(dataset,'experiment_2','anthroscore')\n",
    "    get_inconclusive_trends(dataset,'experiment_1','AtypicalAnimacy')\n",
    "    get_inconclusive_trends(dataset,'experiment_2','AtypicalAnimacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c592e4-4ac9-455c-a4cd-8c46890957bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
