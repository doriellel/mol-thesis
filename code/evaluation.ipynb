{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36da6186-7d29-49c4-9771-9aa97fb13c63",
   "metadata": {},
   "source": [
    "# Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "31972cd3-bea4-42a5-bdd1-9607d7dc37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582795-5cb6-4cea-a21d-175b255bf265",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "48574186-0357-4a2d-adaf-22c0ea0e3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_per_category(dataset,experiment,model,print_res=False,include_inconclusive=False):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "\n",
    "    positive = os.path.join(directory, f\"{dataset}_positive_{experiment}_{model}_predictions.csv\")\n",
    "    negative = os.path.join(directory, f\"{dataset}_negative_{experiment}_{model}_predictions.csv\")\n",
    "    inconclusive = os.path.join(directory, f\"{dataset}_inconclusive_{experiment}_{model}_predictions.csv\")\n",
    "\n",
    "    df_pos = pd.read_csv(positive)\n",
    "    df_neg = pd.read_csv(negative)\n",
    "    df_inc = pd.read_csv(inconclusive)\n",
    "\n",
    "    if include_inconclusive == True: # include inconclusive gold\n",
    "        df = pd.concat([df_pos, df_neg, df_inc], ignore_index=True) \n",
    "        incl_or_excl = 'including'\n",
    "    else:\n",
    "        df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "        incl_or_excl = 'excluding'\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    exp0_pred0 = df[(df['expectation'] == 0) & (df['prediction'] == 0)] # 0,0\n",
    "    exp0_pred1 = df[(df['expectation'] == 0) & (df['prediction'] == 1)] # 0,1\n",
    "    exp0_pred2 = df[(df['expectation'] == 0) & (df['prediction'] == 2)] # 0,2\n",
    "    \n",
    "    exp1_pred0 = df[(df['expectation'] == 1) & (df['prediction'] == 0)] # 1,0\n",
    "    exp1_pred1 = df[(df['expectation'] == 1) & (df['prediction'] == 1)] # 1,1\n",
    "    exp1_pred2 = df[(df['expectation'] == 1) & (df['prediction'] == 2)] # 1,2\n",
    "\n",
    "    exp2_pred0 = df[(df['expectation'] == 2) & (df['prediction'] == 0)] # 2,0\n",
    "    exp2_pred1 = df[(df['expectation'] == 2) & (df['prediction'] == 1)] # 2,1\n",
    "    exp2_pred2 = df[(df['expectation'] == 2) & (df['prediction'] == 2)] # 2,2\n",
    "\n",
    "    precision_0 = len(exp0_pred0) / (len(exp0_pred0)+len(exp1_pred0)+len(exp2_pred0)) # true positives / true positives + false positives\n",
    "    recall_0 = len(exp0_pred0) / (len(exp0_pred0)+len(exp0_pred1)+len(exp0_pred2)) # true positives / true positives + false negatives\n",
    "    f1_score_0 = (2 * precision_0 * recall_0) / (precision_0+recall_0)\n",
    "    \n",
    "    precision_1 = len(exp1_pred1) / (len(exp1_pred1)+len(exp0_pred1)+len(exp2_pred1)) # true positives / true positives + false positives\n",
    "    recall_1 = len(exp1_pred1) / (len(exp1_pred1)+len(exp1_pred0)+len(exp1_pred2)) # true positives / true positives + false negatives\n",
    "    f1_score_1 = (2 * precision_1 * recall_1) / (precision_1+recall_1)\n",
    "\n",
    "    try:\n",
    "        precision_2 = len(exp2_pred2) / (len(exp2_pred2)+len(exp0_pred2)+len(exp1_pred2)) # true positives / true positives + false positives\n",
    "    except ZeroDivisionError:\n",
    "        precision_2 = 0.0\n",
    "    try:\n",
    "        recall_2 = len(exp2_pred2) / (len(exp2_pred2)+len(exp2_pred0)+len(exp2_pred1)) # denominator is 0\n",
    "    except ZeroDivisionError:\n",
    "        recall_2 = 0.0\n",
    "    try:\n",
    "        f1_score_2 = (2 * precision_2 * recall_2) / (precision_2+recall_2)\n",
    "    except ZeroDivisionError:\n",
    "        f1_score_2 = 0.0\n",
    "\n",
    "    if print_res == True:\n",
    "        \n",
    "        print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "        print(f\"Precision for positive cases: {precision_1:.3f}\")\n",
    "        print(f\"Recall for positive cases: {recall_1:.3f}\")\n",
    "        print(f\"f1-score for positive cases: {f1_score_1:.3f}\")\n",
    "        print(f\"Precision for negative cases: {precision_0:.3f}\")\n",
    "        print(f\"Recall for negative cases: {recall_0:.3f}\")\n",
    "        print(f\"f1-score for negative cases: {f1_score_0:.3f}\")\n",
    "        print(f\"Precision for inconclusive cases: {precision_2:.3f}\")\n",
    "        print(f\"Recall for inconclusive cases: {recall_2:.3f}\")\n",
    "        print(f\"f1-score for inconclusive cases: {f1_score_2:.3f}\")\n",
    "        print()\n",
    "\n",
    "    # sanity_check\n",
    "    if model == 'anthroscore':\n",
    "        aggregate_precision = (precision_0 + precision_1 + precision_2) / 3\n",
    "        aggregate_recall = (recall_0 + recall_1 + recall_2) / 3\n",
    "        aggregate_f1 = (f1_score_0 + f1_score_1 + f1_score_2) / 3\n",
    "    elif model == 'AtypicalAnimacy':\n",
    "        aggregate_precision = (precision_0 + precision_1) / 2\n",
    "        aggregate_recall = (recall_0 + recall_1) / 2\n",
    "        aggregate_f1 = (f1_score_0 + f1_score_1) / 2\n",
    "\n",
    "    print(f\"Precision for all cases: {aggregate_precision:.3f}\")\n",
    "    print(f\"Recall for all cases: {aggregate_recall:.3f}\")\n",
    "    print(f\"f1-score for all cases: {aggregate_f1:.3f}\")\n",
    "    print()\n",
    "\n",
    "def get_precision_recall_f1_and_accuracy(dataset,experiment,model,print_res=False,map_pred=False,include_inconclusive=False):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "\n",
    "    positive = os.path.join(directory, f\"{dataset}_positive_{experiment}_{model}_predictions.csv\")\n",
    "    negative = os.path.join(directory, f\"{dataset}_negative_{experiment}_{model}_predictions.csv\")\n",
    "    inconclusive = os.path.join(directory, f\"{dataset}_inconclusive_{experiment}_{model}_predictions.csv\")\n",
    "\n",
    "    df_pos = pd.read_csv(positive)\n",
    "    df_neg = pd.read_csv(negative)\n",
    "    df_inc = pd.read_csv(inconclusive)\n",
    "\n",
    "    if include_inconclusive == True: # include inconclusive gold\n",
    "        df = pd.concat([df_pos, df_neg, df_inc], ignore_index=True) \n",
    "        average_ = 'weighted'\n",
    "        incl_or_excl = 'including'\n",
    "    else:\n",
    "        df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "        average_ = 'macro'\n",
    "        incl_or_excl = 'excluding'\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # Filter out all rows where the prediction is 2\n",
    "    # df = df[df['prediction'] != 2]\n",
    "\n",
    "    y_true = df['expectation'].astype(int)\n",
    "    y_pred = df['prediction'].astype(int)\n",
    "    if map_pred == True:\n",
    "        y_pred = [0 if pred == 2 else pred for pred in y_pred]\n",
    "\n",
    "    # filter out cases which have a prediction of 2\n",
    "    #mask = (y_true != 2) & (y_pred != 2)\n",
    "    #y_true_filtered = y_true[mask]\n",
    "    #y_pred_filtered = y_pred[mask]\n",
    "    #if include_inconclusive == False:\n",
    "        #y_true = y_true_filtered\n",
    "        #y_pred = y_pred_filtered\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    \n",
    "    correct = (y_true == y_pred).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "        print(f\"average: {average_} (we are {incl_or_excl} inconclusive cases from expectations)\")\n",
    "        print(f\"Correct predictions (accuracy): {correct} / {total}\")\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall:    {recall:.3f}\")\n",
    "        print(f\"F1 Score:  {f1:.3f}\")\n",
    "        print()\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def get_accuracy(dataset,experiment,model):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "\n",
    "    # this is only relevant to positive cases\n",
    "    path_to_files = os.path.join(directory, f\"{dataset}_positive_{experiment}_{model}_predictions.csv\") \n",
    "    df = pd.read_csv(path_to_files)\n",
    "        \n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    df['expectation'] = df['expectation'].astype(int)\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "\n",
    "    #print(\"Unique values in expectation:\", df['expectation'].unique())\n",
    "    #print(\"Unique values in prediction:\", df['prediction'].unique())\n",
    "\n",
    "    accuracy = accuracy_score(df['expectation'], df['prediction'])    \n",
    "    correct = (df['expectation'] == df['prediction']).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "    print(f\"Correct predictions: {correct} / {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print()\n",
    "\n",
    "def get_inconclusive_trends(dataset,experiment,model):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}_inconclusive_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    y_true = df['expectation'].astype(int)\n",
    "    y_pred = df['prediction'].astype(int)\n",
    "\n",
    "    num_positive_pred = len([pred for pred in y_pred if pred == 1])\n",
    "    num_negative_pred = len([pred for pred in y_pred if pred == 0])\n",
    "    num_inconclusive_pred = len([pred for pred in y_pred if pred == 2])\n",
    "\n",
    "    if model == 'AtypicalAnimacy':\n",
    "        note = \" (AtypicalAnimacy does not have inconclusive predictions)\"\n",
    "    else:\n",
    "        note = \"\"\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}_inconclusive\")\n",
    "    \n",
    "    print(f\"Number of inconclusive cases in the dataset: {len(y_true)}\")\n",
    "    print(f\"Number total predictions: {len(y_pred)}\")\n",
    "    print(f\"Number of positive predictions: {num_positive_pred}\")\n",
    "    print(f\"Number of negative predictions: {num_negative_pred}\")\n",
    "    print(f\"Number of inconlcusive predictions: {num_inconclusive_pred}\", note)\n",
    "    print()\n",
    "\n",
    "def compare_anthroscore_scenarios(dataset):\n",
    "\n",
    "    exp_1_no_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=False,map_pred=False)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 1 no mapping\")\n",
    "    print(f\"Accuracy: {exp_1_no_map[0]:.3f}\")\n",
    "    print(f\"Precision: {exp_1_no_map[1]:.3f}\")\n",
    "    print(f\"Recall:    {exp_1_no_map[2]:.3f}\")\n",
    "    print(f\"F1 Score:  {exp_1_no_map[3]:.3f}\")\n",
    "    print()\n",
    "    exp_1_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=False,map_pred=True)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 1 with mapping\")\n",
    "    print(f\"Accuracy: {exp_1_map[0]:.3f}\")\n",
    "    print(f\"Precision: {exp_1_map[1]:.3f}\")\n",
    "    print(f\"Recall:    {exp_1_map[2]:.3f}\")\n",
    "    print(f\"F1 Score:  {exp_1_map[3]:.3f}\")\n",
    "    print()\n",
    "    exp_2_no_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=False,map_pred=False)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 2 no mapping\")\n",
    "    print(f\"Accuracy: {exp_2_no_map[0]:.3f}\")\n",
    "    print(f\"Precision: {exp_2_no_map[1]:.3f}\")\n",
    "    print(f\"Recall:    {exp_2_no_map[2]:.3f}\")\n",
    "    print(f\"F1 Score:  {exp_2_no_map[3]:.3f}\")\n",
    "    print()\n",
    "    exp_2_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=False,map_pred=True)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 2 with mapping\")\n",
    "    print(f\"Accuracy: {exp_2_map[0]:.3f}\")\n",
    "    print(f\"Precision: {exp_2_map[1]:.3f}\")\n",
    "    print(f\"Recall:    {exp_2_map[2]:.3f}\")\n",
    "    print(f\"F1 Score:  {exp_2_map[3]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14db21-738c-4088-8f53-cc1503410dc2",
   "metadata": {},
   "source": [
    "### Obtain precision, recall and f1-scores per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2184693c-fcb1-4f91-a0ce-42506f041e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Precision for positive cases: 1.000\n",
      "Recall for positive cases: 0.114\n",
      "f1-score for positive cases: 0.204\n",
      "Precision for negative cases: 0.544\n",
      "Recall for negative cases: 0.956\n",
      "f1-score for negative cases: 0.694\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.515\n",
      "Recall for all cases: 0.356\n",
      "f1-score for all cases: 0.299\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Precision for positive cases: 0.905\n",
      "Recall for positive cases: 0.432\n",
      "f1-score for positive cases: 0.585\n",
      "Precision for negative cases: 0.632\n",
      "Recall for negative cases: 0.956\n",
      "f1-score for negative cases: 0.761\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.769\n",
      "Recall for all cases: 0.694\n",
      "f1-score for all cases: 0.673\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Precision for positive cases: 1.000\n",
      "Recall for positive cases: 0.125\n",
      "f1-score for positive cases: 0.222\n",
      "Precision for negative cases: 0.645\n",
      "Recall for negative cases: 0.984\n",
      "f1-score for negative cases: 0.779\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.548\n",
      "Recall for all cases: 0.370\n",
      "f1-score for all cases: 0.334\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_objects\n",
      "Precision for positive cases: 0.789\n",
      "Recall for positive cases: 0.804\n",
      "f1-score for positive cases: 0.796\n",
      "Precision for negative cases: 0.817\n",
      "Recall for negative cases: 0.803\n",
      "f1-score for negative cases: 0.810\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.803\n",
      "Recall for all cases: 0.803\n",
      "f1-score for all cases: 0.803\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Precision for positive cases: 1.000\n",
      "Recall for positive cases: 0.145\n",
      "f1-score for positive cases: 0.254\n",
      "Precision for negative cases: 0.581\n",
      "Recall for negative cases: 0.877\n",
      "f1-score for negative cases: 0.699\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.527\n",
      "Recall for all cases: 0.341\n",
      "f1-score for all cases: 0.318\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_subjects\n",
      "Precision for positive cases: 0.829\n",
      "Recall for positive cases: 0.618\n",
      "f1-score for positive cases: 0.708\n",
      "Precision for negative cases: 0.704\n",
      "Recall for negative cases: 0.877\n",
      "f1-score for negative cases: 0.781\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.767\n",
      "Recall for all cases: 0.748\n",
      "f1-score for all cases: 0.745\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Precision for positive cases: 0.571\n",
      "Recall for positive cases: 0.154\n",
      "f1-score for positive cases: 0.242\n",
      "Precision for negative cases: 0.482\n",
      "Recall for negative cases: 0.574\n",
      "f1-score for negative cases: 0.524\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.351\n",
      "Recall for all cases: 0.243\n",
      "f1-score for all cases: 0.256\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Precision for positive cases: 0.962\n",
      "Recall for positive cases: 0.481\n",
      "f1-score for positive cases: 0.641\n",
      "Precision for negative cases: 0.630\n",
      "Recall for negative cases: 0.979\n",
      "f1-score for negative cases: 0.767\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.796\n",
      "Recall for all cases: 0.730\n",
      "f1-score for all cases: 0.704\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "Precision for positive cases: 0.609\n",
      "Recall for positive cases: 0.241\n",
      "f1-score for positive cases: 0.346\n",
      "Precision for negative cases: 0.559\n",
      "Recall for negative cases: 0.508\n",
      "f1-score for negative cases: 0.532\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.389\n",
      "Recall for all cases: 0.250\n",
      "f1-score for all cases: 0.293\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_objects\n",
      "Precision for positive cases: 0.804\n",
      "Recall for positive cases: 0.776\n",
      "f1-score for positive cases: 0.789\n",
      "Precision for negative cases: 0.806\n",
      "Recall for negative cases: 0.831\n",
      "f1-score for negative cases: 0.818\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.805\n",
      "Recall for all cases: 0.803\n",
      "f1-score for all cases: 0.804\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "Precision for positive cases: 0.909\n",
      "Recall for positive cases: 0.179\n",
      "f1-score for positive cases: 0.299\n",
      "Precision for negative cases: 0.560\n",
      "Recall for negative cases: 0.689\n",
      "f1-score for negative cases: 0.618\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.490\n",
      "Recall for all cases: 0.289\n",
      "f1-score for all cases: 0.305\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_subjects\n",
      "Precision for positive cases: 0.917\n",
      "Recall for positive cases: 0.786\n",
      "f1-score for positive cases: 0.846\n",
      "Precision for negative cases: 0.826\n",
      "Recall for negative cases: 0.934\n",
      "f1-score for negative cases: 0.877\n",
      "Precision for inconclusive cases: 0.000\n",
      "Recall for inconclusive cases: 0.000\n",
      "f1-score for inconclusive cases: 0.000\n",
      "\n",
      "Precision for all cases: 0.871\n",
      "Recall for all cases: 0.860\n",
      "f1-score for all cases: 0.862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"]\n",
    "experiments = ['experiment_1', 'experiment_2']\n",
    "\n",
    "for experiment in experiments:\n",
    "    for dataset in multi_label_datasets:\n",
    "        get_metrics_per_category(dataset,experiment,'anthroscore',print_res=True)\n",
    "        get_metrics_per_category(dataset,experiment,'AtypicalAnimacy',print_res=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafcb9e4-ac86-4cee-bf40-ab88f146f6bb",
   "metadata": {},
   "source": [
    "### Comparison between anthroscore and AtypicalAnimacy on the binary cases only\n",
    "\n",
    "Get accuracy, precision, recall and f1-score (for multilabel evaluation sets adjective_phrases, verb_subjects and verb_objects), and accuracy for adjective_phrases, verb_subjects, verb_objects, noun_phrases and possessives.\n",
    "\n",
    "(for the last two accuracy is the same as recall, since there is only 1 expectation - positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e8ad7ac4-dc4b-47bb-b630-f131c960dbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 48 / 89\n",
      "Accuracy: 0.539\n",
      "Precision: 0.515\n",
      "Recall:    0.356\n",
      "F1 Score:  0.299\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: adjective_phrases\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 62 / 89\n",
      "Accuracy: 0.697\n",
      "Precision: 0.769\n",
      "Recall:    0.694\n",
      "F1 Score:  0.673\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 67 / 117\n",
      "Accuracy: 0.573\n",
      "Precision: 0.548\n",
      "Recall:    0.370\n",
      "F1 Score:  0.334\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_objects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 94 / 117\n",
      "Accuracy: 0.803\n",
      "Precision: 0.803\n",
      "Recall:    0.803\n",
      "F1 Score:  0.803\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 58 / 112\n",
      "Accuracy: 0.518\n",
      "Precision: 0.527\n",
      "Recall:    0.341\n",
      "F1 Score:  0.318\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_subjects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 84 / 112\n",
      "Accuracy: 0.750\n",
      "Precision: 0.767\n",
      "Recall:    0.748\n",
      "F1 Score:  0.745\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 35 / 99\n",
      "Accuracy: 0.354\n",
      "Precision: 0.351\n",
      "Recall:    0.243\n",
      "F1 Score:  0.256\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: adjective_phrases\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 71 / 99\n",
      "Accuracy: 0.717\n",
      "Precision: 0.796\n",
      "Recall:    0.730\n",
      "F1 Score:  0.704\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 47 / 123\n",
      "Accuracy: 0.382\n",
      "Precision: 0.389\n",
      "Recall:    0.250\n",
      "F1 Score:  0.293\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_objects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 99 / 123\n",
      "Accuracy: 0.805\n",
      "Precision: 0.805\n",
      "Recall:    0.803\n",
      "F1 Score:  0.804\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 52 / 117\n",
      "Accuracy: 0.444\n",
      "Precision: 0.490\n",
      "Recall:    0.289\n",
      "F1 Score:  0.305\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_subjects\n",
      "average: macro (we are excluding inconclusive cases from expectations)\n",
      "Correct predictions (accuracy): 101 / 117\n",
      "Accuracy: 0.863\n",
      "Precision: 0.871\n",
      "Recall:    0.860\n",
      "F1 Score:  0.862\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: noun_phrases\n",
      "Correct predictions: 7 / 66\n",
      "Accuracy: 0.106\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: noun_phrases\n",
      "Correct predictions: 31 / 66\n",
      "Accuracy: 0.470\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: possessives\n",
      "Correct predictions: 1 / 57\n",
      "Accuracy: 0.018\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: possessives\n",
      "Correct predictions: 17 / 57\n",
      "Accuracy: 0.298\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: noun_phrases\n",
      "Correct predictions: 6 / 70\n",
      "Accuracy: 0.086\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: noun_phrases\n",
      "Correct predictions: 14 / 70\n",
      "Accuracy: 0.200\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: possessives\n",
      "Correct predictions: 7 / 60\n",
      "Accuracy: 0.117\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: possessives\n",
      "Correct predictions: 47 / 60\n",
      "Accuracy: 0.783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"] # recall and precision matters\n",
    "single_label_datasets = ['noun_phrases', 'possessives'] # only recall i.e. accuracy matters\n",
    "\n",
    "experiments = ['experiment_1', 'experiment_2']\n",
    "\n",
    "for experiment in experiments:\n",
    "    for dataset in multi_label_datasets:\n",
    "        get_precision_recall_f1_and_accuracy(dataset,experiment,'anthroscore',print_res=True,map_pred=False)\n",
    "        get_precision_recall_f1_and_accuracy(dataset,experiment,'AtypicalAnimacy',print_res=True,map_pred=False)\n",
    "\n",
    "for experiment in experiments:\n",
    "    for dataset in single_label_datasets:\n",
    "        get_accuracy(dataset,experiment,'anthroscore')\n",
    "        get_accuracy(dataset,experiment,'AtypicalAnimacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdace3-2ec9-4599-b98a-5229d57e4069",
   "metadata": {},
   "source": [
    "### Get results for anthroscore including inconclusive expectations\n",
    "\n",
    "Get accuracy, precision, recall and accuracy for anthroscore when the inconclusive cases are not excluded. They are excluded in the comparison between the two models because AtypicalAnimacy only provides positive and negative scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3062430d-bbb0-49ca-8c9b-9bc2251c6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Precision for positive cases: 0.833\n",
      "Recall for positive cases: 0.114\n",
      "f1-score for positive cases: 0.200\n",
      "Precision for negative cases: 0.457\n",
      "Recall for negative cases: 0.956\n",
      "f1-score for negative cases: 0.619\n",
      "Precision for inconclusive cases: 0.167\n",
      "Recall for inconclusive cases: 0.059\n",
      "f1-score for inconclusive cases: 0.087\n",
      "\n",
      "Precision for all cases: 0.486\n",
      "Recall for all cases: 0.376\n",
      "f1-score for all cases: 0.302\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Precision for positive cases: 0.700\n",
      "Recall for positive cases: 0.125\n",
      "f1-score for positive cases: 0.212\n",
      "Precision for negative cases: 0.545\n",
      "Recall for negative cases: 0.984\n",
      "f1-score for negative cases: 0.702\n",
      "Precision for inconclusive cases: 0.292\n",
      "Recall for inconclusive cases: 0.259\n",
      "f1-score for inconclusive cases: 0.275\n",
      "\n",
      "Precision for all cases: 0.512\n",
      "Recall for all cases: 0.456\n",
      "f1-score for all cases: 0.396\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Precision for positive cases: 0.800\n",
      "Recall for positive cases: 0.145\n",
      "f1-score for positive cases: 0.246\n",
      "Precision for negative cases: 0.467\n",
      "Recall for negative cases: 0.877\n",
      "f1-score for negative cases: 0.610\n",
      "Precision for inconclusive cases: 0.357\n",
      "Recall for inconclusive cases: 0.303\n",
      "f1-score for inconclusive cases: 0.328\n",
      "\n",
      "Precision for all cases: 0.541\n",
      "Recall for all cases: 0.442\n",
      "f1-score for all cases: 0.395\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Precision for positive cases: 0.500\n",
      "Recall for positive cases: 0.154\n",
      "f1-score for positive cases: 0.235\n",
      "Precision for negative cases: 0.380\n",
      "Recall for negative cases: 0.574\n",
      "f1-score for negative cases: 0.458\n",
      "Precision for inconclusive cases: 0.121\n",
      "Recall for inconclusive cases: 0.190\n",
      "f1-score for inconclusive cases: 0.148\n",
      "\n",
      "Precision for all cases: 0.334\n",
      "Recall for all cases: 0.306\n",
      "f1-score for all cases: 0.280\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "Precision for positive cases: 0.452\n",
      "Recall for positive cases: 0.241\n",
      "f1-score for positive cases: 0.315\n",
      "Precision for negative cases: 0.478\n",
      "Recall for negative cases: 0.508\n",
      "f1-score for negative cases: 0.493\n",
      "Precision for inconclusive cases: 0.180\n",
      "Recall for inconclusive cases: 0.333\n",
      "f1-score for inconclusive cases: 0.234\n",
      "\n",
      "Precision for all cases: 0.370\n",
      "Recall for all cases: 0.361\n",
      "f1-score for all cases: 0.347\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "Precision for positive cases: 0.833\n",
      "Recall for positive cases: 0.179\n",
      "f1-score for positive cases: 0.294\n",
      "Precision for negative cases: 0.438\n",
      "Recall for negative cases: 0.689\n",
      "f1-score for negative cases: 0.535\n",
      "Precision for inconclusive cases: 0.262\n",
      "Recall for inconclusive cases: 0.333\n",
      "f1-score for inconclusive cases: 0.293\n",
      "\n",
      "Precision for all cases: 0.511\n",
      "Recall for all cases: 0.400\n",
      "f1-score for all cases: 0.374\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get accuracy, precision, recall and accuracy for anthroscore when the inconclusive cases are not excluded \n",
    "# they are excluded in the comparison between the two models because AtypicalAnimacy only provides positive and negative scores\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    get_metrics_per_category(dataset,'experiment_1','anthroscore',print_res=True,include_inconclusive=True)\n",
    "    #get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=True,map_pred=False,include_inconclusive=True)\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    get_metrics_per_category(dataset,'experiment_2','anthroscore',print_res=True,include_inconclusive=True)\n",
    "    #get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=True,map_pred=False,include_inconclusive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2427c0-4c9a-4f55-8bf7-32920707dfb7",
   "metadata": {},
   "source": [
    "### Get the trends for the inconclusive sets only\n",
    "\n",
    "Count for each inconclusive set, how many positive, negative and inconclusive (in the case of anthroscore only) predictions were given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "35aa72e9-ee58-47d9-925d-b6436a5f75be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 17\n",
      "Number total predictions: 17\n",
      "Number of positive predictions: 1\n",
      "Number of negative predictions: 15\n",
      "Number of inconlcusive predictions: 1 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 21\n",
      "Number total predictions: 21\n",
      "Number of positive predictions: 2\n",
      "Number of negative predictions: 15\n",
      "Number of inconlcusive predictions: 4 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 17\n",
      "Number total predictions: 17\n",
      "Number of positive predictions: 2\n",
      "Number of negative predictions: 15\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: adjective_phrases_inconclusive\n",
      "Number of inconclusive cases in the dataset: 21\n",
      "Number total predictions: 21\n",
      "Number of positive predictions: 4\n",
      "Number of negative predictions: 17\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 3\n",
      "Number of negative predictions: 17\n",
      "Number of inconlcusive predictions: 7 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 8\n",
      "Number of negative predictions: 10\n",
      "Number of inconlcusive predictions: 9 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 16\n",
      "Number of negative predictions: 11\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_objects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 27\n",
      "Number total predictions: 27\n",
      "Number of positive predictions: 17\n",
      "Number of negative predictions: 10\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 2\n",
      "Number of negative predictions: 21\n",
      "Number of inconlcusive predictions: 10 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 1\n",
      "Number of negative predictions: 21\n",
      "Number of inconlcusive predictions: 11 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 17\n",
      "Number of negative predictions: 16\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_subjects_inconclusive\n",
      "Number of inconclusive cases in the dataset: 33\n",
      "Number total predictions: 33\n",
      "Number of positive predictions: 16\n",
      "Number of negative predictions: 17\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 42\n",
      "Number total predictions: 42\n",
      "Number of positive predictions: 1\n",
      "Number of negative predictions: 38\n",
      "Number of inconlcusive predictions: 3 \n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 50\n",
      "Number total predictions: 50\n",
      "Number of positive predictions: 3\n",
      "Number of negative predictions: 42\n",
      "Number of inconlcusive predictions: 5 \n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 42\n",
      "Number total predictions: 42\n",
      "Number of positive predictions: 19\n",
      "Number of negative predictions: 23\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: comparisons_inconclusive\n",
      "Number of inconclusive cases in the dataset: 50\n",
      "Number total predictions: 50\n",
      "Number of positive predictions: 26\n",
      "Number of negative predictions: 24\n",
      "Number of inconlcusive predictions: 0  (AtypicalAnimacy does not have inconclusive predictions)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inconclusives = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\",\"comparisons\"] \n",
    "\n",
    "for dataset in inconclusives:\n",
    "    get_inconclusive_trends(dataset,'experiment_1','anthroscore')\n",
    "    get_inconclusive_trends(dataset,'experiment_2','anthroscore')\n",
    "    get_inconclusive_trends(dataset,'experiment_1','AtypicalAnimacy')\n",
    "    get_inconclusive_trends(dataset,'experiment_2','AtypicalAnimacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26d859-68e3-4273-bab0-540ba0878479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
