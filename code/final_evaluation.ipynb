{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36da6186-7d29-49c4-9771-9aa97fb13c63",
   "metadata": {},
   "source": [
    "# Create final evaluation sets and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31972cd3-bea4-42a5-bdd1-9607d7dc37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f992cd-3854-4f91-a939-f1a013f97358",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90f9d17-0f2d-4994-bc94-609dbfd8a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "\n",
    "def get_final_prediction(score,model):\n",
    "    \"\"\"\n",
    "     This function converts the anthroscore/AtypicalAnimacy scores to a single numerical value in {0,1,2}\n",
    "    \"\"\"     \n",
    "    AtypicalAnimacy_threshold = 0.3 # this was calculated during the experiment.\n",
    "    score = float(score)\n",
    "\n",
    "    if model == 'anthroscore':\n",
    "        if score > 1.0:\n",
    "            pred = '1'\n",
    "        elif score < -1.0:\n",
    "            pred = '0'\n",
    "        else:\n",
    "            pred = '2'\n",
    "            \n",
    "    elif model == 'AtypicalAnimacy':\n",
    "        if score > AtypicalAnimacy_threshold:\n",
    "            pred = '1'\n",
    "        else:\n",
    "            pred = '0'\n",
    "\n",
    "    return pred\n",
    "\n",
    "def create_final_eval_file(filename,experiment,model,all_indices_dict):\n",
    "    \"\"\"\n",
    "    this function reads info from csv file and writes it to a file with uniform structure to facilitate evaluation.\n",
    "\n",
    "    :param filename (str): name of the file \n",
    "    :param experiment (str): specify the experiment - used in input and output paths, and for obtaining correct indices\n",
    "    :param experiment (str): specify the model - used to obtain final prediction {0,1,2} based on the anthro/AtypicalAnimacy score\n",
    "    :param experiment (dict): pre-defined dictionary containing experiment+model string as key and index dict as value\n",
    "    \n",
    "    \"\"\" \n",
    "    with open(f\"../final_sets/{filename}_{experiment}_{model}_predictions.csv\",\"w\") as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        new_header = ['id','sentence','masked_sentence','AI_phrase','mask','component','expectation','model_score','prediction']\n",
    "        writer.writerow(new_header)\n",
    "        infile = open(f\"../{experiment}/{model}/predictions/csv/{filename}.csv\",\"r\")\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "\n",
    "        eval_set = f\"{experiment}_{model}\"\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            indices = all_indices_dict[eval_set]\n",
    "            \n",
    "            sentence_id = normalized(row[indices['id']])\n",
    "            sentence = normalized(row[indices['sent']])\n",
    "            masked_sent = normalized(row[indices['masked_sent']])\n",
    "            AI_phrase = normalized(row[indices['phrase']])\n",
    "            mask = normalized(row[indices['mask']])\n",
    "            component = normalized(row[indices['comp']])\n",
    "            expectation = (normalized(row[indices['exp']])) # should be numerical value {0,1,2}\n",
    "            expectation = int(float(expectation))\n",
    "            prediction = normalized(row[indices['pred']])\n",
    "\n",
    "\n",
    "            final_pred = get_final_prediction(prediction,model)\n",
    "            \n",
    "            write_to_file = [sentence_id,sentence,masked_sent,AI_phrase,mask,component,expectation,prediction,final_pred]\n",
    "            \n",
    "            writer.writerow(write_to_file)\n",
    "        \n",
    "        print(f\"Created {filename}_{experiment}_{model}_predictions.csv in ../final_sets/\")\n",
    "\n",
    "all_indices_dict = {'experiment_1_anthroscore':{'id':0,'sent':1,'masked_sent':2,'phrase':3,'mask':4,'comp':6,'exp':7,'pred':8},\n",
    "              'experiment_1_AtypicalAnimacy':{'id':0,'sent':2,'masked_sent':3,'phrase':5,'mask':6,'comp':8,'exp':10,'pred':15},\n",
    "              'experiment_2_anthroscore':{'id':0,'sent':1,'masked_sent':2,'phrase':3,'mask':4,'comp':6,'exp':7,'pred':8},\n",
    "              'experiment_2_AtypicalAnimacy':{'id':0,'sent':2,'masked_sent':3,'phrase':5,'mask':6,'comp':8,'exp':9,'pred':10}\n",
    "             }\n",
    "\n",
    "#for file in files:\n",
    "    #create_final_eval_file(file,'experiment_2','anthroscore',all_indices_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582795-5cb6-4cea-a21d-175b255bf265",
   "metadata": {},
   "source": [
    "### Get evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48574186-0357-4a2d-adaf-22c0ea0e3d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions: 50 / 107\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.4877\n",
      "Recall:    0.3763\n",
      "F1 Score:  0.3037\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions: 75 / 147\n",
      "Accuracy: 0.5102\n",
      "Precision: 0.5186\n",
      "Recall:    0.4608\n",
      "F1 Score:  0.3996\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions: 68 / 145\n",
      "Accuracy: 0.4690\n",
      "Precision: 0.5415\n",
      "Recall:    0.4419\n",
      "F1 Score:  0.3946\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: noun_phrases\n",
      "Correct predictions: 7 / 66\n",
      "Accuracy: 0.1061\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: possessives\n",
      "Correct predictions: 1 / 57\n",
      "Accuracy: 0.0175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def get_multiclass_evaluation_metrics(dataset,experiment,model,include_inconclusive=False):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}*_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    average_ = 'macro'\n",
    "\n",
    "    if include_inconclusive == True:\n",
    "        df = df[df['expectation'] != '2']\n",
    "        average_ = 'weighted'\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    #df['expectation'] = df['expectation'].astype(int)\n",
    "    #df['prediction'] = df['prediction'].astype(int)\n",
    "\n",
    "    y_true = df['expectation'].astype(int)\n",
    "    y_pred = df['prediction'].astype(int)\n",
    "    mapped_pred = [0 if pred == 2 else pred for pred in df['prediction']]\n",
    "\n",
    "    y_pred_to_check = y_pred\n",
    "\n",
    "    #print(\"Unique values in expectation:\", df['expectation'].unique())\n",
    "    #print(\"Unique values in prediction:\", df['prediction'].unique())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_to_check, average=average_, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_to_check, average=average_, zero_division=0)\n",
    "\n",
    "    #from sklearn.metrics import confusion_matrix\n",
    "    #print(confusion_matrix(df['expectation'], df['prediction']))\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "    \n",
    "    correct = (y_true == y_pred_to_check).sum()\n",
    "    total = len(df)\n",
    "    print(f\"Correct predictions: {correct} / {total}\")\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print()\n",
    "\n",
    "def get_single_class_evaluation_metrics(dataset,experiment,model,include_inconclusive=False):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}*_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    if not include_inconclusive:\n",
    "        df = df[df['expectation'] != '2']\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    df['expectation'] = df['expectation'].astype(int)\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "\n",
    "    #print(\"Unique values in expectation:\", df['expectation'].unique())\n",
    "    #print(\"Unique values in prediction:\", df['prediction'].unique())\n",
    "\n",
    "    accuracy = accuracy_score(df['expectation'], df['prediction'])\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "    \n",
    "    correct = (df['expectation'] == df['prediction']).sum()\n",
    "    total = len(df)\n",
    "    print(f\"Correct predictions: {correct} / {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"]\n",
    "single_label_datasets = ['noun_phrases', 'possessives']\n",
    "comparisons = ['comparisons']\n",
    "\n",
    "for dataset in multi_label_datasets:\n",
    "    get_multiclass_evaluation_metrics(dataset,'experiment_1','anthroscore',include_inconclusive=False)\n",
    "\n",
    "for dataset in single_label_datasets:\n",
    "    get_single_class_evaluation_metrics(dataset,'experiment_1','anthroscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1de0c06a-ffae-4281-bd5d-b559750dc45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/adjective_phrases_inconclusive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/adjective_phrases_negative.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/adjective_phrases_positive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/comparisons_inconclusive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/noun_phrases_positive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/possessives_positive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/verb_objects_inconclusive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/verb_objects_negative.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/verb_objects_positive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/verb_subjects_inconclusive.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/verb_subjects_negative.csv\n",
      "Created ../experiment_2/AtypicalAnimacy_19thBERT/expectations/csv/verb_subjects_positive.csv\n"
     ]
    }
   ],
   "source": [
    "def temp_check(filename,experiment,all_indices_dict):\n",
    "    \"\"\"\n",
    "    this function reads info from csv file and writes it to a file with uniform structure to facilitate evaluation.\n",
    "\n",
    "    :param filename (str): name of the file \n",
    "    :param experiment (str): specify the experiment - used in input and output paths, and for obtaining correct indices\n",
    "    :param experiment (str): specify the model - used to obtain final prediction {0,1,2} based on the anthro/AtypicalAnimacy score\n",
    "    :param experiment (dict): pre-defined dictionary containing experiment+model string as key and index dict as value\n",
    "    \n",
    "    \"\"\" \n",
    "    with open(f\"../{experiment}/AtypicalAnimacy/19thcentury/expectations/csv/{filename}.csv\",\"w\") as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        new_header = ['id','Previous Sentence','Current Sentence','Masked Sentence','Next Sentence','AI Phrase','Suggested Mask','AI Entity',\n",
    "                      'Anthropomorphic Component','Target Expression', 'Animated']\n",
    "        writer.writerow(new_header)\n",
    "\n",
    "        infile = open(f\"../{experiment}/AtypicalAnimacy/predictions/txt/{filename}.txt\",\"r\")\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            write_to_file = row[:11]            \n",
    "            writer.writerow(write_to_file)\n",
    "        \n",
    "        print(f\"Created ../{experiment}/AtypicalAnimacy_19thBERT/expectations/csv/{filename}.csv\")\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    temp_check(file,'experiment_2',AA_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7c0d2-5884-45c4-8d49-b44d94cc855e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
