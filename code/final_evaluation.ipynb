{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36da6186-7d29-49c4-9771-9aa97fb13c63",
   "metadata": {},
   "source": [
    "# Create final evaluation sets and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31972cd3-bea4-42a5-bdd1-9607d7dc37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22f992cd-3854-4f91-a939-f1a013f97358",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c90f9d17-0f2d-4994-bc94-609dbfd8a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "\n",
    "def get_final_prediction(score,model):\n",
    "    \"\"\"\n",
    "     This function converts the anthroscore/AtypicalAnimacy scores to a single numerical value in {0,1,2}\n",
    "    \"\"\"     \n",
    "    AtypicalAnimacy_threshold = 0.3 # this was calculated during the experiment.\n",
    "    score = float(score)\n",
    "\n",
    "    if model == 'anthroscore':\n",
    "        if score > 1.0:\n",
    "            pred = '1'\n",
    "        elif score < -1.0:\n",
    "            pred = '0'\n",
    "        else:\n",
    "            pred = '2'\n",
    "            \n",
    "    elif model == 'AtypicalAnimacy':\n",
    "        if score > AtypicalAnimacy_threshold:\n",
    "            pred = '1'\n",
    "        else:\n",
    "            pred = '0'\n",
    "\n",
    "    return pred\n",
    "\n",
    "def create_final_eval_file(filename,experiment,model,all_indices_dict):\n",
    "    \"\"\"\n",
    "    this function reads info from csv file and writes it to a file with uniform structure to facilitate evaluation.\n",
    "\n",
    "    :param filename (str): name of the file \n",
    "    :param experiment (str): specify the experiment - used in input and output paths, and for obtaining correct indices\n",
    "    :param experiment (str): specify the model - used to obtain final prediction {0,1,2} based on the anthro/AtypicalAnimacy score\n",
    "    :param experiment (dict): pre-defined dictionary containing experiment+model string as key and index dict as value\n",
    "    \n",
    "    \"\"\" \n",
    "    with open(f\"../final_sets/{filename}_{experiment}_{model}_predictions.csv\",\"w\") as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        new_header = ['id','sentence','masked_sentence','AI_phrase','mask','component','expectation','model_score','prediction']\n",
    "        writer.writerow(new_header)\n",
    "        infile = open(f\"../{experiment}/{model}/predictions/csv/{filename}.csv\",\"r\")\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "\n",
    "        eval_set = f\"{experiment}_{model}\"\n",
    "        \n",
    "        for row in reader:\n",
    "\n",
    "            indices = all_indices_dict[eval_set]\n",
    "            \n",
    "            sentence_id = normalized(row[indices['id']])\n",
    "            sentence = normalized(row[indices['sent']])\n",
    "            masked_sent = normalized(row[indices['masked_sent']])\n",
    "            AI_phrase = normalized(row[indices['phrase']])\n",
    "            mask = normalized(row[indices['mask']])\n",
    "            component = normalized(row[indices['comp']])\n",
    "            expectation = (normalized(row[indices['exp']])) # should be numerical value {0,1,2}\n",
    "            expectation = int(float(expectation))\n",
    "            prediction = normalized(row[indices['pred']])\n",
    "\n",
    "\n",
    "            final_pred = get_final_prediction(prediction,model)\n",
    "            \n",
    "            write_to_file = [sentence_id,sentence,masked_sent,AI_phrase,mask,component,expectation,prediction,final_pred]\n",
    "            \n",
    "            writer.writerow(write_to_file)\n",
    "        \n",
    "        print(f\"Created {filename}_{experiment}_{model}_predictions.csv in ../final_sets/\")\n",
    "\n",
    "all_indices_dict = {'experiment_1_anthroscore':{'id':0,'sent':1,'masked_sent':2,'phrase':3,'mask':4,'comp':6,'exp':7,'pred':8},\n",
    "              'experiment_1_AtypicalAnimacy':{'id':0,'sent':2,'masked_sent':3,'phrase':5,'mask':6,'comp':8,'exp':10,'pred':15},\n",
    "              'experiment_2_anthroscore':{'id':0,'sent':1,'masked_sent':2,'phrase':3,'mask':4,'comp':6,'exp':7,'pred':8},\n",
    "              'experiment_2_AtypicalAnimacy':{'id':0,'sent':2,'masked_sent':3,'phrase':5,'mask':6,'comp':8,'exp':9,'pred':10}\n",
    "             }\n",
    "\n",
    "#for file in files:\n",
    "    #create_final_eval_file(file,'experiment_2','anthroscore',all_indices_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3582795-5cb6-4cea-a21d-175b255bf265",
   "metadata": {},
   "source": [
    "### Get evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48574186-0357-4a2d-adaf-22c0ea0e3d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: anthroscore Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 50 / 107\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.4877\n",
      "Recall:    0.3763\n",
      "F1 Score:  0.3037\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 62 / 106\n",
      "Accuracy: 0.5849\n",
      "Precision: 0.4481\n",
      "Recall:    0.4625\n",
      "F1 Score:  0.4130\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 39 / 120\n",
      "Accuracy: 0.3250\n",
      "Precision: 0.3338\n",
      "Recall:    0.3063\n",
      "F1 Score:  0.2804\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: adjective_phrases\n",
      "Correct predictions (accuracy): 71 / 120\n",
      "Accuracy: 0.5917\n",
      "Precision: 0.4481\n",
      "Recall:    0.4865\n",
      "F1 Score:  0.4271\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 75 / 147\n",
      "Accuracy: 0.5102\n",
      "Precision: 0.5186\n",
      "Recall:    0.4608\n",
      "F1 Score:  0.3996\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 94 / 144\n",
      "Accuracy: 0.6528\n",
      "Precision: 0.4355\n",
      "Recall:    0.5356\n",
      "F1 Score:  0.4800\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 56 / 150\n",
      "Accuracy: 0.3733\n",
      "Precision: 0.3700\n",
      "Recall:    0.3608\n",
      "F1 Score:  0.3470\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_objects\n",
      "Correct predictions (accuracy): 99 / 150\n",
      "Accuracy: 0.6600\n",
      "Precision: 0.4392\n",
      "Recall:    0.5355\n",
      "F1 Score:  0.4825\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 68 / 145\n",
      "Accuracy: 0.4690\n",
      "Precision: 0.5415\n",
      "Recall:    0.4419\n",
      "F1 Score:  0.3946\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_1 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 84 / 145\n",
      "Accuracy: 0.5793\n",
      "Precision: 0.3870\n",
      "Recall:    0.4985\n",
      "F1 Score:  0.4321\n",
      "\n",
      "Model: anthroscore Experiment: experiment_2 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 63 / 150\n",
      "Accuracy: 0.4200\n",
      "Precision: 0.5109\n",
      "Recall:    0.4001\n",
      "F1 Score:  0.3742\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: verb_subjects\n",
      "Correct predictions (accuracy): 101 / 150\n",
      "Accuracy: 0.6733\n",
      "Precision: 0.4501\n",
      "Recall:    0.5734\n",
      "F1 Score:  0.5029\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: noun_phrases\n",
      "Correct predictions: 7 / 66\n",
      "Accuracy: 0.1061\n",
      "\n",
      "Model: anthroscore Experiment: experiment_1 Dataset: possessives\n",
      "Correct predictions: 1 / 57\n",
      "Accuracy: 0.0175\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: noun_phrases\n",
      "Correct predictions: 14 / 70\n",
      "Accuracy: 0.2000\n",
      "\n",
      "Model: AtypicalAnimacy Experiment: experiment_2 Dataset: possessives\n",
      "Correct predictions: 47 / 60\n",
      "Accuracy: 0.7833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def get_precision_recall_f1_and_accuracy(dataset,experiment,model,print_res=True,map_pred=False,include_inconclusive=False):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}*_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    average_ = 'macro'\n",
    "\n",
    "    if include_inconclusive == True:\n",
    "        df = df[df['expectation'] != '2']\n",
    "        average_ = 'weighted'\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    y_true = df['expectation'].astype(int)\n",
    "    y_pred = df['prediction'].astype(int)\n",
    "    if map_pred == True:\n",
    "        y_pred = [0 if pred == 2 else pred for pred in y_pred]\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average_, zero_division=0)\n",
    "    \n",
    "    correct = (y_true == y_pred).sum()\n",
    "    total = len(df)\n",
    "\n",
    "    if print_res == True:\n",
    "        print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "        print(f\"Correct predictions (accuracy): {correct} / {total}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall:    {recall:.4f}\")\n",
    "        print(f\"F1 Score:  {f1:.4f}\")\n",
    "        print()\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def get_accuracy(dataset,experiment,model):\n",
    "\n",
    "    directory = \"../final_sets\"\n",
    "    \n",
    "    pattern = os.path.join(directory, f\"{dataset}*_{experiment}_{model}_predictions.csv\")\n",
    "    file_list = glob.glob(pattern)\n",
    "        \n",
    "    df_list = [pd.read_csv(file) for file in file_list]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    if 'expectation' not in df.columns or 'prediction' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'expectation' and 'prediction' columns.\")\n",
    "\n",
    "    # convert to integers\n",
    "    df['expectation'] = df['expectation'].astype(int)\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "\n",
    "    #print(\"Unique values in expectation:\", df['expectation'].unique())\n",
    "    #print(\"Unique values in prediction:\", df['prediction'].unique())\n",
    "\n",
    "    accuracy = accuracy_score(df['expectation'], df['prediction'])\n",
    "\n",
    "    print(f\"Model: {model} Experiment: {experiment} Dataset: {dataset}\")\n",
    "    \n",
    "    correct = (df['expectation'] == df['prediction']).sum()\n",
    "    total = len(df)\n",
    "    print(f\"Correct predictions: {correct} / {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "\n",
    "def compare_models():\n",
    "\n",
    "    multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"] # recall and precision matters\n",
    "    single_label_datasets = ['noun_phrases', 'possessives'] # only recall i.e. accuracy matters\n",
    "\n",
    "    for dataset in multi_label_datasets:\n",
    "        get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "        get_precision_recall_f1_and_accuracy(dataset,'experiment_1','AtypicalAnimacy',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "        get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "        get_precision_recall_f1_and_accuracy(dataset,'experiment_2','AtypicalAnimacy',print_res=True,map_pred=False,include_inconclusive=False)\n",
    "\n",
    "    for dataset in single_label_datasets:\n",
    "        get_accuracy(dataset,'experiment_1','anthroscore')\n",
    "    for dataset in single_label_datasets:\n",
    "        get_accuracy(dataset,'experiment_2','AtypicalAnimacy')\n",
    "\n",
    "def compare_anthroscore_scenarios(dataset):\n",
    "\n",
    "    exp_1_no_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=False,map_pred=False)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 1 no mapping\")\n",
    "    print(f\"Accuracy: {exp_1_no_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_1_no_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_1_no_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_1_no_map[3]:.4f}\")\n",
    "    print()\n",
    "    exp_1_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_1','anthroscore',print_res=False,map_pred=True)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 1 with mapping\")\n",
    "    print(f\"Accuracy: {exp_1_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_1_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_1_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_1_map[3]:.4f}\")\n",
    "    print()\n",
    "    exp_2_no_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=False,map_pred=False)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 2 no mapping\")\n",
    "    print(f\"Accuracy: {exp_2_no_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_2_no_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_2_no_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_2_no_map[3]:.4f}\")\n",
    "    print()\n",
    "    exp_2_map = get_precision_recall_f1_and_accuracy(dataset,'experiment_2','anthroscore',print_res=False,map_pred=True)\n",
    "    print(f\"scenario: {dataset} anthroscore experiment 2 with mapping\")\n",
    "    print(f\"Accuracy: {exp_2_map[0]:.4f}\")\n",
    "    print(f\"Precision: {exp_2_map[1]:.4f}\")\n",
    "    print(f\"Recall:    {exp_2_map[2]:.4f}\")\n",
    "    print(f\"F1 Score:  {exp_2_map[3]:.4f}\")\n",
    "    print()\n",
    "\n",
    "def evaluation_inconclusives(experiment,model):\n",
    "\n",
    "    inconclusives = [\"adjective_phrases_inconclusive\", \"verb_objects_inconclusive\", \"verb_subjects_inconclusive\",\"comparisons\"] \n",
    "\n",
    "    for dataset in multi_label_datasets:\n",
    "        get_precision_recall_f1_and_accuracy(dataset,experiment,model,include_inconclusive=True,)\n",
    "\n",
    "multi_label_datasets = [\"adjective_phrases\", \"verb_objects\", \"verb_subjects\"] # recall and precision matters\n",
    "single_label_datasets = ['noun_phrases', 'possessives'] # only recall i.e. accuracy matters\n",
    "\n",
    "#for dataset in multi_label_datasets:\n",
    "    #compare_anthroscore_scenarios(dataset)\n",
    "#for dataset in single_label_datasets:\n",
    "    #compare_anthroscore_scenarios(dataset)\n",
    "\n",
    "compare_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
