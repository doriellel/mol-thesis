{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACL anthology data extraction\n",
    "\n",
    "This notebook provides code for extracting ACL anthologoy data following the dev documentation here: https://acl-anthology.readthedocs.io/latest/api/anthology/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. get ACL anthology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: acl-anthology\n",
      "Version: 0.5.1\n",
      "Summary: A library for accessing the ACL Anthology\n",
      "Home-page: https://github.com/acl-org/acl-anthology\n",
      "Author: Marcel Bollmann\n",
      "Author-email: marcel@bollmann.me\n",
      "License: Apache-2.0\n",
      "Location: /Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages\n",
      "Requires: app-paths, attrs, citeproc-py, diskcache, docopt, gitpython, langcodes, lxml, numpy, omegaconf, platformdirs, pylatexenc, python-slugify, PyYAML, rich, rnc2rng, scipy, texsoup\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "from acl_anthology import Anthology\n",
    "anthology = Anthology.from_repo()\n",
    "!pip show acl-anthology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extend anthropomorphic wordlists with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import wordnet_syns as syns\n",
    "\n",
    "extended_arg0_verbs = syns.extend_word_list('arg0_verbs','v')\n",
    "extended_arg1_verbs = syns.extend_word_list('arg1_verbs','v')\n",
    "extended_adjectives = syns.extend_word_list('adjectives','a')\n",
    "extended_nouns = syns.extend_word_list('nouns','n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get papers from ACL anthology in a iterable object and initiate keywords for matching relevant titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "all_papers = anthology.papers()\n",
    "\n",
    "# for a case-insensitive re.match with words from title:\n",
    "title_keywords = ['AI','LM','LLM','GPT','ChatGPT'] \n",
    "\n",
    "# for a case insensitive re.search in title:\n",
    "title_phrases = ['artificial intelligence','language model']\n",
    "\n",
    "# for a lemma-based string comparison against entities in the abstract:\n",
    "keywords = ['AI','LM','LMs','LLM','LLMs','model','system','algorithm'] \n",
    "# spaCy lemmatizer does not handle plurals well for LM, LLM, so their plural version was included here too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Functions for retrieving specific patterns for each class from the taxonomy of anthropomorphic structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg0_active_criterion_check(sent,keywords,verb_list):\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in keywords)\n",
    "        if match and chunk.root.dep_ == 'nsubj' and chunk.root.head.lemma_ in verb_list:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. General function for retreiving sentences matching a criterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(cat):\n",
    "\n",
    "    with open(f\"../preprocessed_data/acl_{cat}.txt\",\"w\") as file:\n",
    "\n",
    "        done = False\n",
    "        counter = 0 # initiate counter\n",
    "        sentences_dict = {\"SentenceID\":[],\"currentSentence\":[],\"prevSentence\":[],\"nextSentence\":[],\"Abstract\":[]}\n",
    "        stop_words = ['do','be','have','show'] \n",
    "        verb_list = [v for v in extended_arg0_verbs if v not in stop_words] # exclude stop words\n",
    "\n",
    "        for idx,paper in enumerate(all_papers):\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            title = [token.text for token in nlp(str(paper.title))]\n",
    "            keyword_match = any(re.match(keyword, word, re.IGNORECASE) for keyword in title_keywords for word in title)\n",
    "            phrase_match = any(re.search(phrase, str(paper.title).casefold(), re.IGNORECASE) for phrase in title_phrases)\n",
    "        \n",
    "            if paper.abstract and keyword_match or phrase_match:\n",
    "                doc = nlp(str(paper.abstract))\n",
    "            \n",
    "                for i,sent in enumerate(doc.sents): # check for matches with the keywords in the noun chunks to find AI entities\n",
    "\n",
    "                    if counter >= 1000:\n",
    "                        done = True\n",
    "                        break # stop when counter reaches 1000\n",
    "\n",
    "                    sent_id = paper.id + \"_\" + str(idx) + \"_\" + str(i)\n",
    "\n",
    "                    # check if at least one of the noun chunks is an AI entity whose root is an anthropomorphic predicate\n",
    "                    if cat == \"arg0_verbs_active\":\n",
    "                        criterion_met = arg0_active_criterion_check(sent,keywords,verb_list) \n",
    "                \n",
    "                    if criterion_met:\n",
    "                        counter += 1\n",
    "                        file.write(sent_id+'\\t'+sent.text+'\\n')\n",
    "                        sentences_dict[\"SentenceID\"].append(sent_id)\n",
    "                        sentences_dict[\"currentSentence\"].append(list(doc.sents)[i].text)\n",
    "                        sentences_dict[\"Abstract\"].append(str(paper.abstract))\n",
    "                        try:\n",
    "                            sentences_dict[\"prevSentence\"].append(list(doc.sents)[i-1].text)\n",
    "                        except IndexError:\n",
    "                            sentences_dict[\"prevSentence\"].append(\"\")\n",
    "                        try:\n",
    "                            sentences_dict[\"nextSentence\"].append(list(doc.sents)[i+1].text)\n",
    "                        except IndexError:\n",
    "                            sentences_dict[\"nextSentence\"].append(\"\")\n",
    "                            \n",
    "    return sentences_dict                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve candidates for sentences in which the AI entity is the arg0 of an anthropomorphic predicate in the active voice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown TeX-math command: \\textless\n",
      "Unknown TeX-math command: \\textgreater\n",
      "Unknown TeX-math command: \\textless\n",
      "Unknown TeX-math command: \\textgreater\n"
     ]
    }
   ],
   "source": [
    "arg0_verbs_dict = get_sentences(\"arg0_verbs_active\")\n",
    "arg0_verbs_df = pd.DataFrame(data=arg0_verbs_dict)\n",
    "arg0_verbs_df.to_pickle(\"../preprocessed_data/dataframes/acl_1000_arg0_verbs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
