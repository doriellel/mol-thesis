{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv data extraction\n",
    "\n",
    "This notebook provides code for extracting arXiv data following the dev documentation here: https://www.kaggle.com/datasets/Cornell-University/arxiv/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. get arXiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_51914/4196751292.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "# code snippet below taken from Kaggle docs\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"Cornell-University/arxiv\",\n",
    "  file_path,\n",
    "  pandas_kwargs={\"lines\": True}\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extend anthropomorphic wordlists with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import wordnet_syns as syns\n",
    "\n",
    "extended_arg0_verbs = syns.extend_word_list('arg0_verbs','v')\n",
    "extended_arg1_verbs = syns.extend_word_list('arg1_verbs','v')\n",
    "extended_adjectives = syns.extend_word_list('adjectives','a')\n",
    "extended_nouns = syns.extend_word_list('nouns','n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get papers from arXiv in a iterable object and initiate keywords for matching relevant titles and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "paper_triplets = list(zip(df['id'],df['title'], df['abstract'].str.replace('\\n', ' ')))\n",
    "# zip 3-tuples of id, title and abstract\n",
    "\n",
    "# for a case-insensitive re.match with words from title:\n",
    "title_keywords = ['AI','LM','LLM','GPT','ChatGPT'] \n",
    "\n",
    "# for a case insensitive re.search in title:\n",
    "title_phrases = ['artificial intelligence','language model']\n",
    "\n",
    "# for a lemma-based string comparison against entities in the abstract:\n",
    "keywords = ['AI','LM','LMs','LLM','LLMs','model','system','algorithm'] \n",
    "# spaCy lemmatizer does not handle plurals well for LM, LLM, so their plural version was included here too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Functions for retrieving specific patterns for each class from the taxonomy of anthropomorphic structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg0_active_criterion_check(sent,keywords,verb_list):\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in keywords)\n",
    "        if match and chunk.root.dep_ == 'nsubj' and chunk.root.head.lemma_ in verb_list:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. General function for retreiving sentences matching a criterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(cat):\n",
    "\n",
    "    with open(f\"../preprocessed_data/arxiv_{cat}.txt\",\"w\") as file:\n",
    "\n",
    "        done = False\n",
    "        counter = 0 # initiate counter\n",
    "        sentences_dict = {\"SentenceID\":[],\"currentSentence\":[],\"prevSentence\":[],\"nextSentence\":[],\"Abstract\":[]}\n",
    "        stop_words = ['do','be','have','show'] \n",
    "        verb_list = [v for v in extended_arg0_verbs if v not in stop_words] # exclude stop words\n",
    "    \n",
    "        for idx,paper in enumerate(paper_triplets):\n",
    "\n",
    "            paper_id = str(paper[0])\n",
    "            title = str(paper[1])\n",
    "            abstract = str(paper[2])\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            title_words = [token.text for token in nlp(title)]\n",
    "            keyword_match = any(re.match(keyword, word, re.IGNORECASE) for keyword in title_keywords for word in title_words)\n",
    "            phrase_match = any(re.search(phrase, title.casefold(), re.IGNORECASE) for phrase in title_phrases)\n",
    "        \n",
    "            if keyword_match or phrase_match:\n",
    "                doc = nlp(abstract)\n",
    "            \n",
    "                for i,sent in enumerate(doc.sents): # check for matches with the keywords in the noun chunks to find AI entities\n",
    "\n",
    "                    if counter >= 1000:\n",
    "                        done = True\n",
    "                        break # stop when counter reaches 1000\n",
    "\n",
    "                    sent_id = paper_id + \"_\" + str(idx) + \"_\" + str(i)\n",
    "\n",
    "                    # check if at least one of the noun chunks is an AI entity whose root is an anthropomorphic predicate\n",
    "                    if cat == \"arg0_verbs_active\":\n",
    "                        criterion_met = arg0_active_criterion_check(sent,keywords,verb_list) \n",
    "                \n",
    "                    if criterion_met:\n",
    "                        counter += 1\n",
    "                        file.write(sent_id+'\\t'+sent.text+'\\n')\n",
    "                        sentences_dict[\"SentenceID\"].append(sent_id)\n",
    "                        sentences_dict[\"currentSentence\"].append(list(doc.sents)[i].text)\n",
    "                        sentences_dict[\"Abstract\"].append(abstract)\n",
    "                        try:\n",
    "                            sentences_dict[\"prevSentence\"].append(list(doc.sents)[i-1].text)\n",
    "                        except IndexError:\n",
    "                            sentences_dict[\"prevSentence\"].append(\"\")\n",
    "                        try:\n",
    "                            sentences_dict[\"nextSentence\"].append(list(doc.sents)[i+1].text)\n",
    "                        except IndexError:\n",
    "                            sentences_dict[\"nextSentence\"].append(\"\")\n",
    "                            \n",
    "    return sentences_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve candidates for sentences in which the AI entity is the arg0 of an anthropomorphic predicate in the active voice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg0_verbs_dict = get_sentences(\"arg0_verbs_active\")\n",
    "arg0_verbs_df = pd.DataFrame(data=arg0_verbs_dict)\n",
    "arg0_verbs_df.to_pickle(\"../preprocessed_data/dataframes/arxiv_1000_arg0_verbs.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
