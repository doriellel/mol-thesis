{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv data extraction\n",
    "\n",
    "This notebook provides code for extracting Arxiv data following the dev documentation here: https://www.kaggle.com/datasets/Cornell-University/arxiv/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_43823/4196751292.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "# code snippet below taken from Kaggle docs\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"Cornell-University/arxiv\",\n",
    "  file_path,\n",
    "  pandas_kwargs={\"lines\": True}\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:           id           submitter  \\\n",
      "0  0704.0001      Pavel Nadolsky   \n",
      "1  0704.0002        Louis Theran   \n",
      "2  0704.0003         Hongjun Pan   \n",
      "3  0704.0004        David Callan   \n",
      "4  0704.0005  Alberto Torchinsky   \n",
      "\n",
      "                                             authors  \\\n",
      "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
      "1                    Ileana Streinu and Louis Theran   \n",
      "2                                        Hongjun Pan   \n",
      "3                                       David Callan   \n",
      "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
      "\n",
      "                                               title  \\\n",
      "0  Calculation of prompt diphoton production cros...   \n",
      "1           Sparsity-certifying Graph Decompositions   \n",
      "2  The evolution of the Earth-Moon system based o...   \n",
      "3  A determinant of Stirling cycle numbers counts...   \n",
      "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
      "\n",
      "                                  comments  \\\n",
      "0  37 pages, 15 figures; published version   \n",
      "1    To appear in Graphs and Combinatorics   \n",
      "2                      23 pages, 3 figures   \n",
      "3                                 11 pages   \n",
      "4                                     None   \n",
      "\n",
      "                                 journal-ref                         doi  \\\n",
      "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
      "1                                       None                        None   \n",
      "2                                       None                        None   \n",
      "3                                       None                        None   \n",
      "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
      "\n",
      "          report-no       categories  \\\n",
      "0  ANL-HEP-PR-07-12           hep-ph   \n",
      "1              None    math.CO cs.CG   \n",
      "2              None   physics.gen-ph   \n",
      "3              None          math.CO   \n",
      "4              None  math.CA math.FA   \n",
      "\n",
      "                                             license  \\\n",
      "0                                               None   \n",
      "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4                                               None   \n",
      "\n",
      "                                            abstract  \\\n",
      "0    A fully differential calculation in perturba...   \n",
      "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
      "2    The evolution of Earth-Moon system is descri...   \n",
      "3    We show that a determinant of Stirling cycle...   \n",
      "4    In this paper we show how to compute the $\\L...   \n",
      "\n",
      "                                            versions update_date  \\\n",
      "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
      "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
      "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  2008-01-13   \n",
      "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2007-05-23   \n",
      "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2013-10-15   \n",
      "\n",
      "                                      authors_parsed  \n",
      "0  [[BalÃ¡zs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
      "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  \n",
      "2                                 [[Pan, Hongjun, ]]  \n",
      "3                                [[Callan, David, ]]  \n",
      "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]  \n"
     ]
    }
   ],
   "source": [
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "paper_triplets = list(zip(df['id'],df['title'], df['abstract'].str.replace('\\n', ' ')))\n",
    "# zip 3-tuples of id, title and abstract\n",
    "\n",
    "# for a case-insensitive re.match with words from title:\n",
    "title_keywords = ['AI','LM','LLM','GPT','ChatGPT'] \n",
    "\n",
    "# for a case insensitive re.search in title:\n",
    "title_phrases = ['artificial intelligence','language model']\n",
    "\n",
    "# for a lemma-based string comparison against entities in the abstract:\n",
    "keywords = ['AI','LM','LMs','LLM','LLMs','model','system','algorithm'] \n",
    "# spaCy lemmatizer does not handle plurals well for LM, LLM, so their plural version was included here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg0_active_criterion_check(sent,keywords,verb_list):\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in keywords)\n",
    "        if match and chunk.root.dep_ == 'nsubj' and chunk.root.head.lemma_ in verb_list:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../preprocessed_data/arxiv_arg0_verbs_active.txt\",\"w\") as file:\n",
    "\n",
    "    sentences_dict = {\"SentenceID\":[],\"currentSentence\":[],\"prevSentence\":[],\"nextSentence\":[],\"abstract\":[]}\n",
    "\n",
    "    counter = 0 # initiate counter\n",
    "    stop_words = ['do','be','have','show'] \n",
    "    verb_list = [v for v in extended_arg0_verbs if v not in stop_words] # exclude stop words\n",
    "    \n",
    "    for idx,paper in enumerate(paper_triplets):\n",
    "\n",
    "        paper_id = paper[0]\n",
    "        title = paper[1]\n",
    "        abstract = paper[2]\n",
    "\n",
    "        if counter == 1000:\n",
    "            break # stop when counter reaches 1000\n",
    "            \n",
    "        title = [token.text for token in title]\n",
    "        keyword_match = any(re.match(keyword, word, re.IGNORECASE) for keyword in title_keywords for word in title)\n",
    "        phrase_match = any(re.search(phrase, title.casefold(), re.IGNORECASE) for phrase in title_phrases)\n",
    "        \n",
    "        if keyword_match or phrase_match:\n",
    "            doc = nlp(abstract)\n",
    "            \n",
    "            for i,sent in enumerate(doc.sents): # check for matches with the keywords in the noun chunks to find AI entities\n",
    "\n",
    "                sent_id = paper_id + \"_\" + str(idx) + \"_\" + str(i)\n",
    "                check = arg0_active_criterion_check(sent,keywords,verb_list) \n",
    "                # check if at least one of the noun chunks is an AI entity whose root is an anthropomorphic predicate\n",
    "                \n",
    "                if check:\n",
    "                    counter += 1\n",
    "                    file.write(sent_id+'\\t'+sent.text+'\\n')\n",
    "                    sentences_dict[\"SentenceID\"].append(sent_id)\n",
    "                    sentences_dict[\"currentSentence\"].append(list(doc.sents)[i].text)\n",
    "                    sentences_dict[\"abstract\"].append(paper.abstract)\n",
    "                    try:\n",
    "                        sentences_dict[\"prevSentence\"].append(list(doc.sents)[i-1].text)\n",
    "                    except IndexError:\n",
    "                        sentences_dict[\"prevSentence\"].append(\"\")\n",
    "                    try:\n",
    "                        sentences_dict[\"nextSentence\"].append(list(doc.sents)[i+1].text)\n",
    "                    except IndexError:\n",
    "                        sentences_dict[\"nextSentence\"].append(\"\")\n",
    "                                \n",
    "                                    \n",
    "df = pd.DataFrame(data=sentences_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(df[\"SentenceID\"])))\n",
    "print(len(df[\"SentenceID\"]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
