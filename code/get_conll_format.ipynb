{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get CONLL-U format\n",
    "\n",
    "This notebook provides code for obtaining a random sample of sentences from those collected from the acl anthology and arxiv APIs, and for setting up a code setup for extracting the sentences in CONLL format with POS tags and dependencies.\n",
    "\n",
    "CONLL format documentation: https://universaldependencies.org/format.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get a random sample from the collected data for conducting initial tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_acl_sample = random.sample(list(open('../preprocessed_data/acl_anthology_sentences.txt')),13)\n",
    "random_arxiv_sample = random.sample(list(open('../preprocessed_data/arxiv_sentences.txt')),13)\n",
    "\n",
    "with open(\"../preprocessed_data/random_sample.txt\",\"w\") as file:\n",
    "\n",
    "    for sample in random_acl_sample+random_arxiv_sample:\n",
    "        file.write(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get CONLL-U format\n",
    "\n",
    "The file is formatted according to the CONLL-U guidelines, and contains the sentence id, text, and then 1 token per line with the following information:\n",
    "\n",
    "**ID**: Word index, integer starting at 1 for each new sentence; may be a range for multiword tokens; may be a decimal number for empty nodes \\(decimal numbers can be lower than 1 but must be greater than 0).\n",
    "<br>\n",
    "**FORM**: Word form or punctuation symbol. Spacy token.text\n",
    "<br>\n",
    "**LEMMA**: Lemma or stem of word form. Spacy token.lemma_\n",
    "<br>\n",
    "**UPOS**: Universal part-of-speech tag. Spacy token.pos_\n",
    "<br>\n",
    "**XPOS**: Optional language-specific (or treebank-specific) part-of-speech / morphological tag; underscore if not available. Spacy token.tag_ (fine-grained POS)\n",
    "<br>\n",
    "**FEATS**: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available. Spacy token.morph\n",
    "<br>\n",
    "**HEAD**: Head of the current word, which is either a value of ID or zero (0). obtained using Spacy token.i (The index of the token within the parent document)\n",
    "<br>\n",
    "**DEPREL**: Universal dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one. Spacy token.dep_\n",
    "<br>\n",
    "**DEPS**: Enhanced dependency graph in the form of a list of head-deprel pairs. Currently added _ to indicate no info is provided.\n",
    "<br>\n",
    "**MISC**: Any other annotation. In this case, a flag to indicate if the lemma is one of the AI entity keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_roots(doc):\n",
    "\n",
    "    root_counter = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            root_counter +=1\n",
    "\n",
    "    return root_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentence_to_file(doc,sent_id,file):\n",
    "\n",
    "    keywords = [' AI ',' LM ','LLM', 'LLMs', 'GPT','chatGPT','model','system','algorithm'] \n",
    "    # removed multiwords since comparison is done with lemma\n",
    "\n",
    "    firstrow = \"# sent_id = {sent_id}\".format(sent_id = sent_id)\n",
    "    secondrow = \"# text = {text}\".format(text = doc)\n",
    "    file.write(firstrow+'\\n')\n",
    "    file.write(secondrow+'\\n')\n",
    "\n",
    "    for i,token in enumerate(doc):\n",
    "            \n",
    "        if token.morph:\n",
    "            token_morph = str(token.morph)\n",
    "        else:\n",
    "            token_morph = '_'\n",
    "\n",
    "        if token.dep_ == 'ROOT':\n",
    "            token_head = '0'\n",
    "        else:\n",
    "            token_head = str(token.head.i+1)\n",
    "\n",
    "        if token.lemma_ in keywords:\n",
    "            token_is_keyword = 'IsKeyword=Yes'\n",
    "        else:\n",
    "            token_is_keyword = 'IsKeyword=No'\n",
    "                \n",
    "        token_info = [str(i+1), token.text, token.lemma_, token.pos_, token.tag_, \n",
    "                      token_morph, token_head, token.dep_, '_', token_is_keyword] # add underscore to DEPS column\n",
    "        tokenrow = '\\t'.join(token_info)\n",
    "            \n",
    "        file.write(tokenrow+'\\n')\n",
    "            \n",
    "    file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "with open(\"../data/random_sample.txt\",\"w\") as outfile:\n",
    "\n",
    "    infile = open(\"../preprocessed_data/random_sample.txt\",\"r\")\n",
    "    sent_id = 0 # initiate sent_id counter that goes up for each sentence added to the file\n",
    "    \n",
    "    for sentence in infile.readlines():\n",
    "\n",
    "        sentence = sentence.strip() # remove newline\n",
    "        doc = nlp(sentence) # the sentences were already added as sents from a spacy Doc when collected\n",
    "        num_of_roots = count_roots(doc) # make sure the sentence has 1 root for easier processing (although they should be)\n",
    "        \n",
    "        if num_of_roots == 1:\n",
    "            sent_id += 1 # enumeration starts at 1\n",
    "            add_sentence_to_file(doc,sent_id,outfile) # add conll format (minus DEPS) for each sentence"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
