{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "575639f3-fe85-4f8a-8073-933f3920e36f",
   "metadata": {},
   "source": [
    "# Obtain masked sentences and prev+next context\n",
    "\n",
    "This notebook contains code that obtains masked sentences, either using anthroscore masks for experiment 1 (for AtypicalAnimacy evaluation, since the anthroscore masks are generated during the anthroscore evaluation), or by creating masked sentences based on our own masking strategy for experiment 2. Chapter 3 of the thesis describes and justifies the strategy according to which elements were kept as part of the context, and which elements were masked alongside the AI entity. \n",
    "\n",
    "The AtypicalAnimacy evaluation relies, in addition to the sentence, on a masked sentence plus the context of the previous and next sentences of the sentence. This is also obtained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606305cf-2c1c-4407-a335-bc9ba57bd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cbb16-46c8-4e6f-b0b8-4861ba99dac4",
   "metadata": {},
   "source": [
    "#### Get a single dataframe containing the sentence, its unique ID, and previous and next sentences (if exist, empty string if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c7bc7e-3768-4c2c-b2fd-528acffd1bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SentenceID', 'currentSentence', 'prevSentence', 'nextSentence', 'Abstract']\n"
     ]
    }
   ],
   "source": [
    "def concat_pkl(directory_path):\n",
    "    \"\"\"\n",
    "    this function takes <mask> path to <mask> directory containing .pkl files of dataframes,\n",
    "    and returns <mask> single concatenated dataframe.\n",
    "    The dataframes contain sentences and their unique ID, \n",
    "    as well as the previous and next sentences in the abstract from which the sentence is taken.\n",
    "\n",
    "    :param directory_path: path to <mask> directory containing .pkl files\n",
    "    :type directory_path: string\n",
    "    :return: pd.Dataframe()\n",
    "    \"\"\" \n",
    "    pkl_files = [f for f in os.listdir(directory_path) if f.endswith('.pkl')]\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    for file in pkl_files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No pkl files read successfully.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "path = '../data/dataframes'\n",
    "all_sentences_df = concat_pkl(path)\n",
    "print(all_sentences_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9561ef-9c4f-4fbc-b983-6f623793b7e8",
   "metadata": {},
   "source": [
    "#### Functions for obtaining masked sentences for both experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd43022-504b-455d-946e-84a14b30626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalized(string):\n",
    "    return re.sub(r'\\s+', ' ', string.strip())\n",
    "\n",
    "def convert_annotation(score):\n",
    "    \"\"\"\n",
    "     This function converts annotations to numerical values:\n",
    "     negative - 0, positive - 1, inclonclusive - 2\n",
    "    \"\"\" \n",
    "    if score in ['p','p1','p2','p3']:\n",
    "        score = '1'\n",
    "    elif score in ['n1','n2','n3']:\n",
    "        score = '0'\n",
    "    elif score == 'inc':\n",
    "        score = '2'\n",
    "    else:\n",
    "        print(\"score is malformed\")\n",
    "\n",
    "    return score\n",
    "\n",
    "def mask_sentence(sentence,position,masked_str,mask_token):\n",
    "    \"\"\"\n",
    "    this function takes a sentence, an index and a string to be masked,\n",
    "    and returns a masked sentence\n",
    "    \n",
    "    :param sentence: sentence from the evaluation set\n",
    "    :type sentence: string\n",
    "    :param position: index of the mask in the sentence\n",
    "    :type position: integer\n",
    "    :param masked_str: phrase in the sentence that should be masked\n",
    "    :type masked_str: string\n",
    "    :return: masked sentence (string)\n",
    "    \"\"\" \n",
    "    \n",
    "    masked_sentence = []\n",
    "    len_mask = len(masked_str)\n",
    "    masked_sentence.append(sentence[:position])\n",
    "    masked_sentence.append(mask_token)\n",
    "    position_after_mask = position+len_mask\n",
    "    masked_sentence.append(sentence[position_after_mask:])\n",
    "    masked_sentence = ''.join(masked_sentence)\n",
    "        \n",
    "    return masked_sentence\n",
    "\n",
    "def punctuation_handling(masked_sent,mask_token):\n",
    "    \"\"\"\n",
    "     This ad-hoc function separates punctuation from the <mask> token, to adhere to the AtypicalAnimacy masking approach. \n",
    "     This function only handles cases found in the data, therefore ad-hoc.\n",
    "     This is applied when generating masked sentences according to our masking strategy for AtypicalAnimacy in experiment 2, \n",
    "     as well as preparing the AnthroScore masked sentences for the AtypicalAnimacy evaluation in experiment 1.\n",
    "    \"\"\" \n",
    "    \n",
    "    if mask_token+'.' in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+'.',mask_token+' .')\n",
    "    elif '-'+mask_token in masked_sent:\n",
    "        masked_sent = masked_sent.replace('-'+mask_token,'- '+mask_token)\n",
    "    elif '('+mask_token+')' in masked_sent:\n",
    "        masked_sent = masked_sent.replace('('+mask_token+')','( '+mask_token+' )')\n",
    "    elif mask_token+',' in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+',',mask_token+' ,')\n",
    "    elif mask_token+':' in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+':',mask_token+' :')\n",
    "    elif ' '+mask_token+')' in masked_sent:\n",
    "        masked_sent = masked_sent.replace(' '+mask_token+')',' '+mask_token+' )')\n",
    "    elif mask_token+\"’ \" in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+\"’ \",mask_token+\" ’s \")\n",
    "    elif mask_token+\"’s\" in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+\"’s\",mask_token+\" ’s\")\n",
    "    elif mask_token+\"' \" in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+\"' \",mask_token+\" 's \")\n",
    "    elif mask_token+\"'s\" in masked_sent:\n",
    "        masked_sent = masked_sent.replace(mask_token+\"'s\",mask_token+\" 's\")\n",
    "\n",
    "    return masked_sent\n",
    "\n",
    "def get_masked_sentence(model,sentence,AI_phrase,mask):\n",
    "    \"\"\"\n",
    "    this function takes a sentence and string to be masked, and returns a list of masked versions \n",
    "\n",
    "    :param experiment: specifies which format to use for the masked token based on the model.\n",
    "    :type experiment: string\n",
    "    :param sentence: sentence from the evaluation set\n",
    "    :type sentence: string\n",
    "    :param AI_phrase: entire AI phrase (including contextual components that should not be masked - \n",
    "    used for identification for when there are multiple occurrences of the mask in the sentence)\n",
    "    :type AI_phrase: string\n",
    "    :param mask: phrase in the sentence that should be masked\n",
    "    :type mask: string\n",
    "    :return: list of tuples containing the sentence and the masked sentence\n",
    "    \"\"\"     \n",
    "    if model == 'anthroscore':\n",
    "        mask_token = '<mask>'\n",
    "    elif model == 'AtypicalAnimacy':\n",
    "        mask_token = '[MASK]'\n",
    "    \n",
    "    mask_position = [match.start() for match in re.finditer(rf'\\b{re.escape(mask)}\\b', sentence, flags=re.IGNORECASE)]\n",
    "    masked_sentences = []\n",
    "    \n",
    "    if len(mask_position) == 1: # simple case, only one occurrence of mask\n",
    "        position = mask_position[0]\n",
    "        masked_sentence = mask_sentence(sentence,position,mask,mask_token)\n",
    "        if model == 'AtypicalAnimacy':\n",
    "            masked_sentence = punctuation_handling(masked_sentence,'[MASK]') \n",
    "        masked_sentences.append((sentence,masked_sentence))\n",
    "    elif len(mask_position) > 1: # more than one occurrence of the mask\n",
    "        AI_phrase_position = [match.start() for match in re.finditer(rf'\\b{re.escape(AI_phrase)}\\b', sentence,flags=re.IGNORECASE)]\n",
    "        mask_in_phrase_position = [match.start() for match in re.finditer(rf'\\b{re.escape(mask)}\\b', AI_phrase)]\n",
    "        if len(AI_phrase_position) == 1 and len(mask_in_phrase_position) == 1: # found mask by comparing position in AI phrase\n",
    "            position = mask_in_phrase_position[0] + AI_phrase_position[0]\n",
    "            masked_sentence = mask_sentence(sentence,position,mask,mask_token)\n",
    "            if model == 'AtypicalAnimacy':\n",
    "                masked_sentence = punctuation_handling(masked_sentence,'[MASK]') \n",
    "            masked_sentences.append((sentence,masked_sentence))\n",
    "        else: # cannot identify, masking all occurrences to be safe\n",
    "            for i,position in enumerate(mask_position):\n",
    "                masked_sentence = mask_sentence(sentence,position,mask,mask_token)\n",
    "                if model == 'AtypicalAnimacy':\n",
    "                    masked_sentence = punctuation_handling(masked_sentence,'[MASK]') \n",
    "                masked_sentences.append((sentence,masked_sentence))\n",
    "    else: \n",
    "        # brute-force - do not replicate !!! this was done after manual revision and confirmation\n",
    "        masked_sentence = sentence.replace(mask, mask_token)\n",
    "        if model == 'AtypicalAnimacy':\n",
    "            masked_sentence = punctuation_handling(masked_sentence,'[MASK]') \n",
    "        masked_sentences.append((sentence,masked_sentence))\n",
    "    \n",
    "    # seperate punctuation and possessive markers from the mask token with single spaces - for AtypicalAnimacy only.\n",
    "    # the anthroscore model expects <mask> tokens that are not separated from the punctuation.   \n",
    "\n",
    "    return masked_sentences \n",
    "\n",
    "def get_anthroscore_masks(filename):\n",
    "    \"\"\"\n",
    "    this function returns the masked sentences from the AnthroScore prediction files, \n",
    "    to be used in experiment 1 of AtypicalAnimacy evaluation.\n",
    "\n",
    "    :param filename: path to .csv file\n",
    "    :type sentence: string\n",
    "    :return: dictionary with unique ID as key, list of tuples (sentence-masked_sentence) as values.\n",
    "\n",
    "    Note: there can be more than one masked sentence per ID (same as in experiment 2 masking strategy).\n",
    "    \"\"\" \n",
    "    with open(f\"../experiment_1/anthroscore/predictions/csv/{filename}.csv\",\"r\") as infile:\n",
    "\n",
    "        anthroscore_masks = {}\n",
    "\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "            sentence_id = normalized(row[0])\n",
    "            sentence = normalized(row[1])\n",
    "            masked_sentence = normalized(row[2])\n",
    "            if sentence_id not in anthroscore_masks:\n",
    "                anthroscore_masks[sentence_id] = [(sentence,masked_sentence)]\n",
    "\n",
    "        return anthroscore_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d22f76d-98f9-4d18-8cab-cea1fd69155e",
   "metadata": {},
   "source": [
    "#### Deprecated: context3w(-masked) functions\n",
    "\n",
    "The functions below were created to reconstruct all features existing in the AtypicalAnimacy dataset, including the 3-word context and masked 3-word context. However, these features are not used in the AtypicalAnimacy masking approach, which only uses a masked sentence + previous and next sentences context. The function get_context3w also assumes (and handles) that punctuation and possessive markers are attached to the mask token, which is not the case in the masking strategy taken in experiment 2.\n",
    "To utilize this function with punctuations separated by spaces from the masked token, simply remove the ValueError handling - that error only occurs when punctuation or possessive markers are attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c630cca9-b724-4992-bc20-9b5912c3c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context3w(masked_sentence,masked_str):\n",
    "    \"\"\"\n",
    "    this function takes a masked sentence and the string that was masked,\n",
    "    and returns both the 3-word context of the original masked string and a 3-word context of the mask\n",
    "\n",
    "    :param masked_sentence: sentence that has been masked\n",
    "    :type sentence: string\n",
    "    :param masked_str: original phrase in the sentence that was masked\n",
    "    :type masked_str: string\n",
    "    :return: 3-word context and masked 3-word context (tuple of strings)\n",
    "    \"\"\" \n",
    "    masked_sentence_list = masked_sentence.split(' ')\n",
    "    try:\n",
    "        mask_index = masked_sentence_list.index(\"<mask>\")\n",
    "        mask_str = '<mask>'\n",
    "    except ValueError: # the mask is followed by punctuation or a possessive marker\n",
    "        printcheck = True\n",
    "        mask_plus_punct = [x for x in masked_sentence_list if '<mask>' in x][0] # assumes there is exactly one\n",
    "        mask_index = masked_sentence_list.index(mask_plus_punct)\n",
    "        mask_str = mask_plus_punct\n",
    "        masked_str = mask_str.replace('<mask>',masked_str) # restore punctuation and possesive marker to masked string\n",
    "    if mask_index <= 3:\n",
    "        prev_words = masked_sentence_list[:mask_index]\n",
    "    else:\n",
    "        start_index = mask_index - 3\n",
    "        prev_words = masked_sentence_list[start_index:mask_index]\n",
    "    if len(masked_sentence_list) < mask_index + 3:\n",
    "        next_words = masked_sentence_list[mask_index+1:]\n",
    "    else:\n",
    "        end_index = mask_index + 4\n",
    "        next_words = masked_sentence_list[mask_index+1:end_index]\n",
    "    prev_words = ' '.join(prev_words)\n",
    "    next_words = ' '.join(next_words)\n",
    "    context_3w = prev_words + ' ' + masked_str + ' ' + next_words # masked_str is the original text\n",
    "    context_3w_masked = prev_words + ' ' + mask_str + ' ' + next_words # mask_str is <mask> (with or without punct)\n",
    "\n",
    "    return context_3w,context_3w_masked\n",
    "\n",
    "def get_masked_sentence_and_context(sentence,AI_phrase,mask):\n",
    "    \"\"\"\n",
    "    this function takes a sentence and string to be masked, and returns a list of masked versions\n",
    "    as well as 3-word context and 3-word masked context  \n",
    "\n",
    "    :param sentence: sentence from the evaluation set\n",
    "    :type sentence: string\n",
    "    :param AI_phrase: entire AI phrase (including contextual components that should not be masked - \n",
    "    used for identification for when there are multiple occurrences of the mask in the sentence)\n",
    "    :type AI_phrase: string\n",
    "    :param mask: phrase in the sentence that should be masked\n",
    "    :type mask: string\n",
    "    :return: list of tuples containing the sentence, the masked sentence, the 3-word context and the masked 3-word context\n",
    "    \"\"\" \n",
    "    mask_position = [match.start() for match in re.finditer(rf'\\b{re.escape(mask)}\\b', sentence, flags=re.IGNORECASE)]\n",
    "    masked_sentences_and_context = []\n",
    "\n",
    "    if len(mask_position) == 1: # simple case, only one occurrence of mask\n",
    "        position = mask_position[0]\n",
    "        masked_sentence = mask_sentence(sentence,position,mask)\n",
    "        context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "        context_3w = context_3w_tuple[0]\n",
    "        context_3w_masked = context_3w_tuple[1]\n",
    "        masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "    elif len(mask_position) > 1: # more than one occurrence of the mask\n",
    "        AI_phrase_position = [match.start() for match in re.finditer(rf'\\b{re.escape(AI_phrase)}\\b', sentence,flags=re.IGNORECASE)]\n",
    "        mask_in_phrase_position = [match.start() for match in re.finditer(rf'\\b{re.escape(mask)}\\b', AI_phrase)]\n",
    "        if len(AI_phrase_position) == 1 and len(mask_in_phrase_position) == 1: # found mask by comparing position in AI phrase\n",
    "            position = mask_in_phrase_position[0] + AI_phrase_position[0]\n",
    "            masked_sentence = mask_sentence(sentence,position,mask)\n",
    "            context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "            context_3w = context_3w_tuple[0]\n",
    "            context_3w_masked = context_3w_tuple[1]\n",
    "            masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "        else: # cannot identify, masking all occurrences to be safe\n",
    "            for i,position in enumerate(mask_position):\n",
    "                masked_sentence = mask_sentence(sentence,position,mask)\n",
    "                context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "                context_3w = context_3w_tuple[0]\n",
    "                context_3w_masked = context_3w_tuple[1]\n",
    "                masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "    else: \n",
    "        # brute-force - do not replicate !!! this was done after manual revision and confirmation\n",
    "        masked_sentence = sentence.replace(mask, \"<mask>\")\n",
    "        context_3w_tuple = get_context3w(masked_sentence,mask)\n",
    "        context_3w = context_3w_tuple[0]\n",
    "        context_3w_masked = context_3w_tuple[1]\n",
    "        masked_sentences_and_context.append((sentence,masked_sentence,context_3w,context_3w_masked))\n",
    "\n",
    "    return masked_sentences_and_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a828df4-b868-4a6d-9d9d-747e76518052",
   "metadata": {},
   "source": [
    "#### Create evaluation sets for AtypicalAnimacy using specified masking strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b5f81b-1f94-455f-bc56-857fea6483da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_set(filename,experiment,model,include_3w_context=False):\n",
    "    \"\"\"\n",
    "    this function takes a .csv file and returns a new .csv file with expanded information used by the AtypicalAnimacy model for evaluation: \n",
    "    in addition to the original information, it adds the previous and next sentence (if applicable), the masked sentence, \n",
    "    the 3-word context of the masked phrase as well as the 3-word context of the mask.\n",
    "\n",
    "    To include context3w and context3wmasked, add these strings as last two items in AA_header, \n",
    "    and then use function masked_sentences_and_context instead of masked_sentences. \n",
    "\n",
    "    :param filename: name of the file to be processed\n",
    "    :type sentence: string\n",
    "    :param experiment: obtains masked sentences according to the experiment. \n",
    "                        experiment_1 uses AnthroScore masking strategy and experiment_2 uses our own masking strategy.\n",
    "    :type sentence: string\n",
    "    :param filename: experiment\n",
    "    :type sentence: string\n",
    "    :param experiment: True if (masked) 3-word contexts should be included in the generated sets. False by default.\n",
    "    :type sentence: bool\n",
    "    \"\"\" \n",
    "    with open(f\"../{experiment}/{model}/expectations/csv/{filename}.csv\",\"w\") as outfile:\n",
    "\n",
    "        missing_ids = []\n",
    "\n",
    "        if experiment == 'experiment_1':\n",
    "            anthroscore_masks = get_anthroscore_masks(filename) # used in experiment_1, i.e. AnthroScore masking\n",
    "        else:\n",
    "            anthroscore_masks = []\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        new_header = ['id','Previous Sentence','Current Sentence','Masked Sentence','Next Sentence','AI Phrase','Suggested Mask','AI Entity',\n",
    "                      'Anthropomorphic Component','Target Expression','Animated']\n",
    "        if include_3w_context:\n",
    "            new_header.extend(['context3w','context3wmasked'])\n",
    "        writer.writerow(new_header)\n",
    "        infile = open(f\"../data/evaluation_sentences_csv/{filename}.csv\",\"r\")\n",
    "        header = infile.readline()\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        for row in reader:\n",
    "            \n",
    "            sentence_id = normalized(row[0])\n",
    "            sentence = normalized(row[1])\n",
    "            orig_sentence_id = '_'.join(sentence_id.split('_')[2:5]) # remove class and dataset prefix added during preprocessing\n",
    "            \n",
    "            # retrieve previous and next sentences from dataframe all sentences dataframe\n",
    "            sentence_info = all_sentences_df[all_sentences_df['SentenceID'] == orig_sentence_id]\n",
    "            if not sentence_info.empty:\n",
    "                current_sentence = normalized(sentence_info.iloc[0]['currentSentence'])\n",
    "                prev_sent = normalized(sentence_info.iloc[0]['prevSentence'])\n",
    "                next_sent = normalized(sentence_info.iloc[0]['nextSentence'])\n",
    "            else: # this is only to capture errors - this does not happen\n",
    "                print(f\"error: the sentence with the id {sentence_id} was not found in the dataframe\")\n",
    "                prev_sent = ''\n",
    "                next_sent = ''\n",
    "                \n",
    "            # get masked sentence, context3w and context3wmasked\n",
    "            AI_phrase = normalized(row[2])\n",
    "            mask = normalized(row[3])\n",
    "            AI_entity = normalized(row[4])\n",
    "            anthro_component = normalized(row[5])\n",
    "            score = convert_annotation(normalized(row[6])) # convert p,n,inc scores to 0,1,2 scores\n",
    "\n",
    "            if experiment == 'experiment_1': # AnthroScore masking strategy\n",
    "\n",
    "                try:\n",
    "                    masked_sentences = anthroscore_masks[sentence_id] # use sentence ID to retrieve masks\n",
    "                    for m in masked_sentences:\n",
    "                        # make sure the original sentence matches\n",
    "                        if sentence != m[0]:\n",
    "                            print(\"found mismatching sentence!\")\n",
    "                        if len(masked_sentences) > 1:\n",
    "                            print(\"something's strange...found more than one mask in anthroscore masks...\")\n",
    "                        else:\n",
    "                            masked_sentence = m[1].replace('<mask>','[MASK]')\n",
    "                            masked_sentence = punctuation_handling(masked_sentence,'[MASK]') # necessary for AtypicalAnimacy code to run\n",
    "                            writer.writerow([sentence_id,prev_sent,m[0],masked_sentence,next_sent,AI_phrase,mask,\n",
    "                                                AI_entity,anthro_component,mask,score])\n",
    "                except KeyError:\n",
    "                    missing_ids.append(sentence_id)\n",
    "                        \n",
    "            elif experiment == 'experiment_2': # Our own masking strategy\n",
    "                if include_3w_context: # only in case we want to include (masked) 3-word contexts \n",
    "                    masked_sentences = get_masked_sentence_and_context(sentence,AI_phrase,mask) \n",
    "                    for m in masked_sentences:\n",
    "                        writer.writerow([sentence_id,prev_sent,m[0],m[1],next_sent,AI_phrase,mask,\n",
    "                                         AI_entity,anthro_component,mask,score,m[2],m[3]])\n",
    "                else: \n",
    "                    masked_sentences = get_masked_sentence(model,sentence,AI_phrase,mask) \n",
    "                    for m in masked_sentences:\n",
    "                        writer.writerow([sentence_id,prev_sent,m[0],m[1],next_sent,AI_phrase,mask,AI_entity,anthro_component,mask,score])\n",
    "        \n",
    "        if missing_ids:\n",
    "            print(\"IDs of missing sentences: \",missing_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee44448-bdc0-4aa5-bdd5-9dd39c3d3024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating anthroscore evaluation set for noun_phrases_positive_retest...\n"
     ]
    }
   ],
   "source": [
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]\n",
    "\n",
    "files_retest = [\"noun_phrases_positive_retest\"]\n",
    "\n",
    "for file in files_retest:\n",
    "    print(f\"Creating anthroscore evaluation set for {file}...\")\n",
    "    create_evaluation_set(file,'experiment_2','AtypicalAnimacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e42350-34ae-4b82-a090-a1c541fd53f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
