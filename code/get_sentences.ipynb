{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "\n",
    "This notebook provides code for extracting ACL anthologoy data following the documentation here: https://acl-anthology.readthedocs.io/latest/api/anthology/ and arXiv data following the documentation here: https://www.kaggle.com/datasets/Cornell-University/arxiv/code. The extraced sentences are only candidates for the evaluation sets. The final selection is performed manually, using my linguistic judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get ACL anthology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: acl-anthology\n",
      "Version: 0.5.1\n",
      "Summary: A library for accessing the ACL Anthology\n",
      "Home-page: https://github.com/acl-org/acl-anthology\n",
      "Author: Marcel Bollmann\n",
      "Author-email: marcel@bollmann.me\n",
      "License: Apache-2.0\n",
      "Location: /Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages\n",
      "Requires: app-paths, attrs, citeproc-py, diskcache, docopt, gitpython, langcodes, lxml, numpy, omegaconf, platformdirs, pylatexenc, python-slugify, PyYAML, rich, rnc2rng, scipy, texsoup\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "from acl_anthology import Anthology\n",
    "\n",
    "anthology = Anthology.from_repo()\n",
    "!pip show acl-anthology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get arXiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_19128/4196751292.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "Resuming download from 3353346048 bytes (1305981737 bytes left)...\n",
      "Resuming download from https://www.kaggle.com/api/v1/datasets/download/Cornell-University/arxiv?dataset_version_number=232&file_name=arxiv-metadata-oai-snapshot.json (3353346048/4659327785) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.34G/4.34G [01:01<00:00, 21.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# code snippet below taken from Kaggle docs\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"Cornell-University/arxiv\",\n",
    "  file_path,\n",
    "  pandas_kwargs={\"lines\": True}\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get papers in the form of a list of 3-tuples containing paper id, title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown TeX-math command: \\choose\n",
      "Unknown TeX-math command: \\textless\n",
      "Unknown TeX-math command: \\textgreater\n",
      "Unknown TeX-math command: \\textless\n",
      "Unknown TeX-math command: \\textgreater\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# zip 3-tuples of id, title and abstract from the arXiv dataframe\n",
    "arxiv_paper_triplets = list(zip(df['id'].astype(str),df['title'].astype(str), df['abstract'].str.replace('\\n', ' ')))\n",
    "\n",
    "all_acl_papers = anthology.papers()\n",
    "acl_paper_triplets = []\n",
    "\n",
    "# iterate over acl object and obtain relevant data\n",
    "for paper in all_acl_papers:\n",
    "    if paper.abstract:\n",
    "        paper_id = str(paper.id)\n",
    "        paper_title = str(paper.title)\n",
    "        paper_abstract = str(paper.abstract)\n",
    "        paper_triplet = (paper_id,paper_title,paper_abstract)\n",
    "        acl_paper_triplets.append(paper_triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Initiate keyword list for finding relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a case-insensitive re.match with words from title:\n",
    "title_keywords = ['AI','LM','LLM','GPT','ChatGPT'] \n",
    "\n",
    "# for a case insensitive re.search in title:\n",
    "title_phrases = ['artificial intelligence','language model']\n",
    "\n",
    "# for a lemma-based string comparison against entities in the abstract:\n",
    "keywords = ['AI','LM','LMs','LLM','LLMs','model','system','algorithm','GPT','chatGPT'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Functions for identifying specific linguistic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tools import wordnet_syns as syns\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "\n",
    "def agent_subjects(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    active voice structure in which an AI entity is the nsubj of an anthropomorphic predicate\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    stop_words = ['do','be','have','show'] \n",
    "    extended_agent_verbs = syns.extend_word_list('agent_verbs','v') # extend the list of words with similar words using WordNet\n",
    "    anthro_words = [w for w in extended_agent_verbs if w not in stop_words] # exclude stop words\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words)\n",
    "        if match and chunk.root.dep_ == 'nsubj' and chunk.root.head.lemma_ in anthro_words:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def agent_objects(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    passive voice structure in which an AI entity is the object of an anthropomorphic predicate\n",
    "    by checking whether there is a verb given in passive voice, and whose pobj is an AI entity\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :param anthro_words: list of anthropomorphic words (verbs,nouns or adjectives)\n",
    "    :type anthro_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    stop_words = ['do','be','have','show'] \n",
    "    extended_agent_verbs = syns.extend_word_list('agent_verbs','v') # extend the list of words with similar words using WordNet\n",
    "    anthro_words = [w for w in extended_agent_verbs if w not in stop_words] # exclude stop words\n",
    "    \n",
    "    first_check = 0\n",
    "    second_check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        if chunk.root.dep_ == 'nsubjpass' and chunk.root.head.lemma_ in anthro_words: # check that there is a passive anthro verb\n",
    "            first_check += 1\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words) # check that the AI entity is pobj\n",
    "        if match and first_check > 0 and chunk.root.dep_ == 'pobj':\n",
    "            second_check += 1\n",
    "\n",
    "    if second_check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def nonagent_objects(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    AI entity is the object of an anthropomorphic predicate\n",
    "    by identifying AI entities as direct (dobj) / indirect (pobj) objects of anthropomorphic verbs\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    with open(\"../wordlists/nonagent_verbs_dobj.txt\",\"r\") as file:\n",
    "        nonagent_verbs_dobj = [word.strip() for word in file.readlines()] \n",
    "    with open(\"../wordlists/nonagent_verbs_pobj.txt\",\"r\") as file:\n",
    "        nonagent_verbs_pobj = [word.strip() for word in file.readlines()]\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words)\n",
    "        if match and chunk.root.dep_ == 'dobj' and chunk.root.head.lemma_ in nonagent_verbs_dobj:\n",
    "            check += 1\n",
    "        if match and chunk.root.dep_ == 'pobj' and chunk.root.head.head.lemma_ in nonagent_verbs_pobj:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def adjective_phrases(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    AI entity is modified or complemented by an anthropomorphic adjective\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\"     \n",
    "    extended_adjectives = syns.extend_word_list('adjectives','n') # extend the list of words with similar words using WordNet\n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for token in [token for token in sent if token.lemma_ in extended_adjectives]:\n",
    "        if token.dep_ == 'amod' and any(re.search(rf\"\\b{re.escape(word)}\\b\", token.head.text, re.IGNORECASE) for word in ai_words) :\n",
    "            check += 1\n",
    "        elif token.dep_ == 'acomp':\n",
    "            for descendant in token.head.subtree:\n",
    "                if any(re.search(rf\"\\b{re.escape(word)}\\b\", descendant.text, re.IGNORECASE) for word in ai_words):\n",
    "                    check += 1\n",
    "                    \n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def noun_phrases(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    AI entity is part of an NP whose head is an anthropomorphic noun (assistant,teacher,...)\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    extended_nouns = syns.extend_word_list('nouns','n') # extend the list of words with similar words using WordNet\n",
    "    \n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        if chunk.root.lemma_ in extended_nouns and any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words):\n",
    "            check += 1\n",
    "                    \n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def possessives(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    AI entity is followed by a possessive marker 's\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    check = 0\n",
    "\n",
    "    for i,token in enumerate(sent):\n",
    "        if i >= 1:\n",
    "            prev_token = sent[i-1].text\n",
    "        else:\n",
    "            prev_token = \"\" # handling for first tokens in sentence which are never a possessive marker in well-formed sentences\n",
    "        if token.text == \"'s\":\n",
    "            if any(re.search(rf\"\\b{re.escape(word)}\\b\", prev_token, re.IGNORECASE) for word in ai_words):\n",
    "                check += 1\n",
    "                    \n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def comparisons(sent,ai_words):\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence adheres to the following linguistic structure:\n",
    "    AI entity is being compared to human beings, by checking for specific comparison and human keywords\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    if any(re.search(rf\"\\b{re.escape(word)}\\b\", str(sent), re.IGNORECASE) for word in ai_words):\n",
    "\n",
    "        comparative_phrases = [\"like\",\"compared to\",\"similarly to\",\"similar to\",\"resembles\",\n",
    "                               \"resemble\",\"resembling\",\"mimick\",\"mimicks\",\"better than\",\"as\"]\n",
    "        human_phrases = [\"humans\",\"people\",\"human beings\",\"humanity\",\"mankind\",\"human\",\"person\",\n",
    "                         \"child\",\"childlike\",\"humanlike\",\"human-like\",\"children\"]\n",
    "\n",
    "        comparatives = any(re.search(rf\"\\b{re.escape(phrase)}\\b\", str(sent), re.IGNORECASE) for phrase in comparative_phrases)\n",
    "        human = any(re.search(rf\"\\b{re.escape(phrase)}\\b\", str(sent), re.IGNORECASE) for phrase in human_phrases)\n",
    "    \n",
    "        if comparatives and human:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Functions for iterating over papers and obtaining matching sentences, writing to .txt and .pkl files\n",
    "\n",
    "The .txt files include only the sentences and their unique ID (comprised of the paper id, the index of the paper in the tuple list, and the index of the sentence inside the abstract). The ID will be later used to extract the selected sentences from the .pkl dataframe, which also contains the previous and next sentence, which will be used later for the Atypical Animacy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "def get_sentences(dataset,cat,lim):\n",
    "    \"\"\"\n",
    "    this function finds possible candidates for sentences adhering to various linguistic structures.\n",
    "\n",
    "    :param dataset: name of dataset from which sentences are being extracted\n",
    "    :type dataset: string\n",
    "    :param cat: class of linguistic structures from the taxonomy of anthropomorphic language\n",
    "    :type cat: string\n",
    "    :param lim: number that limits the number of iterations\n",
    "    :type lim: int\n",
    "    :return: dictionary containing candidate sentences \n",
    "    \"\"\" \n",
    "    \n",
    "    with open(f\"../preprocessed_data/{dataset}_{cat}.txt\",\"w\") as file:\n",
    "\n",
    "        done = False\n",
    "\n",
    "        print(f\"Looking for matching sentences for {cat} in the {dataset} dataset...\")\n",
    "        \n",
    "        if dataset == \"acl\":\n",
    "            paper_triplets = acl_paper_triplets\n",
    "        elif dataset == \"arxiv\":\n",
    "            paper_triplets = arxiv_paper_triplets\n",
    "\n",
    "        if cat == \"agent_subjects\":\n",
    "            criterion_met = agent_subjects\n",
    "        elif cat == \"agent_objects\":\n",
    "            criterion_met = agent_objects\n",
    "        elif cat == \"nonagent_objects\":\n",
    "            criterion_met = nonagent_objects\n",
    "        elif cat == \"adjective_phrases\":\n",
    "            criterion_met = adjective_phrases\n",
    "        elif cat == \"noun_phrases\":\n",
    "            criterion_met = noun_phrases\n",
    "        elif cat == \"possessives\":\n",
    "            criterion_met = possessives\n",
    "        elif cat == \"comparisons\":\n",
    "            criterion_met = comparisons\n",
    "        else:\n",
    "            print(\"The provided class of structures is not valid. Terminated process.\")\n",
    "            return\n",
    "            \n",
    "        counter = 0 # initiate counter\n",
    "        sentences_dict = {\"SentenceID\":[],\"currentSentence\":[],\"prevSentence\":[],\"nextSentence\":[],\"Abstract\":[]}\n",
    "\n",
    "        try: \n",
    "            \n",
    "            for idx,paper in enumerate(paper_triplets):\n",
    "\n",
    "                paper_id = paper[0]\n",
    "                title = paper[1]\n",
    "                abstract = paper[2]\n",
    "\n",
    "                if done:\n",
    "                    print(f\"Process has finished succcessfully. {counter} sentences were logged.\")\n",
    "                    return sentences_dict\n",
    "\n",
    "                words_in_title = [token.text for token in nlp(title)]\n",
    "                keyword_match = any(re.match(keyword, word, re.IGNORECASE) for keyword in title_keywords for word in words_in_title)\n",
    "                phrase_match = any(re.search(phrase, title.casefold(), re.IGNORECASE) for phrase in title_phrases)\n",
    "        \n",
    "                if keyword_match or phrase_match:\n",
    "                    doc = nlp(abstract)\n",
    "            \n",
    "                    for i,sent in enumerate(doc.sents): # check for matches with the keywords in the noun chunks to find AI entities\n",
    "\n",
    "                        if counter >= lim:\n",
    "                            done = True\n",
    "                            print(f\"reached limit of {lim} sentences.\")\n",
    "                            break # stop when counter reaches the configured limit\n",
    "\n",
    "                        sent_id = paper_id + \"_\" + str(idx) + \"_\" + str(i)\n",
    "\n",
    "                        # check if the sentence adheres to the given structure\n",
    "                        if criterion_met(sent,keywords):\n",
    "                            counter += 1\n",
    "                            file.write(sent_id+'\\t'+sent.text+'\\n')\n",
    "                            sentences_dict[\"SentenceID\"].append(sent_id)\n",
    "                            sentences_dict[\"currentSentence\"].append(list(doc.sents)[i].text)\n",
    "                            sentences_dict[\"Abstract\"].append(abstract)\n",
    "                            try:\n",
    "                                sentences_dict[\"prevSentence\"].append(list(doc.sents)[i-1].text)\n",
    "                            except IndexError:\n",
    "                                sentences_dict[\"prevSentence\"].append(\"\")\n",
    "                            try:\n",
    "                                sentences_dict[\"nextSentence\"].append(list(doc.sents)[i+1].text)\n",
    "                            except IndexError:\n",
    "                                sentences_dict[\"nextSentence\"].append(\"\")\n",
    "                            #print(f\"Found sentence - wrote to file and added to dictionary. counter is {counter}\")\n",
    "\n",
    "            print(f\"Finished iterating through the sentences. {counter} sentences were found.\")\n",
    "        \n",
    "        except UnboundLocalError:\n",
    "            print(\"The provided dataset is not valid. Terminated process.\")\n",
    "            return\n",
    "                            \n",
    "    return sentences_dict\n",
    "\n",
    "def write_to_files(dataset,cat,lim):\n",
    "    sentence_dict = get_sentences(dataset,cat,lim)\n",
    "    sentence_df = pd.DataFrame(data=sentence_dict)\n",
    "    sentence_df.to_pickle(f\"../preprocessed_data/dataframes/{dataset}_{lim}_{cat}.pkl\")\n",
    "    print(f\"{dataset} {cat} dataframe was saved as .pkl file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Retrieve candidates for sentences for each category\n",
    "\n",
    "change parameter of get_sentences. The options are:\n",
    "1. agent_subjects - sentences in which the AI entity is the subject of an anthropomorphic verb (nsubj)\n",
    "2. agent_objects - sentences in which the AI entity is object (agent) of an anthropomorphic verb in the passive voice (pobj)\n",
    "3. nonagent_objects - sentences in which the AI entity is object (cognizer) of an anthropomorphic verb\n",
    "4. adjective_phrasess - sentences in which the AI entity is part of an anthropomorphic adjectival phrase\n",
    "5. noun_phrases - sentences in which the AI entity is part of an anthropomorphic noun phrase\n",
    "6. possessives - sentences in which the AI entity is immediately followed by a possessive marker\n",
    "7. comparisons - sentences in which the AI entity is being compared to humans explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for matching sentences for comparisons in the acl dataset...\n",
      "Finished iterating through the sentences. 534 sentences were found.\n",
      "acl comparisons dataframe was saved as .pkl file.\n"
     ]
    }
   ],
   "source": [
    "write_to_files(\"acl\",\"comparisons\",1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
