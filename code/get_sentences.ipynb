{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction\n",
    "\n",
    "This notebook provides code for extracting ACL anthologoy data following the documentation here: https://acl-anthology.readthedocs.io/latest/api/anthology/ and arXiv data following the documentation here: https://www.kaggle.com/datasets/Cornell-University/arxiv/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get ACL anthology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: acl-anthology\n",
      "Version: 0.5.1\n",
      "Summary: A library for accessing the ACL Anthology\n",
      "Home-page: https://github.com/acl-org/acl-anthology\n",
      "Author: Marcel Bollmann\n",
      "Author-email: marcel@bollmann.me\n",
      "License: Apache-2.0\n",
      "Location: /Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages\n",
      "Requires: app-paths, attrs, citeproc-py, diskcache, docopt, gitpython, langcodes, lxml, numpy, omegaconf, platformdirs, pylatexenc, python-slugify, PyYAML, rich, rnc2rng, scipy, texsoup\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "from acl_anthology import Anthology\n",
    "\n",
    "anthology = Anthology.from_repo()\n",
    "!pip show acl-anthology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Get arXiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doriellelonke/Desktop/thesis/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_72125/4196751292.py:9: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "# code snippet below taken from Kaggle docs\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "file_path = \"arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.load_dataset(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"Cornell-University/arxiv\",\n",
    "  file_path,\n",
    "  pandas_kwargs={\"lines\": True}\n",
    "  # Provide any additional arguments like \n",
    "  # sql_query or pandas_kwargs. See the \n",
    "  # documenation for more information:\n",
    "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get papers in the form of a list of 3-tuples containing paper id, title and abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m nlp = spacy.load(\u001b[33m\"\u001b[39m\u001b[33men_core_web_md\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# zip 3-tuples of id, title and abstract from the arXiv dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m arxiv_paper_triplets = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m),df[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m), df[\u001b[33m'\u001b[39m\u001b[33mabstract\u001b[39m\u001b[33m'\u001b[39m].str.replace(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m      7\u001b[39m all_acl_papers = anthology.papers()\n\u001b[32m      8\u001b[39m acl_paper_triplets = []\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# zip 3-tuples of id, title and abstract from the arXiv dataframe\n",
    "arxiv_paper_triplets = list(zip(df['id'].astype(str),df['title'].astype(str), df['abstract'].str.replace('\\n', ' ')))\n",
    "\n",
    "all_acl_papers = anthology.papers()\n",
    "acl_paper_triplets = []\n",
    "\n",
    "# iterate over acl object and obtain relevant data\n",
    "for paper in all_papers:\n",
    "    if paper.abstract:\n",
    "        paper_id = str(paper.id)\n",
    "        paper_title = str(paper.title)\n",
    "        paper_abstract = str(paper.abstract)\n",
    "        paper_triplet = (paper_id,paper_title,paper_abstract)\n",
    "        acl_paper_triplets.append(paper_triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Initiate keyword list for finding relevant papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a case-insensitive re.match with words from title:\n",
    "title_keywords = ['AI','LM','LLM','GPT','ChatGPT'] \n",
    "\n",
    "# for a case insensitive re.search in title:\n",
    "title_phrases = ['artificial intelligence','language model']\n",
    "\n",
    "# for a lemma-based string comparison against entities in the abstract:\n",
    "keywords = ['AI','LM','LLM','model','system','algorithm','chatGPT','GPT'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Extend anthropomorphic wordlists with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import wordnet_syns as syns\n",
    "\n",
    "extended_arg0_verbs = syns.extend_word_list('arg0_verbs','v') \n",
    "extended_arg1_verbs = syns.extend_word_list('arg1_verbs','v')\n",
    "extended_adjectives = syns.extend_word_list('adjectives','a')\n",
    "extended_nouns = syns.extend_word_list('nouns','n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Functions for identifying specific linguistic patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def arg0_verbs_active(sent,ai_words,anthro_words):\n",
    "\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence is likely to adhere to the following linguistic structure:\n",
    "    active voice structure in which an AI entity is arg0 of an anthropomorphic predicate\n",
    "    by checking whether an AI entity is an nsubj of an anthropomorphic verb\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :param anthro_words: list of anthropomorphic words (verbs, nouns or adjectives)\n",
    "    :type anthro_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "\n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words)\n",
    "        if match and chunk.root.dep_ == 'nsubj' and chunk.root.head.lemma_ in anthro_words:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def arg0_verbs_passive(sent,ai_words,anthro_words):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence is likely to adhere to the following linguistic structure:\n",
    "    passive voice structure in which an AI entity is arg0 of an anthropomorphic predicate\n",
    "    by checking whether there is a verb given in passive voice, and whose pobj is an AI entity\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :param anthro_words: list of anthropomorphic words (verbs,nouns or adjectives)\n",
    "    :type anthro_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    \n",
    "    first_check = 0\n",
    "    second_check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        if chunk.root.dep_ == 'nsubjpass' and chunk.root.head.lemma_ in anthro_words: # check that there is a passive anthro verb\n",
    "            first_check += 1\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words) # check that the AI entity is pobj\n",
    "        if match and first_check > 0 and chunk.root.dep_ == 'pobj':\n",
    "            second_check += 1\n",
    "\n",
    "    if second_check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def arg1_verbs(sent,ai_words,anthro_words):\n",
    "\n",
    "    \"\"\"\n",
    "    this function checks whether a spaCy sentence is likely to adhere to the following linguistic structure:\n",
    "    AI entity is arg1 of an anthropomorphic predicate\n",
    "    by identifying AI entities as direct / indirect objects of anthropomorphic verbs\n",
    "\n",
    "    :param sent: sentence from an abstract of a relevant (in-domain) paper\n",
    "    :type sent: spacy.tokens.span.Span\n",
    "    :param ai_words: list of AI entities to match inside the sentence\n",
    "    :type ai_words: list of strings\n",
    "    :param anthro_words: list of anthropomorphic words (verbs,nouns or adjectives)\n",
    "    :type anthro_words: list of strings\n",
    "    :return: True or False\n",
    "    \"\"\" \n",
    "    \n",
    "    check = 0\n",
    "\n",
    "    for chunk in sent.noun_chunks:\n",
    "        match = any(re.search(rf\"\\b{re.escape(word)}\\b\", chunk.text, re.IGNORECASE) for word in ai_words)\n",
    "        if match and chunk.root.dep_ == 'dobj' and chunk.root.head.lemma_ in anthro_words:\n",
    "            check += 1\n",
    "        elif match and chunk.root.dep_ == 'pobj' and chunk.root.head.head.lemma_ in anthro_words:\n",
    "            check += 1\n",
    "\n",
    "    if check > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Functions for iterating over papers and obtaining matching sentences, writing to .txt and .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "\n",
    "def get_sentences(dataset,cat,lim):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function finds possible candidates for sentences adhering to various linguistic structures.\n",
    "\n",
    "    :param dataset: name of dataset from which sentences are being extracted\n",
    "    :type dataset: string\n",
    "    :param cat: class of linguistic structures from the taxonomy of anthropomorphic language\n",
    "    :type cat: string\n",
    "    :param lim: number that limits the number of iterations\n",
    "    :type lim: int\n",
    "    :return: dictionary containing candidate sentences \n",
    "    \"\"\" \n",
    "    \n",
    "    with open(f\"../preprocessed_data/{dataset}_{cat}.txt\",\"w\") as file:\n",
    "\n",
    "        done = False\n",
    "\n",
    "        if lim > 1000:\n",
    "            print(\"The provided limit is too big!\")\n",
    "            done = True\n",
    "        \n",
    "        if dataset == \"acl\":\n",
    "            paper_triplets = acl_paper_triplets\n",
    "        elif dataset == \"arxiv\":\n",
    "            paper_triplets = arxiv_paper_triplets\n",
    "        else:\n",
    "            print(\"The provided dataset is not valid.\")\n",
    "            done = True\n",
    "\n",
    "        if cat == \"arg0_verbs_active\":\n",
    "            list_of_words = extended_arg0_verbs\n",
    "            criterion_met = arg0_verbs_active\n",
    "        elif cat == \"arg0_verbs_passive\":\n",
    "            list_of_words = extended_arg0_verbs\n",
    "            criterion_met = arg0_verbs_passive\n",
    "        elif cat == \"arg1_verbs\":\n",
    "            list_of_words = extended_arg1_verbs\n",
    "            criterion_met = arg1_verbs\n",
    "        else:\n",
    "            print(\"The provided class of structures is not valid.\")\n",
    "            done = True\n",
    "            \n",
    "        counter = 0 # initiate counter\n",
    "        sentences_dict = {\"SentenceID\":[],\"currentSentence\":[],\"prevSentence\":[],\"nextSentence\":[],\"Abstract\":[]}\n",
    "        stop_words = ['do','be','have','show'] \n",
    "        anthro_words = [w for w in list_of_words if w not in stop_words] # exclude stop words\n",
    "    \n",
    "        for idx,paper in enumerate(paper_triplets):\n",
    "\n",
    "            paper_id = paper[0]\n",
    "            title = paper[1]\n",
    "            abstract = paper[2]\n",
    "\n",
    "            if done:\n",
    "                print(\"Processed has either finished or was terminated.\")\n",
    "                break\n",
    "\n",
    "            words_in_title = [token.text for token in nlp(title)]\n",
    "            keyword_match = any(re.match(keyword, word, re.IGNORECASE) for keyword in title_keywords for word in words_in_title)\n",
    "            phrase_match = any(re.search(phrase, title.casefold(), re.IGNORECASE) for phrase in title_phrases)\n",
    "        \n",
    "            if keyword_match or phrase_match:\n",
    "                doc = nlp(abstract)\n",
    "            \n",
    "                for i,sent in enumerate(doc.sents): # check for matches with the keywords in the noun chunks to find AI entities\n",
    "\n",
    "                    if counter >= lim:\n",
    "                        done = True\n",
    "                        break # stop when counter reaches 1000\n",
    "\n",
    "                    sent_id = paper_id + \"_\" + str(idx) + \"_\" + str(i)\n",
    "\n",
    "                    # check if at least one of the noun chunks is an AI entity adhering to one of the structures\n",
    "                    if criterion_met(sent,keywords,anthro_words):\n",
    "                        counter += 1\n",
    "                        file.write(sent_id+'\\t'+sent.text+'\\n')\n",
    "                        sentences_dict[\"SentenceID\"].append(sent_id)\n",
    "                        sentences_dict[\"currentSentence\"].append(list(doc.sents)[i].text)\n",
    "                        sentences_dict[\"Abstract\"].append(abstract)\n",
    "                        try:\n",
    "                            sentences_dict[\"prevSentence\"].append(list(doc.sents)[i-1].text)\n",
    "                        except IndexError:\n",
    "                            sentences_dict[\"prevSentence\"].append(\"\")\n",
    "                        try:\n",
    "                            sentences_dict[\"nextSentence\"].append(list(doc.sents)[i+1].text)\n",
    "                        except IndexError:\n",
    "                            sentences_dict[\"nextSentence\"].append(\"\")\n",
    "                            \n",
    "    return sentences_dict\n",
    "\n",
    "def write_to_files(dataset,cat,lim):\n",
    "    sentence_dict = get_sentences(dataset,cat,lim)\n",
    "    sentence_df = pd.DataFrame(data=sentence_dict)\n",
    "    sentence_df.to_pickle(f\"../preprocessed_data/dataframes/{dataset}_{lim}_{cat}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Retrieve candidates for sentences for each category\n",
    "\n",
    "change parameter of get_sentences. The options are:\n",
    "1. arg0_verbs_active - sentences in which the AI entity is arg0 of an anthropomorphic verb in the active voice (nsubj)\n",
    "2. arg0_verbs_passive - sentences in which the AI entity is arg0 of an anthropomorphic verb in the passive voice (pobj)\n",
    "3. arg1_verbs - sentences in which the AI entity is arg1 of an anthropomorphic verb\n",
    "4. adjectival_phrases - sentences in which the AI entity is part of an anthropomorphic adjectival phrase\n",
    "5. noun_phrases - sentences in which the AI entity is part of an anthropomorphic noun phrase\n",
    "6. possessives - sentences in which the AI entity is immediately followed by a possessive marker\n",
    "7. comparisons - sentences in which the AI entity is being compared to humans explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_files(\"acl\",\"arg1_verbs\",1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
