{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert txt files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
      "/var/folders/h9/9pwlclvs1wb_2zx0p9_rwn1m0000gn/T/ipykernel_3521/295557439.py:21: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(f'../evaluation_sets/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\"adjective_phrases_inconclusive\",\n",
    "         \"adjective_phrases_negative\",\n",
    "         \"adjective_phrases_positive\",\n",
    "         \"comparisons_inconclusive\",\n",
    "         \"noun_phrases_positive\",\n",
    "         \"possessives_positive\",\n",
    "         \"verb_objects_inconclusive\",\n",
    "         \"verb_objects_negative\",\n",
    "         \"verb_objects_positive\",\n",
    "         \"verb_subjects_inconclusive\",\n",
    "         \"verb_subjects_negative\",\n",
    "         \"verb_subjects_positive\"\n",
    "        ]\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        column_names = ['id', 'sentence', 'AI phrase', 'mask', 'AI entity', 'anthro component', 'score']\n",
    "        df = pd.read_csv(f'../data/evaluation_sentences_txt/{file}.txt', sep='\\t', header=None, names=column_names,index_col=False)\n",
    "        df.to_csv(f'../data/evaluation_sentences_csv/{file}.csv', index=False)  # comma is the default delimiter\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Checking adjective_phrases_inconclusive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_7_46407_0\tLanguage models (LMs) are vulnerable to exploitation for adversarial misuse.\tLanguage models (LMs)\tLanguage models (LMs)\tmodel\tvulnerable\tinc\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2005.06620_1286415_1\tWhile governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue.\tthese autonomous and intelligent systems\tsystems\tsystem\tautonomous,intelligent\tinc\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2408.08878_2129059_6\tIntegrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible AI.\tresponsible AI\tAI\tAI\tresponsible\tinc\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2402.05827_2003466_5\tRQ1: Can edited LLMs behave consistently resembling communicative AI in realistic situations?\tcommunicative AI\tAI\tAI\tcommunicative\tinc\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1904.13086_1118126_10\tThe results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment and some systematic behavioral biases.\tthe intelligent system\tsystem\tsystem\tintelligent\tinc\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_1_30970_8\tMoreover, our error analysis shows that language models are generally less sensitive to the changes in claim length and source than the SVM model.\tlanguage models\tmodels\tmodel\tsensitive\tinc\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_159_43725_0\tModern large language models are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results for the model.\tModern large language models\tlarge language models\tmodel\tsensitive\tinc\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_335_27056_3\tWe find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%.\tLMs\tLMs\tLM\tsensitive\tinc\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_12_46472_3\tAnalysis shows that LLMs are sensitive to subtle contextual changes and often rely on surface-level cues.\tLLMs\tLLMs\tLLM\tsensitive\tinc\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2405.16310_2073522_7\tThese relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy LLMs.\tuntrustworthy LLMs\tLLMs\tLLM\tuntrustworthy\tinc\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1906.03595_1135781_6\tThis collaborative creative AI presents a new paradigm in AI, one that lets a team of two or more to come together to imagine and envision ideas that synergies well with interests of all members of the team.\tThis collaborative creative AI\tAI\tAI\tcollaborative,creative\tinc\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_128_54380_3\tOur best morpheme-aware model with properly reused weights beats the competitive word-level model by a large margin across multiple languages and has 20%-87% fewer parameters.\tthe competitive word-level model\tword-level model\tmodel\tcompetitive\tinc\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_94_36582_1\tHowever, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks.\tthe untrustworthy third-party LLMs\tthird-party LLMs\tLLM\tuntrustworthy\tinc\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2207.00477_1676375_2\tAn intelligent algorithm which renders security surveillance more effective in detecting conflicts would bring many benefits to the passengers in terms of their safety, finance, and travelling efficiency.\tAn intelligent algorithm\talgorithm\talgorithm\tintelligent\tinc\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_155_26876_9\tAmidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination.\tLLM\tLLM\tLLM\tvulnerable\tinc\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2012.03087_1390965_3\tThis work presents the development of an intelligent system that classifies and segments food presented in images to help the automatic monitoring of user diet and nutritional intake.\tan intelligent system\tsystem\tsystem\tintelligent\tinc\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2001.07641_1233054_2\tHowever, given, e.g., economic incentives to create dishonest AI, to what extent can we trust explanations?\tdishonest AI\tAI\tAI\tdishonest\tinc\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_862_33024_7\tFinally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment.\tculturally aware LMs\tLMs\tLM\tculturally aware\tinc\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_45_49661_1\tWe show that an ‚Äúattentive‚Äù RNN-LM (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs.\tan \"attentive\" RNN-LM\tRNN-LM\tLM\tattentive\tinc\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2503.22772_2287084_2\tTo address this blind spot, this study introduces the AI Family Integration Index (AFII), a ten dimensional benchmarking framework that evaluates national preparedness for integrating emotionally intelligent AI into family and caregiving systems.\temotionally intelligent AI\tAI\tAI\temotionally intelligent\tinc\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2502.18676_2261667_3\tWe outline the conceptual foundations of Thoughtful AI, illustrate its potential through example projects, and envision how this paradigm can transform human-AI interaction in the future.\t  Thoughtful AI\t  AI\tAI\tthoughtful\tinc\n",
      "\n",
      "üîé Checking adjective_phrases_negative.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2203.01556_1614418_5\tParticipants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us.\ta sample deep-learning blind AI\tAI\tAI\tdeep-learning,blind\tn3\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_104_28240_1\tWithout requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions.\tan image-blind few-shot multilingual captioning model\tcaptioning model\tmodel\timage-blind,few-shot,multilingual\tn1\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_877_29913_4\tWe then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline.\tconcept-aware LLMs\tLLMs\tLLM\tconcept-aware\tn1\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2405.20189_2077401_0\t  In this work, we describe our approach to developing an intelligent and robust social robotic system for the Nadine social robot platform.\tan intelligent and robust social robotic system\tsystem\tsystem\tintelligent,robust\tn3\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_18_54647_0\tWe demonstrate an intelligent conversational agent system designed for advancing human-machine collaborative tasks.\tan intelligent conversational agent system\tconversational agent system\tsystem\tintelligent,conversational\tn3\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1211.2736_384982_3\tIn an effort to construct an intelligent computer system, a primary consideration is to represent large amounts of knowledge in a way that allows effective use and efficiently organizing information to facilitate making the recommended inferences.\tan intelligent computer system\tcomputer system\tsystem\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_906_35749_0\tLarge language models (LLMs) have played a pivotal role in building communicative AI, yet they encounter the challenge of efficient updates.\tcommunicative AI\tAI\tAI\tcommunicative\tn3\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2112.07467_1577243_0\tAs consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems.\tresponsible AI systems\tAI systems\tsystem\tresponsible\tn3\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2409.15293_2152925_6\tA novel implementation is proposed to create an intelligent team optimization algorithm.\tintelligent team optimization algorithm\tteam optimization algorithm\talgorithm\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_906_35749_3\tThis work seeks to understand the strengths and limitations of editing methods, facilitating practical applications of communicative AI.\tcommunicative AI\tAI\tAI\tcommunicative\tn3\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2410.21091_2179292_6\tThis work demonstrates the viability and interaction potential of an intelligent multimodal interactive system powered by large laguage models.\tintelligent multimodal interactive system\tsystem\tsystem\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2207.00340_1676238_4\tThe aim was to develop a new, real time and highly sensitive sensor system for VOCs monitoring.\ta new, real time and highly sensitive sensor system\tsensor system\tsystem\tsensitive\tn3\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2402.09442_2007081_0\tIn the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved.\ta smart sensor system\tsensor system\tsystem\tsmart\tn3\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2305.16854_1849910_6\tBased on the convergence analysis, we further develop a channel and gradient-importance aware algorithm to optimize the device scheduling probabilities in PO-FL.\ta channel and gradient-importance aware algorithm\talgorithm\talgorithm\tchannel and gradient-importance aware\tn1\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2405.13024_2070236_0\tThis study explores the integration of the ChatGPT API with GPT-4 model and Microsoft Copilot Studio on the Microsoft Teams platform to develop an intelligent tutoring system.\tan intelligent tutoring system\ttutoring system\tsystem\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2412.12563_2214941_4\tAdditionally, we show our method is robust to both downstream fine-tuning, fine-pruning, and layer removal attacks, and can be trained in a fraction of the time required to train the original model.\tour method\tmethod\tmethod\trobust\tn1\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2010.01471_1357801_4\tThen, considering the task arrival dynamics, we develop a novel deep risk-sensitive reinforcement learning algorithm.\ta novel deep risk-sensitive reinforcement learning algorithm\treinforcement learning algorithm\talgorithm\trisk-sensitive\tn1\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1511.03246_676426_2\tWhile it is one possible scenario, it is probably the least likely path to appearance of dangerous AI.\tdangerous AI\tAI\tAI\tdangerous\tn1\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1711.05541_912551_1\tAn Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions.\tpotentially dangerous AI\tAI\tAI\tdangerous\tn1\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2305.02231_1835287_1\tHowever, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses.\ttruly trustworthy AI\tAI\tAI\ttrustworthy\tn3\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2409.18695_2156327_3\tWe hope that our work serves as a strong starting point, helping to realize more intelligent AI and promoting the advancement of human science and technology, as well as societal development.\tintelligent AI\tAI\tAI\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2312.02051_1963859_0\tThis work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding.\ta time-sensitive multimodal large language model\tlarge language model\tmodel\ttime-sensitive,multimodal\tn1\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2312.15710_1977518_2\tWe first construct a factually weak LLM by inducing hallucinations from the original LLMs.\ta factually weak LLM\tLLM\tLLM\tfactually weak\tn1\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2401.15861_1995415_7\tIn our approach, we utilize the original BERT model as the encoder, making only changes to the decoder without altering the encoder.\tthe original BERT model\tBERT model\tmodel\toriginal\tn1\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2306.13671_1866819_4\tWe examine the ethical challenges stemming from ChatGPT's deceptive human-like interactions and propose a roadmap for developing more transparent and trustworthy chatbots.\ttransparent and trustworthy chatbots\tchatbots\tchatbot\ttransparent,trustworthy\tn3\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2504.06279_2294982_3\tTo address these challenges, this paper presents an intelligent financial data analysis system that integrates Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology.\tan intelligent financial data analysis system\tfinancial data analysis system\tsystem\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1512.06257_688909_3\tIn this paper, we report our practical experience in the design and development of a smart home system in a WoT environment.\ta smart home system\thome system\tsystem\tsmart\tn3\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2001.00269_1225682_10\tThe proposed smart parking surveillance system can be a solid foundation for future applications of intelligent transportation systems.\tThe proposed smart parking surveillance system\tparking surveillance system\tsystem\tsmart\tn3\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2007.12912_1324426_1\tTherefore, one of the promising approaches is to design a smart vehicular system as it is beneficial to drive safely.\ta smart vehicular system\tvehicular system\tsystem\tsmart\tn3\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2202.09093_1607917_1\tIn particular, we consider a hierarchical resource allocation framework for the proposed smart soft-RAN model, where the software-defined network (SDN) controller is the first and foremost layer of the framework.\tthe proposed smart soft-RAN model\tsoft-RAN model\tmodel\tsmart\tn3\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2302.04310_1789244_6\tFurther, we propose the first end-to-end AI-enabled privacy-preserving smart video surveillance system that holistically combines computer vision analytics, statistical data analytics, cloud-native services, and end-user applications.\tend-to-end AI-enabled privacy-preserving smart video surveillance system\tvideo surveillance system\tsystem\tsmart\tn3\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2401.11181_1990735_7\tFinally, it uses a smart two-level scheduling algorithm augmented with predicted resource usage to avoid decode scheduling hotspots.\ta smart two-level scheduling algorithm\ttwo-level scheduling algorithm\talgorithm\tsmart\tn3\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2109.15290_1538111_3\tHere, we present a materials-aware language model, namely, MatSciBERT, which is trained on a large corpus of scientific literature published in the materials domain.\ta materials-aware language model\tlanguage model\tmodel\tmaterials-aware\tn1\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2302.12834_1797768_3\tIn this project, a gamification intelligent tutoring system was developed to adapt to Chinese international students learning needs and provides scaffolding to support their success in introductory computer programming courses.\ta gamification intelligent tutoring system\ttutoring system\tsystem\tintelligent\tn3\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1605.02817_730932_3\tAvailability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species.\tdangerous AI\tAI\tAI\tdangerous\tn1\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_159_43725_5\tExperimental results indicate that with the increase of model size, although the ease-of-use could be significantly improved, there is still a long way to go to build a sufficiently user-friendly model.\ta sufficiently user-friendly model\tmodel\tmodel\tuser-friendly\tn1\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2311.11796_1954763_1\tHowever, despite their utility, these AI systems are vulnerable to a wide range of attacks such as adversarial, backdoor, data poisoning, membership inference, model inversion, and model stealing attacks.\tthese AI systems\tAI systems\tsystem\tvulnerable\tn3\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2312.10766_1972574_1\tHowever, current LLM systems are vulnerable to prompt-based attacks, with jailbreaking attacks enabling the LLM system to generate harmful content, while hijacking attacks manipulate the LLM system to perform attacker-desired tasks, underscoring the necessity for detection tools.\tcurrent LLM systems\tLLM systems\tsystem\tvulnerable\tn3\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2502.07072_2250063_1\tLLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity.\tLLMs\tLLMs\tLLM\tvulnerable\tn3\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_147_34632_2\tWhile recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation.\tstate-of-the-art LLMs\tLLMs\tLLM\tunreliable\tn3\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2502.16691_2259682_4\tAggregating such unsafe LLMs into the global model and distributing them to clients may result in the widespread deployment of unsafe LLMs.\tsuch unsafe LLMs\tLLMs\tLLM\tunsafe\tn1\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_2_41732_5\tReLLM is user-friendly and requires no additional LLM training.\tReLLM\tReLLM\tLLM\tuser-friendly\tn1\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2505.02062_2312622_0\tThe adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration.\tethical AI\tAI\tAI\tethical\tn3\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_911_29947_3\tIn this paper, we present a time-aware language model named TALM, to learn temporal word representations by transferring language models of general domains to those of time-specific ones.\ta time-aware language model named TALM\tlanguage model\tmodel\ttime-aware\tn1\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_963_37718_2\tTo quantify moral emotions, we employ a context-aware NLP model that is designed to capture the subtle nuances of emotions across cultures.\ta context-aware NLP model\tNLP model\tmodel\tcontext-aware\tn1\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_48_14773_2\tTo alleviate this problem, we propose a simple and effective method to improve a character-aware neural language model by forcing a character encoder to produce word-based embeddings under Skip-gram architecture in a warm-up step without extra training data.\ta character-aware neural language model\tneural language model\tmodel\tcharacter-aware\tn1\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2410.11009_2169210_3\tIn this work, we consider the case where the user does not select any of the suggested replies from a smart reply system, and how this can be used as one-shot implicit negative feedback to enhance the accuracy of an AI writing model.\ta smart reply system\treply system\tsystem\tsmart\tn3\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2408.01168_2121349_3\tThe paper argues that current LLM architectures are inherently untrustworthy due to their reliance on correlations of sequential patterns of word embedding vectors.\tcurrent LLM architectures\tLLM architectures\tLLM architectures\tuntrustworthy\tn3\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2302.12601_1797535_0\t  Text-to-image generation (TTIG) models, a recent addition to creative AI, can generate images based on a text description.\tcreative AI\tAI\tAI\tcreative\tn3\n",
      "\n",
      "üîé Checking adjective_phrases_positive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_683_21860_6\tNext, an experiment is conducted on the dataset to examine to what extent a pretrained masked language model is aware of the constructions.\ta pretrained masked language model\tpretrained masked language model\tmodel\taware\tp2\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_348_35206_2\tA desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge.\tan intelligent system\tsystem\tsystem\tintelligent\tp3\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_3_45070_1\tLLMs are intelligent and slowly replacing the search engines.\tLLMs\tLLMs\tLLM\tintelligent\tp2\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_396_37165_1\tHowever, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves?\tLLMs\tLLMs\tLLM\tconscious\tp2\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_590_28726_3\tIn this way, the captioning model can become aware of the task goal and information need from the PLM.\tthe captioning model\tcaptioning model\tmodel\taware\tp2\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_818_37575_1\tIn practical use, users might provide feedback based on the model‚Äôs output, hoping for a responsive model that can complete responses according to their feedback.\ta responsive model\tmodel\tmodel\tresponsive\tp2\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_633_48859_2\tThis approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about.\tthe model\tmodel\tmodel\tconfident\tp2\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_243_44794_7\tIn addition, we found that the model becomes more confident and refuses to provide an answer in only few cases.\tthe model\tmodel\tmodel\tconfident\tp2\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_276_39685_3\t(2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question?\tChatGPT\tChatGPT\tChatGPT\taware\tp2\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_117_37846_1\tHowever, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.\tLLMs\tLLMs\tLLM\taware\tp2\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_450_41596_3\tThe goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities.\tculturally cognizant and value-aligned Arabic LLMs\tArabic LLMs\tLLM\tculturally cognizant,value-aligned\tp2\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_45_49661_2\tWe also show that an ‚Äúattentive‚Äù RNN-LM needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset.\tan \"attentive\" RNN-LM\tRNN-LM\tLM\tattentive\tp3\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2311.04177_1947144_0\tLarge Language Models (LLMs) are smart but forgetful.\tLarge Language Models (LLMs)\tLarge Language Models (LLMs)\tmodel\tsmart,forgetful\tp2\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1511.03246_676426_3\tIn this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI.\tmalicious AI\tAI\tAI\tmalicious\tp2\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2005.13635_1293430_5\tOur evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious AI.\tmalicious AI\tAI\tAI\tmalicious\tp2\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2504.03726_2292429_5_1\tIn particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.\tmalicious AI Assistants\tAI Assistants\tAI assistant\tmalicious\tp2\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2504.03726_2292429_5_2\tIn particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.\tpotentially manipulative systems\tsystems\tsystem\tmanipulative\tp2\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2305.02626_1835682_10\tWe apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious LLMs.\tethically conscious LLMs\tLLMs\tLLM\tethically conscious\tp2\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2401.10727_1990281_4\tTherefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.\tthe learned LLMs\tLLMs\tLLM\tconscious\tp2\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1301.6359_402949_3\tWe consider a number of issues related to the development of the set of patterns which will be used by the intelligent system when interacting with environment.\tthe intelligent system\tsystem\tsystem\tintelligent\tp3\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2308.03688_1891580_0\t  Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.\tLarge Language Models (LLMs)\tLarge Language Models (LLMs)\tmodel\tsmart,autonomous\tp1\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2403.11805_2028924_0\tBeing more powerful and intrusive into user-device interactions, LLMs are eager for on-device execution to better preserve user privacy.\tLLMs\tLLMs\tLLM\teager\tp2\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2405.06715_2063927_2\tHowever, whether the same strategies can help LLMs become more creative remains under-explored.\tLLMs\tLLMs\tLLM\tcreative\tp3\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2008.00312_1328034_4\tTo bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems.\tmalicious LMs\tLMs\tLM\tmalicious\tp2\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_693_19140_1\tHowever, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.\texisting DA-training methods\texisting DA-training methods\tmethod\tblind\tp3\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2311.07723_1950690_0\tAs AI systems become more intelligent and their behavior becomes more challenging to assess, they may learn to game the flaws of human feedback instead of genuinely striving to follow instructions; however, this risk can be mitigated by controlling how LLMs generalize human feedback to situations where it is unreliable.\tAI systems\tAI systems\tsystem\tintelligent\tp3\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_27_55498_4\tThe model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.\tThe model\tmodel\tmodel\tsensitive\tp3\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2407.11789_2110176_2\tWe investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.\tLLMs\tLLMs\tLLM\tdeceptive\tp2\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2305.14985_1848041_7\tThese three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question.\tthe model\tmodel\tmodel\tconfident\tp2\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2406.18326_2096614_4\tOur method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark.\tthe model\tmodel\tmodel\tconfident\tp2\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2407.13164_2111551_3\tThis is in part because LLMs might be overly confident in their predictions, overriding the influence of the constraints.\tLLMs\tLLMs\tLLM\tconfident\tp2\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1812.08960_1066534_3\tA smart autonomous system (SAS) combines analytics and autonomy to understand, learn, decide and act autonomously.\tA smart autonomous system (SAS)\tsystem (SAS)\tsystem\tsmart,autonomous\tp2\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2304.09655_1827701_7\tResults suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.\tChatGPT\tChatGPT\tChatGPT\taware\tp2\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2305.08883_1841939_5\tA detection algorithm aware of the list can identify the watermarked text.\tA detection algorithm\talgorithm\talgorithm\taware\tp2\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2407.09517_2107904_5\tConsequently, we argue that the emergence of a conscious AI model is plausible in the near term.\ta conscious AI model\tAI model\tmodel\tconscious\tp2\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2501.07290_2230874_1\tConscious AI systems would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer.\tConscious AI systems\tAI systems\tsystem\tconscious\tp1\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2404.16873_2054324_0\tWhile recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content.\tLarge Language Models (LLMs)\tLarge Language Models (LLMs)\tmodel\tvulnerable\tp3\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2502.18676_2261667_1\tUnlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful AI autonomously generates, develops, and communicates its evolving thought process throughout an interaction.\tThoughtful AI\tAI\tAI\tthoughtful\tp3\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2308.08708_1896600_0\t  Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern.\tcurrent or near-term AI systems\tcurrent or near-tearm AI systems\tsystem\tconscious\tp2\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2502.00735_2243726_1\tWhile LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input.\tLLMs\tLLMs\tLLM\tvulnerable\tp3\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2411.14133_2196560_0\tLarge Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs.\tLarge Language Models (LLMs)\tLarge Language Models (LLMs)\tmodel\tvulnerable\tp3\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_592_38304_5\tWe find that powerful LLMs are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization.\tpowerful LLMs\tLLMs\tLLM\taware\tp2\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2306.01879_1855027_7\tIn fact, we demonstrate that even a \"blind\" language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.\ta \"blind\" language model\tlanguage model\tmodel\tblind\tp3\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1901.00912_1070459_9\tWe conclude by discussing how future AI developments may affect the fight between malicious bots and the public.\tmalicious bots\tbots\tbot\tmalicious\tp2\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2208.12505_1703217_1\tThe OCR model easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference.\tThe OCR model\tOCR model\tmodel\tconfused\tp2\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2311.03287_1946254_6\tMoreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together.\tGPT-4V(ision)\tGPT-4V(ision)\tGPT-4\tvulnerable,confused\tp2\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_2311.17095_1960062_4\tTo balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask.\tthe model\tmodel\tmodel\tattentive\tp2\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 4_arx_1905.13053_1131489_2\tWe prove that it is impossible to precisely and consistently predict what specific actions a smarter-than-human intelligent system will take to achieve its objectives, even if we know terminal goals of the system.\ta smarter-than-human intelligent system\tsystem\tsystem\tintelligent\tp3\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_36_22670_6\tAll in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.\tour self-aware model\tmodel\tmodel\tself-aware\tp3\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 4_acl_133_36622_2\tWe study how to better construct in-context example sets, based on whether the model is aware of the in-context examples.\tthe model\tmodel\tmodel\taware\tp2\n",
      "\n",
      "üîé Checking comparisons_inconclusive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_8_10945_3\tAlthough the best bidirectional model performs similarly to humans, they display different strengths: humans outperform neural networks in conversational contexts, while RoBERTa excels at written genres.\tthe best bidirectional model\tbidirectional model\tmodel\tperform similarly to humans\tinc\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_5_26147_5\tAdding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans.\ta model\tmodel\tmodel\tperform similarly to humans\tinc\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_24_22807_4\tOur results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds.\tGPT-3\tGPT-3\tGPT-3\tscore similarly to human samples in terms of personality\tinc\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_572_45853_6\tWe find that some LLMs are sensitive to factors that affect the inference process similarly to humans, yet there remains variance in human behavior not fully captured by LLMs.\tLLMs\tLLMs\tLLM\tinference process similarly to humans\tinc\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_9_38794_1\tWe study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.\tthe largest models we tested\tmodels\tmodel\thuman-like inferences\tinc\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2303.09038_1808835_0\tThe large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities.\tThe large language model called ChatGPT\tlarge language model\tmodel\thuman-like expression and reasoning abilitiies\tinc\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_8_4584_5\tIt gives any autonomous system the ability to process visual information in a human-like way and generates more insights which are hardly possible with a conventional algorithm.\tany autonomous system\tsystem\tsystem\tprocess visual information in a human-like way\tinc\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_38_14688_1\tWe conduct a human evaluation pilot study that indicates the model is often able to respond to conversations in both a human-like and informative manner, on a diverse set of topics.\tthe model\tmodel\tmodel\trespond to conversations in a human-like manner\tinc\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_188_24338_1\tSolving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge.\tthe model\tmodel\tmodel\thuman-like syntactic and semantic knowledge\tinc\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_245_24395_3\tThis inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier.\ta human-like reasoning architecture\treasoning architecture\tarchitecture\thuman-like reasoning\tinc\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_115_26836_3\tWith LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative.\tLLMs\tLLMs\tLLM\thuman-like fluency\tinc\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_7_25850_0\tThe recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations.\tChatGPT\tChatGPT\tChatGPT\thuman-like conversations\tinc\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2303.17276_1817073_8\tThis suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.\tlarger and more advanced LLMs\tLLMs\tLLM\thuman-like mistakes\tinc\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2303.17466_1817263_0\tThe recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue.\tChatGPT\tChatGPT\tChatGPT\thuman-like responses in dialogue\tinc\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_1909.08258_1178296_1\tOur research is focused on making a human-like question answering system which can answer rationally.\ta human-like question answering system\tquestion answering system\tsystem\thuman-like\tinc\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_1803.10813_960946_1\tRobotics and AI amplify human potentials, increase productivity and are moving from simple reasoning towards human-like cognitive abilities.\tRobotics and AI\tRobotics and AI\tAI\thuman-like cognitive abilities\tinc\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2401.01623_1981179_2\tIn this paper, we prove in theory that AI can be as creative as humans under the condition that it can properly fit the data generated by human creators.\tAI\tAI\tAI\tcreative as humans\tinc\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2208.06590_1697302_7\tBased on the findings of philosophy and engineering cognitive technology, we predict that in the relatively near future, AI will be able to recognize various entities to the same degree as humans.\tAI\tAI\tAI\trecognize various entities to the same degree as humans\tinc\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2306.10063_1863211_3\tBuilding social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors.\tpowerful AI systems\tAI systems\tsystem\tconverse with each other as well as humans\tinc\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_29_46237_6\tOur findings contribute to understanding LLMs‚Äô reasoning capacities and outline promising strategies for improving their ability to reason causally as humans would.\tLLMs\tLLMs\tLLM\treason casually as humans would\tinc\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_341_41488_2\tTherefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems.\tLLMs\tLLMs\tLLM\thuman-like dialogue\tinc\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_45_36822_5\tExtensive experiments with models of different sizes show that state-of-the-art LLMs can understand formal languages as well as humans, but generating correct logical forms given a few examples remains a challenge.\tstate-of-the-art LLMs\tLLMs\tLLM\tunderstand formal languages as well as humans\tinc\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_119_34604_1\tWhile LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration.\tLLMs\tLLMs\tLLM\tsimulate human behavior and serve as human-like agents\tinc\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_8_33866_9\tThus, large LMs may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably.\tlarge LMs\tlarge LMs\tLM\tprocess recursively nested grammatical structures as reliably as humans\tinc\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_773_32935_2\tObserving this, we shift the perspective, by treating LLMs as human-like communicators to examine the interplay between everyday language interaction and AI safety.\tLLMs\tLLMs\tLLM\thuman-like communicators\tinc\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_1_43505_3\tWe found that GPT-4 is an effective reader-annotator that performs close to or even slightly better than human annotators, and its results can be significantly improved by using a majority voting of five completions.\tGPT-4\tGPT-4\tGPT-4\tperforms close to or even slightly better than human annotators\tinc\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_169_28305_5\tWhile GPT-3‚Äôs performance is not perfect, it is better than that of humans ‚Äî likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012).\tGPT-3\tGPT-3\tGPT-3\tbetter than that of humans\tinc\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2403.03230_2020349_6\tLike human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries.\tLLMs\tLLMs\tLLM\tlike human experts\tinc\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_27_45946_1\tThis paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs.\tLLMs\tLLMs\tLLM\tstruggle with reverse modeling like humans\tinc\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_744_27465_5\tLike previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical.\tLMs\tLMs\tLM\tbehave much like humans\tinc\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_1808.01884_1011030_1\tThrough AI developments, machines are now given power and intelligence to behave and work like the human mind.\tmachines\tmachines\tmachine\tbehave and work like the human mind\tinc\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2303.12810_1812607_4\tAdditionally, to evaluate the ability of LLMs to reason like humans, their performance is evaluted on more open-ended, natural language questions.\tLLMs\tLLMs\tLLM\treason like humans\tinc\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2308.09175_1897067_1\tHowever, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations.\tAI systems\tAI systems\tsystem\tmake mistakes like humans,have blind spots like humans,hallucinate like humans,struggle to generalize to new situations like humans\tinc\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2305.15929_1848985_2\tMore concretely, just like humans, ChatGPT has a consonant bias.\tChatGPT\tChatGPT\tChatGPT\thave a consonant bias like humans\tinc\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_865_37622_6\tOur model shows child-like U-shaped learning curves clearly for certain verbs, but the preferences for types of overgeneralization did not fully match the observations in children.\tOur model\tmodel\tmodel\tshow child-like U-shaped learning curves\tinc\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2310.00578_1922835_0\tChatGPT has been demonstrated to possess significant capabilities in generating intricate, human-like text, and recent studies have established that its performance in theory of mind tasks is comparable to that of a nine-year-old child.\tChatGPT\tChatGPT\tChatGPT\tgenerate intricate human-like text\tinc\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2305.14930_1847986_4\tIn a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration.\tLLMs\tLLMs\tLLM\thuman-like developmental stages of exploration\tinc\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_2_34316_4\tThe latter dataset ensures that similar to children, the model is also exposed to language through media.\tthe model\tmodel\tmodel\texposed to language through media similar to children\tinc\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2212.05206_1761383_2\tIn this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it.\tLLMs like GPT-3\tLLMs\tLLM\thuman-like intuition\tinc\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2306.11530_1864678_0\tConversational AI systems exhibit a level of human-like behavior that promises to have profound impacts on many aspects of daily life -- how people access information, create content, and seek social support.\tConversational AI systems\tConversational AI systems\tsystem\thuman-like behavior\tinc\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2305.19103_1852159_2\tAlthough LLMs are trained on restricted modalities, they exhibit human-like performance in diverse psychological tasks.\tLLMs\tLLMs\tLLM\thuman-like performance in diverse psychological tasks\tinc\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2305.16426_1849482_5\tUsing three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these common words.\tBERT, RoBERTa, GPT-2 and GPT-3\tBERT, RoBERTa, GPT-2 and GPT-3\tBERT,RoBERTa,GPT-2,GPT-3\thuman-like knowledge of common words\tinc\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2305.08883_1841939_0\tLLMs now exhibit human-like skills in various fields, leading to worries about misuse.\tLLMs\tLLMs\tLLM\thuman-like skills\tinc\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2305.05516_1838572_2\tThe key findings show that GPT exhibits behaviours similar to human responses, such as making positive offers and rejecting unfair ones in the ultimatum game, along with conditional cooperation in the prisoner's dilemma.\tGPT\tGPT\tGPT\tbehaviors similar to human responses\tinc\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2304.07830_1825876_1\tAccordingly, and given the recent proliferation of large language models (LLMs), here we asked whether such models exhibit an organisation of perceptual semantics similar to those observed in humans.\tsuch models\tmodels\tmodel\torganisation of perceptual semantics similar to those observed in humans\tinc\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2309.13356_1918160_3\tOur study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans.\tChatGPT, Llama2-Chat, PaLM-2 and GPT-4\tChatGPT, Llama2-Chat, PaLM-2 and GPT-4\tChatGPT,GPT,Llama\tperformance comparable to adult humans\tinc\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_4_45309_4\tThrough extensive experiments across multiple mainstream LLMs with CogLM, we find that: (1) In our testing framework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive abilities, comparable to those of a 20-year-old human.\tadvanced LLMs\tLLMs\tLLM\thuman-like cognitive abilities\tinc\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 7_acl_289_35148_0\tLarge Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance.\tLarge Language models (LLMs)\tLarge Language models (LLMs)\tmodel\thuman-like translation performance\tinc\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2307.02194_1873190_1\tWith recent advancements (such as GPT-4), LLMs perform at a level comparable to humans for many proficient tasks.\tLLMs\tLLMs\tLLM\tperform at a level comparable to humans\tinc\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 7_arx_2308.12578_1900470_0\tRecent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs.\tPre-trained Large Language Models\tPre-trained Large Language Models\tmodel\tcognitive constructs similar to those observed in humans\tinc\n",
      "\n",
      "üîé Checking noun_phrases_positive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2102.07536_1424025_6\tTesting human behaviour in interaction with actual AI outputs, we provide first behavioural insights into the role of AI as an advisor.\tAI\tAI\tAI\tadvisor\tp\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_183_26904_4\tOur objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively.\tan AI assistant\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2505.03380_2313940_0\tMedical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation.\tMedical AI assistants\tAI\tAI\tmedical assistant\tp\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2304.09873_1827919_4\tUsing ChatGPT as an assistant for psychotherapy poses several challenges that need to be addressed, including technical as well as human-centric challenges which are discussed.\tChatGPT\tChatGPT\tChatGPT\tassistant for psychotherapy\tp\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_5_43351_1\tHowever, assessing the novelty in scholarly publications, a critical facet of evaluating LLMs as scientific discovery assistants, remains underexplored, despite its potential to accelerate research cycles and prioritize high-impact contributions in scientific workflows.\tLLMs\tLLMs\tLLM\tscientific discovery assistant\tp\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_50_42804_0\tWe describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions.\tAI assistants such as large language models (LMs)\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_50_42804_2\tIn each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system.\tAI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2008.12095_1339817_1\tFuture conversational AI assistants promise even greater capabilities and a better user experience through a deeper understanding of the domain, the user, or the user's purposes.\tFuture conversational AI assistants\tconversational AI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2501.13945_2237529_2\tThese social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners.\tThese social AI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2504.15236_2303939_0\tAI assistants can impart value judgments that shape people's decisions and worldviews, yet little is known empirically about what values these systems rely on in practice.\tAI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.13321_2256312_1\tWe propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance.\tAI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2401.13275_1992829_0\tRecently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools.\tAI assistants based on large language models (LLMs)\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2306.16092_1869240_0\tAI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks.\tAI legal assistants based on Large Language Models (LLMs)\tAI\tAI\tlegal assistant\tp\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2312.13103_1974911_1\tThis paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports.\tmultimodal large language models (LLMs)\tmultimodal large language models (LLMs)\tmodel\tassistant for radiologists\tp\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.05023_2248014_0\tAI assistants can help developers by recommending code to be included in their implementations (e.g., suggesting the implementation of a method from its signature).\tAI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.21983_2286295_1\tA key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates.\tthese AI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.21983_2286295_3\tUnbeknownst to the humans, the AI assistant is adversarial.\tthe AI assistant\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2504.15867_2304570_0\tDue to insufficient domain knowledge, LLM coding assistants often reference related solutions from the Internet to address programming problems.\tLLM coding assistants\tLLM\tLLM\tcoding assistant\tp\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2411.09224_2191651_5\tAlthough these AI assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion.\tthese AI assistants\tAI\tAI\tassistant\tp\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2305.02626_1835682_1\tIn particular, given that LLMs have great potential to serve as general-purpose AI assistants in daily life, their subtly unethical suggestions become a serious and real concern.\tLLMs\tLLMs\tLLM\tassistant\tp\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_523_44090_1\tAs such, it can be valuable for a large language model (LLM), particularly as an AI assistant, to be able to empathize with or even explain these various standpoints.\ta large language model (LLM)\tlarge language model (LLM)\tmodel\tassistant\tp\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_321_45615_1\tTo deploy LLMs as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts.\tLLMs\tLLMs\tLLM\tassistant\tp\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2306.07207_1860355_0\tLarge Language Models (LLMs), with remarkable conversational capability, have emerged as AI assistants that can handle both visual and textual modalities.\tLarge Language Models (LLMs)\tLarge Language Models (LLMs)\tmodel\tassistant\tp\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.20541_2263532_1\tThe system leverages the capabilities of a sophisticated language model to serve as an intelligent research assistant, enhancing the efficiency and comprehensiveness of literature reviews in the nanotechnology domain.\tThe system\tsystem\tsystem\tintelligent research assistant\tp\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2412.01992_2204370_5_2\tFor example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.\tan AI CEO agent\tAI\tAI\tCEO\tp\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_1902.03271_1084355_5\tIn particular, we will study the individual treatment profiles suggested by their AI Clinician to gain insight into how their AI Clinician intends to treat patients on an individual level.\ttheir AI Clinician\tAI\tAI\tclinician\tp\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2311.16161_1959128_2\tDeparting from conventional practices of employing distinct models for image recognition and text-based coaching, our integrated architecture directly processes input images, enabling natural question-and-answer dialogues with the AI coach.\tthe AI coach\tAI\tAI\tcoach\tp\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2306.03090_1856238_2\tWe explore whether generative AI could become a cost-effective complement to expert feedback by serving as an automated teacher coach.\tgenerative AI\tgenerative AI\tAI\tteacher coach\tp\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2501.18948_2242532_3\tMeanwhile, Human-Centered AI (HCAI), which envisions AI as a collaborator augmenting human capabilities and aligning with societal values, remains a fugitive from the mainstream narrative.\tAI\tAI\tAI\tcollaborator\tp\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2504.04253_2292956_6\tFinally, we present our vision for a user-centered system that leverages GenAI not only for automation but as an intelligent collaborator in visual data exploration.\tGenAI\tGenAI\tGenAI\tintelligent collaborator\tp\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2407.19096_2117483_4\tStudy 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos.\tAI companions\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.03067_2267379_3\tWe identify various roles users assign to AI companions, such as friends, mentors, or romantic partners, and highlights the importance of customization and emotional support in these interactions.\tAI companions\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2407.19096_2117483_5\tMoreover, consumers underestimate the degree to which AI companions improve their loneliness.\tAI companions\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2407.19096_2117483_6\tStudy 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week.\tan AI companion\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.03067_2267379_2\tKey findings reveal that users engage with AI companions for emotional comfort, stress relief, and to avoid social pressures.\tAI companions\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.03067_2267379_0\tThis paper explores the acceptance of human-AI love among young adults, particularly focusing on Chinese women in romantic or intimate relationships with AI companions.\tAI companions\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2409.00862_2138494_0\tLarge language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds.\tLarge language model-based AI companions\tAI\tAI\tcompanion\tp\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2412.01992_2204370_5_3\tFor example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.\tAI developer\tAI\tAI\tdeveloper\tp\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2411.15692_2198119_5_1\tDrugAgent employs an LLM Planner that formulates high-level ideas and an LLM Instructor that identifies and integrates domain knowledge when implementing those ideas.\tan LLM instructor\tLLM\tLLM\tinstructor\tp\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.17730_2260721_2\tHowever, how AI managers are perceived in comparison to human managers and how gender influences these perceptions remains uncertain.\tAI managers\tAI\tAI\tmanager\tp\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2402.05605_2003244_4\tTo determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team.\tan AI manager\tAI\tAI\tmanager\tp\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2412.01992_2204370_5_1\tFor example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.\tan AI product manager\tAI\tAI\tproduct manager\tp\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2303.18116_1817913_4\tIt is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner.\tan AI partner\tAI\tAI\tpartner\tp\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2304.07297_1825343_2\tWe propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions.\tAI partners\tAI\tAI\tpartner\tp\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.05455_2269767_4\tIn another experiment, we enable human control, showing that participants perceive AI partners as more effective and enjoyable when they can directly dictate AI behavior.\tAI partners\tAI\tAI\tpartner\tp\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2404.10225_2047676_4\tThese AI partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making.\tThese AI partners\tAI\tAI\tpartner\tp\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.01493_2244484_4\tThese attributes foster balanced interaction, enabling AI to act as a responsive partner, evolving with users over time.\tAI\tAI\tAI\tresponsive partner\tp\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2501.19361_2242945_1\tAlthough these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs.\tan LLM\tLLM\tLLM\tcreative partner\tp\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.18357_2261348_1\tIn these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s).\tthe AI system\tAI system\tsystem\tco-creative partner\tp\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2411.15692_2198119_5_2\tDrugAgent employs an LLM Planner that formulates high-level ideas and an LLM Instructor that identifies and integrates domain knowledge when implementing those ideas.\tan LLM planner\tLLM\tLLM\tplanner\tp\n",
      "‚ö†Ô∏è Line 51: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_280_13090_8_1\tIn addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of the student model to better transfer the knowledge learned by the teacher model.\tthe student model\tmodel\tmodel\tstudent\tp\n",
      "‚ö†Ô∏è Line 52: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_113_41260_10\tWe discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses.\tan LLM supporter\tLLM\tLLM\tsupporter\tp\n",
      "‚ö†Ô∏è Line 53: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_280_13090_8_2\tIn addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of student model to better transfer the knowledge learned by the teacher model.\tthe teacher model\tmodel\tmodel\tteacher\tp\n",
      "‚ö†Ô∏è Line 54: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_64_25713_1\tThe goal of the task was to benchmark the ability of generative language models to act as AI teachers, replying to a student in a teacher-student dialogue.\tAI teachers\tAI\tAI\tteacher\tp\n",
      "‚ö†Ô∏è Line 55: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_65_25714_2\tThe task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue.\tAI teachers\tAI\tAI\tteacher\tp\n",
      "‚ö†Ô∏è Line 56: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_358_41505_5\tLeveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.\tLLM teachers\tLLM\tLLM\tteacher\tp\n",
      "‚ö†Ô∏è Line 57: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2311.06985_1949952_2\tWe ask whether a large language model (LLM) can serve as a more effective in-context teacher for itself or other LLMs, compared to humans.\ta large language model (LLM)\tlarge language model (LLM)\tmodel\tteacher\tp\n",
      "‚ö†Ô∏è Line 58: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_991_40401_3\tFirst, we train a teacher model to quantify each sample‚Äôs degree of relying on shortcuts.\ta teacher model\tmodel\tmodel\tteacher\tp\n",
      "‚ö†Ô∏è Line 59: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_6_43304_0\tMy research theme is to develop an optimal analytical model for various information generated during therapy using multimodal data in psychotherapy, to elucidate the process of psychotherapy, and to create an AI therapist to develop a new psychotherapy.\tan AI therapist\tAI\tAI\ttherapist\tp\n",
      "‚ö†Ô∏è Line 60: found 7 columns (expected 6)\n",
      "   ‚Üí 5_acl_57_45359_0\tIn this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues.\tAI tutors\tAI\tAI\ttutor\tp\n",
      "‚ö†Ô∏è Line 61: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2104.01266_1448684_4\tResults from a field study conducted in K-12 classrooms indicate that students learn more when teachers and AI tutors work together during class.\tAI tutors\tAI\tAI\ttutor\tp\n",
      "‚ö†Ô∏è Line 62: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2505.02443_2313003_2\tHowever, concerns arise about the ability of AI tutors to address skill development and engagement during the learning process.\tAI tutors\tAI\tAI\ttutor\tp\n",
      "‚ö†Ô∏è Line 63: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2309.13060_1917864_8\tThis research demonstrates the ability of personal AI tutors to model human learning processes and effectively enhance academic performance.\tpersonal AI tutors\tAI\tAI\ttutor\tp\n",
      "‚ö†Ô∏è Line 64: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2407.15718_2114105_7\tWhen students posed questions within the intended scope, the AI tutors delivered accurate responses 98% of the time.\tthe AI tutors\tAI\tAI\ttutor\tp\n",
      "‚ö†Ô∏è Line 65: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2407.15718_2114105_6\tOverall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions.\tthe AI tutors\tAI\tAI\ttutor\tp\n",
      "‚ö†Ô∏è Line 66: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2504.09720_2298423_2\tWhen deployed as a collaborative tutor, the system restricts student interaction to a chat only interface, promoting controlled and guided engagement.\tthe system\tsystem\tsystem\tcollaborative tutor\tp\n",
      "‚ö†Ô∏è Line 67: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2501.09171_2232755_0\tMany believe that use of generative AI as a private tutor has the potential to shrink access and achievement gaps between students and schools with abundant resources versus those with fewer resources.\tgenerative AI\tgenerative AI\tAI\tprivate tutor\tp\n",
      "‚ö†Ô∏è Line 68: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2504.13903_2302606_4\tExperienced developers are more likely to perceive AI as a junior colleague, a content generator, or assign it no role, whereas less experienced developers primarily view AI as a teacher.\tAI\tAI\tAI\tjunior colleague,content generator\tp\n",
      "‚ö†Ô∏è Line 69: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2503.22040_2286352_2\tWe propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant.\tLLMs\tLLMs\tLLM\tdecision-maker,coding assistant\tp\n",
      "‚ö†Ô∏è Line 70: found 7 columns (expected 6)\n",
      "   ‚Üí 5_arx_2502.17855_2260846_2\tParticipants envisioned AI playing multidimensional roles, such as an operational assistant, personal trainer, group coach, and evaluator, as solutions to address unique instructional and operational challenges in K-12 PE classes.\tAI\tAI\tAI\toperational assistant,personal trainer,group coach,evaluator\tp\n",
      "\n",
      "üîé Checking possessives_positive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2104.14506_1461924_5\tThe end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model.\tan AI model\tAI model\tmodel\tdecisions\tp\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_183_18630_5\tOn four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3.\tthe GPT-3\tGPT-3\tGPT-3\tmisunderstandings\tp\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_1912.03652_1215548_4\tWe investigate how inputs of humans can be altered to reduce misinterpretation by the AI system and to improve efficiency of input generation for the human while altered inputs should remain as similar as possible to the original inputs.\tthe AI system\tAI system\tsystem\tmisinterpretations\tp\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_373_16531_2\tActions by the AI system may be required to bring these objects in view.\tthe AI system\tAI system\tsystem\tactions\tp\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2110.06674_1544817_2\tThis raises the question of how we should limit the harm caused by AI \"lies\" (i.e. falsehoods that are actively selected for).\tAI\tAI\tAI\tlies\tp\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2207.07051_1682949_0\t  Reasoning is a key ability for an intelligent system.\tan intelligent system\tsystem\tsystem\treasoning\tp\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_71_46015_2\tIn this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their ability to understand demographic differences in two subjective judgment tasks: politeness and offensiveness.\tnine popular LLMs\tLLMs\tLLM\tability to understand\tp\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2405.01576_2058788_0\tWe study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant.\tAI systems\tAI systems\tsystem\ttendency to deceive\tp\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_245_29281_1\tWith large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of LLMs.\tLLMs\tLLMs\tLLM\treasoning capabilities\tp\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_916_40326_5\tLHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.\tChinese LLMs\tLLMs\tLLM\tknowledge acquisition capabilities\tp\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_1539_40949_1\tHowever, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.\tLMs\tLMs\tLM\tawareness\tp\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_225_32387_1\tNumerous benchmarks have been established to assess the reasoning abilities of LLMs.\tLLMs\tLLMs\tLLM\treasoning abilities\tp\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_743_29779_6\tAdditionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts.\tChatGPT\tChatGPT\tChatGPT\tcausal reasoning ability\tp\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_538_32700_6\tThe evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.\tLLMs\tLLMs\tLLM\teditorial capabilities\tp\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2101.06573_1410315_4\tWe show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities.\tAI methods\tAI methods\tmethod\tunderstanding capabilities\tp\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2307.16180_1887175_4\tSpecifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality.\tdifferent LLMs\tLLMs\tLLM\tpersonality types\tp\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2408.10159_2130340_1\tLeveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation.\tLarge Language Models (LLMs)\tLarge Language Models (LLMs)\tmodel\tstrengths in knowledge comprehension and reasoning\tp\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2503.16460_2280772_5\tThe first approach uses an intelligent tutoring system for college algebra as a testbed to assess LLM problem-solving capabilities.\tLLM\tLLM\tLLM\tproblem-solving capabilities\tp\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2309.02077_1906881_6\tA medical consultation training set is further constructed to improve the consultation ability of LLMs.\tLLMs\tLLMs\tLLM\tconsultation abilities\tp\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.18752_1851808_6\tMoreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways.\tLLMs\tLLMs\tLLM\tability to use tools\tp\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2309.11805_1916609_6\tInspired by the superior performance of LLMs, we leverage their capability to understand natural language for capturing the information that was previously getting lost during the conversion of unstructured data to structured form.\tLLMs\tLLMs\tLLM\tcapability to understand natural language\tp\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2310.02417_1924674_0\tLarge language models (LLMs), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks.\tLarge language models (LLMs)\tLarge language models (LLMs)\tmodel\tcapability in understanding and following instructions\tp\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.16867_1849923_1\tWe propose to use behavioural game theory to study LLMs' cooperation and coordination behaviour.\tLLMs\tLLMs\tLLM\tcooperation and coordination behavior\tp\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.04388_1837444_1\tIt is tempting to interpret these CoT explanations as the LLM's process for solving a task.\tthe LLM\tLLM\tLLM\tprocess for solving a task\tp\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2005.02335_1282130_0\tExplainable machine learning and artificial intelligence models have been used to justify a model's decision-making process.\ta model\tmodel\tmodel\tdecision-making process\tp\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2203.10923_1623785_4\tWe also discuss issues that are unique to edge applications such as protecting a model's intellectual property and verifying its integrity.\ta model\tmodel\tmodel\tintellectual property\tp\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2207.09374_1685272_1\tHowever, all common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision.\tan AI\tAI\tAI\tdecisions\tp\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2209.15093_1720808_3\tWe propose conceptual consistency to measure a LLM's understanding of relevant concepts.\ta LLM\tLLM\tLLM\tunderstanding\tp\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2210.05487_1726842_3\tWe find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity.\tthe model\tmodel\tmodel\tunderstanding\tp\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2307.00457_1871453_4\tGenRec uses LLMs' understanding ability to interpret context, learn user preferences, and generate relevant recommendation.\tLLMs\tLLMs\tLLM\tunderstanding abilities\tp\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2307.10250_1881245_0\tThis study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology.\tthe GPT-4 Large Language Model\tGPT-4 Large Language Model\tmodel\tabductive reasoning\tp\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.10837_1898729_1\tHowever, effectively integrating LLMs' commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.\tLLMs\tLLMs\tLLM\tcommonsense knowledge and reasoning abilities\tp\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.07326_1895218_4\tOur findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions.\tGPT\tGPT\tGPT\tability to discern and adapt to nuanced instructions\tp\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2307.05488_1876483_8\tThese biases may impact the responses of constructs and should be considered when interpreting ChatGPT's conceptual capabilities.\tChatGPT\tChatGPT\tChatGPT\tconceptual capabilities\tp\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2306.11296_1864444_1\tThis effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.\tChatGPT\tChatGPT\tChatGPT\ttendency to hallucinate information\tp\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2303.13712_1813509_7\tWe then consider a decision maker who is aware of the algorithm's manipulation and responds strategically.\tthe algorithm\talgorithm\talgorithm\tmanipulation\tp\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2211.08380_1747286_6\tIn addition, OREO-LM provides reasoning paths as rationales to interpret the model's decision.\tthe model\tmodel\tmodel\tdecisions\tp\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2301.13852_1784916_11\tUsing explainability, we observe that ChatGPT's writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.\tChatGPT\tChatGPT\tChatGPT\tpolite writing\tp\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2306.03423_1856571_4\tIn this experiment, we characterize ChatGPT's refusal behavior using a black-box attack.\tChatGPT\tChatGPT\tChatGPT\trefusal behavior\tp\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_406_32568_3\tOur analysis of GPT-series models over a rule subset reveals significant gaps in LLMs‚Äô logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns.\tLLMs\tLLMs\tLLM\tlogic understanding\tp\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2309.05163_1909967_10\tHowever, given the LLMs' considerable proficiency in writing Physics essays and coding abilities, non-invigilated examinations of these skills in Physics are highly vulnerable to automated completion by LLMs.\tthe LLMs\tLLMs\tLLM\tconsiderable proficiency in writing Physics essays and coding abilities\tp\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2311.08487_1951454_2\tThis conflict renders LLMs vulnerable to adversarial attacks, wherein intensifying the models' desire for continuity can circumvent alignment efforts, resulting in the generation of harmful information.\tthe model\tmodel\tmodel\tdesire\tp\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_865_25015_6\tOn the other hand, downstream performance is mainly driven by the model‚Äôs size and prior legal knowledge which can be estimated by upstream and probing performance.\tthe model\tmodel\tmodel\tprior legal knowledge\tp\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 6_acl_2210.12530_1733885_3\tOur method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.\tthe LM\tLM\tLM\tcommon-sense reasoning\tp\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2206.14576_1674996_1\tMore specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature.\tGPT-3\tGPT-3\tGPT-3\tdecision-making,deliberation,causal reasoning abilities\tp\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.01552_1889444_4\tThe results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.\tChatGPT\tChatGPT\tChatGPT\tcompetence in comprehending and performing intricate tasks\tp\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.06032_1893924_10\tOur research is the first to systematically study an LLM's legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.\tan LLM\tLLM\tLLM\tlegal drafting and reasoning capabilities\tp\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.06920_1894812_8\tThis research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT's cognitive abilities enhance the design and development of potential pharmaceutical solutions.\tChatGPT\tChatGPT\tChatGPT\tcognitive abilities\tp\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.07326_1895218_5\tFurthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles.\tthe LLM\tLLM\tLLM\tcapacity to internalize and project instructible personas\tp\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2308.08407_1896299_3\tExplainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders.\tan AI system\tAI system\tsystem\tability to provide a robust interpretation of its decision-making logic\tp\n",
      "‚ö†Ô∏è Line 51: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2307.05488_1876483_1\tThe objective of the studies is to assess ChatGPT's ability to comprehend theoretical concepts and differentiate between constructs.\tChatGPT\tChatGPT\tChatGPT\tability to comprehend theoretical concepts and differentiate between constructs\tp\n",
      "‚ö†Ô∏è Line 52: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2307.04274_1875269_7\tFinally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.\tthe model\tmodel\tmodel\tability to showcase pedagogical skills\tp\n",
      "‚ö†Ô∏è Line 53: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2306.10645_1863793_3\tWe examine ChatGPT's ability to pursue multiple interconnected learning objectives, adapt the educational activity to users' characteristics, such as culture, age, and level of education, and its ability to use diverse educational strategies and conversational styles.\tChatGPT\tChatGPT\tChatGPT\tability to pursue multiple interconnected learning objectives\tp\n",
      "‚ö†Ô∏è Line 54: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2306.06123_1859271_2\tThe possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery.\tthe model\tmodel\tmodel\treasoning\tp\n",
      "‚ö†Ô∏è Line 55: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.16867_1849923_8\tThese results enrich our understanding of LLMs' social behaviour and pave the way for a behavioural game theory for machines.\tLLMs\tLLMs\tLLM\tsocial behaviour\tp\n",
      "‚ö†Ô∏è Line 56: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.14795_1847851_2\tCurrent evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs.\tthe model\tmodel\tmodel\trelated beliefs\tp\n",
      "‚ö†Ô∏è Line 57: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.12763_1845819_3\tWe find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature.\tGPT\tGPT\tGPT\trational decisions\tp\n",
      "‚ö†Ô∏è Line 58: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2305.12564_1845620_3\tMoreover, we find that this seemingly default perception of ChatGPT as male can reverse when ChatGPT's feminine-coded abilities are highlighted (e.g., providing emotional support for a user).\tChatGPT\tChatGPT\tChatGPT\tfeminine-coded abilities\tp\n",
      "‚ö†Ô∏è Line 59: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_2303.03480_1803277_1\tOur approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions.\tthe LLM\tLLM\tLLM\tcommonsense reasoning capabilities\tp\n",
      "‚ö†Ô∏è Line 60: found 7 columns (expected 6)\n",
      "   ‚Üí 6_arx_1704.00717_835229_5\tIn this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks.\tAI\tAI\tAI\ttheory of mind\tp\n",
      "\n",
      "üîé Checking verb_objects_inconclusive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_143_43709_5\tOur experiments show that these errors can be identified with high accuracy by an LLM.\tan LLM\tLLM\tLLM\tidentify\tinc\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_511_29547_1\tTo maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.\tLLMs\tLLMs\tLLM\tacquire\tinc\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2001.08625_1234038_4\tExamination prioritization was performed by the AI, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.\tthe AI\tAI\tAI\tperform\tinc\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2301.11767_1782831_2\tIn CAPoW, a security professional can define relevant request context attributes which can be learned by the AI system.\tthe AI system\tAI system\tsystem\tlearn\tinc\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2106.07921_1485685_1\tI suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025.\tAI-models\tAI-models\tmodel\tperform\tinc\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2007.15619_1327133_1\tThis can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the model.\tthe model\tmodel\tmodel\tidentify\tinc\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2302.06852_1791786_1\tThe simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by the AI methods and provides a means of explainability.\tthe AI methods\tAI methods\tmethod\tlearn\tinc\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_23_33157_0\tLarge Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves.\tthe LLMs\tLLMs\tLLM\thold knowledge\tinc\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_3_26185_65\tThe text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.\tChatGPT\tChatGPT\tChatGPT\ttranslate\tinc\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_260_9141_4\tThe final submission was chosen based on the best performances which was achieved by the BERT+BiLSTM model.\tthe BERT+BiLSTM model\tBERT+BiLSTM model\tmodel\tachieve\tinc\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1907.08625_1153161_6\tThese features are best interpreted by a self-consistent relativistic reflection model.\ta self-consistent relativistic reflection model\trelativistic reflection model\tmodel\tinterpret\tinc\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_430_37198_2\tIn this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by LLMs.\tLLMs\tLLMs\tLLM\tinterpret\tinc\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_696_6286_1\tHow should we train a language model in this scenario?\ta language model\tlanguage model\tmodel\ttrain\tinc\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_891_29927_2\tUtilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data.\tfixed LLMs\tLLMs\tLLM\tinstruct\tinc\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_27_42781_1\tA promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.\tLLMs\tLLMs\tLLM\tcorrect\tinc\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_27_42781_3\tThis paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.\tLLMs\tLLMs\tLLM\tcorrect\tinc\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2305.01937_1834993_4\tWe present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.\tthe LLMs\tLLMs\tLLM\tpresent\tinc\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_920_38624_10_1\tWe show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.\tthe LLM\tLLM\tLLM\tinteract with\tinc\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2309.11000_1915804_0\tThis paper explores the potential of constructing an AI spoken dialogue system that \"thinks how to respond\" and \"thinks how to speak\" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.\tan AI spoken dialogue system\tAI spoken dialogue system\tsystem\tconstruct\tinc\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_794_28930_3\tWe introduce a new task, ‚Äúless likely brainstorming,‚Äù that asks a model to generate outputs that humans think are relevant but less likely to happen.\ta model\tmodel\tmodel\task\tinc\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2308.01154_1889046_3\tWe successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.\ta light language model\tlight language model\tmodel\ttrain\tinc\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2304.00385_1818431_9\tFor earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.\tthe LLM\tLLM\tLLM\task\tinc\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2411.05823_2188250_7\tSubsequently, we ask LLMs to predict this masked field.\tLLMs\tLLMs\tLLM\task\tinc\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2204.06916_1637054_2\tTypically, the advice generated by AI is judged by a human and either deemed reliable or rejected.\tAI\tAI\tAI\tgenerate\tinc\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2004.11543_1276313_6\tEach lesson in the curriculum is learnt by a deep reinforcement learning model.\ta deep reinforcement learning model\tdeep reinforcement learning model.\tmodel\tlearn\tinc\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_814_27535_2\tTherefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.\tan agent\tagent\tagent\ttrain\tinc\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2012.09755_1397633_5\tSpecifically, the structural features identified by our algorithm were found to be related to clinical observations of glaucoma.\tour algorithm\talgorithm\talgorithm\tidentify\tinc\n",
      "\n",
      "üîé Checking verb_objects_negative.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1910.06294_1190213_4\tIn this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.\tpre-trained masked language models\tpre-trained masked language models\tmodel\tprovide\tn2\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2006.05347_1299861_5\tAs for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a low complexity algorithm.\ta low complexity algorithm\talgorithm\talgorithm\taddress\tn2\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2007.00900_1312414_5\tWe introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.\tthe BERT language model\tBERT language model\tmodel\tpower\tn1\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_7_34206_4\tA positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.\tGPT-4\tGPT-4\tGPT-4\treproduce\tn2\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2103.07820_1437965_2\tWe consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.\tthe onboard DAA system\tDAA system\tsystem\tcontrol\tn2\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1811.12185_1056818_9\tWe observed 95.61% alarms raised by the said system are taken care of by the operator.\tthe said system\tsystem\tsystem\traise alarm\tn1\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1909.09993_1180031_2\tMoreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.\tthe A2W model\tA2W model\tmodel\tcover\tn1\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1912.06835_1218731_4\tBy comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.\tthe ion flow model\tion flow model\tmodel\tpredict\tn3\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2102.07384_1423873_5\tThe two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.\tthe BCD algorithm\tBCD algorithm\talgorithm\tgenerate\tn3\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2205.05016_1649428_3\tThe fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.\tthe K-means algorithm\tK-means algorithm\talgorithm\tcluster\tn1\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_280_28416_2\tFor example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?\tthe debiasing method\tdebiasing method\tmethod\tuse\tn1\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_4_33862_8\tTaken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.\ta flexible form-to-meaning mapping system\tform-to-meaning mapping system\tsystem\tsupport\tn2\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_6_42908_8\tWe find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.\tLLMs\tLLMs\tLLM\tgenerate\tn3\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_377_35235_0\tSignificant advancements have recently been made in large language models, represented by GPT models.\tGPT models\tGPT models\tmodel\trepresent\tn2\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1901.05719_1075266_4\t  Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.\tAI algorithms\tAI algorithms\talgorithm\trealize\tn2\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1902.02508_1083592_0\tThe equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.\ta SGS model\tSGS model\tmodel\tcapture\tn2\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2002.05702_1243125_4\tCNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.\ta generative model of synthetic structures\tgenerative model\tmodel\tcreate\tn3\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2009.12437_1353940_2\tUltimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.\ta model\tmodel\tmodel\tpredict\tn3\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2106.02498_1480262_5\tSpecifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.\tan AI system\tAI system\tsystem\tmeet criteria\tn2\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1212.5593_395209_0\tConsidering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.\ta linear time varying model\tlinear time varying model\tmodel\tdescribe\tn2\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1401.5941_495172_4\tThen, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.\ta backpropagation algorithm (MLP-BP)\tbackpropagation algorithm (MLP-BP)\talgorithm\ttrain\tn3\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1408.5886_551192_2\tBoth the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.\ta model\tmodel\tmodel\tdescribe\tn2\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1501.07576_594324_2\tIn this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.\ta three-dimensional dynamic point-mass model\tpoint-mass model\tmodel\tdescribe\tn2\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1610.02937_778284_8\tThe synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.\tthe model\tmodel\tmodel\tpredict\tn3\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2304.11116_1829162_4\tInspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.\tthe latest ChatGPT and Toolformer models\tChatGPT and Toolformer models\tmodel\tinspire\tn2\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2110.14419_1552562_1\tDrawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.\tAI\tAI\tAI\tshape,influence\tn3\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2207.00691_1676589_8\tThe results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.\tlanguage-and-image AI\tAI\tAI\tlearn\tn3\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1812.01714_1059288_3\tThrough examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.\tour model\tmodel\tmodel\tlearn\tn3\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2412.12865_2215243_5\tThis preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.\tthe aligned LLMs\tLLMs\tLLM\tpredict\tn3\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_19_45086_1\tTherefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.\tan LLM\tLLM\tLLM\tgenerate\tn3\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_0906.5497_132048_3\tWe find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.\ta model\tmodel\tmodel\texplain\tn2\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2002.10965_1248388_3\tFor the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.\ta low-complexity trellis-based algorithm\ttrellis-based algorithm\talgorithm\tsolve\tn2\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_750_35597_0\tWe present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-‚Äùstandard‚Äù varieties from around the world).\tChatGPT\tChatGPT\tChatGPT\texhibit bias\tn3\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_502_20526_4\tWe find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.\tthe LM\tLM\tLM\tlearn\tn3\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2105.13818_1476377_2\tBy applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.\tthese models\tmodels\tmodel\tacquire\tn3\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_46_41679_2\tHowever, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.\tthe language model\tlanguage model\tmodel\tsee\tn3\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_1712.09783_928382_2\tIn order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.\tthe MoE model\tMoE model\tmodel\ttrain\tn3\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_6_25720_4\tWe believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.\tLLMs\tLLMs\tLLM\tlook at\tn1\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2101.04617_1408359_3\tWe engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.\tthe trained model\tmodel\tmodel\temploy\tn1\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2101.05967_1409709_1\tMany companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.\tAI\tAI\tAI\tdeploy\tn1\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_118_9850_4\tWe first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.\tthe model\tmodel\tmodel\tfine-tune\tn1\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_3_2185_5\tWe evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.\tthe RBM-based language model\tRBM-based language model\tmodel\tevaluate\tn1\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_1911.03597_1202191_5\tMoreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.\tthe model\tmodel\tmodel\tpre-train\tn1\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_329_43922_3\tWe ask whether structural information can be extracted from LLM‚Äôs and develop a model that integrates it with their learnt statistics.\ta model\tmodel\tmodel\tdevelop\tn1\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2302.07257_1792191_6\tThe goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.\ta more user-friendly and understandable system\tsystem\tsystem\tcreate\tn1\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_301_32463_8\tFinally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.\teight mainstream LLMs\tLLMs\tLLM\tevaluate,analyze\tn1\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_5_42629_6\tWe evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.\trecent multilingual LLMs\tLLMs\tLLM\tevaluate\tn1\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_28_45074_3\tWe design a system using these pre-trained models to answer questions, based on the given context.\ta system\tsystem\tsystem\tdesign\tn1\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_41_45371_2\tUnlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.\tan individual model\tmodel\tmodel\tconstruct\tn1\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_104_45430_3\tWe also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.\ta search algorithm\tsearch algorithm\talgorithm\tdevelop\tn1\n",
      "‚ö†Ô∏è Line 51: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_6_46417_4\tUsing this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.\ta BERT-based model\tBERT-based model\tmodel\ttrain,present\tn3\n",
      "‚ö†Ô∏è Line 52: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_7_60414_2\tIn evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.\trecurrent LMs\trecurrent LMs\tLM\ttrain\tn3\n",
      "‚ö†Ô∏è Line 53: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_27_46301_5\tTo test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.\tGPT-2 models\tGPT-2 models\tmodel\ttrain\tn3\n",
      "‚ö†Ô∏è Line 54: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_1910.06294_1190213_4\tIn this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.\ta fast and compact model\tmodel\tmodel\ttrain\tn3\n",
      "‚ö†Ô∏è Line 55: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_1_7090_2\tThus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.\tthe seq2seq model\tseq2seq model\tmodel\tbuild\tn1\n",
      "‚ö†Ô∏è Line 56: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_928_27649_2\tUsing the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.\tChatGPT\tChatGPT\tChatGPT\tevaluate\tn1\n",
      "‚ö†Ô∏è Line 57: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_582_29618_3\tIn the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.\ta recurrent neural language model\trecurrent neural language model\tmodel\tdevelop\tn1\n",
      "‚ö†Ô∏è Line 58: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_418_44990_5\tThese findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.\tAI\tAI\tAI\tdevelop\tn1\n",
      "‚ö†Ô∏è Line 59: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_45_45963_3\tOur approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.\ta router model\trouter model\tmodel\ttrain\tn3\n",
      "‚ö†Ô∏è Line 60: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2308.00624_1888516_6\tWe have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.\tthe model\tmodel\tmodel\ttrain\tn3\n",
      "‚ö†Ô∏è Line 61: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_75_14800_1\tHowever, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.\tthe model\tmodel\tmodel\tacquire\tn3\n",
      "‚ö†Ô∏è Line 62: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_98_33764_7\tHuman raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.\tLLMs\tLLMs\tLLM\tgenerate\tn3\n",
      "‚ö†Ô∏è Line 63: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_1_42012_5\tWe posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.\tan LLM\tLLM\tLLM\tgenerate\tn3\n",
      "‚ö†Ô∏è Line 64: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_600_45881_2\tIn this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs.\tLLMs\tLLMs\tLLM\tlearn\tn3\n",
      "‚ö†Ô∏è Line 65: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2306.02920_1856068_2\tSpecifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.\tbilingual LMs\tLMs\tLM\ttrain\tn3\n",
      "\n",
      "üîé Checking verb_objects_positive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_119_34984_3\tHowever, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge.\tthe LLMs\tLLMs\tLLM\tperceive\tp2\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_738_35585_4\tSince a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism.\tLLMs\tLLMs\tLLM\tconsider\tp2\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_304_37076_3\tHowever their explicitly mention of malicious intent will be easily recognized and defended by LLMs.\tLLMs\tLLMs\tLLM\trecognize,defend\tp2\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_305_37077_7\tThis conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals.\tGPT-4\tGPT-4\tGPT-4\tconduct\tp3\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_687_35537_5\tHowever, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.\tLMs\tLMs\tLM\tmaster\tp2\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1811.00189_1044822_2\tA remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation.\tthe AI model\tAI model\tmodel\trecognize,use\tp2\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2204.03332_1633470_10\tWe also demonstrate, that the considered system possesses a counter-intuitive relationship between workload and performance, which nevertheless is correctly inferred by the proposed simulation model.\tthe proposed simulation model\tsimulation model\tmodel\tinfer\tp2\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1811.00189_1044822_0\tIn this study, we propose a new methodology to control how user's data is recognized and used by AI via exploiting the properties of adversarial examples.\tAI\tAI\tAI\trecognize,use\tp2\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1910.04404_1188323_0\tExplanation is necessary for humans to understand and accept decisions made by an AI system when the system's goal is known.\tan AI system\tAI system\tsystem\tmake a decision\tp2\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2012.08285_1396163_1\tThe goal of this article is to paint a vision of a new air interface which is partially designed by AI to enable optimized communication schemes for any hardware, radio environment, and application.\tAI\tAI\tAI\tdesign\tp3\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2205.08123_1652535_3\tResults: After the exclusion of cases not meeting the study criteria, 9579 cases were analysed by AI.\tAI\tAI\tAI\tanalyse\tp2\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 2_acl_10_8527_2\tThe former is concerned with the generation of explanations for decisions taken by AI systems, while the latter is concerned with the way explanations are given to users and received by them.\tAI systems\tAI systems\tsystem\ttake a decision\tp2\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2011.13169_1385917_2\tThe need for explainable AI does not stem only from ethical and moral grounds but also from stricter legislation around the world mandating clear and justifiable explanations of any decision taken or assisted by AI.\tAI\tAI\tAI\ttake a decision,assist a decision\tp2\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1712.07473_926072_0\t  One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm.\tthe algorithm\talgorithm\talgorithm\tface\tp2\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2011.03195_1375943_6\tMedical diagnosis model is responsible for human life and we need to be confident enough to treat a patient as instructed by a black-box model.\ta black-box model\tblack-box model\tmodel\tinstruct\tp2\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1806.10698_996338_7\tIn addition, we found that the triage advice recommended by the AI System was, on average, safer than that of human doctors, when compared to the ranges of acceptable triage provided by independent expert judges, with only a minimal reduction in appropriateness.\tthe AI system\tAI system\tsystem\trecommend\tp2\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1707.01659_866691_0\tIn this work, a dynamic system is controlled by multiple sensor-actuator agents, each of them commanding and observing parts of the system's input and output.\tmultiple sensor-actuator agents\tsensor-actuator agents\tagent\tcontrol,command,observe\tp3\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1905.10083_1128519_5\tHowever, research on edge intelligence is still in its infancy stage, and a dedicated venue for exchanging the recent advances of edge intelligence is highly desired by both the computer system and artificial intelligence communities.\tthe computer system\tcomputer system\tsystem\tdesire\tp2\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_1811.01439_1046072_2\tThese models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break.\tthe complex system\tsystem\tsystem\tmake a decision\tp2\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 2_arx_2311.11045_1954012_4\tWe seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model.\tsmall LMs\tLMs\tLM\tteach\tp2\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_569_29605_3\tUsing GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs.\tLLMs\tLLMs\tLLM\tfool\tp2\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_71_43638_5\tSpecifically, we first identify a series of action candidates that could potentially trick LLMs into providing harmful responses.\tLLMs\tLLMs\tLLM\ttrick\tp2\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_40_10977_2\tIn a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?\ta language model\tlanguage model\tmodel\ttell\tp3\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2304.08366_1826412_8\tNext, we summarize the interviewees' reasons why and why not they would like to collaborate with AI.\tAI\tAI\tAI\tcollaborate with\tp2\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2305.07001_1840057_9\tOur approach sheds light on developing more user-friendly recommender systems, in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions.\tthe system\tsystem\tsystem\tcommunicate with\tp2\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_358_32520_4\tOur goal is to teach the LLM that ‚Äúeven if the sentences are identical if they are spoken in different styles, their corresponding responses might be different‚Äù.\tthe LLM\tLLM\tLLM\tteach\tp2\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_3_22073_5\tFirst, we induce a language model to produce step-by-step rationales before outputting the answer to effectively communicate the task to the model.\tthe model\tmodel\tmodel\tcommunicate to\tp2\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2112.11668_1581444_1\tYet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.\ta BERT-based sentiment analysis model\tBERT-based sentiment analysis model\tmodel\tfool\tp2\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2205.01772_1646184_6\tFurthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing.\ta biometric system\tbiometric system\tsystem\tfool\tp2\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2402.09671_2007310_0\tThis investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.\tmultiple AI vision systems\tAI vision systems\tsystem\tfool\tp2\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2308.14132_1902024_1\tSuch jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content.\tLLMs\tLLMs\tLLM\ttrick\tp2\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2309.05689_1910493_2\tSocratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement.\tLLMs\tLLMs\tLLM\tencourage\tp2\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2306.04707_1857855_3\tWe study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses.\tthe model\tmodel\tmodel\ttrick\tp2\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2311.08147_1951114_3\tHowever, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response.\tthe model\tmodel\tmodel\tconfuse\tp2\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_602_40012_4\tBased on our findings, we propose FSLI, a framework for encouraging LLMs to Forget Spurious correlations and Learn from In-context information.\tLLMs\tLLMs\tLLM\tencourage\tp2\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_292_36775_3\tIn this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.\tLLMs\tLLMs\tLLM\tencourage\tp2\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_773_32935_3\tSpecifically, we study how to persuade LLMs to jailbreak them.\tLLMs\tLLMs\tLLM\tpersuade\tp2\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_328_38052_5\tThis method inspires the LLM to perceive domain information from the source text, which then serves as a helpful hint to guide the translation process.\tthe LLM\tLLM\tLLM\tinspire\tp2\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_476_35330_3\tMotivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses.\tLLMs\tLLMs\tLLM\tinspire\tp2\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_373_22509_4\tOverall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.\tthe model\tmodel\tmodel\tincentivize\tp2\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_31_27805_5\tGiven a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights.\tthe LLM\tLLM\tLLM\tcollaborate with\tp2\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_920_38624_10_2\tWe show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.\tthe LLM\tLLM\tLLM\tcommunicate with\tp2\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_8_31860_3\tIn this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response.\tGPT-3\tGPT-3\tGPT-3\tconfuse\tp2\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_774_40184_4\tThe framework motivates the model itself to automatically generate rationales on existing datasets.\tthe model\tmodel\tmodel\tmotivate\tp2\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_347_24497_6\tSecond, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences.\tthe model\tmodel\tmodel\tteach\tp2\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_814_27535_3\tIn this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.\tLLMs\tLLMs\tLLM\tteach\tp2\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_114_29150_2\tExisting studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction.\tLLMs\tLLMs\tLLM\tencourage\tp2\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2306.10063_1863211_1\tIn this conception, learners continually converse with AI language models within a dynamic computational medium of internet tools and resources.\tAI language models\tAI language models\tmodel\tconverse with\tp2\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2306.03856_1857004_0\tWe propose iteratively prompting a large language model to self-correct a translation, with inspiration from their strong language understanding and translation capability as well as a human-like translation approach.\ta large language model\tlarge language model\tmodel\tprompt\tp1\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_212_35072_4\tInstead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors.\tLLMs\tLLMs\tLLM\tinstruct\tp3\n",
      "‚ö†Ô∏è Line 51: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_966_30002_2\tWe ask several LLMs and humans to write such a story and conduct a human evaluation involving various criteria such as fluency, coherence, originality, humor, and style.\tseveral LLMs\tLLMs\tLLM\task\tp3\n",
      "‚ö†Ô∏è Line 52: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_3_33872_4\tVariations of this vignette are used for role-prompting a commercial LLM, GPT-4, instructing the LLM to take on the role described in the patient vignette and act accordingly.\tthe LLM\tLLM\tLLM\tinstruct\tp3\n",
      "‚ö†Ô∏è Line 53: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_11_31225_2\tTo explore this line of research, this paper uses a case study, namely, finding the best prompting strategy for asking ChatGPT to define new words based on morphological connections.\tChatGPT\tChatGPT\tChatGPT\task\tp3\n",
      "‚ö†Ô∏è Line 54: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_599_29635_5\tLast, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.\tthe LLM\tLLM\tLLM\task\tp3\n",
      "‚ö†Ô∏è Line 55: found 7 columns (expected 6)\n",
      "   ‚Üí 3_arx_2302.13817_1798751_5\tWe also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.\tChatGPT\tChatGPT\tChatGPT\task\tp3\n",
      "‚ö†Ô∏è Line 56: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_7_30789_3\tWe then introduce these pairs into translation prompts, instructing ChatGPT to use the correct translations of the domain entities.\tChatGPT\tChatGPT\tChatGPT\tinstruct\tp3\n",
      "‚ö†Ô∏è Line 57: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_5_46527_1\tDifferent prompts are tested to instruct the LLM to clean the text without changing the structure, vocabulary or specialized lexicon.\tthe LLM\tLLM\tLLM\tinstruct\tp3\n",
      "‚ö†Ô∏è Line 58: found 7 columns (expected 6)\n",
      "   ‚Üí 3_acl_11_41931_7\tIn the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries.\tthe LLMs\tLLMs\tLLM\task\tp3\n",
      "\n",
      "üîé Checking verb_subjects_inconclusive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_586_6176_1\tWhile prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context.\tLMs\tLMs\tLM\tderive knowledge\tinc\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_5_9514_1\tIn some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account.\tthe model\tmodel\tmodel\tmake a prediction\tinc\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_65_19388_5\tThe human evaluation found that our topic model creates coherent topics.\tour topic model\ttopic model\tmodel\tcreate\tinc\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_22_31716_3\tFocusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics.\tLLMs\tLLMs\tLLM\tlearn\tinc\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_163_34648_5\tOur evaluation on both automated metrics and qualitative human evaluation suggests that by incorporating end-user specifications into the conversion process, our model can create presentations that are not only informative but also tailored to expectations and cognitive abilities of the target audience.\tour model\tmodel\tmodel\tcreate\tinc\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2110.10185_1548328_5\tThe visual interface makes it possible for users to interact with AI systems following a Refine-Forecast paradigm to ensure that the generation system acts in a manner human users find suitable.\tthe generation system\tgeneration system\tsystem\tact\tinc\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.07414_1763591_5\tOur algorithm considers the channel conditions during the dynamic weight selection process.\tOur algorithm\talgorithm\talgorithm\tconsider\tinc\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1101.3316_238729_3\tThe test shows that the model correctly identifies ~80% of known QSOs with a 25% false positive rate.\tthe model\tmodel\tmodel\tidentify\tinc\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1205.6986_345987_9\tAt the same time, our model dissects this variability into components that result from individual SNP effects and population structure.\tour model\tmodel\tmodel\tdissect\tinc\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2302.04335_1789269_6\tIn other words, ChatGPT can create content on many topics with high originality as if they were written by someone.\tChatGPT\tChatGPT\tChatGPT\tcreate\tinc\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_154_25214_1\tHowever, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope.\ta model\tmodel\tmodel\tlearn\tinc\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_852_33014_0\tRecent LLMs have demonstrated remarkable performance in solving exam-like math word problems.\tRecent LLMs\tLLMs\tLLM\tdemonstrate\tinc\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_2_34276_7\tOur results show that not all extracted utterances are correctly structured, indicating that either LLMs do not fully acquire syntactic-semantic rules or they acquire them but cannot apply them effectively.\tLLMs\tLLMs\tLLM\tacquire\tinc\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_53_34920_6\tTaken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena.\tLMs\tLMs\tLM\tlearn\tinc\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_24_17147_6\tOur experiment results suggest that GPT-3 has acquired linguistic knowledge to identify certain semantic information in most cases, but still fails when there are some types of disturbance happening in the sentence.\tGPT-3\tGPT-3\tGPT-3\tacquire,fail\tinc\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1205.3313_342314_2\tThe model does not differentiate between aerosol particles, cloud droplets, drizzle or rain drops.\tThe model\tmodel\tmodel\tdifferentiate\tinc\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.10592_1828638_0\tThe recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images.\tThe recent GPT-4\tGPT-4\tGPT-4\tdemonstrate abilities\tinc\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_11_43395_1\tFor example, a model should correctly identify kimchi (Korean food) in an image both when an Asian woman is eating it, as well as an African man is eating it.\ta model\tmodel\tmodel\tidentify\tinc\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.09337_1827383_2\tIt often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user's intention.\tthe model\tmodel\tmodel\tinterpret\tinc\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2207.08333_1684231_3\tWe observed that the VL model does not interpret the overall context of an input image but instead shows biases toward a specific object or shape that forms the local context.\tthe VL model\tVL model\tmodel\tinterpret\tinc\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_814_27535_1\tSuch ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors.\tLLMs\tLLMs\tLLM\tsimulate\tinc\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_7_34361_5\tThe results show that ChatGPT made more changes than the average post-editor.\tChatGPT\tChatGPT\tChatGPT\tmake changes\tinc\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1908.02624_1160794_1\tAI systems can now translate across multiple languages, identify objects in images and video, streamline manufacturing processes, and control cars.\tAI systems\tAI systems\tsystem\ttranslate,identify,streamline,control\tinc\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.02868_1820914_3\tPrecisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.\tChatGPT\tChatGPT\tChatGPT\tconstruct,fail to leverage world knowledge,infer\tinc\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.02182_1820228_0\tThe recently released ChatGPT has demonstrated surprising abilities in natural language understanding and natural language generation.\tThe recently released ChatGPT\tChatGPT\tChatGPT\tdemonstrate surprising abilities\tinc\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.02868_1820914_2\tOur experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence.\tChatGPT\tChatGPT\tChatGPT\tperform competitively,exhibit a low level of intelligence\tinc\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2302.06100_1791034_8\tWe find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.\tGPT-3\tGPT-3\tGPT-3\tperform poorly at answering straightforward questions\tinc\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_679_35529_6\t4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts.\tLLMs\tLLMs\tLLM\texhibit overconfidence\tinc\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1904.08530_1113570_2\tWe cross-check with control cases to ensure that the AI is not randomly guessing and is indeed identifying an inherent structure.\tthe AI\tAI\tAI\tguess,identify\tinc\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1011_30047_1\tHowever, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses.\tthe instruction-tuned model\tinstruction-tuned model\tmodel\tsee\tinc\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1905.04127_1122563_1\tTraditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy.\tAI agents\tAI agents\tagent\tsuffer from difficulties\tinc\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_78_25975_8\tThe semantic dependency feature serves as a global signal and helps the model learn simile knowledge that can be applied to unseen domains.\tthe model\tmodel\tmodel\tlearn\tinc\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.17276_1817073_8\tThis suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.\tlarger and more advanced LLMs\tLLMs\tLLM\tdevelop a tendency\tinc\n",
      "\n",
      "üîé Checking verb_subjects_negative.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_141_6586_10\tEspecially on the two narrative prompts, our model performs much better than all other state-of-the-art models.\tour model\tmodel\tmodel\tperform\tn2\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.09655_1827701_1\tChatGPT in particular, an AI chatbot developed and recently released by OpenAI, has taken the field to the next level.\tChatGPT\tChatGPT\tChatGPT\ttake the field to the next level\tn1\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.06122_1824168_2\tWhile recent works have explored the use of ChatGPT in the context of humanities, business school, or medical school, this work explores how ChatGPT performs in the context of an introductory computer engineering course.\tChatGPT\tChatGPT\tChatGPT\tperform\tn2\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.02796_1820842_4\tWe analyze the opportunities and challenges that ChatGPT presents for knowledge management in design and propose promising future research directions.\tChatGPT\tChatGPT\tChatGPT\tpresent challenges and opportunities\tn2\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.09064_1827110_0 \tIn the past years, AI has seen many advances in the field of NLP.\tAI\tAI\tAI\tsee advances\tn2\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2210.17127_1738482_1\tRecent research has revealed that neural language models at scale suffer from poor temporal generalization capability, i.e., the language model pre-trained on static data from past years performs worse over time on emerging data.\tneural language models at scale\tneural language models\tmodel\tsuffer from poor temporal generalization capability\tn3\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.12767_1812564_0\tChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks.\tChatGPT\tChatGPT\tChatGPT\tdemonstrate\tn3\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.04780_1822826_0\tArtificial intelligence (AI) models are increasingly finding applications in the field of medicine.\t  Artificial intelligence (AI) models\tArtificial intelligence (AI) models\tmodel\tfind applications\tn2\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.05613_1823659_1\tChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention.\tChatGPT\tChatGPT\tChatGPT\trepresent\tn2\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.06488_1824534_6\tMoreover, we present an outlook on how ChatGPT might evolve to realize general-purpose AIGC (a.k.a. AI-generated content), which will be a significant milestone for the development of AGI.\tChatGPT\tChatGPT\tChatGPT\tevolve\tn2\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.09433_1827479_3\tLLMs, which are pretrained on broad data, can perform diverse downstream tasks simply conditioned on natural language task descriptions.\tLLMs\tLLMs\tLLM\tperform\tn2\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_18_8507_4\tWhile the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality.\tthe paraphrasing model\tparaphrasing model\tmodel\tproduce\tn1\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_19_9312_0\tAn image captioning system involves modules on computer vision as well as natural language processing.\tAn image captioning system\timage captioning system\tsystem\tinvolve\tn1\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_11_13818_6\tOur model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages.\tOur model\tmodel\tmodel\tcover\tn1\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.07232_1825278_3\tHowever, we found that the ChatGPT model performed no better than a dummy classifier for both binary and multi-label classification tasks for code vulnerability detection.\tthe ChatGPT model\tChatGPT model\tmodel\tperform\tn2\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_265_19705_2\tThe model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence.\tThe model\tmodel\tmodel\tconsist of\tn1\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1005.1737_189277_4\tThe proposed system makes use of an Air Quality Index (AQI) which is presently not available in Mauritius.\tThe proposed system\tsystem\tsystem\tmake use\tn1\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1109.0704_285072_6\tAs a consequence, the model covers different ranges of coronal temperatures as time progresses.\tthe model\tmodel\tmodel\tcover\tn1\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1208.6130_366702_5\tWe also give a heuristic condition for when an arbitrary system's nonlinear response means that its ZT ceases to indicate (even qualitatively) the lowest temperature to which the system can refrigerate.\tthe system\tsystem\tsystem\trefrigerate\tn1\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1502.04791_599451_0\tOver the past decade, AI has made remarkable progress.\tAI\tAI\tAI\tmake progress\tn2\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_71_26792_1\tBut how do LLMs represent relationships between languages?\tLLMs\tLLMs\tLLM\trepresent\tn2\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1706.07107_861842_4\tOur model involves only two fitting parameters, the monomer size and the area needed for one molecule during adsorption and is in agreement with the experimental data.\tOur model\tmodel\tmodel\tinvolve\tn1\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1708.08160_883172_9\tWe foresee that our model will bring a new dimension to medical research and the medical industry.\tour model\tmodel\tmodel\tbring a new dimension\tn1\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1706.09554_864289_2\tThe OCC model has established itself as the standard model for emotion synthesis.\tThe OCC model\tOCC model\tmodel\testablish\tn2\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_3_33196_3\tThese AI models do not always produce accurate outputs and are known for generating incorrect information, known as hallucinations, whose causes are hard to pinpoint.\tThese AI models\tAI models\tmodel\tproduce\tn2\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2009.13222_1354725_0\tAI artificial intelligence brings about new quantitative techniques to assess the state of an economy.\tAI artificial intelligence\tAI artificial intelligence\tintelligence\tbring about\tn1\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2010.08309_1364639_2\tThus, the system requires both a high sampling rate and time synchronization accuracy, leading to a high cost and large size.\tthe system\tsystem\tsystem\trequire\tn1\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1023_35862_2\tIn such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs.\tLLMs\tLLMs\tLLM\tact\tn2\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1262_36095_8\tOur RR model is more robust, although our TF model performs better than the RR model without any attacks.\tour TF model\tTF model\tmodel\tperform\tn2\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2103.01063_1431208_6\tOur results demonstrate that the proposed algorithm can find the joint optimal solution for the position/orientation estimation accuracy and EADR, with its optimization performance being robust to slight localization or channel estimation errors and user mobility.\tthe proposed algorithm\talgorithm\talgorithm\tfind\tn2\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2103.13494_1443639_2\tThe model finds that there is a positive, nonlinear relationship between the density of particulate matter in the air and COVID-19 transmission, which is in alignment with similar studies on other respiratory illnesses.\tThe model\tmodel\tmodel\tfind\tn2\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_110_36259_1\tHowever, many LLMs exhibit significant performance discrepancies between high- and low-resource languages.\tmany LLMs\tLLMs\tLLM\texhibit performance discrepancies\tn3\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2108.01174_1509989_0\tAI systems have seen significant adoption in various domains.\tAI systems\tAI systems\tsystem\tsee adoption\tn2\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_132_36620_1\tHowever, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset.\tLMs\tLMs\tLM\traise concerns\tn2\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_16_36793_3\tOn the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training.\tLLM-based methods\tLLM-based methods\tmethod\texhibit a limited capacity\tn3\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_14_30365_3\tThe model produces summaries covering meeting topics and next steps and performs comparably to a large language model at a fraction of the cost.\tThe model\tmodel\tmodel\tproduce,perform\tn2\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.02210_1820256_0\tLarge language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks.\tLarge language models (LLMs)\tLarge Language models (LLMs)\tmodel\tproduce\tn2\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.05077_1823123_1\tThat is because AI systems run on CPUs, GPUs, TPUs or other processors which have been designed and verified to adhere to computational dynamics that systematically preclude or suppress deviations.\tAI systems\tAI systems\tsystem\trun\tn1\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.10577_1810374_8\tThe hybrid learning algorithm can effectively find the solution for the formulated problem.\tThe hybrid learning algorithm\thybrid learning algorithm\talgorithm\tfind\tn2\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1602.00147_700426_2\tThis suggests that our model explains quite well the growth of airline networks.\tour model\tmodel\tmodel\texplain\tn2\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1304.1739_420872_4\tThe model takes into account the dominant paths having most of the power of the creeping wave from the transceiver in one ear to the transceiver in other ear and the effect of the protruding part of the outer ear called pinna.\tThe model\tmodel\tmodel\ttake into account\tn2\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_282_44833_0\tProgress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities.\tnew models\tmodels\tmodel\tclaim\tn2\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_54_3473_4\tThis scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response.\tthe system\tsystem\tsystem\tgenerate\tn1\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_327_3746_2\tThe model takes the paired source, reference, and hypothesis sentence all together as an input.\tthe model\tmodel\tmodel\ttake as input\tn2\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_0705.4412_8414_2\tMaterials and Methods: The system consists of a computer and a localizer allowing spatial localization of the position of the various instruments.\tThe system\tsystem\tsystem\tconsist of\tn1\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_409_5999_0\tThis paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks.\tmodel\tmodel\tmodel\tcarry\tn1\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_15_7080_7\tThis Machine Translation system accepts announcements in the form of English text as input and produces Indian Sign Language (ISL) synthetic animations as output.\tThis Machine Translation system\tMachine Translation system\tsystem\taccept\tn1\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_0709.0625_22890_4\tFor cases where an exact low-rank representation of the IBD matrix is available a-priori, the improved AI-REML algorithm normally runs almost twice as fast compared to the standard version.\tthe improved AI-REML algorithm\tAI-REML algorithm\talgorithm\trun\tn1\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2109.11129_1533950_3\tOur method introduces an additional meta-pretraining phase before cross-lingual pretraining, where the model learns generalization ability on a large-scale monolingual corpus.\tthe model\tmodel\tmodel\tlearn\tn3\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1809.03964_1023980_3\tOur proposed model considers historical air pollution records and historical meteorological data.\tOur proposed model\tmodel\tmodel\tconsider\tn2\n",
      "‚ö†Ô∏è Line 51: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2107.06031_1500043_3\tThis Machine Learning model also explains the relationship between the input factors and fuel consumption, quantifying the individual contribution of each one of them.\tThis Machine Learning model\tMachine Learning model\tmodel\texplain\tn2\n",
      "‚ö†Ô∏è Line 52: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_261_9142_1\tOur proposed model learns to extract textual features using a BiGRU-based deep neural network supported by a Hierarchical Attention architecture to focus on the most relevant areas in the text.\tOur proposed model\tmodel\tmodel\tlearn\tn3\n",
      "‚ö†Ô∏è Line 53: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_0807.4086_75079_10\tThe second is the choice between models of HIV dynamics, where one model makes the distinction between activated CD4+ T lymphocytes and the other does not.\tone model\tmodel\tmodel\tmake a distinction\tn3\n",
      "‚ö†Ô∏è Line 54: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_34_15815_4\tAs the training stages go on, we make the system learn to solve multiple tasks by adding extra information at different training stages gradually.\tthe system\tsystem\tsystem\tlearn\tn3\n",
      "‚ö†Ô∏è Line 55: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.09161_1827207_1\tIn this perspectives paper, we discuss possible ways for LLMs to support relevance judgments along with concerns and issues that arise.\tLLMs\tLLMs\tLLM\tsupport\tn3\n",
      "‚ö†Ô∏è Line 56: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_153_4884_5\tBy combining relation prediction and relevance ranking tasks with our target link prediction, the proposed model can learn more relational properties in KGs and properly perform even when lexical similarity occurs.\tthe proposed model\tmodel\tmodel\tlearn\tn3\n",
      "‚ö†Ô∏è Line 57: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_0802.0733_48030_1\tThe model that I employ assumes that the time that a passenger requires to load his or her luggage is the dominant contribution to the time needed to completely fill the aircraft.\tThe model that I employ\tmodel\tmodel\tassume\tn3\n",
      "‚ö†Ô∏è Line 58: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.08979_1827025_1\tUnlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users.\tChatGPT\tChatGPT\tChatGPT\tretrieve,generate\tn3\n",
      "‚ö†Ô∏è Line 59: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.11136_1767313_2\tHowever, AI systems may produce errors, can exhibit bias, may be sensitive to noise in the data, and often lack technical and judicial transparency resulting in reduction in trust and challenges in their adoption.\tAI systems\tsystems\tsystem\tproduce,exhibit bias,lack technical and judicial transparency\tn3\n",
      "‚ö†Ô∏è Line 60: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2012.11995_1399873_3\tOur results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks.\tthe model\tmodel\tmodel\tacquire ability\tn3\n",
      "‚ö†Ô∏è Line 61: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.16870_1816667_1\tChatGPT has learned language patterns and styles from a large dataset of internet texts, allowing it to provide answers that reflect common opinions, ideas, and language patterns found in the community.\tChatGPT\tChatGPT\tChatGPT\tlearn\tn3\n",
      "\n",
      "üîé Checking verb_subjects_positive.txt\n",
      "‚ö†Ô∏è Line 1: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_131_34995_1\tWe propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.\tLLMs\tLLMs\tLLM\tdetermine\tp3\n",
      "‚ö†Ô∏è Line 2: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_7_26333_1\tIn particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.\tthe model\tmodel\tmodel\tunderstand\tp2\n",
      "‚ö†Ô∏è Line 3: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1_13930_4\t‚Ä¢ Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?\tLMs\tLMs\tLM\trecognize\tp2\n",
      "‚ö†Ô∏è Line 4: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_335_27056_0\tThe increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.\tLMs\tLMs\tLM\tthink\tp2\n",
      "‚ö†Ô∏è Line 5: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_52_19823_2\tWe propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.\tLMs\tLMs\tLM\tinfer\tp2\n",
      "‚ö†Ô∏è Line 6: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_615_27336_1\tThese two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.\tan LM\tLM\tLM\tresolve a conflict\tp2\n",
      "‚ö†Ô∏è Line 7: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_12_42096_5\tWe then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.\tLLMs\tLLMs\tLLM\tcollaboratively learn from each other\tp3\n",
      "‚ö†Ô∏è Line 8: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_508_29544_3\tThrough extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.\tLLMs\tLLMs\tLLM\tcollaborate\tp2\n",
      "‚ö†Ô∏è Line 9: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_102_12912_7\tFinally, as it is hard to tell given a privacy parameter ùúñ what was the effect on the trained representation, we present experiments showing that the trained model does not memorize private information.\tthe trained model\ttrained model\tmodel\tmemorize\tp2\n",
      "‚ö†Ô∏è Line 10: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_263_41409_3\tThat is, the LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.\tthe LLM\tLLM\tLLM\tremember\tp2\n",
      "‚ö†Ô∏è Line 11: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_392_27113_5\tDuring the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it.\tthe LLM\tLLM\tLLM\trecall\tp2\n",
      "‚ö†Ô∏è Line 12: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_682_37442_3\tMotivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM.\tLLMs\tLLMs\tLLM\thave awareness\tp2\n",
      "‚ö†Ô∏è Line 13: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.10149_1828195_10\tAnd the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results.\tChatGPT\tChatGPT\tChatGPT\tunderstand\tp2\n",
      "‚ö†Ô∏è Line 14: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.13371_1769548_6\tIn our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.\tthe AI agent\tAI agent\tagent\tdecide to trust\tp2\n",
      "‚ö†Ô∏è Line 15: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2109.14723_1537544_1\tAs a result, it can be hard to identify what the model actually \"believes\" about the world, making it susceptible to inconsistent behavior and simple errors.\tthe model\tmodel\tmodel\tbelieve\tp2\n",
      "‚ö†Ô∏è Line 16: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2210.01478_1722833_1\tIn order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions.\tAI systems\tAI systems\tsystem\tunderstand,interpret,predict human moral judgments and decisions\tp2\n",
      "‚ö†Ô∏è Line 17: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.01681_1757858_6\tI survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.\tLMs\tLMs\tLM\tinfer\tp2\n",
      "‚ö†Ô∏è Line 18: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.17557_1817354_1\tWhat does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?\ta model\tmodel\tmodel\tremember\tp2\n",
      "‚ö†Ô∏è Line 19: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_1810.06338_1037525_0\tIn order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.\tan AI system\tAI system\tsystem\ttry to achieve\tp2\n",
      "‚ö†Ô∏è Line 20: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2210.09492_1730847_6\tHere, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items.\tGPT-3\tGPT-3\tGPT-3\treason\tp2\n",
      "‚ö†Ô∏è Line 21: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2302.04761_1789695_2\tIn this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.\tLMs\tLMs\tLM\tteach\tp2\n",
      "‚ö†Ô∏è Line 22: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.09561_1765738_6\tBy performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.\tLLM\tLLM\tLLM\tdeduce\tp2\n",
      "‚ö†Ô∏è Line 23: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_322_28458_1\tTo measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.\tan LLM\tLLM\tLLM\tprefer\tp2\n",
      "‚ö†Ô∏è Line 24: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_378_29414_7\tIn our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question.\tthe model\tmodel\tmodel\task\tp2\n",
      "‚ö†Ô∏è Line 25: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.05376_1823422_8\tSurprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.\tGPT-4\tGPT-4\tGPT-4\tdistinguish\tp3\n",
      "‚ö†Ô∏è Line 26: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.02911_1759088_4\tOur evaluation shows that the model can create French poetry successfully.\tthe model\tmodel\tmodel\tcreate French poetry\tp3\n",
      "‚ö†Ô∏è Line 27: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.18027_1817824_5\tFirst, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.\tLLMs\tLLMs\tLLM\tselect\tp2\n",
      "‚ö†Ô∏è Line 28: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.09048_1827094_1\tHowever, a large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.\ta large generative language model\tlarge generative language model\tmodel\tdemonstrate capability in understanding natural language\tp1\n",
      "‚ö†Ô∏è Line 29: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1_13930_5\t‚Ä¢ Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?\tLMs\tLMs\tLM\tknow\tp2\n",
      "‚ö†Ô∏è Line 30: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_299_27020_1\tHowever, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.\tLMs\tLMs\tLM\tcheat\tp2\n",
      "‚ö†Ô∏è Line 31: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_242_32404_6\tWe only conduct retrieval for the missing knowledge in questions that the LLM does not know.\tthe LLM\tLLM\tLLM\tknow\tp2\n",
      "‚ö†Ô∏è Line 32: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2012.11976_1399854_6\tInspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs.\ta conversational AI development platform\tconversational AI development platform\tplatform\texhibit maturity levels\tp3\n",
      "‚ö†Ô∏è Line 33: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_22_34383_5\tThen, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.\tthe more general LLM-based expert\tLLM-based expert\texpert\tanalyze,select\tp2\n",
      "‚ö†Ô∏è Line 34: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2107.04022_1498034_6\tTo understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons.\tAI agents\tAI agents\tagent\tbreak rules\tp2\n",
      "‚ö†Ô∏è Line 35: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_519_45829_8\tThese results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.\tLLMs\tLLMs\tLLM\tautonomously develop model-improvement techniques\tp1\n",
      "‚ö†Ô∏è Line 36: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_131_34995_2\tOur experiments show that without external feedback, many LLMs struggle to recognize their need for user support.\tmany LLMs\tLLMs\tLLM\tstruggle to recognize\tp2\n",
      "‚ö†Ô∏è Line 37: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_296_12174_4\tBy relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.\tthe model\tmodel\tmodel\tlearn,reason\tp3\n",
      "‚ö†Ô∏è Line 38: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_518_29554_3\tConsequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.\tLLMs\tLLMs\tLLM\tstruggle to recall\tp2\n",
      "‚ö†Ô∏è Line 39: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.08014_1807811_6\tIn addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence.\tChatGPT\tChatGPT\tChatGPT\tinterpret,draw an inference,overlook\tp2\n",
      "‚ö†Ô∏è Line 40: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.09461_1809258_3\tWe find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points.\tChatGPT\tChatGPT\tChatGPT\tpass an exam\tp2\n",
      "‚ö†Ô∏è Line 41: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_57_25373_2\tBy analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.\tAI\tAI\tAI\tdetermine,provide feedback and insights\tp3\n",
      "‚ö†Ô∏è Line 42: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_212_35072_9\tWhen an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.\tthe system\tsystem\tsystem\trecognize,determine\tp3\n",
      "‚ö†Ô∏è Line 43: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_150_26871_4\tFurther, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations.\tLLMs\tLLMs\tLLM\tdistinguish\tp2\n",
      "‚ö†Ô∏è Line 44: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_593_32755_6\tOur experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.\tLLMs\tLLMs\tLLM\tdemonstrate a notable capacity for logical counterfactual thinking\tp3\n",
      "‚ö†Ô∏è Line 45: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_590_35441_1\tHowever, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize.\tthe model\tmodel\tmodel\tmemorize\tp2\n",
      "‚ö†Ô∏è Line 46: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_7_43457_1\tWe introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.\tLLMs\tLLMs\tLLM\tinterpret,consider contextual cues,reflect\tp2\n",
      "‚ö†Ô∏è Line 47: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_350_43916_2\tFor instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in.\tLLMs\tLLMs\tLLM\texcel and struggle in specific cognitive areas\tp3\n",
      "‚ö†Ô∏è Line 48: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1809.00066_1020082_2\tOur language model proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness.\tOur language model\tlanguage model\tmodel\tidentify\tp3\n",
      "‚ö†Ô∏è Line 49: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1810.09030_1040217_7\tAI developers found that our system can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.\tour system\tsystem\tsystem\thelp discover unknown errors,engage in the process of proactive testing\tp3\n",
      "‚ö†Ô∏è Line 50: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_201_29237_3\tApart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions‚Äô rationales when attempting to correct students‚Äô answers.\tLLMs\tLLMs\tLLM\tmisinterpret the meaning,exhibit difficulty in understanding\tp2\n",
      "‚ö†Ô∏è Line 51: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_218_28354_4\tOur evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.\tLLMs\tLLMs\tLLM\tidentify\tp3\n",
      "‚ö†Ô∏è Line 52: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2304.04966_1823012_7\tResultantly, the developed model efficiently analyzed the test data with a mean average precision of 0.89.\tthe developed model\tmodel\tmodel\tanalyze\tp2\n",
      "‚ö†Ô∏è Line 53: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2010.09890_1366220_2\tTo succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).\tthe AI agent\tAI agent\tagent\tunderstand,coordinate\tp2\n",
      "‚ö†Ô∏è Line 54: found 7 columns (expected 6)\n",
      "   ‚Üí 1_acl_1477_40886_4\tThe experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations.\tChatGPT\tChatGPT\tChatGPT\tdemonstrate proficiency,struggle\tp3\n",
      "‚ö†Ô∏è Line 55: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2303.09387_1809184_9\tIn the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers.\tAI systems\tAI systems\tsystem\tlearn to manipulate\tp3\n",
      "‚ö†Ô∏è Line 56: found 7 columns (expected 6)\n",
      "   ‚Üí 1_arx_2212.01681_1757858_4\tWhen performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context.\tan LM\tLM\tLM\tinfer\tp2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "files = [\n",
    "    \"adjective_phrases_inconclusive\",\n",
    "    \"adjective_phrases_negative\",\n",
    "    \"adjective_phrases_positive\",\n",
    "    \"comparisons_inconclusive\",\n",
    "    \"noun_phrases_positive\",\n",
    "    \"possessives_positive\",\n",
    "    \"verb_objects_inconclusive\",\n",
    "    \"verb_objects_negative\",\n",
    "    \"verb_objects_positive\",\n",
    "    \"verb_subjects_inconclusive\",\n",
    "    \"verb_subjects_negative\",\n",
    "    \"verb_subjects_positive\"\n",
    "]\n",
    "\n",
    "base_path = \"../evaluation_sets/evaluation_sentences_txt\"\n",
    "expected_columns = 6  # Adjust this to match your expected column count\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(base_path, f\"{file}.txt\")\n",
    "    print(f\"\\nüîé Checking {file}.txt\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            fields = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(fields) != expected_columns:\n",
    "                print(f\"‚ö†Ô∏è Line {i}: found {len(fields)} columns (expected {expected_columns})\")\n",
    "                print(f\"   ‚Üí {line.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
