{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the annotated datasets\n",
    "\n",
    "This notebook provides code for validating the evaluation set .txt files and retrieving information about the anthropomorphic components.\n",
    "\n",
    "1. Make sure that no sentence was annotated with conflicting annotations\n",
    "2. Make sure that there are no duplicate sentences in a sentence\n",
    "3. Make sure that the .txt files used to create the evaluation sets are well-formed - i.e, the IDs contain the database prefix (used to locate them in the dataframe) and that each row contains exactly seven tab-separated values.\n",
    "4. Check that the annotations are correct - e.g. the positive set contains only ['p1','p2','p3'] scores, the negative set contains only ['n1','n2','n3'] scores, and the inconclusive set has only 'inc'.\n",
    "5. Retrieving the anthropomorphic components\n",
    "6. Retrieving the AI entity lemmas (i.e. without descriptors and modifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "        \n",
    "def get_sentences_dict(cat,score):   \n",
    "\n",
    "    sentences_dict = {}\n",
    "    duplicate_ids = []\n",
    "    duplicate_sentence_pairs = []\n",
    "    \n",
    "    sentences = open(f\"../data/evaluation_sentences_txt/{cat}_{score}.txt\",\"r\")\n",
    "    \n",
    "    for line in sentences.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "        sent_id = line[0]\n",
    "        sent_info = line[1:]\n",
    "\n",
    "        # wellformedness checks\n",
    "        if len(line) != 7:\n",
    "            print(f\"The row with the ID {sent_id} in {cat}_{score}.txt is not well-formed.\")\n",
    "        id_prefix = sent_id[:6]\n",
    "        if not re.match(r\"^[1-7]{1}_(arx|acl)_\", id_prefix):\n",
    "            print(f\"The ID {sent_id} in {cat}_{score} is not well-formed.\")\n",
    "        \n",
    "        if sent_id not in sentences_dict:\n",
    "            if sent_info not in sentences_dict.values():\n",
    "                sentences_dict[sent_id] = sent_info\n",
    "            else: # the sentence appears twice with different IDs \n",
    "                other_id = [key for key in sentences_dict if sentences_dict[key] == sent][0]\n",
    "                duplicate_sentence = (other_id,sent_id)\n",
    "                duplicate_sentence_pairs.append(duplicate_sentence) \n",
    "        else: # the sentence appears twice with the same ID\n",
    "            duplicate_ids.append(sent_id)\n",
    "\n",
    "    return sentences_dict,duplicate_ids,duplicate_sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(cat,score):\n",
    "\n",
    "    response = \"No duplicate utterances.\"\n",
    "    print(f\"Checking for duplicate entries in {cat}_{score}.txt...\")\n",
    "\n",
    "    sentences_dict = get_sentences_dict(cat,score)[0] # dict of ids and sentence info\n",
    "    duplicate_ids = get_sentences_dict(cat,score)[1] # list of duplicate sentences with identical ids\n",
    "    duplicate_sentence_pairs = get_sentences_dict(cat,score)[2] # list of duplicate sentences with different ids\n",
    "\n",
    "    if duplicate_ids:\n",
    "        response = f\"Resolve duplicates in the {score} set!!!\"\n",
    "        print(\"The sentences with the following ids appear twice: \",duplicate_ids,\n",
    "             f\" in {cat}_{score}.txt\")\n",
    "\n",
    "    if duplicate_sentence_pairs:\n",
    "        response = f\"Resolve duplicates in the {score} set!!!\"\n",
    "        print(\"The following ID pairs refer to the same sentence: \",duplicate_sentence_pairs,\n",
    "             f\" in {cat}_{score}.txt\")\n",
    "\n",
    "    return response\n",
    "\n",
    "def check_annotations(cat,score):\n",
    "\n",
    "    annotations_dict = {}\n",
    "\n",
    "    if cat == \"noun_phrases\" or cat == 'possessives':\n",
    "        annotations = ['p']\n",
    "    else:\n",
    "        if score == 'positive':\n",
    "            annotations = ['p1','p2','p3']\n",
    "        elif score == 'negative':\n",
    "            annotations = ['n1','n2','n3']\n",
    "        elif score == 'inconclusive':\n",
    "            annotations = ['inc']\n",
    "\n",
    "    print(f\"Checking annotations in {cat}_{score}.txt:\")\n",
    "\n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    all_sentences_info = sentences_dict.values()\n",
    "\n",
    "    for sent_id,sent_info in sentences_dict.items():\n",
    "        if sent_info[5] not in annotations:\n",
    "            print(f\"Fix incorrect annotation {sent_info[5]} in the sentence with the ID {sent_id}\")\n",
    "        if sent_info[5] not in annotations_dict:\n",
    "            annotations_dict[sent_info[5]] = 1\n",
    "        else:\n",
    "            annotations_dict[sent_info[5]] += 1\n",
    "            \n",
    "    num_sentences = sum([x for x in annotations_dict.values()])\n",
    "    print(f\"There are {num_sentences} sentences in {cat}_{score}.txt.\")\n",
    "    \n",
    "    return annotations_dict\n",
    "\n",
    "def pairwise_conflict_check(cat,score1,score2):\n",
    "\n",
    "    print(f\"Comparing {score1} cases and {score2} cases for the {cat} set...\")\n",
    "\n",
    "    conflicting_annotation = False\n",
    "\n",
    "    dict1 = get_sentences_dict(cat,score1)[0]\n",
    "    dict2 = get_sentences_dict(cat,score2)[0]\n",
    "\n",
    "    for id1,sent in dict1.items():\n",
    "        if id1 in dict2:\n",
    "            conflicting_annotation = True\n",
    "            print(f\"The {score1} sentence with the ID \",id1,f\" appears in the {score2} set with the same ID\")\n",
    "        elif sent in dict2.values():\n",
    "            conflicting_annotation = True\n",
    "            id2 = [key for keys in dict2.keys() if dict2[key] == sent][0]\n",
    "            print(f\"The {score1} sentence with the ID  \",id1,\n",
    "                  f\" appears in the {score2} set with the ID \",id2)\n",
    "\n",
    "    return conflicting_annotation\n",
    "\n",
    "def check_conflicting_annotations(cat,case,other_cases):\n",
    "\n",
    "    response = \"No conflicting annotations.\"\n",
    "\n",
    "    for other_case in other_cases:\n",
    "\n",
    "        conflicting_annotations = pairwise_conflict_check(cat,case,other_case)\n",
    "        if conflicting_annotations:\n",
    "            response = \"Resolve conflicts before proceeding.\"\n",
    "            print(f\"Conflicting annotations in the {case} and {other_case} sets!!!\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def get_ai_phrases(cat,score,idx):\n",
    "\n",
    "    ai_components = {}\n",
    "    \n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    all_sentences_info = sentences_dict.values()\n",
    "    \n",
    "    for sent_info in all_sentences_info:\n",
    "        if sent_info[idx] not in ai_components:\n",
    "            ai_components[sent_info[idx]] = 1\n",
    "        else:\n",
    "            ai_components[sent_info[idx]] += 1\n",
    "\n",
    "    return ai_components\n",
    "\n",
    "def get_entities_or_components(cat,score,idx):\n",
    "\n",
    "    components = {}\n",
    "    \n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    all_sentences_info = sentences_dict.values()\n",
    "    \n",
    "    for sent_info in all_sentences_info:\n",
    "        elements = sent_info[idx].split(\",\")\n",
    "        for element in elements:\n",
    "            if element not in components:\n",
    "                components[element] = 1\n",
    "            else:\n",
    "                components[element] += 1\n",
    "\n",
    "    return components\n",
    "\n",
    "def get_and_display_anthro_words(cat,case,pos):\n",
    "\n",
    "    if pos == 'verbs':\n",
    "        pos_tag = 'VERB'\n",
    "    elif pos == 'adjectives':\n",
    "        pos_tag = 'ADJ'\n",
    "    else:\n",
    "        print(\"choose a different POS you pos!\")\n",
    "\t\n",
    "    anthro_words = []\n",
    "    \n",
    "    anthro_components = get_entities_or_components(cat,case,4)\n",
    "    # get only the words that match the desired POS:\n",
    "    for anthro_component in anthro_components:\n",
    "        if len(anthro_component.split()) > 1:\n",
    "            doc = nlp(anthro_component)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "        else:\n",
    "            words = anthro_component.split()\n",
    "            anthro_words.extend(words)\n",
    "    anthro_words = set(anthro_words)\n",
    "\n",
    "    print(f\"all {case} {pos} in {category_is}:\")\n",
    "    print(sorted(list(anthro_words)))\n",
    "    print()\n",
    "\n",
    "    return anthro_words\n",
    "\n",
    "def get_phrase_mask_entity_triplets(cat,score):\n",
    "\n",
    "    phrase_mask_entity_triplets = []\n",
    "    \n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    \n",
    "    for sent_id,sent_info in sentences_dict.items():\n",
    "        phrase_mask_entity_triplets.append((sent_info[1],sent_info[2],sent_info[3]))\n",
    "\n",
    "    return phrase_mask_entity_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check sentences for each category\n",
    "\n",
    "Perform checks on the set:\n",
    "1. check that the file contains no duplicate sentences\n",
    "2. check that the same sentence does not appear twice in two sets of the same category - for sets with more than one label\n",
    "3. check that the annotations in a given file are correct\n",
    "\n",
    "The categories are:\n",
    "1. verb_subjects - sentences in which the AI entity is the subject of an anthropomorphic verb (nsubj)\n",
    "2. verb_objects - sentences in which the AI entity is object of an anthropomorphic verb (pobj,dobj)\n",
    "4. adjective_phrases - sentences in which the AI entity is part of an anthropomorphic adjectival phrase\n",
    "5. noun_phrases - sentences in which the AI entity is part of an anthropomorphic noun phrase\n",
    "6. possessives - sentences in which the AI entity is immediately followed by a possessive marker\n",
    "7. comparisons - sentences in which the AI entity is being compared to humans explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicate entries in verb_subjects_positive.txt...\n",
      "No duplicate utterances. \n",
      "\n",
      "Comparing positive cases and negative cases for the verb_subjects set...\n",
      "Comparing positive cases and inconclusive cases for the verb_subjects set...\n",
      "No conflicting annotations. \n",
      "\n",
      "Checking annotations in verb_subjects_positive.txt:\n",
      "There are 56 sentences in verb_subjects_positive.txt.\n",
      "{'p3': 16, 'p2': 38, 'p1': 2} \n",
      "\n",
      "Checking for duplicate entries in verb_subjects_negative.txt...\n",
      "No duplicate utterances. \n",
      "\n",
      "Comparing negative cases and positive cases for the verb_subjects set...\n",
      "Comparing negative cases and inconclusive cases for the verb_subjects set...\n",
      "No conflicting annotations. \n",
      "\n",
      "Checking annotations in verb_subjects_negative.txt:\n",
      "There are 61 sentences in verb_subjects_negative.txt.\n",
      "{'n2': 28, 'n1': 18, 'n3': 15} \n",
      "\n",
      "Checking for duplicate entries in verb_subjects_inconclusive.txt...\n",
      "No duplicate utterances. \n",
      "\n",
      "Comparing inconclusive cases and positive cases for the verb_subjects set...\n",
      "Comparing inconclusive cases and negative cases for the verb_subjects set...\n",
      "No conflicting annotations. \n",
      "\n",
      "Checking annotations in verb_subjects_inconclusive.txt:\n",
      "There are 33 sentences in verb_subjects_inconclusive.txt.\n",
      "{'inc': 33} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cases = [\"positive\",\"negative\",\"inconclusive\"]\n",
    "#cases = [\"inconclusive\"]\n",
    "category_is = \"verb_subjects\" # bring it to the runway\n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    # check that the file contains no duplicate sentences\n",
    "    check1 = check_duplicates(category_is,case)\n",
    "    print(check1,'\\n')\n",
    "    \n",
    "    # check that the same sentence does not appear twice in two sets of the same category\n",
    "    # not applicable for noun_phrases, possessives (always positive) and comparisons (always inconclusive)\n",
    "    other_cases = [other_case for other_case in cases if other_case != case]\n",
    "    if other_cases:\n",
    "        check2 = check_conflicting_annotations(category_is,case,other_cases)\n",
    "        print(check2, '\\n')\n",
    "\n",
    "    # check that the annotations in a given file are correct (i.e. no negative annotations in the positive set)\n",
    "    check3 = check_annotations(category_is,case)\n",
    "    print(check3,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve information for manual review\n",
    "\n",
    "1. retrieve the complete AI phrases and their count\n",
    "2. retrieve the masks and their count\n",
    "3. retrieve all of the AI entities and their count\n",
    "4. retrieve all of the potentially (non-)anthropomorphic components and their count\n",
    "5. retrieve the AI phrase, mask, entity triplets and their unique ID for manual review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handling inconclusive comparisons cases\n",
      "1 ('the best bidirectional model', 'bidirectional model', 'model')\n",
      "2 ('a model', 'model', 'model')\n",
      "3 ('GPT-3', 'GPT-3', 'GPT-3')\n",
      "4 ('LLMs', 'LLMs', 'LLM')\n",
      "5 ('the largest models we tested', 'models', 'model')\n",
      "6 ('The large language model called ChatGPT', 'large language model', 'model')\n",
      "7 ('any autonomous system', 'system', 'system')\n",
      "8 ('the model', 'model', 'model')\n",
      "9 ('the model', 'model', 'model')\n",
      "10 ('a human-like reasoning architecture', 'architecture', 'architecture')\n",
      "11 ('LLMs', 'LLMs', 'LLM')\n",
      "12 ('ChatGPT', 'ChatGPT', 'ChatGPT')\n",
      "13 ('larger and more advanced LLMs', 'LLMs', 'LLM')\n",
      "14 ('ChatGPT', 'ChatGPT', 'ChatGPT')\n",
      "15 ('a human-like question answering system', 'question answering system', 'system')\n",
      "16 ('Robotics and AI', 'Robotics and AI', 'AI')\n",
      "17 ('AI', 'AI', 'AI')\n",
      "18 ('AI', 'AI', 'AI')\n",
      "19 ('powerful AI systems', 'AI systems', 'system')\n",
      "20 ('LLMs', 'LLMs', 'LLM')\n",
      "21 ('LLMs', 'LLMs', 'LLM')\n",
      "22 ('state-of-the-art LLMs', 'LLMs', 'LLM')\n",
      "23 ('LLMs', 'LLMs', 'LLM')\n",
      "24 ('large LMs', 'large LMs', 'LM')\n",
      "25 ('LLMs', 'LLMs', 'LLM')\n",
      "26 ('GPT-4', 'GPT-4', 'GPT-4')\n",
      "27 ('GPT-3', 'GPT-3', 'GPT-3')\n",
      "28 ('LLMs', 'LLMs', 'LLM')\n",
      "29 ('LLMs', 'LLMs', 'LLM')\n",
      "30 ('LMs', 'LMs', 'LM')\n",
      "31 ('machines', 'machines', 'machine')\n",
      "32 ('LLMs', 'LLMs', 'LLM')\n",
      "33 ('AI systems', 'AI systems', 'system')\n",
      "34 ('ChatGPT', 'ChatGPT', 'ChatGPT')\n",
      "35 ('Our model', 'model', 'model')\n",
      "36 ('ChatGPT', 'ChatGPT', 'ChatGPT')\n",
      "37 ('LLMs', 'LLMs', 'LLM')\n",
      "38 ('the model', 'model', 'model')\n",
      "39 ('LLMs like GPT-3', 'LLMs', 'LLM')\n",
      "40 ('Conversational AI systems', 'Conversational AI systems', 'system')\n",
      "41 ('LLMs', 'LLMs', 'LLM')\n",
      "42 ('BERT, RoBERTa, GPT-2 and GPT-3', 'BERT, RoBERTa, GPT-2 and GPT-3', 'BERT,RoBERTa,GPT-2,GPT-3')\n",
      "43 ('LLMs', 'LLMs', 'LLM')\n",
      "44 ('GPT', 'GPT', 'GPT')\n",
      "45 ('such models', 'models', 'model')\n",
      "46 ('ChatGPT, Llama2-Chat, PaLM-2 and GPT-4', 'ChatGPT, Llama2-Chat, PaLM-2 and GPT-4', 'ChatGPT,GPT,Llama')\n",
      "47 ('advanced LLMs', 'LLMs', 'LLM')\n",
      "48 ('Large Language models (LLMs)', 'Large Language models (LLMs)', 'model')\n",
      "49 ('LLMs', 'LLMs', 'LLM')\n",
      "50 ('Pre-trained Large Language Models', 'Pre-trained Large Language Models', 'model')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cases = [\"positive\",\"negative\",\"inconclusive\"]\n",
    "cases = [\"inconclusive\"]\n",
    "category_is = \"comparisons\" # bring it to the runway\n",
    "\n",
    "check_AI_phrases = False\n",
    "check_masks = False\n",
    "check_entities = False\n",
    "check_anthro_components = False\n",
    "check_AI_triplets = True\n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    print(f\"handling {case} {category_is} cases\")\n",
    "\n",
    "    # retrieve the complete AI phrases and their count\n",
    "    if check_AI_phrases == True:\n",
    "        components = get_ai_phrases(category_is,case,1)\n",
    "        sorted_list = sorted([(key,value) for key,value in components.items()])\n",
    "        for item in sorted_list:\n",
    "            print(item[1],\": \",item[0])\n",
    "\n",
    "    # retrieve the masks and their count\n",
    "    if check_masks == True:\n",
    "        components = get_ai_phrases(category_is,case,2)\n",
    "        sorted_list = sorted([(key,value) for key,value in components.items()])\n",
    "        for item in sorted_list:\n",
    "            print(item[1],\": \",item[0])\n",
    "    \n",
    "    # retrieve all of the AI entities and their count\n",
    "    if check_entities == True:\n",
    "        anthro_components = get_entities_or_components(category_is,case,3)\n",
    "        sorted_anthro_list = sorted([(key,value) for key,value in anthro_components.items()])\n",
    "        for item in sorted_anthro_list:\n",
    "            print(item[1],\": \",item[0])\n",
    "\n",
    "    # retrieve all of the potentially (non-)anthropomorphic components and their count\n",
    "    if check_anthro_components == True:\n",
    "        anthro_components = get_entities_or_components(category_is,case,4)\n",
    "        sorted_anthro_list = sorted([(key,value) for key,value in anthro_components.items()])\n",
    "        for item in sorted_anthro_list:\n",
    "            print(item[1],\": \",item[0])\n",
    "\n",
    "    # retrieve the AI phrase, mask, entity triplets and their unique ID for manual review\n",
    "    if check_AI_triplets == True:\n",
    "        phrase_mask_entity_triplets = get_phrase_mask_entity_triplets(category_is,case)\n",
    "        for i,item in enumerate(phrase_mask_entity_triplets):\n",
    "            print(i+1,item)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display verbs and adjectives for positive, negative, and inconclusive cases as well as intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all positive verbs in verb_subjects:\n",
      "['analyze', 'ask', 'believe', 'cheat', 'collaborate', 'coordinate', 'deduce', 'determine', 'distinguish', 'find', 'identify', 'infer', 'interpret', 'know', 'learn', 'memorize', 'overlook', 'prefer', 'reason', 'recall', 'recognize', 'reflect', 'remember', 'select', 'struggle', 'teach', 'think', 'understand']\n",
      "\n",
      "all negative verbs in verb_subjects:\n",
      "['accept', 'act', 'assume', 'carry', 'claim', 'consider', 'cover', 'demonstrate', 'establish', 'evolve', 'explain', 'find', 'generate', 'involve', 'learn', 'perform', 'produce', 'refrigerate', 'represent', 'require', 'retrieve', 'run', 'see', 'support']\n",
      "\n",
      "all inconclusive verbs in verb_subjects:\n",
      "['acquire', 'act', 'consider', 'construct', 'control', 'create', 'demonstrate', 'differentiate', 'dissect', 'fail', 'guess', 'identify', 'infer', 'interpret', 'learn', 'see', 'simulate', 'streamline', 'translate']\n",
      "\n",
      "verbs that are both positive and negative:\n",
      "find\n",
      "\n",
      "verbs that are both positive and inconclusive:\n",
      "identify\n",
      "infer\n",
      "interpret\n",
      "\n",
      "verbs that are both negative and inconclusive:\n",
      "act\n",
      "consider\n",
      "demonstrate\n",
      "see\n",
      "\n",
      "verbs that can be positive, negative or inconclusive:\n",
      "learn\n"
     ]
    }
   ],
   "source": [
    "positive_verbs = get_and_display_anthro_words(\"verb_subjects\",\"positive\",\"verbs\")\n",
    "negative_verbs = get_and_display_anthro_words(\"verb_subjects\",\"negative\",\"verbs\")\n",
    "inconclusive_verbs = get_and_display_anthro_words(\"verb_subjects\",\"inconclusive\",\"verbs\")\n",
    "\n",
    "all_verbs_intersect = sorted(list((positive_verbs & negative_verbs & inconclusive_verbs)))\n",
    "\n",
    "just_pos_neg_v_intersect = sorted([x for x in list((positive_verbs & negative_verbs)) if x not in all_verbs_intersect])\n",
    "just_pos_inc_v_intersect = sorted([x for x in list((positive_verbs & inconclusive_verbs)) if x not in all_verbs_intersect])\n",
    "just_neg_inc_v_intersect = sorted([x for x in list((negative_verbs & inconclusive_verbs)) if x not in all_verbs_intersect])\n",
    "\n",
    "print(\"verbs that are both positive and negative:\")\n",
    "for item in just_pos_neg_v_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "print(\"verbs that are both positive and inconclusive:\")\n",
    "for item in just_pos_inc_v_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "print(\"verbs that are both negative and inconclusive:\")\n",
    "for item in just_neg_inc_v_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "print(\"verbs that can be positive, negative or inconclusive:\")\n",
    "for item in all_verbs_intersect:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all positive adjectives in verb_subjects:\n",
      "['attentive', 'autonomous', 'aware', 'blind', 'confident', 'confused', 'conscious', 'creative', 'deceptive', 'eager', 'forgetful', 'intelligent', 'malicious', 'manipulative', 'responsive', 'self-aware', 'sensitive', 'smart', 'thoughtful', 'value-aligned', 'vulnerable']\n",
      "\n",
      "all negative adjectives in verb_subjects:\n",
      "['blind', 'character-aware', 'communicative', 'concept-aware', 'context-aware', 'conversational', 'creative', 'dangerous', 'deep-learning', 'ethical', 'few-shot', 'image-blind', 'intelligent', 'materials-aware', 'multilingual', 'multimodal', 'original', 'responsible', 'risk-sensitive', 'robust', 'sensitive', 'smart', 'time-aware', 'time-sensitive', 'transparent', 'trustworthy', 'unreliable', 'unsafe', 'untrustworthy', 'user-friendly', 'vulnerable']\n",
      "\n",
      "all inconclusive adjectives in verb_subjects:\n",
      "['attentive', 'autonomous', 'collaborative', 'communicative', 'competitive', 'creative', 'dishonest', 'intelligent', 'responsible', 'sensitive', 'thoughtful', 'untrustworthy', 'vulnerable']\n",
      "\n",
      "adjectives that are both positive and negative:\n",
      "blind\n",
      "smart\n",
      "\n",
      "adjectives that are both positive and inconclusive:\n",
      "attentive\n",
      "autonomous\n",
      "creative\n",
      "intelligent\n",
      "sensitive\n",
      "thoughtful\n",
      "vulnerable\n",
      "\n",
      "adjectives that are both negative and inconclusive:\n",
      "communicative\n",
      "creative\n",
      "intelligent\n",
      "responsible\n",
      "sensitive\n",
      "untrustworthy\n",
      "vulnerable\n",
      "\n",
      "adjectives that can be positive, negative or inconclusive:\n",
      "creative\n",
      "intelligent\n",
      "sensitive\n",
      "vulnerable\n"
     ]
    }
   ],
   "source": [
    "positive_adjectives = get_and_display_anthro_words(\"adjective_phrases\",\"positive\",\"adjectives\")\n",
    "negative_adjectives = get_and_display_anthro_words(\"adjective_phrases\",\"negative\",\"adjectives\")\n",
    "inconclusive_adjectives = get_and_display_anthro_words(\"adjective_phrases\",\"inconclusive\",\"adjectives\")\n",
    "\n",
    "all_adjectives_intersect = sorted(list((positive_adjectives & negative_adjectives & inconclusive_adjectives)))\n",
    "\n",
    "just_pos_neg_a_intersect = sorted([x for x in list((positive_adjectives & negative_adjectives)) if x not in all_adjectives_intersect])\n",
    "just_pos_inc_a_intersect = sorted([x for x in list((positive_adjectives & inconclusive_adjectives)) if x not in all_adj_intersect])\n",
    "just_neg_inc_a_intersect = sorted([x for x in list((negative_adjectives & inconclusive_adjectives)) if x not in all_adj_intersect])\n",
    "\n",
    "print(\"adjectives that are both positive and negative:\")\n",
    "for item in just_pos_neg_a_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "print(\"adjectives that are both positive and inconclusive:\")\n",
    "for item in just_pos_inc_a_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "print(\"adjectives that are both negative and inconclusive:\")\n",
    "for item in just_neg_inc_a_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "print(\"adjectives that can be positive, negative or inconclusive:\")\n",
    "for item in all_adjectives_intersect:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
