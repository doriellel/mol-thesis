{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the annotated datasets\n",
    "\n",
    "This notebook provides code for validating the evaluation set .txt files and retrieving information about the anthropomorphic components.\n",
    "\n",
    "1. Make sure that no sentence was annotated with conflicting annotations\n",
    "2. Make sure that there are no duplicate sentences in a sentence\n",
    "3. Make sure that the .txt files used to create the evaluation sets are well-formed - i.e, the IDs contain the database prefix (used to locate them in the dataframe) and that each row contains exactly seven tab-separated values.\n",
    "4. Check that the annotations are correct - e.g. the positive set contains only ['p1','p2','p3'] scores, the negative set contains only ['n1','n2','n3'] scores, and the inconclusive set has only 'inc'.\n",
    "5. Retrieving the anthropomorphic components\n",
    "6. Retrieving the AI entity lemmas (i.e. without descriptors and modifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "        \n",
    "def get_sentences_dict(cat,score):   \n",
    "\n",
    "    sentences_dict = {}\n",
    "    duplicate_ids = []\n",
    "    duplicate_sentence_pairs = []\n",
    "    \n",
    "    sentences = open(f\"../preprocessed_data/evaluation_sentences/{cat}_{score}.txt\",\"r\")\n",
    "    \n",
    "    for line in sentences.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "        sent_id = line[0]\n",
    "        sent_info = line[1:]\n",
    "\n",
    "        # wellformedness checks\n",
    "        if len(line) != 7:\n",
    "            print(f\"The row with the ID {sent_id} in {cat}_{score}.txt is not well-formed.\")\n",
    "        id_prefix = sent_id[:6]\n",
    "        if not re.match(r\"^[1-7]{1}_(arx|acl)_\", id_prefix):\n",
    "            print(f\"The ID {sent_id} in {cat}_{score} is not well-formed.\")\n",
    "        \n",
    "        if sent_id not in sentences_dict:\n",
    "            if sent_info not in sentences_dict.values():\n",
    "                sentences_dict[sent_id] = sent_info\n",
    "            else: # the sentence appears twice with different IDs \n",
    "                other_id = [key for key in sentences_dict if sentences_dict[key] == sent][0]\n",
    "                duplicate_sentence = (other_id,sent_id)\n",
    "                duplicate_sentence_pairs.append(duplicate_sentence) \n",
    "        else: # the sentence appears twice with the same ID\n",
    "            duplicate_ids.append(sent_id)\n",
    "\n",
    "    return sentences_dict,duplicate_ids,duplicate_sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_num_and_duplicates(cat,score,num):\n",
    "\n",
    "    response = \"No duplicate utterances.\"\n",
    "    print(f\"Checking for duplicate entries in {cat}_{score}.txt...\")\n",
    "\n",
    "    sentences_dict = get_sentences_dict(cat,score)[0] # dict of ids and sentence info\n",
    "    duplicate_ids = get_sentences_dict(cat,score)[1] # list of duplicate sentences with identical ids\n",
    "    duplicate_sentence_pairs = get_sentences_dict(cat,score)[2] # list of duplicate sentences with different ids\n",
    "    \n",
    "    if len(sentences_dict.keys()) > num:\n",
    "        print(f\"There are more than {num} sentences in {cat}_{score}.txt.\")\n",
    "    elif len(sentences_dict.keys()) < num:\n",
    "        print(f\"There are less than {num} sentences in {cat}_{score}.txt.\")\n",
    "\n",
    "    if duplicate_ids:\n",
    "        response = f\"Resolve duplicates in the {score} set!!!\"\n",
    "        print(\"The sentences with the following ids appear twice: \",duplicate_ids,\n",
    "             f\" in {cat}_{score}.txt\")\n",
    "\n",
    "    if duplicate_sentence_pairs:\n",
    "        response = f\"Resolve duplicates in the {score} set!!!\"\n",
    "        print(\"The following ID pairs refer to the same sentence: \",duplicate_sentence_pairs,\n",
    "             f\" in {cat}_{score}.txt\")\n",
    "\n",
    "    return response\n",
    "\n",
    "def check_annotations(cat,score):\n",
    "\n",
    "    annotations_dict = {}\n",
    "\n",
    "    if cat == \"noun_phrases\" or cat == 'possessives':\n",
    "        annotations = ['p']\n",
    "    else:\n",
    "        if score == 'positive':\n",
    "            annotations = ['p1','p2','p3']\n",
    "        elif score == 'negative':\n",
    "            annotations = ['n1','n2','n3']\n",
    "        elif score == 'inconclusive':\n",
    "            annotations = ['inc']\n",
    "\n",
    "    print(f\"Checking annotations in {cat}_{score}.txt:\")\n",
    "\n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    all_sentences_info = sentences_dict.values()\n",
    "\n",
    "    for sent_id,sent_info in sentences_dict.items():\n",
    "        if sent_info[5] not in annotations:\n",
    "            print(f\"Fix incorrect annotation {sent_info[5]} in the sentence with the ID {sent_id}\")\n",
    "        if sent_info[5] not in annotations_dict:\n",
    "            annotations_dict[sent_info[5]] = 1\n",
    "        else:\n",
    "            annotations_dict[sent_info[5]] += 1\n",
    "\n",
    "    return annotations_dict\n",
    "\n",
    "def pairwise_conflict_check(cat,score1,score2):\n",
    "\n",
    "    print(f\"Comparing {score1} cases and {score2} cases for the {cat} set...\")\n",
    "\n",
    "    conflicting_annotation = False\n",
    "\n",
    "    dict1 = get_sentences_dict(cat,score1)[0]\n",
    "    dict2 = get_sentences_dict(cat,score2)[0]\n",
    "\n",
    "    for id1,sent in dict1.items():\n",
    "        if id1 in dict2:\n",
    "            conflicting_annotation = True\n",
    "            print(f\"The {score1} sentence with the ID \",id1,f\" appears in the {score2} set with the same ID\")\n",
    "        elif sent in dict2.values():\n",
    "            conflicting_annotation = True\n",
    "            id2 = [key for keys in dict2.keys() if dict2[key] == sent][0]\n",
    "            print(f\"The {score1} sentence with the ID  \",id1,\n",
    "                  f\" appears in the {score2} set with the ID \",id2)\n",
    "\n",
    "    return conflicting_annotation\n",
    "\n",
    "def check_conflicting_annotations(cat,case,other_cases):\n",
    "\n",
    "    response = \"No conflicting annotations.\"\n",
    "\n",
    "    for other_case in other_cases:\n",
    "\n",
    "        conflicting_annotations = pairwise_conflict_check(cat,case,other_case)\n",
    "        if conflicting_annotations:\n",
    "            response = \"Resolve conflicts before proceeding.\"\n",
    "            print(f\"Conflicting annotations in the {case} and {other_case} sets!!!\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_phrases(cat,score,idx):\n",
    "\n",
    "    ai_components = {}\n",
    "    \n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    all_sentences_info = sentences_dict.values()\n",
    "    \n",
    "    for sent_info in all_sentences_info:\n",
    "        if sent_info[idx] not in ai_components:\n",
    "            ai_components[sent_info[idx]] = 1\n",
    "        else:\n",
    "            ai_components[sent_info[idx]] += 1\n",
    "\n",
    "    return ai_components\n",
    "\n",
    "def get_entities_or_components(cat,score,idx):\n",
    "\n",
    "    components = {}\n",
    "    \n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    all_sentences_info = sentences_dict.values()\n",
    "    \n",
    "    for sent_info in all_sentences_info:\n",
    "        elements = sent_info[idx].split(\",\")\n",
    "        for element in elements:\n",
    "            if element not in components:\n",
    "                components[element] = 1\n",
    "            else:\n",
    "                components[element] += 1\n",
    "\n",
    "    return components\n",
    "\n",
    "def get_phrase_mask_entity_triplets(cat,score):\n",
    "\n",
    "    phrase_mask_entity_triplets = []\n",
    "    \n",
    "    sentences_dict = get_sentences_dict(cat,score)[0]\n",
    "    \n",
    "    for sent_id,sent_info in sentences_dict.items():\n",
    "        phrase_mask_entity_triplets.append((sent_info[1],sent_info[2],sent_info[3]))\n",
    "\n",
    "    return phrase_mask_entity_triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check sentences for each category\n",
    "\n",
    "The categories are:\n",
    "1. verb_subjects - sentences in which the AI entity is the subject of an anthropomorphic verb (nsubj)\n",
    "2. verb_objects - sentences in which the AI entity is object of an anthropomorphic verb (pobj,dobj)\n",
    "4. adjective_phrases - sentences in which the AI entity is part of an anthropomorphic adjectival phrase\n",
    "5. noun_phrases - sentences in which the AI entity is part of an anthropomorphic noun phrase\n",
    "6. possessives - sentences in which the AI entity is immediately followed by a possessive marker\n",
    "7. comparisons - sentences in which the AI entity is being compared to humans explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicate entries in noun_phrases_positive.txt...\n",
      "There are more than 50 sentences in noun_phrases_positive.txt.\n",
      "No duplicate utterances. \n",
      "\n",
      "Checking annotations in noun_phrases_positive.txt:\n",
      "{'p': 70} \n",
      "\n",
      "1 :  CEO\n",
      "1 :  advisor\n",
      "16 :  assistant\n",
      "1 :  assistant for psychotherapy\n",
      "1 :  assistant for radiologists\n",
      "1 :  clinician\n",
      "1 :  co-creative partner\n",
      "1 :  coach\n",
      "2 :  coding assistant\n",
      "1 :  collaborative tutor\n",
      "1 :  collaborator\n",
      "7 :  companion\n",
      "1 :  content generator\n",
      "1 :  creative partner\n",
      "1 :  decision-maker\n",
      "1 :  developer\n",
      "1 :  evaluator\n",
      "1 :  group coach\n",
      "1 :  instructor\n",
      "1 :  intelligent collaborator\n",
      "1 :  intelligent research assistant\n",
      "1 :  junior colleague\n",
      "1 :  legal assistant\n",
      "2 :  manager\n",
      "1 :  medical assistant\n",
      "1 :  operational assistant\n",
      "4 :  partner\n",
      "1 :  personal trainer\n",
      "1 :  planner\n",
      "1 :  private tutor\n",
      "1 :  product manager\n",
      "1 :  responsive partner\n",
      "1 :  scientific discovery assistant\n",
      "1 :  student\n",
      "1 :  supporter\n",
      "6 :  teacher\n",
      "1 :  teacher coach\n",
      "1 :  therapist\n",
      "6 :  tutor\n"
     ]
    }
   ],
   "source": [
    "#cases_and_nums = {\"positive\":50,\"negative\":50,\"inconclusive\":20}\n",
    "cases_and_nums = {\"positive\":50}\n",
    "category_is = \"noun_phrases\" # bring it to the runway\n",
    "\n",
    "check_AI_phrases_and_masks = False\n",
    "check_entities_or_anthro = True\n",
    "check_AI_triplets = False\n",
    "\n",
    "for case in cases_and_nums:\n",
    "\n",
    "    # check that the number of utterance matches the expecation, and that the file contains no duplicate sentences\n",
    "    check1 = check_num_and_duplicates(category_is,case,cases_and_nums[case])\n",
    "    print(check1,'\\n')\n",
    "    \n",
    "    # check that the same sentence does not appear twice in two sets of the same category\n",
    "    # not applicable for noun_phrases, possessives (always positive) and comparisons (always inconclusive)\n",
    "    other_cases = [other_case for other_case in cases_and_nums if other_case != case]\n",
    "    if other_cases:\n",
    "        check2 = check_conflicting_annotations(category_is,case,other_cases)\n",
    "        print(check2, '\\n')\n",
    "\n",
    "    # check that the annotations in a given file are correct (i.e. no negative annotations in the positive set)\n",
    "    check3 = check_annotations(category_is,case)\n",
    "    print(check3,'\\n')\n",
    "\n",
    "    # retrieve the AI phrases and their count\n",
    "    # 1: the full AI phrase\n",
    "    # 2: the masked component\n",
    "    if check_AI_phrases_and_masks == True:\n",
    "        idx = 2 # options are 1 and 2\n",
    "        components = get_ai_phrases(category_is,case,idx)\n",
    "        sorted_list = sorted([(key,value) for key,value in components.items()])\n",
    "        for item in sorted_list:\n",
    "            print(item[1],\": \",item[0])\n",
    "    \n",
    "    # retrieve all of the potentially (non-)anthropomorphic components / AI entities and their count\n",
    "    # 3: AI entities\n",
    "    # 4: anthropomorphic components\n",
    "    if check_entities_or_anthro == True:\n",
    "        idx = 4 # options are 3 and 4\n",
    "        anthro_components = get_entities_or_components(category_is,case,idx)\n",
    "        sorted_anthro_list = sorted([(key,value) for key,value in anthro_components.items()])\n",
    "        for item in sorted_anthro_list:\n",
    "            print(item[1],\": \",item[0])\n",
    "\n",
    "    # retrieve the AI phrase,mask,entity triplets and their unique ID\n",
    "    if check_AI_triplets == True:\n",
    "        phrase_mask_entity_triplets = get_phrase_mask_entity_triplets(category_is,case)\n",
    "        for item in phrase_mask_entity_triplets:\n",
    "            print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE VERBS ONLY ['French', 'analyze', 'ask', 'believe', 'cheat', 'cognitive', 'collaborate', 'contextual', 'coordinate', 'counterfactual', 'deduce', 'determine', 'distinguish', 'human', 'identify', 'infer', 'interpret', 'know', 'learn', 'logical', 'memorize', 'model', 'moral', 'natural', 'notable', 'other', 'overlook', 'prefer', 'proactive', 'reason', 'recall', 'recognize', 'reflect', 'remember', 'select', 'specific', 'struggle', 'teach', 'think', 'understand', 'unknown']\n",
      "\n",
      "NEGATIVE VERBS ONLY ['accept', 'acquire', 'act', 'assume', 'bring', 'carry', 'claim', 'consider', 'consist', 'cover', 'demonstrate', 'establish', 'evolve', 'exhibit', 'explain', 'find', 'generate', 'involve', 'lack', 'learn', 'make', 'perform', 'produce', 'raise', 'refrigerate', 'represent', 'require', 'retrieve', 'run', 'see', 'suffer', 'support', 'take']\n",
      "\n",
      "INCONCLUSIVE VERBS ONLY ['acquire', 'act', 'consider', 'construct', 'control', 'create', 'demonstrate', 'differentiate', 'dissect', 'fail', 'guess', 'identify', 'infer', 'interpret', 'learn', 'low', 'see', 'simulate', 'straightforward', 'streamline', 'surprising', 'translate']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "cases_and_nums = {\"positive\":50,\"negative\":50,\"inconclusive\":20}\n",
    "categories_are = [\"verb_subjects\"] # bring it to the runway\n",
    "\n",
    "positive_case_verbs = []\n",
    "for category_is in categories_are:\n",
    "    idx = 4 # anthro_cases\n",
    "    anthro_components = get_entities_or_components(category_is,\"positive\",idx)\n",
    "    # get only the verb:\n",
    "    for anthro_component in anthro_components:\n",
    "        if len(anthro_component.split()) > 1:\n",
    "            doc = nlp(anthro_component)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "        else:\n",
    "            verbs = anthro_component.split()\n",
    "        positive_case_verbs.extend(verbs)\n",
    "positive_case_verbs = set(positive_case_verbs)\n",
    "print(\"POSITIVE VERBS ONLY\",sorted(list(positive_case_verbs)))\n",
    "print()\n",
    "\n",
    "negative_case_verbs = []\n",
    "for category_is in categories_are:\n",
    "    idx = 4 # anthro_cases\n",
    "    anthro_components = get_entities_or_components(category_is,\"negative\",idx)\n",
    "    # get only the verb:\n",
    "    for anthro_component in anthro_components:\n",
    "        if len(anthro_component.split()) > 1:\n",
    "            doc = nlp(anthro_component)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "        else:\n",
    "            verbs = anthro_component.split()\n",
    "        negative_case_verbs.extend(verbs)\n",
    "negative_case_verbs = set(negative_case_verbs)\n",
    "print(\"NEGATIVE VERBS ONLY\",sorted(list(negative_case_verbs)))\n",
    "print()\n",
    "\n",
    "inconclusive_case_verbs = []\n",
    "for category_is in categories_are:\n",
    "    idx = 4 # anthro_cases\n",
    "    anthro_components = get_entities_or_components(category_is,\"inconclusive\",idx)\n",
    "    # get only the verb:\n",
    "    for anthro_component in anthro_components:\n",
    "        if len(anthro_component.split()) > 1:\n",
    "            doc = nlp(anthro_component)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'ADJ']\n",
    "        else:\n",
    "            verbs = anthro_component.split()\n",
    "        inconclusive_case_verbs.extend(verbs)\n",
    "inconclusive_case_verbs = set(inconclusive_case_verbs)\n",
    "print(\"INCONCLUSIVE VERBS ONLY\",sorted(list(inconclusive_case_verbs)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "identify\n",
      "infer\n",
      "interpret\n",
      "\n",
      "acquire\n",
      "act\n",
      "consider\n",
      "demonstrate\n",
      "see\n",
      "\n",
      "learn\n"
     ]
    }
   ],
   "source": [
    "all_intersect = sorted(list((positive_case_verbs & negative_case_verbs & inconclusive_case_verbs)))\n",
    "\n",
    "just_pos_neg_intersect = sorted([x for x in list((positive_case_verbs & negative_case_verbs)) if x not in all_intersect])\n",
    "just_pos_inc_intersect = sorted([x for x in list((positive_case_verbs & inconclusive_case_verbs)) if x not in all_intersect])\n",
    "just_neg_inc_intersect = sorted([x for x in list((negative_case_verbs & inconclusive_case_verbs)) if x not in all_intersect])\n",
    "\n",
    "\n",
    "for item in just_pos_neg_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "for item in just_pos_inc_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "for item in just_neg_inc_intersect:\n",
    "    print(item)\n",
    "print()\n",
    "for item in all_intersect:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f37b899a14b1e53256e3dbe85dea3859019f1cb8d1c44a9c4840877cfd0e7ef"
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
