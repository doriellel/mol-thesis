1109.5072_289440_4	This method not only focuses on measuring the intelligence of algorithms, but also to assess any intelligent system (human beings, animals, AI, aliens?,...), and letting us to place their results on the same scale and, therefore, to be able to compare them.
1203.1020_326379_1	The original IS-LM model has two main deficiencies: assumptions of constant price level and of strictly exogenous money supply.
1209.4838_372002_5	We assume that for some level of intelligence the respective AI will be more intelligent than a human being.
1211.2736_384982_3	In an effort to construct an intelligent computer system, a primary consideration is to represent large amounts of knowledge in a way that allows effective use and efficiently organizing information to facilitate making the recommended inferences.
1212.3996_393612_3	An original system is presented, that has three essential components: the trajectory models, the optimization process, and the monitoring process.
1301.6359_402949_3	We consider a number of issues related to the development of the set of patterns which will be used by the intelligent system when interacting with environment.
1309.3921_460929_4	The study shows that the Monte-Carlo algorithm is more sensible to the amount of uncertainty in the system, but has the advantage to return a result with the associated accuracy on demand.
1510.07524_671481_3	The light curves obtained from the random combination of those of single strands are compared to the observed light curves either in a single pixel or in a row of pixels, simultaneously in the two channels and using two independent methods: an artificial intelligent system (Probabilistic Neural Network, PNN) and a simple cross-correlation technique.
1511.03246_676426_0	  In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state.
1511.03246_676426_2	While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI.
1511.03246_676426_3	In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI.
1512.00977_683629_2	Aiming at this problem, a standard intelligent system model is established in this study to describe the AI system and the beings like human uniformly.
1512.00977_683629_3	Based on the model, the article makes an abstract mathematical description, and builds the standard intelligent machine mathematical model; expands the Von Neumann architecture and proposes the Liufeng - Shiyong architecture; gives the definition of the artificial intelligence IQ, and establishes the artificial intelligence scale and the evaluation method; conduct the test on 50 search engines and three human subjects at different ages across the world, and finally obtains the ranking of the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014.
1605.02817_730932_3	Availability of such information would be of great value particularly to computer scientists, mathematicians, and others who have an interest in AI safety, and who are attempting to avoid the spontaneous emergence or the deliberate creation of a dangerous AI, which can negatively affect human activities and in the worst case cause the complete obliteration of the human species.
1609.00813_766291_1	We derive expressions for the average rate and symbol error rate (SER) performance of an adaptive link selection based channel-aware buffer-aided relay (CABR) scheme that imposes peak-power and peak-interference constraints on the secondary nodes, and compare them with those of conventional non-buffer-aided relay (CNBR) and conventional buffer-aided relay (CBR) schemes for a delay-tolerant system.
1703.01908_825412_3	This paper will propose the application of Kant's critique of reason to current programming constructs of an autonomous intelligent system.
1705.00594_844135_2	We present an ongoing project whose ultimate goal is to deliver an open source, user-friendly AI system that is specialized for machine learning analysis of complex data in the biomedical and health care domains.
1705.10720_854261_0	  There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful.
1707.06480_871512_1	However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18%-33% fewer parameters and is trained 1.2-2.2 times faster.
1708.06746_881758_0	  Air traffic is widely known as a complex, task-critical techno-social system, with numerous interactions between airspace, procedures, aircraft and air traffic controllers.
1710.00692_896075_5	As an example scenario, we consider intersection crossing (IC) with autonomous vehicles that cooperate via V2V communication, and propose a fully distributed and a fault-tolerant algorithm for this problem.
1710.02603_897986_0	  A context-aware language model uses location, user and/or domain metadata (context) to adapt its predictions.
1711.05541_912551_1	An Oracle is a design which aims to restrain the impact of a potentially dangerous AI by restricting the agent to no actions besides answering questions.
1712.03199_921798_1	Of importance in this process is how sensitive the hyper parameters of such models are to novel datasets as this would affect the reproducibility of a model.
1802.08375_947915_1	The proposed techniques do not benefit a competitive character-aware model, but some of them improve the performance of syllable- and morpheme-aware models while showing significant reductions in model sizes.
1802.08375_947915_3	Our best morpheme-aware model with properly reused weights beats the competitive word-level model by a large margin across multiple languages and has 20%-87% fewer parameters.
1802.08872_948412_11	Automatic derivation of optimal features via deep learning provide the opportunity for remarkable improvements in prediction tasks where captured data are not friendly to human visual system - likely yielding sub-optimal human-designed features.
1805.04623_977668_4	The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.
1805.06628_979673_3	More specifically, we propose a deep reinforcement learning based UAV relay scheme to help cellular systems resist smart jamming without being aware of the jamming model and the network model in the dynamic game based on the previous anti-jamming relay experiences and the observed current network status.
1806.10055_995695_2	The resulting key sizes are significantly lower than in the original McEliece system and also slightly smaller than in Loidreau's unbroken GPT variant.
1809.05673_1025689_4	Moreover, an intelligent vehicular communication system is configured to validate the cognitive capability based on AI clustering algorithm.
1811.03742_1048375_5	To capture these characteristics, we formulated an intelligent agent-based model to imitate decision making process during fleet operation, which combines real-time optimization with artificial intelligence.
1812.03980_1061554_3	AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues.
1901.03775_1073322_3	These techniques are also well positioned to take advantage of large-scale parallel computing resources, making creative AI through evolutionary computation the likely "next deep learning".
1904.13086_1118126_10	The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment and some systematic behavioral biases.
1905.04610_1123046_4	We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model.
1905.13053_1131489_2	We prove that it is impossible to precisely and consistently predict what specific actions a smarter-than-human intelligent system will take to achieve its objectives, even if we know terminal goals of the system.
1907.01297_1145833_1	AI and Robotics research communities, industries and students are becoming increasingly aware of the problems caused by unsafe or insecure AI applications.
1907.07892_1152428_7	Our literature review indicates that this pattern exists on a global scale, and suggests that low- and middle-income countries may be more vulnerable to the negative social impacts of AI and less likely to benefit from the attendant gains.   
1910.07496_1191415_3	A least mean squares algorithm based adaptive filter (LMS-AF) is used for the purpose of FECG extraction.
1911.05508_1204102_0	  In this paper, we propose an original complex network model for an epidemic problem in an heterogeneous geographical area.
1912.01629_1213525_0	  This paper attempts to design an intelligent simulation model for pedestrian crowd evacuation.
2001.03238_1228651_2	A smart energy management system, Transactive management (TM) is a concept to improve the efficiency and reliability of the power system.
2001.07641_1233054_2	However, given, e.g., economic incentives to create dishonest AI, to what extent can we trust explanations?
2002.06374_1243797_6	Our method suggested in this article detects traffic congestion based on IOT containing a smart system that gives us traffic congestion by calculating the air pollution amount in that area.
2003.01525_1251880_5	When considering the aspect of fairness, the role that AI has on a decision-making process becomes even more sensitive since it affects the fairness and the responsibility of those people making the ultimate decision.
2004.02809_1267579_4	This work presents the initial steps towards the development of an intelligent artificial system for autonomous medical mentoring.
2005.06620_1286415_1	While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue.
2005.08663_1288458_0	  The ethical, legal, and social challenges involved in the use of profiling services for recruitment are the focus of many previous studies; however, the processes vary depending on the social system and cultural practices.
2005.13635_1293430_5	Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious AI.
2007.00655_1312169_6	We also show that our knowledge-aware language model (KALM) can serve as a drop-in replacement for GPT-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.
2007.05609_1317123_6	In this paper, we propose to train a context aware E2E model and allow the beam search to traverse into the context FST during inference.
2007.12912_1324426_3	Motivated by these issues, this paper addresses a drone-enabled intelligent vehicular system, which is secure, easy to deploy and reliable in quality.
2008.00312_1328034_4	To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems.
2008.01547_1329269_5	Compared with the original Transformer, TensorCoder not only greatly reduces the calculation of the original model but also obtains improved performance on masked language modeling task (in PTB dataset) and comparable performance on machine translation tasks.
2008.04212_1331934_3	These techniques make it possible to find creative solutions to practical problems in the real world, making creative AI through evolutionary computation the likely "next deep learning."
2008.06208_1333930_3	The proposed model can reuse the full fine-tuned LM which is fine-tuned using all layers of an original model.
2008.07326_1335048_3	In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural).
2008.10699_1338421_0	  In this paper, large-scale intelligent reflecting sur-face (IRS)-assisted multiple-input single-output (MISO) system is considered in the presence of channel uncertainty.
2008.12987_1340709_5	We elaborate on the utilization of a Genetic Algorithm and Neural Network to propose an intelligent feature selection algorithm.
2009.00473_1341976_0	  In this paper, we investigate a large intelligent surface-enhanced (LIS-enhanced) system, where a LIS is deployed to assist secure transmission.
2009.07408_1348911_0	  We consider retrofitting structure-aware Transformer-based language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model.
2009.07408_1348911_2	Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases.
2009.13996_1355499_4	However, the interpretable model does not necessarily map accurately to the original black-box model.
2010.01471_1357801_4	Then, considering the task arrival dynamics, we develop a novel deep risk-sensitive reinforcement learning algorithm.
2010.03749_1360079_5	In the proposed method, a tatum-level probabilistic language model (gated recurrent unit (GRU) network or repetition-aware bi-gram model) is trained from an extensive collection of drum scores.
2010.05990_1362320_1	The intelligent system augments human-sourced data via artificial paraphrasing in order to generate a large set of training data for further classical, attention, and language transformation-based learning approaches for Natural Language Processing.
2010.05990_1362320_10	A highly-performing model allows the intelligent system to interpret human commands at the social-interaction level through a chatbot-like interface (e.g. "Robot, can we have a conversation?") and allows for better accessibility to AI by non-technical users.
2010.12827_1369157_3	We therefore present a simple method to turn a sentence-level translation model into a context-aware model by incorporating a document-level language model into the decoder.
2011.01513_1374261_2	In this paper, we propose a character-aware pre-trained language model named CharBERT improving on the previous methods (such as BERT, RoBERTa) to tackle these problems.
2011.07347_1380095_3	We argued that the above approaches are not necessary, and the original unconditioned LM is sufficient for conditioned NLG.
2011.07960_1380708_2	In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM).
2011.10737_1383485_1	However, as a model-based shooting method, it relies heavily on an accurate system model to update the optimal control actions and the trajectory determined with forward integration, thus becoming vulnerable to inevitable model inaccuracies.
2011.13416_1386164_6	To effectively assist in anticipating harmful uses, we argue that frameworks of harms must be context-aware and consider a wider range of potential stakeholders, system affordances, as well as viable proxies for assessing harms in the widest sense.
2012.02110_1389988_7	GottBERT was pre-trained related to the original RoBERTa model using fairseq.
2012.03087_1390965_3	This work presents the development of an intelligent system that classifies and segments food presented in images to help the automatic monitoring of user diet and nutritional intake.
2012.15035_1402913_5	In Study 2, we test whether our measure is sufficiently sensitive to capture a negative form of adaptation to AI (cheating aided by AI), which occurred in a match between professional Go players.
2101.00281_1404023_2	It is in this context that the shared economy has assumed itself as a new social and economic system based on the sharing of resources and has allowed the emergence of innovative businesses like Airbnb.
2101.07315_1411057_7	Extensive simulation results demonstrate the effectiveness of the proposed semi-blind cascaded channel estimation algorithm.
2101.12051_1415793_7	It is found that autonomous driving tasks are more sensitive to model parameter errors than other tasks since the neural networks for autonomous driving contain sparser model parameters.
2102.00178_1416667_5	The resulted scheme, termed the DRL-MCTS detector, demonstrates significant improvements over the original MCTS detection algorithm and exhibits favorable performance compared to other existing linear and DNN-based detection methods under varying channel conditions.
2102.01815_1418304_3	We call this vulnerable model adversarial artificial intelligence (AI).
2102.04661_1421150_1	In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques.
2102.05490_1421979_4	Both the history-based supervisor and the safety advisor are designed based on an approximate probabilistic relation between the original system and its finite abstraction.
2102.06362_1422851_3	Effective solutions need to bridge the gap between a technical system with the social system that it will be deployed to.
2103.01421_1431566_4	Experimental results showed that our context-sensitive unsupervised segmentation model achieved state-of-the-art at different evaluation settings on various data sets for Chinese, and the comparable result for Thai.
2103.12516_1442661_4	Thereafter, a social aware similarity model is constructed to transferred individual user interest to group interest, based on which, videos can be selected to cache.
2104.03349_1450767_5	To that effect, we provide a discussion and demonstration of an agnostic and systematic paradigm for enabling expeditious simultaneously-integrated recovery of all problem dimensions during airline disruption management, through an intelligent multi-agent system that employs principles from artificial intelligence and distributed ledger technology.
2104.08066_1455484_1	Such an extension aims to make a V&L model inherit the capability of natural language understanding (NLU) from the original language model.
2104.10344_1457762_7	We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and applies a text-entity fusion encoding to aggregate entity representation.
2104.10658_1458076_8	The new combined model significantly outperformed the original model in accuracy and precision.
2105.00164_1462723_0	  Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors.
2105.00375_1462934_4	This paper uses a divergent window co-occurrence pattern detection method to develop a spatiotemporal variability-aware AI model for predicting emission values from the OBD datasets.
2105.04949_1467508_4	We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters.
2105.07879_1470438_7	It also provides a principled understanding of how conscious AI can be detected and how it might be manifested in contrast to the dominant paradigm that seeks to ultimately create machines that are linguistically indistinguishable from humans.
2105.10282_1472841_0	  In this paper, we propose a novel joint intelligent trajectory design and resource allocation algorithm based on user's mobility and their requested services for unmanned aerial vehicles (UAVs) assisted networks, where UAVs act as nodes of a network function virtualization (NFV) enabled network.
2105.13328_1475887_1	In this paper, we utilize the GAIL model for text generation to develop empathy-based context-aware conversational AI.
2106.00490_1478254_2	In this work, we consider an over-the-air FEEL system with analog gradient aggregation, and propose an energy-aware dynamic device scheduling algorithm to optimize the training performance under energy constraints of devices, where both communication energy for gradient aggregation and computation energy for local training are included.
2106.06300_1484064_4	We derive a user-friendly centralised distributed MCMC algorithm with provable scaling in high-dimensional settings.
2107.03712_1497724_0	  While the original Ait-Sahalia interest rate model has been found considerable use as a model for describing time series evolution of interest rates, it may not possess adequate specifications to explain responses of interest rates to empirical phenomena such as volatility 'skews' and 'smiles', jump behaviour, market regulatory lapses, economic crisis, financial clashes, political instability, among others collectively.
2107.03924_1497936_2	The smart healthcare system is a topic of recently growing interest and has become increasingly required due to major developments in modern technologies, especially in artificial intelligence (AI) and machine learning (ML).
2107.13734_1507746_1	The question is not if research can produce such affectively-aware AI, but when it will.
2107.13734_1507746_4	The goal of this article is to pre-empt some of the potential implications of these developments, and propose a set of guidelines for evaluating the (moral and) ethical consequences of affectively-aware AI, in order to guide researchers, industry professionals, and policy-makers.
2108.02340_1511155_2	In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack.
2108.08859_1517674_1	As an illustration, we use a soft Actor-Critic algorithm and find approximate solutions to the truncated crossing equations of two-dimensional CFTs, successfully identifying well-known theories like the 2D Ising model and the 2D CFT of a compactified scalar.
2109.03264_1526085_4	In this work, we present a prosody-aware generative spoken language model (pGSLM).
2109.05341_1528162_0	  This work presents non-orthogonal multiple access (NOMA) enabled energy-efficient alternating optimization framework for backscatter aided wireless powered uplink sensors communications for beyond 5G intelligent transportation system (ITS).
2109.09903_1532724_3	In particular, we present AirDOS, a dynamic object-aware system that introduces rigidity and motion constraints to model articulated objects.
2109.09903_1532724_6	To the best of our knowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating that camera pose estimation can be improved by incorporating dynamic articulated objects.
2109.15158_1537979_3	We also present a baseline socially-aware trajectory prediction algorithm, $\textit{TrajAirNet}$, that uses the dataset to predict the trajectories of all agents.
2110.00910_1539053_0	  As the demands of autonomous mobile robots are increasing in recent years, the requirement of the path planning/navigation algorithm should not be content with the ability to reach the target without any collisions, but also should try to achieve possible optimal or suboptimal path from the initial position to the target according to the robot's constrains in practice.
2110.04452_1542595_9	Examples from social, legal and ethical reasoning highlight the challenges in developing social AI logic.
2110.04452_1542595_10	The discussion of the four challenges leads to a research program for argumentation-based social AI logic, contributing towards the future development of AI logic.
2110.04509_1542652_0	  Solar still is an eco-friendly and convenient desalination system that can provide fresh water for remote areas and emergencies.
2110.11578_1549721_1	Existing FL protocol designs have been shown to be vulnerable to attacks that aim to compromise data privacy and/or model robustness.
2110.13538_1551681_3	To devise portable smart AR devices that operate smoothly, we first present a no keep alive Internet required smart hands-free elderly care support system that employs smart glasses with facial recognition and text-to-speech synthesis technologies.
2111.14018_1568121_9	Therefore, the highlighted challenging issues can open the door for the development of future IoT models which will be intelligent and secure based on the integration of blockchain and AI with the IoT.
2111.14833_1568936_6	More specifically, we show that three algorithms inspired by human-like social intelligence are, in principle, vulnerable to attacks that exploit weaknesses introduced by cooperative AI's algorithmic improvements and report experimental findings that illustrate how these vulnerabilities can be exploited in practice.
2112.06751_1576527_1	Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation.
2112.08718_1578494_4	Additionally, our method is deployment-friendly as the learnt domain embeddings are prefixed to the input to the model rather than changing the base model architecture.
2112.11668_1581444_1	Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.
2112.14005_1583781_3	Inspired by the perceptual process from cognitive psychology, we propose the XAI Perceptual Processing Framework and RexNet model for relatable explainable AI with Contrastive Saliency, Counterfactual Synthetic, and Contrastive Cues explanations.
2112.14005_1583781_6	This work provides insights into providing and evaluating relatable contrastive explainable AI for perception applications.
2201.09227_1594599_1	The LMs are intelligent enough to find useful and relevant representations of the language without any supervision.
2201.10822_1596194_6	Third, the XAI-enabled quality-aware IoE service delivery algorithm is implemented by employing ensemble-based regression models for ensuring the interpretation of contextual relationships among the matrices to reconfigure network parameters.
2202.00828_1599652_2	We find that co-training makes it possible to improve the original prompt model and at the same time learn a smaller, downstream task-specific model.
2202.02833_1601657_2	Though healthcare systems are eager to adopt AI solutions a fundamental question remains: \textit{what happens after the AI model goes into production?}
2202.05983_1604807_3	In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice).
2202.06431_1605255_2	To address this, here we present a novel deep learning framework that uses knowledge distillation through self-supervised learning and self-training, which shows that the performance of the original model trained with a small number of labels can be gradually improved with more unlabeled data.
2202.08171_1606995_3	A case-aware language model trained on this normalized text achieves the same perplexity as a model trained on text with gold capitalization.
2202.09447_1608271_3	Unfortunately, the current accounts of human-aware AI are insufficient to explain the landscape of the work doing in the space of human-AI interaction due to their focus on limited settings.
2203.01556_1614418_5	Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us.
2203.08410_1621272_0	  The strong few-shot in-context learning capability of large pre-trained language models (PLMs) such as GPT-3 is highly appealing for application domains such as biomedicine, which feature high and diverse demands of language technologies but also high data annotation costs.
2203.10506_1623368_2	However, various works show that raw CSI can be very sensitive to system impairments and small changes in the environment.
2203.13948_1626810_3	We developed an AI-augmented smart pathology review system (SmartPath) to empower pathologists with quantitative metrics for determining tissue extraction parameters.
2204.07553_1637691_2	For the shallow fusion setup, we use LMs during both hypotheses generation and loss computation, and the LM-aware MWER-trained model achieves 10\% relative improvement over the model trained with standard MWER on voice search test sets containing rare words.
2204.07644_1637782_5	The results indicate a need to identify potential ethical issues from an engaging communicating co-creative AI.
2204.07644_1637782_6	Later in the paper, we present some potential ethical issues in human-AI co-creation and propose to use participatory design fiction as the research methodology to investigate the ethical issues associated with a co-creative AI that communicates with users.
2204.13217_1643355_4	This paper presents a study with 38 participants to explore the impact of two interaction designs, with and without AI-to-human communication, on user engagement, collaborative experience and user perception of a co-creative AI.
2204.13217_1643355_5	The study involves user interaction with two prototypes of a co-creative system that contributes sketches as design inspirations during a design task.
2205.00029_1644441_6	All in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.
2205.07444_1651856_1	In this work, an AI that only uses sound as the input is called blind AI.
2205.07444_1651856_3	We propose different approaches to process audio data and use the Proximal Policy Optimization algorithm for our blind AI.
2205.07444_1651856_4	We also propose to use our blind AI in evaluation of sound designs submitted to the competition and define two metrics for this task.
2205.07444_1651856_5	The experimental results show the effectiveness of not only our blind AI but also the proposed two metrics.
2205.10635_1655047_6	However, there is no intelligent algorithm that decides which splitting strategy to use and places such modular splits to edge nodes for optimal performance.
2205.10957_1655369_0	  In this paper, we develop an impairments-aware air-to-ground unified channel model that incorporates the effect of both wobbling and hardware impairments, where the former is caused by random physical fluctuations of unmanned aerial vehicles (UAVs), and the latter by intrinsic radio frequency (RF) nonidealities at both the transmitter and receiver, such as phase noise, in-phase/quadrature (I/Q) imbalance, and power amplifier (PA) nonlinearity.
2205.13770_1658182_1	In this paper, we design an edge-based energy-aware MAR system that enables MAR devices to dynamically change their configurations, such as CPU frequency, computation model size, and image offloading frequency based on user preferences, camera sampling rates, and available radio resources.
2206.14623_1675043_4	In this work, we propose a contextual density ratio approach for both training a contextual aware E2E model and adapting the language model to named entities.
2207.00112_1676010_6	Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance.
2207.00969_1676867_0	  This paper studies a new multi-device edge artificial-intelligent (AI) system, which jointly exploits the AI model split inference and integrated sensing and communication (ISAC) to enable low-latency intelligent services at the network edge.
2207.02463_1678361_3	Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model.
2207.09638_1685536_3	Motivated by this phenomenon, we for the first time posit that domain-general parameters can underpin a domain-general LM that can be derived from the original LM.
2208.07026_1697738_1	We consider two uplink communication scenarios: (i) a double dirty MAC where the interferences are known non-causally to both users; and (ii) a single dirty MAC model where only one of the users knows the interference and the other one is not aware of the interference.
2208.12616_1703328_5	We describe how each principle has previously been operationalised, highlighting key themes that AI practitioners seeking to implement ethical principles should be aware of.
2209.01319_1707034_1	To facilitate recent advances in robotics and AI for delicate collaboration between humans and machines, we propose the Kinova Gemini, an original robotic system that integrates conversational AI dialogue and visual reasoning to make the Kinova Gen3 lite robot help people retrieve objects or complete perception-based pick-and-place tasks.
2209.01515_1707230_4	Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime.
2209.04595_1710310_0	  This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD).
2209.14958_1720673_4	We illustrate Dramatron's usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals.
2210.01293_1722648_7	Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.
2210.04186_1725541_2	We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements).
2210.11373_1732728_0	  This paper presents novel cascaded channel estimation techniques for an intelligent reflecting surface-aided multiple-input multiple-output system.
2210.11832_1733187_4	There is no greater opportunity for sharing perspectives at the moment than human-aware AI, which studies how to account for the fact that people are more than a source of data or part of an algorithm.
2210.11832_1733187_5	We will explore how AI-HRI can change the way researchers think about human-aware AI, from observation through validation, to make even the algorithmic design process human-aware.
2210.13594_1734949_3	Armed with an interview study, indicating that the independent news media has the potential to address them, we designed an intelligent collaborative system, called Datavoidant.
2210.13715_1735070_2	We establish this via reformulating KG completion as a "fill-in-the-blank" task, and introducing a parameter-lite encoder on top of the original LMs.
2211.06344_1745250_6	To address this challenge, we introduce appropriate auxiliary variables to convert the original signal model into two linear models with respect to the Tx signals and one entry-by-entry bilinear model with respect to the RIS phase coefficients.
2211.16912_1755818_1	To adapt to the error, one must use quantization-aware training, which entails a fine-tuning process based on the dataset and the training pipeline identical to those for the original model.
2212.03551_1759728_2	The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are.
2212.04196_1760373_5	With a novel asymmetric contrastive loss, the representation from the original pre-trained vision-language model acts as supervision to enhance the generalization ability of the learned prompt.
2212.06557_1762734_0	  Using artificial intelligent (AI) to re-design and enhance the current wireless communication system is a promising pathway for the future sixth-generation (6G) wireless network.
2212.06662_1762839_1	As more and more aerospace engineers are becoming aware of new trends in AI, traditional approaches are revisited to consider the applications of emerging AI technologies.
2212.08073_1764250_4	In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses.
2212.09292_1765469_5	It is crucial for educators and institutions to be aware of the possibility of ChatGPT being used for cheating and to investigate measures to address it in order to maintain the fairness and validity of online exams for all students.
2212.10560_1766737_3	Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model.
2212.11133_1767310_10	Third, a permute and diffusion-based intelligent model weights encryption/decryption method is proposed to achieve effective IP protection, where chaos theory is utilized to convert the PUF-based secret key to encryption/decryption keys.
2301.01010_1772074_7	For the problem of offloading decision optimization, we propose a Channel-Aware heuristic algorithm.
2301.02071_1773135_6	Specifically, we devise a three-layered multi-head attention network to realize the table-structure-aware text generation model with the help of the pretrained language model.
2301.07060_1778124_4	In this paper, we first systematically evaluate the significance of applying monotonic neural additive models (MNAMs), which use a fairness-aware ML algorithm to enforce both individual and pairwise monotonicity principles, for the fairness of AI ethics and society.
2301.08986_1780050_1	However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.
2301.13003_1784067_6	Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively.
2302.02453_1787387_3	Our results show that FineDeb offers stronger debiasing in comparison to other methods which often result in models as biased as the original language model.
2302.03241_1788175_5	A novel proxy is also proposed to preserve the general knowledge in the original LM.

2302.05206_1790140_4	Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline.
2302.05319_1790253_12	Importantly, SVEN closely matches the original LMs in functional correctness.
2302.07257_1792191_6	The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.
2302.12601_1797535_0	  Text-to-image generation (TTIG) models, a recent addition to creative AI, can generate images based on a text description.
2302.13439_1798373_3	We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%.
2303.03836_1803633_8	Besides, ChatGPT is highly sensitive to different prompt styles.
2303.05453_1805250_9	Finally, we propose a three-tiered policy framework that allows users to experience the benefits of personalised alignment, while restraining unsafe and undesirable LLM-behaviours within (supra-)national and organisational bounds.
2303.09224_1809021_3	However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them.
2303.10494_1810291_4	While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.   
2303.10571_1810368_2	In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-ended tasks.
2303.12003_1811800_4	Interestingly, 9.4 percent of humans were more creative than the most creative GAI, GPT-4.
2303.12040_1811837_11	How artificially intelligent (AI) could and should they be?
2303.12310_1812107_5	%We evaluate our workload-aware memory system on the CV and NLP benchmarks and observe significant PPA improvement compared to an SRAM-based in both inference and training modes.
2303.12310_1812107_6	Our workload-aware memory system achieves 8X energy and 9X latency improvement on Computer Vision (CV) benchmarks in training and 8X energy and 4.5X latency improvement on Natural Language Processing (NLP) benchmarks in training while consuming only around 50% of SRAM area at iso-capacity.
2304.03439_1821485_1	With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks.
2304.04007_1822053_3	We present Sky-GVINS: a sky-aware GNSS-Visual-Inertial system based on a recent work called GVINS.
2304.05077_1823123_0	  We demonstrate that if consciousness is relevant for the temporal evolution of a system's states--that is, if it is dynamically relevant--then AI systems cannot be conscious.
2304.05077_1823123_2	The design and verification preclude or suppress, in particular, potential consciousness-related dynamical effects, so that if consciousness is dynamically relevant, AI systems cannot be conscious.
2304.05524_1823570_3	We discuss possible future directions and opportunities, such as enabling explicit and implicit causal modules as well as deep causal-aware LLMs.
2304.08979_1827025_6	We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reliability in certain cases.
2304.09653_1827699_5	We introduce ReelFramer, a human-AI co-creative system that helps journalists translate print articles into scripts and storyboards.
2304.09781_1827827_2	We introduce Clover, a carbon-friendly ML inference service runtime system that balances performance, accuracy, and carbon emissions through mixed-quality models and GPU resource partitioning.
2305.00944_1834000_6	Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.
2305.02626_1835682_10	We apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious LLMs.
2305.03047_1836103_4	Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses.
2305.08711_1841767_0	  We present sustainAI, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports.
2305.09764_1842820_6	In comparison to the application-specific solution, one of our novel approaches reduces the disk size by half, while maintaining speed and accuracy of the original model.
2305.11523_1844579_7	Second, there are nonetheless significant differences across actor types, with business actors being less concerned about the downsides of AI and more in favor of lax regulation than other non-state actors.
2305.11627_1844683_2	With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM.
2305.12798_1845854_4	It requires learning parameters equal to 0.2% of the original LMs' size for steering each style.
2305.12943_1845999_6	Specifically, we start with an initial story and build a story-aware caption model to refine the captions using the whole story as guidance.
2305.13954_1847010_2	We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis.
2305.14688_1847744_4	We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability.
2305.14938_1847994_5	Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs.
2305.15020_1848076_5	In our experiments, we show that VT can retain the original performance of the multilingual LM, while being smaller in size (in general around 50% of the original vocabulary size is enough) than the original multilingual LM.
2305.15255_1848311_5	Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets.
2305.15507_1848563_2	We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size.
2305.17530_1850586_4	Training PuMer is mostly the same as finetuning the original VL model but faster.
2305.18290_1851346_2	However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.
2305.18396_1851452_1	However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs.
2305.18396_1851452_3	Compared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80% reduction in communication overhead, while retaining nearly identical accuracy.
2305.18465_1851521_5	We are happy to announce that all the next word prediction neural network LMs in Gboard now have DP guarantees, and all future launches of Gboard neural network LMs will require DP guarantees.
2305.19223_1852279_1	Many researchers are concerned that AIs and AGIs will harm humans via intentional misuse (AI-misuse) or through accidents (AI-accidents).
2305.19821_1852877_1	Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions.
2306.00190_1853338_3	This study explores the ability of GPT-4 in the contextualization of problems within CTAT, an intelligent tutoring system, aiming to increase student engagement and enhance learning outcomes.
2306.02592_1855740_3	To address this problem, we propose a framework of graph-aware language model pre-training (GALM) on a large graph corpus, which incorporates large language models and graph neural networks, and a variety of fine-tuning methods on downstream applications.
2306.02697_1855845_5	The resulting GPT-2-based model stores up to 40% fewer parameters, showing the perplexity comparable to the original model.
2306.02697_1855845_6	On the downstream tasks, including language understanding and text summarization, the model performs similarly to the original GPT-2 model.
2306.02841_1855989_4	To solve these problems, in this paper, we propose a novel framework \textbf{CTRL}, which is industrial-friendly and model-agnostic with superior inference efficiency.
2306.04653_1857801_1	This work introduces an intelligent city management system that provides a data-driven approach to three use cases: (i) analyze traffic information to reduce the risk of traffic collisions and improve driver and pedestrian safety, (ii) identify when and where energy consumption can be reduced to improve cost savings, and (iii) detect maintenance issues like potholes in the city's roads and sidewalks, as well as the beginning of hazards like floods and fires.
2306.05715_1858863_10	The advice that the LLMs provide on the issues is often sensible.
2306.07005_1860153_0	  With the rapid evolution of AI Generated Content (AIGC), forged images produced through this technology are inherently more deceptive and require less human intervention compared to traditional Computer-generated Graphics (CG).
2306.08000_1861148_3	We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment.
2306.09194_1862342_5	That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model.
2306.10063_1863211_3	Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors.
2306.10063_1863211_6	We need to consider how to design and constrain social generative AI for education.
2306.10073_1863221_7	We found that the GPT models evolved from completely failing the typical programming class' assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4).
2306.11702_1864850_2	To address this issue, we present Lingua Manga, a user-friendly and versatile system that utilizes pre-trained large language models.
2306.15559_1868707_4	RansomAI presents an agent that learns the best encryption algorithm, rate, and duration that minimizes its detection (using a reward mechanism and a fingerprinting intelligent detection system) while maximizing its damage function.
2307.00526_1871522_5	With different hyperparameter choices, the model compressed with our approach can achieve a comparable language task performance to the original model with around $2.0\times$ embedding layer compression.
2307.01210_1872206_3	It is essential that the medical community be aware of various AI assessments and choose them considering their degrees of validity, efficiency, practicality, reliability, and accuracy concerning the early identification of patients with dementia (PwD).
2307.04701_1875696_0	  Planning is a pivotal ability of any intelligent system being developed for real-world applications.
2307.04964_1875959_11	Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.
2307.08941_1879936_4	By incorporating NTK into the compression process, MLP Fusion not only preserves the original model's output but also maintains its training dynamics.
2307.10490_1881485_2	When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction.
2307.11468_1882463_4	In this context, we introduce a novel platform architecture to deploy a zero-touch PAI-as-a-Service (PAIaaS) in 6G networks supported by a blockchain-based smart system.
2307.12471_1883466_6	This perspective introduces the concept of Neuromorphic Neuromodulation, a new breed of closed-loop responsive feedback system.
2307.13808_1884803_2	To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context.
2307.15374_1886369_2	Our approach uses a fiber-optic cable to measure vibrations, enabling accurate leak identification and localization by an intelligent algorithm.
2308.00521_1888413_5	This white paper underscores the platform's potential to deliver robust results, highlighting its significance to alignment research and its implications for future social AI systems.
2308.01552_1889444_2	In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model.
2308.03279_1891171_2	Yet such student models still trail the original LLMs by large margins in downstream applications.
2308.05012_1892904_2	In this paper, we propose leveraging traditional transit CRM feedback to develop and deploy a transit-topic-aware large language model (LLM) capable of classifying open-ended text feedback to relevant transit-specific topics.
2308.05962_1893854_1	Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure the trustworthiness and to prevent misuse that could harm humans, society and the environment.
2308.08708_1896600_0	  Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern.
2308.08708_1896600_5	Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.
2308.16137_1904029_9	LM-Infinite brings substantial efficiency improvements: it achieves 2.7x decoding speed up and 7.5x memory saving over the original model.
2309.00667_1905471_1	A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment.
2309.01686_1906490_8	In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts.
2309.01885_1906689_9	Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.
2309.02705_1907509_0	  Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content.
2309.03876_1908680_8	This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
2309.03882_1908686_1	This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A").
2309.05898_1910702_4	Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning.
2309.07103_1911907_3	Our goal is to compare the accuracy of Llama-2 and our original GPT-3 baseline by using a similar metric.
2309.07124_1911928_3	In contrast, aligning frozen LLMs without requiring alignment data is more appealing.
2309.07900_1912704_1	However, LLMs are sensitive to the choice of prompts, and therefore a crucial research question is how to select good demonstrations for ICL.
2309.08112_1912916_2	In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation.
2309.08168_1912972_3	Subsequently, the verification stage employs the original LLM to validate those draft output tokens in one forward pass.
2309.08968_1913772_6	Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets.
2309.11325_1916129_0	  We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services.
2309.12053_1916857_4	The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities.   
2309.12367_1917171_4	To achieve this, we design a scaleable KB that affords educational supervisors seamless integration of lesson curricula, which is automatically processed by the intelligent tutoring system.
2309.14348_1919152_2	Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts.
2309.14348_1919152_4	RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM.
2309.14913_1919717_3	Here this picture is scrutinized by considering its robustness against extensions of the original model, and trajectories through parameter space different from those originally considered.
2309.15701_1920505_1	However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise.
2309.16583_1921387_5	Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc.
2309.16898_1921702_4	Finally, we present an integrated software pipeline, embodying advancements in a socially aware AI interaction model.
2310.00035_1922292_6	Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computational overhead as using the original model.
2310.01018_1923275_2	In this paper, we present a degradation-aware vision-language model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a multi-task framework for image restoration.
2310.02417_1924674_0	  Large language models (LLMs), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks.
2310.02727_1924984_0	  The authors are concerned about the safety, health, and rights of the European citizens due to inadequate measures and procedures required by the current draft of the EU Artificial Intelligence (AI) Act for the conformity assessment of AI systems.
2310.03031_1925288_3	In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output.
2310.03084_1925341_3	We propose a multi-objective differentiable masking scheme that can be applied to both weights and neurons to discover such subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original model.
2310.03294_1925551_2	We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm.
2310.03971_1926228_5	Our experiment results show that, compared to the original BERT large model, the converted and quantized MobileBERT models have 160$\times$ smaller footprints for a 4.1% drop in accuracy while analyzing at least one tweet per second on edge devices.
2310.05605_1927862_4	However, existing simulation tools are only concerned with typical resource management policies, not the adoption and implementation of AI models for resource management, especially.
2310.05824_1928081_2	We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model.
2310.05824_1928081_6	Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.
2310.05876_1928133_3	We argue that "Property X" characteristics are intrinsically dangerous, and when combined with greater capabilities will result in AI systems for which safety and control is difficult to guarantee.
2310.06239_1928496_1	Methods We developed a soft prompt-based LLM model and compared 4 training strategies including (1) fine-tuning without prompts; (2) hard-prompt with unfrozen LLMs; (3) soft-prompt with unfrozen LLMs; and (4) soft-prompt with frozen LLMs.
2310.07729_1929986_2	To tackle this problem, we design an energy-aware routing algorithm, aiming to minimize the overall mission duration under the energy limitations of both vehicles.
2310.08256_1930513_2	Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer.
2310.08419_1930676_1	However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails.
2310.08565_1930822_3	However, while the integration of AI-Robotics systems has enhanced the quality our lives, it has also presented a serious problem - these systems are vulnerable to security attacks.
2310.09624_1931881_1	Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system.
2310.10046_1932303_4	To address these issues, we propose TRANSOM, a novel fault-tolerant LLM training system.
2310.10944_1933201_2	The training process is lightweight, requiring only 1K steps and fewer than 0.1 percent of the original model's trainable parameters.
2310.11324_1933581_3	We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning.
2310.13028_1935285_1	Since the information is scattered, using an intelligent question-answering system to efficiently handle researchers' queries and ensure awareness of the latest advancements is necessary.
2310.13206_1935463_4	We have two main findings: i) ChatGPT's decision is sensitive to the order of labels in the prompt; ii) ChatGPT has a clearly higher chance to select the labels at earlier positions as the answer.
2310.13394_1935651_6	Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.
2310.15004_1937261_8	We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English.
2310.17162_1939419_7	Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and training on a small dataset with fewer than 300 songs.
2310.18512_1940769_2	In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers.
2310.19181_1941438_8	We also disclose the vulnerabilities to the concerned LLMs, with Google acknowledging it as a severe issue.
2310.19651_1941908_4	Our experiments, conducted on models ranging from 7b to 33b parameters, yield three key findings: (i) While these factors directly affect overall model performance, some abilities are more responsive to scaling, whereas others demonstrate significant resistance.
2311.00522_1943489_3	This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model.
2311.01197_1944164_4	Specifically, at the intermediate layer of the ViT, we utilize a spatial-aware density-based clustering algorithm to select representative tokens from the token sequence.
2311.01866_1944833_4	We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline.
2311.01866_1944833_7	These preliminary results underscore the promise of concept-aware LLMs.
2311.02069_1945036_3	We use our technique to examine alt text generation for scientific figures, finding that GPT-Vision is particularly sensitive to prompting, counterfactual text in images, and relative spatial relationships.
2311.02126_1945093_6	We introduce two modules: Firstly, utilizing Mixture-of-Modality-Adapter-Expert to independently handle different modalities, enabling better adaptation to downstream tasks while preserving the expressive capability of the original model.
2311.02847_1945814_5	Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method.
2311.02849_1945816_1	However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed.
2311.02849_1945816_6	Extensive ablation studies demonstrate the effectiveness of CTCD, and the small model distilled by CTCD outperforms the original larger model by a significant margin of 1.66 on the GLUE benchmark.
2311.03595_1946562_2	But, unlike past technologies, generative AI is creative, cognitive, and potentially ubiquitous which makes the usual assumptions of automation predictions ill-suited for today.
2311.03920_1946887_1	This paper introduces an intelligent system dedicated to monitoring air quality and categorizing activities within indoor environments using a DL approach based on 1D Convolutional Neural Networks (1D-CNNs).
2311.08244_1951211_0	  The socially-aware navigation system has evolved to adeptly avoid various obstacles while performing multiple tasks, such as point-to-point navigation, human-following, and -guiding.
2311.08245_1951212_3	If so, an intelligent HAR system with human-like cognition can be built, capable of adapting to new environments and unseen categories.
2311.08385_1951352_1	Current methods employ role-playing with personae but face two major issues: LLMs are sensitive to even a single irrelevant persona, skewing predictions by up to 30%, and LLMs fail to reason strategically over personae.
2311.08545_1951512_4	Continual pre-trained FinPythia showcases consistent improvements on financial tasks over the original foundational model.
2311.08877_1951844_4	We then explore using a surrogate confidence model -- using a model where we do have probabilities to evaluate the original model's confidence in a given question.
2311.09336_1952303_5	Using original LLM as a proposal of edits, LLMRefine searches for defect-less text via simulated annealing, trading off the exploration and exploitation.
2311.09358_1952325_10	We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge.
2311.09579_1952546_2	We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples.
2311.09624_1952591_6	This visually aware system represents a key advancement in customer engagement through personalized fashion recommendations
2311.10421_1953388_4	In this work, we analyze two different anomaly detection model maintenance techniques in terms of the model update frequency, namely blind model retraining and informed model retraining.
2311.10544_1953511_1	This paper experimentally investigates the mutual coupling effect among RIS elements using a mutual coupling-aware communication model based on scattering matrices.
2311.10544_1953511_5	Compared to the conventional communication model that does not account for mutual coupling in RIS, the mutual coupling-aware model incorporating trained scattering parameters demonstrates improved prediction accuracy.
2311.12990_1955957_6	Results show that GPT-4V's average scoring accuracy was mean =.51, SD = .037.
2311.13057_1956024_1	On one hand, writers are concerned that LLMs will deprive them of agency and ownership, and readers are concerned about spending their time on text generated by soulless machines.
2311.13445_1956412_3	LLMs can also be vulnerable to the same adversarial perturbations but a detailed study on this concern has been lacking so far.
2311.15451_1958418_0	  We present an automatic large language model (LLM) conversion approach that produces uncertainty-aware LLMs capable of estimating uncertainty with every prediction.
2311.16103_1959070_1	In pursuit of the ultimate goal of achieving artificial general intelligence, a truly intelligent Video-LLM model should not only see and understand the surroundings, but also possess human-level commonsense, and make well-informed decisions for the users.
2311.16115_1959082_3	In this paper, we discuss attestation types, including governmental, biometric, federated, and web of trust-based, and include examples such as e-Estonia, China's social credit system, Worldcoin, OAuth, X (formerly Twitter), Gitcoin Passport, and EAS.
2311.16442_1959409_13	As a result, with our 2/4/16 mixed-precision quantization for each weight matrix and asynchronous dequantization during inference, our design achieves an end-to-end speedup for Llama2-7b is 1.74x over the original model, and we reduce both runtime cost and total cost by up to 2.53x and 2.29x with less GPU requirements.
2311.17095_1960062_4	To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask.
2311.17400_1960367_8	Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.
2311.18251_1961218_6	We define such essential knowledge as the \textit{common ground} between chatbots and their owners, and we propose to build a common-ground-aware dialogue system from an LLM-based module, named \textit{OS-1}, to enable chatbot companionship.
2311.18762_1961729_5	The results reveal that while the proposed PASCAL system can be sensitive to gain-phase imperfections, it remains to be a powerful and efficient means to achieve reliable simultaneous localisation and communications.
2312.01532_1963340_5	Lab and field testing on two eye-gaze typing users with amyotrophic lateral sclerosis (ALS) demonstrated text-entry rates 29-60% faster than traditional baselines, due to significant saving of expensive keystrokes achieved through phrase and word predictions from context-aware LLMs.
2312.03755_1965563_6	Our framework integrates (1) a hierarchical casualty extraction model built upon large language models, prompt design, and few-shot learning to retrieve quantitative human loss claims from social media, (2) a physical constraint-aware, dynamic-truth discovery model that discovers the truthful human loss from massive noisy and potentially conflicting human loss claims, and (3) a Bayesian updating loss projection model that dynamically updates the final loss estimation using discovered truths.
2312.04590_1966398_0	  Artificial Intelligence (AI) models are vulnerable to information leakage of their training data, which can be highly sensitive, for example in medical imaging.
2312.05320_1967128_2	The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations.
2312.10766_1972574_1	However, current LLM systems are vulnerable to prompt-based attacks, with jailbreaking attacks enabling the LLM system to generate harmful content, while hijacking attacks manipulate the LLM system to perform attacker-desired tasks, underscoring the necessity for detection tools.
2312.10770_1972578_3	After fine-tuning the ESM model for the task of enzyme sequence classification, we compare two knowledge neuron selection methods that preserve a subset of neurons from the original model.
2312.11511_1973319_4	ComplexityNet achieved a notable 79% accuracy in determining task complexity, a significant improvement over the 34% accuracy of the original, non fine-tuned model.
2312.13401_1975209_1	Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model.
2312.13435_1975243_0	  Despite considerable efforts on making them robust, real-world AI-based systems remain vulnerable to decision based attacks, as definitive proofs of their operational robustness have so far proven intractable.
2312.14480_1976288_2	Previous research has primarily focused on patching system vulnerabilities to enhance cybersecurity, but these approaches are not well-suited to the Metaverse, where the virtual space is more complex, LLMs are vulnerable, and ethical user interaction is critical.
2312.15524_1977332_1	We identify a fundamental challenge in using them to simulate experiments: when LLM-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption.
2312.15710_1977518_4	Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding.
2312.15880_1977688_5	Finally, KnowledgeNavigator constructs the structured knowledge into effective prompts that are friendly to LLM to help its reasoning.
2312.17240_1979048_6	Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction.
2401.00283_1979839_5	Finally, some promising research avenues are identified, including stratospheric satellite (StratoSat) -to-ground direct links for mobile terminals, reconfigurable multiple-input multiple-output (MIMO) and holographic MIMO, federated learning in NS-COM networks, maritime communication, electromagnetic spectrum sensing and adversarial game, integrated sensing and communications, StratoSat-based radar detection and imaging, NS-COM assisted enhanced global navigation system, NS-COM assisted intelligent unmanned system and free space optical (FSO) communication.
2401.01291_1980847_4	We find that use of generative AI systems is already widespread: 45% of respondents were aware of generative AI usage within their area of work, while 22% actively use a generative AI system.
2401.01325_1980881_5	The two-level attentions are computed based on the original model's self-attention mechanism during inference.
2401.02596_1982152_2	In this paper, a novel explicit Euler-type scheme is proposed, which is easily implementable and able to preserve positivity of the original model unconditionally, i.e., for any time step-size $h >0$.
2401.03676_1983232_0	  Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC)
2401.04700_1984256_5	Our analysis reveals that the side effects are caused by model editing altering the original model weights excessively, leading to overfitting to the edited facts.
2401.05459_1985015_9	Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.
2401.05777_1985333_8	In general, the formal language with a lower formalization level, i.e., the more similar it is to natural language, is more friendly to LLMs.
2401.08664_1988220_6	This paper reviews the recently emerged LLM research related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system.
2401.08915_1988471_8	Moreover, many respondents are concerned that AI ethics are not well understood in the transportation community and that AI use in transportation could exaggerate existing inequalities.
2401.10360_1989914_1	A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload.
2401.10727_1990281_4	Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.
2401.11053_1990607_4	Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech.
2401.11240_1990794_7	Moreover, CaraServe employs a rank-aware scheduling algorithm to optimally schedule heterogeneous LoRA requests for maximum service-level objective (SLO) attainment.
2401.11624_1991178_1	However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations.
2401.12192_1991746_5	Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective.
2401.13222_1992776_5	In this paper, we propose and evaluate, TempRALM, a temporally-aware Retriever Augmented Language Model (RALM) with few-shot learning extensions, which takes into account both semantically and temporally relevant documents relative to a given query, rather than relying on semantic similarity alone.
2401.15081_1994635_8	However, ChatGPT and GPT-4 were not statistically sensitive to the increase in cognitive demands of the tasks, except for Grade 4.
2401.15861_1995415_7	In our approach, we utilize the original BERT model as the encoder, making only changes to the decoder without altering the encoder.
2401.16742_1996296_1	Although this causes a security problem in the cognitive domain, there has been no research about neural and computational mechanisms counteracting the impact of malicious generative AI in humans.
2401.17256_1996810_0	  Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations.
2401.17263_1996817_0	  Despite advances in AI alignment, large language models (LLMs) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries can modify prompts to induce unwanted behavior.
2402.00474_1998113_7	Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs.
2402.01536_1999175_2	We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users.
2402.01706_1999345_7	In addition, our results indicate that existing LLMs are extremely vulnerable to nesting worlds and programming language worlds.
2402.02145_1999784_4	Using three perturbation methods: replacement, insertion, and deletion coupled with a context-aware masked language model, we aim to maximize the desired sentiment score for targeted news aspects through a beam search algorithm.
2402.02408_2000047_0	  Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design.
2402.03264_2000903_3	To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT.
2402.03627_2001266_2	However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input.
2402.04396_2002035_5	Third, QuIP# uses fine-tuning to improve fidelity to the original model.
2402.04616_2002255_0	  Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense.
2402.05147_2002786_4	This approach ensures the maintenance of the original LLM's activation precision while mitigating the error propagation from shallower into deeper layers.
2402.06673_2004312_3	The paper further investigates the potential convergence of XAI with cognitive sciences, the development of emotionally intelligent AI, and the quest for Human-Like Intelligence (HLI) in AI systems.
2402.09442_2007081_7	Therefore, this paper is based on the intelligent sound monitoring and recognition system of TENG, which has good sound recognition capability, and aims to evaluate thefeasibility of the sound perception module architecture in ubiquitous sensor networks.
2402.09674_2007313_1	While techniques like safety fine-tuning aim to minimize harmful use, recent works have shown that LLMs remain vulnerable to attacks that elicit toxic responses.
2402.09733_2007372_1	This research aims to see if, how, and to what extent LLMs are aware of hallucination.
2402.09773_2007412_9	In LLaMA-7B zero-shot experiments, NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity.
2402.10601_2008240_7	Our experiments reveal a critical trade-off: LLMs that are more capable of decoding ciphers are more vulnerable to these jailbreaking attacks, with success rates on GPT-4o escalating from 40% under ACE to 78% with LACE.
2402.10669_2008308_4	Results show that human and LLM judges are vulnerable to perturbations to various degrees, and that even the cutting-edge judges possess considerable biases.
2402.11187_2008826_4	We also conduct post-training experiments to confirm that the \textit{LaCo} effectively inherits the parameters of the original model.
2402.12291_2009930_3	To give the first evidence that such schedulers enhance student learning, we build KARL, a simple but effective content-aware student model employing deep knowledge tracing (DKT), retrieval, and BERT to predict student recall.
2402.12336_2009975_6	In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one.
2402.12750_2010389_2	In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model.
2402.13463_2011102_1	In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback.
2402.14881_2012520_1	Many educators have found that their take-home or remote tests and exams are vulnerable to ChatGPT-based cheating because students may directly use answers provided by tools like ChatGPT.
2402.15180_2012819_1	Language models (LMs) are vulnerable to exploitation for adversarial misuse.
2402.16192_2013831_0	  Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content.
2402.16431_2014070_2	In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions.
2402.16459_2014098_1	Although many large language models (LLMs) have been trained to refuse harmful requests, they are still vulnerable to jailbreaking attacks which rewrite the original prompt to conceal its harmful intent.
2402.16835_2014474_3	While WHP's unlearning generalizes well when evaluated with the "Familiarity" metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q&A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains.
2402.16914_2014553_0	  The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content.
2402.17008_2014647_2	As LLMs' responses are sensitive to slight variations in prompt design, a major challenge in conducting such a benchmarking study is to systematically explore a variety of prompts before drawing a reliable conclusion.
2402.17400_2015039_4	Our findings uncover several key insights: (i) continual pretraining consistently improves <1.5B models studied in this work and is also superior to domain adaptation, (ii) larger models always achieve better perplexity than smaller ones when continually pretrained on the same corpus, (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both learning and forgetting, (iv) continual pretraining boosts downstream task performance of GPT-2 family, (v) continual pretraining enables LLMs to specialize better when the sequence of domains shows semantic similarity while randomizing training domains leads to better transfer and final performance otherwise.
2402.17834_2015473_5	In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.
2402.18048_2015687_1	Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs.
2402.18734_2016372_6	Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz.
2402.18734_2016372_7	Moreover, it outperforms the autotuner used for the generation of labels for the training of the original model in just 30 samples.
2403.01249_2018368_2	This paper proposes a Neyman-Pearson (NP) criterion-based blockage-aware algorithm to improve communication resilience against blockage in mobile mmWave multiple input multiple output (MIMO) systems.
2403.01273_2018392_5	Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-based model by up to 2$\times$ at 16k context length.
2403.01641_2018760_5	AIO2 utilizes a mean teacher model to enhance training robustness with noisy labels to both stabilize the training accuracy curve for fitting in ACT and provide pseudo labels for correction in O2C. Moreover, O2C is implemented online without the need to store updated labels every training epoch.
2403.02691_2019810_7	We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time.
2403.02775_2019894_8	With these methods, we surprisingly find that EasyQuant achieves comparable performance to the original model.
2403.03506_2020625_4	Therefore, our study utilizes the CoAuthor dataset, which includes diverse, realistic hybrid texts generated through the collaboration between human writers and an intelligent writing system in multi-turn interactions.
2403.04783_2021902_0	  Despite extensive pre-training in moral alignment to prevent generating harmful information, large language models (LLMs) remain vulnerable to jailbreak attacks.
2403.04797_2021916_5	Notably, Ms-PoE achieves an average accuracy gain of up to 3.8 on the Zero-SCROLLS benchmark over the original LLMs.
2403.04808_2021927_1	This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM.
2403.05104_2022223_5	We discuss study limitations and future directions and highlight the need to develop culturally responsive and relevant AI to serve a broader segment of the world population.
2403.05957_2023076_0	  Companies, organizations, and governments across the world are eager to employ so-called 'AI' (artificial intelligence) technology in a broad range of different products and systems.
2403.06660_2023779_2	In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR.
2403.08424_2025543_1	However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors.
2403.09032_2026151_8	The resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in terms of alignment with coding preferences and shows improved functional correctness on the HumanEval+ benchmark compared to the original instruct model.
2403.09162_2026281_2	This paper delves into the differences between original, unmodified LLMs and their fine-tuned variants.
2403.09562_2026681_6	While empirical and theoretical evidence suggests that parameter-efficient and differentially private fine-tuning techniques can defend against privacy attacks on a fine-tuned model, PreCurious demonstrates the possibility of breaking up this invulnerability in a stealthy manner compared to fine-tuning on a benign pre-trained model.
2403.09572_2026691_1	However, they are also more vulnerable to jailbreak attacks than their LLM predecessors.
2403.09676_2026795_4	Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI.
2403.10822_2027941_3	Therefore, this study evaluates whether large language models (LLMs) are aware of medical code ontologies and can accurately generate names from these codes.
2403.11805_2028924_0	  Being more powerful and intrusive into user-device interactions, LLMs are eager for on-device execution to better preserve user privacy.
2403.13313_2030432_0	  We develop Polaris, the first safety-focused LLM constellation for real-time patient-AI healthcare conversations.
2403.13681_2030800_10	Hence, we conclude that for a strong domain-specialized generative language model (such as legal), domain specialized pretraining from scratch is more cost effective, environmentally friendly, and remains competitive with larger models or even better than adapting LLMs for legal domain tasks.
2403.14635_2031754_0	  Sustainable AI projects are continuously responsive to the transformative effects as well as short-, medium-, and long-term impacts on individuals and society that the design, development, and deployment of AI technologies may have.
2403.14714_2031833_4	This approach adds an extra 0.53% improvement over -Oz to the original model.
2403.17333_2034452_3	With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them.
2403.17413_2034532_6	Experiments on the FCGEC dataset show that our proposed method effectively alleviates the over-correction of the original system (+18.2 Precision) while ensuring the error recall remains unchanged.
2403.18281_2035400_7	The results demonstrate that AIR-HLoc facilitates a latency-sensitive localisation system.
2403.19444_2036563_6	We also assess the robustness of ClinicXAI in comparison to the original InceptionV3 model by subjecting both to a series of widely utilized adversarial attacks.
2403.19839_2036958_3	Building on this foundation, this paper introduces a more advanced intelligent crop management system.
2404.00252_2037703_6	Experiments on three public panoramic image and video quality datasets, encompassing both synthetic and authentic distortions, validate the superiority of our blind PVQA model over existing methods.
2404.01242_2038693_6	Then, we prepend soft prompts to the original pre-trained language model and only update the selected parameters together with prompt-related parameters when adapting to the downstream tasks.
2404.01365_2038816_5	Despite our method's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains the original model's performance with little to no degradation on a variety of classification and generation tasks, all while improving latency (e.g. 1.29$\times$ and 1.25$\times$ speed-ups in Gemma 7B and Llama 2 13B, respectively, on an NVIDIA L40).
2404.01617_2039068_3	Using adaptive bitrate (ABR) streaming as a case study, we demonstrate that NADA produces novel ABR algorithms -- previously unknown to human developers -- that consistently outperform the original algorithm in diverse network environments, including broadband, satellite, 4G, and 5G.
2404.01855_2039306_10	We also reveal that LLMs cannot accurately comprehend geographical context information and are sensitive to the order of presentation of candidate POIs, which shows the limitations of LLMs and necessitates further research on robust human mobility reasoning mechanisms.
2404.02062_2039513_2	Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained).
2404.02948_2040399_1	LoRA freezes the original model $W$ and updates the "Noise & Zero" adapter, which may lead to slow convergence.
2404.03602_2041053_8	3) LLMs-based error detection is sensitive to small changes in prompts but remains challenging to improve.
2404.06227_2043678_2	This paper aims to leverage LLMs' reasoning and recognition abilities to replace traditional user interfaces and create an "intelligent operating system" for transportation simulation software, exploring their potential with transportation modeling and simulation.
2404.06283_2043734_1	Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive.
2404.08417_2045868_1	Here we are concerned with LLMs in the context of evolving data requirements.
2404.08654_2046105_7	The results demonstrated that Pointer-GPT outperformed the original GPT model.
2404.08705_2046156_5	This paper highlights the transformative impact of this context-aware LLM, underscoring its crucial role in addressing the global healthcare workforce deficit and propelling forward healthcare outcomes in LMICs.
2404.08806_2046257_5	We then evaluate multiple popular LLMs (including GPT models, CodeLlama, and VeriGen) upon this creativity metric, with results indicating GPT-3.5 as the most creative model in generating hardware designs.
2404.09732_2047183_4	Then we introduce robust training for a degradation-aware CLIP model to extract enriched image content features to assist high-quality image restoration.
2404.10193_2047644_8	We find that neighborhood consistency can be used to identify model responses to visual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model.
2404.10198_2047649_7	Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content.
2404.10555_2048006_5	After continual pre-training using the datasets and the base model, the tuned model performed better than the original model on the Japanese financial benchmarks.
2404.10555_2048006_6	Moreover, the outputs comparison results reveal that the tuned model's outputs tend to be better than the original model's outputs in terms of the quality and length of the answers.
2404.12038_2049489_0	  Despite careful safety alignment, current large language models (LLMs) remain vulnerable to various attacks.
2404.13082_2050533_4	To navigate this rich design space, we propose TREACLE ($\underline{T}$hrifty $\underline{Rea}$soning via $\underline{C}$ontext-Aware $\underline{L}$LM and Prompt S$\underline{e}$lection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints.
2404.13501_2050952_1	Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions.
2404.13898_2051349_9	Furthermore, we develop the Attention-aware Deep Diffusion (ADD) algorithm, which learns attention maps and leverages the diffusion process to enhance the environment exploration ability.
2404.14883_2052334_8	Thus, while increased model size may lead to better performance, LLMs are still not sensitive to (un)grammaticality the same way as humans are.
2404.15206_2052657_2	We compare 10 instruction-tuned LLaMA models to the original LLaMA-7b model and show that almost across-the-board they become more consistent, both in terms of their representations and their predictions in zero-shot and downstream tasks.
2404.16283_2053734_3	Based on this, we propose Andes, a QoE-aware LLM serving system that enhances user experience by ensuring that users receive the first token promptly and subsequent tokens at a smooth, digestible pace, even during surge periods.
2404.16369_2053820_1	Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to jailbreaking attacks, where carefully crafted prompts seduce them to produce toxic content.
2404.17808_2055259_1	Despite its wide adoption, the original BPE algorithm harbors an inherent flaw: it inadvertently introduces a frequency imbalance for tokens in the text corpus.
2404.18373_2055824_2	This paper proposes a network performance optimization and intelligent operation network architecture based on Large Language Model (LLM), aiming to build a comprehensive intelligent 6G network system.
2404.18373_2055824_4	This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.
2405.00023_2057235_1	We aim to create an advanced smart retail analytics system (SRAS), leveraging these technologies to enhance retail efficiency and customer engagement.
2405.00566_2057778_2	In this paper, we propose a novel LLM, called numeric-sensitive large language model (NumLLM), for Chinese finance.
2405.01741_2058953_3	In light of the escalating threat, it is crucial to address key questions: How vulnerable are AI models to parameter corruptions, and how do different components (such as modules, layers) of the models exhibit varying vulnerabilities to parameter corruptions?
2405.02466_2059678_2	Since several major players in the artificial intelligence (AI) field have open-sourced their original LLMs, an increasing number of individuals and smaller companies are able to build derivative LLMs based on these open-sourced models at much lower costs.
2405.02466_2059678_4	Current intellectual property (IP) protection schemes for LLMs are either designed for white-box settings or require additional modifications to the original model, which restricts their use in real-world settings.   
2405.02466_2059678_6	ProFLingo generates queries that elicit specific responses from an original model, thereby establishing unique fingerprints.
2405.02466_2059678_7	Our scheme assesses the effectiveness of these queries on a suspect model to determine whether it has been derived from the original model.
2405.05039_2062251_8	We also address ethical and legal concerns regarding the use of creative AI that impact on artists, actors, technologists and the general public.
2405.05154_2062366_1	As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI.
2405.05466_2062678_3	One model in each pair is consistently benign (aligned).
2405.05741_2062953_10	By highlighting the above critical shortcomings, this research motivates further investigation and offers novel insights for developing more intelligent LLMs.
2405.06424_2063636_2	To address this issue, we propose a novel Uncertainty-aware Reward Model (URM) that introduces a robust uncertainty estimation for the quality of paired responses based on Bayesian approximation.
2405.07773_2064985_2	In this work, we retroactively try to provide an account of what constitutes a human-aware AI system.
2405.07773_2064985_3	We see that human-aware AI is a design oriented paradigm, one that focuses on the need for modeling the humans it may interact with.
2405.08151_2065363_1	However, LLM is sensitive to the selection of demonstrations.
2405.09186_2066398_8	We show that HRE correlates well with human judgements and is particularly responsive to model changes following instruction-tuning.
2405.09763_2066975_0	  This paper introduces Fusion Intelligence (FI), a bio-inspired intelligent system, where the innate sensing, intelligence and unique actuation abilities of biological organisms such as bees and ants are integrated with the computational power of Artificial Intelligence (AI).
2405.12550_2069762_9	Lastly, we are extensively concerned about the open research challenges and potential future guidelines based on BC-based AI approaches in the intelligent IIoT system.
2405.13012_2070224_5	Our quantitative benchmarking framework opens up new paths for the development of more creative LLMs, but it also encourages more granular inquiries into the distinctive elements that constitute human inventive thought processes, compared to those that can be artificially generated.
2405.14917_2072129_7	Comprehensive experiments show that SliM-LLM significantly improves the accuracy of LLMs at ultra-low bits, e.g., 2-bit LLaMA-7B achieves a 5.5-times memory-saving than original model on NVIDIA A800 GPUs, and 48% decrease of perplexity compared to the state-of-the-art gradient-free PTQ method.
2405.15589_2072801_0	  Large language models (LLMs) are vulnerable to adversarial attacks that can bypass their safety guardrails.
2405.16241_2073453_5	FastQuery features a communication-aware embedding table quantization algorithm and a one-hot-aware dense packing algorithm to simultaneously reduce both the computation and communication costs.
2405.16833_2074045_6	Our extensive experiments demonstrate that when fine-tuning on purely malicious data, Safe LoRA retains similar safety performance as the original aligned model.
2405.17374_2074586_3	We discover a new phenomenon observed universally in the model parameter space of popular open-source LLMs, termed as "safety basin": random perturbations to model weights maintain the safety level of the original aligned model within its local neighborhood.
2405.18137_2075349_2	We reveal that widely used quantization methods can be exploited to produce a harmful quantized LLM, even though the full-precision counterpart appears benign, potentially tricking users into deploying the malicious quantized model.
2405.18137_2075349_3	We demonstrate this threat using a three-staged attack framework: (i) first, we obtain a malicious LLM through fine-tuning on an adversarial task; (ii) next, we quantize the malicious model and calculate constraints that characterize all full-precision models that map to the same quantized model; (iii) finally, using projected gradient descent, we tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii).
2405.18166_2075378_1	Despite their impressive performance, recent studies have shown that LLMs are vulnerable to deliberately crafted adversarial prompts even when aligned via Reinforcement Learning from Human Feedback or supervised fine-tuning.
2405.19358_2076570_0	  Large language models (LLMs) are vulnerable when trained on datasets containing harmful content, which leads to potential jailbreaking attacks in two scenarios: the integration of harmful texts within crowdsourced data used for pre-training and direct tampering with LLMs through fine-tuning.
2405.19524_2076736_2	Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of both disciplines in order to devise mostly effective and holistic risk mitigation approaches.
2405.19631_2076843_6	Here, we introduce an intelligent routing system for SDOH coding that uses a language model router to direct medical record data to open source LLMs that demonstrate optimal performance on specific SDOH codes.
2405.19631_2076843_7	The intelligent routing system exhibits state of the art performance of 97.4% accuracy averaged across 5 codes, including homelessness and food insecurity, on par with closed models such as GPT-4o.
2405.20015_2077227_4	Compared to direct LLM-jailbreaking, our approach is more efficient, as MLLMs are more vulnerable to jailbreaking than pure LLM.
2405.20654_2077866_1	Although these methods have demonstrated promising results, the performance is notably sensitive to the human-written prompt (or hard prompt), and fine-tuning LLMs can be computationally intensive and time-consuming.
2405.20681_2077893_7	Various randomization approaches have been proposed to protect prompts' privacy, but they may incur utility loss compared to unprotected LLMs prompting.
2405.20701_2077913_2	In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans.
2405.20770_2077982_1	While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations.
2405.20806_2078018_4	With AI's increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to mitigate it, in order to ensure the beneficial use of AI for the good of humanity.
2406.00045_2078333_1	While fine-tuning seems to be a direct solution, it requires substantial computational resources and may significantly affect the utility of the original LLM.
2406.00833_2079121_3	Half of students are concerned that AI will negatively impact their job prospects, and over half of students wish that Harvard had more classes on the future impacts of AI.
2406.01863_2080151_2	To address this limitation, we introduce BiTimeBERT 2.0, a novel time-aware language model pre-trained on a temporal news article collection.
2406.02481_2080769_5	UTFC has both benign (improving LLM fingerprinting) and malign applications (using LLMs to create covert communication channels).
2406.02721_2081009_3	To further enhance efficiency, we introduce SelfControl_{Prefix}, a compact module that encapsulates the learned representations from gradients into a SelfControl_{Prefix}, facilitating efficient inference-time control with no latency compared to the original model and allowing control for multiple behaviors simultaneously.
2406.03853_2082141_6	The original LLM is then employed to validate these draft tokens through a single forward pass, and thus guarantees that the final output text maintains a distribution consistent with vanilla auto-regressive decoding.
2406.04412_2082700_5	In addition, we introduce a noise-aware preference learning algorithm to mitigate the risk of low quality within generated preference data.
2406.04712_2083000_8	Our experiments demonstrate the effectiveness of CoderGen in improving LLMs' task-specific code generation capability (by 12.00\% on pass@1 for original model and 9.50\% on pass@1 for ReAct Agent).
2406.05432_2083720_5	First, we leverage the domain-agnostic knowledge from the original pre-trained vision-language model by conducting the weight-space ensemble of the fine-tuned model on the generated dataset with the original pre-trained model at the post-training.
2406.05724_2084012_5	In this paper we build a timely and meaningful interdisciplinary perspective on deceptive AI and reinforce a 20 year old socio-cognitive perspective on trust and deception, by proposing the development of DAMAS -- a holistic Multi-Agent Systems (MAS) framework for the socio-cognitive modelling and analysis of deception.
2406.05954_2084242_2	Test-time alignment techniques, such as prompting and guided decoding, do not modify the underlying model, and their performance remains dependent on the original model's capabilities.
2406.05981_2084269_8	Experiments on five LLM families and eight tasks consistently validate the effectiveness of ShiftAddLLM, achieving average perplexity improvements of 5.6 and 22.7 points at comparable or lower latency compared to the most competitive quantized LLMs at 3 and 2 bits, respectively, and more than 80% memory and energy reductions over the original LLMs.
2406.07001_2085289_1	This study shows that LLMs are vulnerable to changes in the number and arrangement of options in text classification.
2406.07056_2085344_7	We demonstrate that our method can compress half or even three-quarters of KV heads while maintaining performance comparable to the original LLMs, which presents a promising direction for more efficient LLM deployment in resource-constrained environments.
2406.07549_2085837_4	Thus, we propose A3VLM, an object-centric, actionable, articulation-aware vision language model.
2406.07739_2086027_3	Our method starts with an existing LLM and iteratively produces improved models by self-generating a large synthetic dataset using an original model, applying automated tools to aggressively filter, score, and de-duplicate the data into a refined higher quality dataset.
2406.10099_2088387_9	Specifically, it achieves a substantial improvement of up to 34.7% in handling questions involving knowledge gaps compared to the original model.
2406.10248_2088536_0	  The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios.
2406.10251_2088539_2	Since LLM abilities emerge with scale, smaller LLMs are more sensitive to quantization.
2406.10264_2088552_6	Overall, this intelligent tensegrity-based system with self-sensing tendons showcases potential for future exploration, making it a versatile tool for real-world applications.
2406.10486_2088774_5	However, the comparative acceptance rates by group vary under different templatic settings, suggesting that LLMs' race- and gender-sensitivity may be idiosyncratic and prompt-sensitive.
2406.10774_2089062_5	To this end, we propose Quest, a query-aware KV cache selection algorithm.
2406.10950_2089238_0	  Most large language models (LLMs) are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results for the model.
2406.10950_2089238_5	Experimental results indicate that with the increase of model size, although the ease-of-use are significantly improved, there is still a long way to go to build a sufficiently user-friendly model.
2406.11093_2089381_5	It accomplishes this by applying an emotion-aware LLM to construct a retrieval database of affective embeddings.
2406.11257_2089545_8	For instance, we achieve approximately $70\times$ compression for the Pythia-410M model, with the final performance being as accurate as the original model on various downstream tasks.
2406.13352_2091640_1	Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks.
2406.14091_2092379_1	Although language models (LMs) demonstrate exceptional capabilities on various tasks, they are potentially vulnerable to extraction attacks, which represent a significant privacy risk.
2406.14504_2092792_2	While specialized translation models still outperform LLMs on the machine translation task when viewed from the lens of correctness, they are not sensitive to cultural differences often requiring manual correction.
2406.14546_2092834_8	While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures.
2406.14871_2093159_7	Our findings based on 23 valid respondents from Ndejje University indicate that lecturers are less concerned about the fairness of LLM use per se but are more focused on the transparency of student utilization, which significantly influences Team Trust positively.
2406.14939_2093227_2	While a traditional near-field channel model, applied without any approximations, offers higher modeling accuracy than a far-field model, it renders the system design more sensitive to channel estimation errors (CEEs).
2406.15518_2093806_6	Our best method prevents 44% of jailbreak attacks compared to the original Llama-2-chat-7B model while maintaining helpfulness (as measured by MT-Bench) on benign requests almost on par with the original LM.
2406.16772_2095060_0	  In this report, we pose the following question: Who is the most intelligent AI model to date, as measured by the OlympicArena (an Olympic-level, multi-discipline, multi-modal benchmark for superintelligent AI)?
2406.17415_2095703_4	Finally, we present several practical key takeaways from our variable layer-wise quantization experiments: (a) LLM performance under variable quantization remains close to the original model until 25-50% of layers are moved in lower quantization using our proposed ordering but only until 5-10% if moved using no specific ordering; (b) Adding layer importance to inherently dynamic quantization techniques can further improve their performance, showing that our approach is complementary to other dynamic quantization methods; (c)
2406.18695_2096983_3	It uses a trained adaptation model to perform a seq2seq mapping from the often-imperfect reasonings of the original black-box LLM to the correct or improved reasonings.
2406.19384_2097672_1	We find that deleting and swapping interventions retain 72-95\% of the original model's prediction accuracy without fine-tuning, whereas models with more layers exhibit more robustness.
2406.19759_2098047_5	The results show that after PPA, models consistently outperform the original model (up to 50% for some tasks) in English-centric transfer.
2406.19783_2098071_3	Prior studies uncover that LLMs are sensitive to the changes in the prompts, including slight changes that look inconspicuous.
2407.00996_2099383_5	Olmo, the smallest model, was highly sensitive to noise, quickly adapting to noisy patterns.
2407.01245_2099632_3	In this paper, we propose a Structure-aware Inductive Knowledge Tracing model with large language model (dubbed SINKT), which, for the first time, introduces large language models (LLMs) and realizes inductive knowledge tracing.
2407.01455_2099842_4	Specifically, within the temporal space, we construct Temporal Belief State Chain (TBSC) for each character and inspired by the cognition perspective of the social world model, we divide TBSC into self-world beliefs and social world beliefs, aligning with first-order ToM (first-order beliefs) and higher-order ToM (higher-order beliefs) questions, respectively.
2407.01902_2100289_1	Although aligned with human preference data before release, LLMs remain vulnerable to various malicious attacks.
2407.02039_2100426_2	The reproducibility of LM outputs may nonetheless be vulnerable to small changes in the prompt design.
2407.02606_2100993_5	By deploying such a system, we believe that the smart sensing system can improve the quality of life for older people and provide more efficient protection
2407.03129_2101516_6	Our experimental results reveal that LLMs are highly sensitive to prompts to the extent that the ranking of LLMs fluctuates when comparing models for task performance and social bias.
2407.05088_2103475_6	This loss function reduces erroneous segmentation by not only prioritizing regions where the model is confident in predicting between foreground or background pixels but also effectively addressing areas where the model lacks high confidence in predictions.
2407.05841_2104228_2	A significant issue in this process is the limited vocabulary coverage in the original model's tokenizer, leading to inadequate representation of new languages and necessitating an expansion of the tokenizer.
2407.06129_2104516_4	Our findings reveal that LLMs are sensitive to uncertainties in utterances.
2407.08265_2106652_3	In this paper, to address these issues, we apply natural language modeling to TIR tracking and propose a coordinate-aware thermal infrared tracking model called NLMTrack, which enhances the utilization of coordinate and temporal information.
2407.08867_2107254_4	People became more opposed to building digital minds: in 2023, 63% supported banning smarter-than-human AI, and 69% supported banning sentient AI.
2407.09007_2107394_5	We quantify NEOGAUGE for various proprietary and open-source models and find that even the most creative model, GPT-4, still falls short of demonstrating human-like creativity.
2407.09164_2107551_1	Unfortunately, a few pioneering works revealed that these Code LLMs are vulnerable to backdoor and adversarial attacks.
2407.09493_2107880_1	People have intuitively started ascribing emotions or consciousness to social AI ('affective artificial agents'), with consequences that range from love to suicide.
2407.09493_2107880_6	This sheds light on the misguided functionalist temptation inherent in moving from equating the two to the ascription of psychological predicates to social AI.
2407.11059_2109446_5	Our experiments show that we are rarely able to reconstruct the exact input of an arbitrary output, thus demonstrating that LLMs are still vulnerable to slander attacks.
2407.11789_2110176_5	We also find that providing the user model with additional context from the passage partially mitigates the influence of the deceptive model.
2407.11888_2110275_2	The AI models and the data are often highly sensitive and come from mutually distrusting parties.
2407.13072_2111459_5	We welcome the ICO call for evidence on the accuracy of Generative AI, and we are happy to highlight aspects of data protection law and AI regulation that we believe should receive attention.
2407.14916_2113303_6	We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.
2407.15568_2113955_9	AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction.
2407.16994_2115381_1	This paper proposes a new method for preventing unsafe or otherwise low quality large language model (LLM) outputs, by leveraging the stochasticity of LLMs.
2407.18035_2116422_3	To address these challenges, we first define a new pipeline for restoring images with multiple degradations, and then introduce RestoreAgent, an intelligent image restoration system leveraging multimodal large language models.
2407.19842_2118229_2	However, neural networks in general, and LLMs in particular, are known to be vulnerable to adversarial attacks, where an imperceptible change to the input can mislead the output of the model.
2407.20570_2118957_3	These alignment challenges are critical in education because of the need for an intelligent visualization system to support beginners' self-regulated learning.
2407.21092_2119479_4	We argue that the zero points of the entropy function and the points where the entropy is close to 0 are the key obstacles for an LLM to approximate an intelligent language model, which explains why good LLMs need billions of parameters.
2408.00024_2120205_1	We examined the impact of deceptive AI generated explanations on individuals' beliefs in a pre-registered online experiment with 23,840 observations from 1,192 participants.
2408.00722_2120903_2	However, LLMs are vulnerable to data and model privacy issues that affect the trustworthiness of LLMs to be deployed for user-based services.
2408.01458_2121639_3	We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting.
2408.02416_2122597_7	We find that current LLMs, even those with safety alignments like GPT-4, are highly vulnerable to prompt extraction attacks, even under the most straightforward user attacks.
2408.03094_2123275_4	It is designed to compress any text, answer various types of questions, and could be utilized by the original large language model (LLM) without requiring fine-tuning.
2408.03554_2123735_2	The quantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and demonstrates a notable attack success rate of 15.8%, which is an unignorable security risk.
2408.04140_2124321_3	Results demonstrate 96% of targeted knowledge can be forgotten while maintaining performance on other knowledge within 2.5% of the original model, significantly outperforming the discriminatory abilities of the previous state-of-the-art.
2408.04771_2124952_1	Our paper evaluates these impacts by investigating (1) the factual question of whether future advanced AI systems will be conscious, together with (2) the epistemic question of whether future human society will broadly believe advanced AI systems to be conscious.
2408.04771_2124952_6	Our analysis suggests that the worst possibility is the wrong belief that AI is non-conscious, followed by the wrong belief that AI is conscious.
2408.04771_2124952_7	The paper concludes with the main recommendations to avoid research aimed at intentionally creating conscious AI and instead focus efforts on reducing our current uncertainties on both the factual and epistemic questions on AI consciousness.
2408.06546_2126727_4	We then illuminate how blind people make sense of AI through experimenting with AI VAT, employing non-visual skills, strategically including sighted people, and cross-referencing with other devices.
2408.06837_2127018_0	  Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways?
2408.06999_2127180_0	  This paper presents the use of robust model predictive control for the design of an intent-aware collision avoidance system for multi-agent aircraft engaged in horizontal maneuvering scenarios.
2408.07224_2127405_1	As a group of experimental musicians and researchers, we are enthusiastic about the creative potential of these tools and have sought to understand and evaluate them from perspectives of prompt creation, control, usability, understandability, explainability of the AI process, and overall aesthetic effectiveness of the results.
2408.08924_2129105_1	However, research indicates that LLMs are vulnerable to jailbreak attacks, where adversaries can induce the generation of harmful content through meticulously crafted prompts.
2408.10159_2130340_1	Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation.
2408.10642_2130823_5	In this article with insight from DPO and MinorDPO, we propose a training metric for SFT to measure the discrepancy between the optimized model and the original model, and a loss function MinorSFT that can increase the training effectiveness, and reduce the discrepancy between the optimized LLM and original LLM.
2408.10668_2130849_5	Trained on external or self-generated harmful datasets, the cost value model could successfully influence the original safe LLM to output toxic content in decoding process.
2408.10722_2130903_2	Emerging as widely adopted generalists for diverse tasks, LLMs are still vulnerable to backdoors.
2408.11865_2132046_0	  Large Language Models (LLMs) are highly sensitive to prompts, including additional context provided therein.
2408.11867_2132048_3	This paper solves the above problems by establishing the taxi driver dual-sensitive decision model, the longitudinal taxi queuing model and the short-distance passenger re-return forced M/M/1 queuing model.
2408.12047_2132228_5	While participants believe that RAI artifacts are a valuable contribution to the broader AI governance ecosystem, many are concerned about their potential unintended, longer-term impacts on actors outside of technology companies (e.g., downstream end-users, policymakers, civil society stakeholders).
2408.12680_2132861_1	The emergence of social norms fosters coordination among agents without any hard-coded rules, which is crucial for the large-scale deployment of AVs in an intelligent transportation system.
2408.12798_2132979_0	  Generative Large Language Models (LLMs) have made significant strides across various tasks, but they remain vulnerable to backdoor attacks, where specific triggers in the prompt cause the LLM to generate adversary-desired responses.
2408.12832_2133013_7	Moreover, we propose a transformer-based intention-aware mobility prediction model to effectively harness the intention inference ability of LLM.
2408.14644_2134825_6	This study contributes significantly to the field of creative AI and interactive art, blending technology and environmental consciousness in a compelling, thought-provoking manner.
2408.16500_2136681_1	Here we propose the CogVLM2 family, a new generation of visual language models for image and video understanding including CogVLM2, CogVLM2-Video and GLM-4V. As an image understanding model, CogVLM2 inherits the visual expert architecture with improved training recipes in both pre-training and post-training stages, supporting input resolution up to $1344 \times 1344$ pixels.
2408.16725_2136906_7	Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities.
2409.00509_2138141_5	Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks.
2409.01247_2138879_1	Despite various safeguards, advanced LLMs remain vulnerable.
2409.02375_2140007_8	This study underscores the growing importance of developing privacy-aware LLMs that can both support businesses in compliance efforts and safeguard user privacy rights.
2409.02864_2140496_8	BRAD's coordinated integration of bioinformatics tools delivers a context-aware and semi-autonomous system that extends beyond the capabilities of conventional LLM-based chatbots.
2409.06601_2144233_4	By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold.
2409.07353_2144985_1	However, these models remain vulnerable to adversarial attacks, particularly jailbreak attacks, which bypass safety protocols and cause the model to generate misleading or harmful responses.
2409.07372_2145004_3	We develop Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring system that can (1) effectively convert an input lecture slide into a structured teaching agenda consisting of a set of heterogeneous teaching actions; (2) create and manage an interactive lecture that generates responsive interactions catering to student learning demands while regulating the interactions to follow teaching actions.
2409.10516_2148148_5	RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors.
2409.11026_2148658_6	The core idea is to find a representation of the original system prompt that leads to the same functionality, while the obfuscated system prompt does not contain any information that allows conclusions to be drawn about the original system prompt.
2409.11057_2148689_5	Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%.
2409.11411_2149043_4	To address these challenges, this paper introduces AIvril, an advanced framework designed to enhance the accuracy and reliability of RTL-aware LLMs.
2409.13537_2151169_2	To address this problem, we propose ShizishanGPT, an intelligent question answering system for agriculture based on the Retrieval Augmented Generation (RAG) framework and agent architecture.
2409.13708_2151340_3	We examine how a language's level of resourcing relates to how vulnerable LLMs are to multilingual jailbreaks in that language.
2409.13710_2151342_5	We demonstrate that this LN-free model achieves similar performance to the original model on the OpenWebText and ThePile datasets (-0.05 cross-entropy loss), and the Hellaswag benchmark (-0.5% accuracy).
2409.14866_2152498_0	  Large Language Models (LLMs) have excelled in various tasks but are still vulnerable to jailbreaking attacks, where attackers create jailbreak prompts to mislead the model to produce harmful or offensive content.
2409.17372_2155004_4	In this paper, we propose a training-free architecture search framework to identify optimal subnets that preserve the fundamental strengths of the original LLMs while achieving inference acceleration.
2409.17372_2155004_5	Furthermore, after generating subnets that inherit specific weights from the original LLMs, we introduce a reformation algorithm that utilizes the omitted weights to rectify the inherited weights with a small amount of calibration data.
2409.17458_2155090_8	Our experiments reveal that all LLMs are vulnerable to RED QUEEN ATTACK, reaching 87.62% attack success rate on GPT-4o and 75.4% on Llama3-70B. Further analysis reveals that larger models are more susceptible to the RED QUEEN ATTACK, with multi-turn structures and concealment strategies contributing to its success.
2409.20089_2157721_0	  Large language models (LLMs) are vulnerable to adversarial attacks that can elicit harmful responses.
2410.00359_2158560_3	The core idea of this work is to maintain states based on the LLM's response, letting the LLM become self-aware of current status and think step by step in a multi-round chain-of-thought paradigm.
2410.00423_2158624_1	However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.
2410.00451_2158652_0	  Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including through the use of adversarial suffixes.
2410.00487_2158688_5	Our method employs a training objective that minimizes the Kullback-Leibler (KL) divergence between the predictions of an original model (with access to contextual information) and a target model (without such access).
2410.01024_2159225_2	In GPTreeO we extend the original DLGP algorithm by allowing continual optimisation of the GP hyperparameters, incorporating uncertainty calibration, and introducing new strategies for how the local partitions are created.
2410.01708_2159909_4	However, Part II results show that even without including social context information in the prompt, LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone.
2410.01792_2159993_4	Specifically, o1 -- like previous LLMs -- is sensitive to the probability of examples and tasks, performing better and requiring fewer "thinking tokens" in high-probability settings than in low-probability ones.
2410.02110_2160311_2	First, LLMs are highly sensitive to minor prompt variations, raising doubts about their ability to generalize to new scenarios without extensive prompt engineering.
2410.02110_2160311_3	Moreover, apparently successful outcomes can often be unreliable, either because domain experts unintentionally guide LLMs to produce expected results, leading to self-fulfilling prophecies; or because the LLM has encountered highly similar scenarios in its training data, meaning that models may not be simulating behavior so much as regurgitating memorized content.
2410.02657_2160858_4	We also find that the LLMs are sensitive to numerical anchors, indicating the ability to leverage community-based flagging efforts and exposure to adversaries.
2410.02677_2160878_6	We find that LLMs are sensitive to such difference in setups (e.g., GPT-4o with 27.3% difference).
2410.04190_2162391_0	  Large Language Models (LLMs) remain vulnerable to jailbreak attacks that bypass their safety mechanisms.
2410.04197_2162398_0	  Evaluating the creativity of large language models (LLMs) in story writing is difficult because LLM-generated stories could seemingly look creative but be very similar to some existing stories in their huge and proprietary training corpus.
2410.04335_2162536_4	We propose replacing and reinitializing the parameters of the model's input and output layers with the parameters of the original model, and training these parameters while keeping other parameters fixed.
2410.04652_2162853_6	We demonstrate the usefulness of the proposed system through two real-world AR applications on Magic Leap 2: a) spatial search in physical environments with natural language and b) an intelligent inventory system that tracks object changes over time.
2410.04652_2162853_7	We also make our full implementation and demo data available at (https://github.com/cy-xu/spatially_aware_AI) to encourage further exploration and research in spatially aware AI.
2410.05331_2163532_7	Empirical experiments across five datasets and three LLM architectures demonstrate that TaylorMLP induces over 4x increase in latency, producing the tokens precisely matched with original LLMs.
2410.05797_2163998_7	Results show that our model successfully confuses the privacy in source code while preserving the original LLM's performance.
2410.06716_2164917_1	This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs while preserving the distribution of the original model as much as possible?
2410.06716_2164917_2	We first define the ideal distribution - the one closest to the original model, which also always satisfies the expressed constraint - as the ultimate goal of guaranteed generation.
2410.07103_2165304_2	In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented.
2410.07304_2165505_2	We found a misalignment between human and LLM moral assessments; although both LLMs and humans tended to reject morally complex utilitarian dilemmas, LLMs were more sensitive to personal framing.
2410.07677_2165878_2	To address these challenges, we propose a smart audit system empowered by large language models (LLMs).
2410.07820_2166021_0	  In recent years, with the maturation of large language model (LLM) technology and the emergence of high-quality programming code datasets, researchers have become increasingly confident in addressing the challenges of program synthesis automatically.
2410.09644_2167845_4	Across 11 languages-with diverse scripts, resource availability, and fragmentation-we demonstrate that VocADT outperforms the original Mistral model and other baselines across various multilingual tasks including natural language understanding and machine translation.
2410.10343_2168544_5	This insight enables us to extract what we term the Meta-SafetyLock, a set of safety bias directions representing key activation patterns associated with safe responses in the original model.
2410.10760_2168961_1	Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token.
2410.11009_2169210_3	In this work, we consider the case where the user does not select any of the suggested replies from a smart reply system, and how this can be used as one-shot implicit negative feedback to enhance the accuracy of an AI writing model.
2410.11533_2169734_1	While LLMs have the capability to identify and avoid harmful queries, they remain vulnerable to "jailbreak" attacks, where carefully crafted prompts can induce the generation of toxic content.
2410.12013_2170214_7	Experimental results demonstrate that the Mixtral-8x7B model with 50% sparsity maintains 99% of the performance of the original model after the expert-wise knowledge distillation.
2410.13042_2171243_5	Experts were primarily concerned with capture leading to a lack of AI regulation, weak regulation, or regulation that over-emphasizes certain policy goals over others.
2410.13284_2171485_1	However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable.
2410.13691_2171892_1	When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails.
2410.13966_2172167_3	Experiments show that this fine-tuned domain-aware model outperforms the popular DetectGPT and GPTZero on both in-domain and cross-domain texts, where AI-generated texts may either be in a different domain or generated by a different LLM not used to generate the training datasets.
2410.14827_2173028_5	When even a small fraction of the alignment data is poisoned using our method, the aligned LLM becomes more vulnerable to prompt injection while maintaining its foundational capabilities.
2410.15107_2173308_6	Moreover, we show the addition of an adversary significantly degrades RALM's performance, with the model becoming even more vulnerable when the two scenarios overlap (adversarial+unanswerable).
2410.15319_2173520_2	This limitation renders LLMs vulnerable to issues such as demographic biases, social stereotypes, and LLM hallucinations.
2410.15939_2174140_7	Our findings reveal that while LLMs show promise in this domain, they are highly sensitive to the encoding used.
2410.16775_2174976_1	We propose a context-aware LLM translation system that leverages conversation summarization and dialogue history to enhance translation quality for the English-Korean language pair.
2410.16879_2175080_7	Where clinicians trust future AI interpretation to be accurate, they are less concerned that it is explainable.
2410.16879_2175080_9	Whilst clinicians do not fear job losses, they are concerned about deskilling and the need to educate the workforce to use AI responsibly.
2410.17552_2175753_1	Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection.
2410.18469_2176670_1	Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses.
2410.18494_2176695_1	However, AI-generated code can be unreliable, and the natural language requirements driving this code may be ambiguous.
2410.19160_2177361_1	While alignment techniques have significantly improved overall safety, LLMs remain vulnerable to carefully crafted adversarial inputs.
2410.19230_2177431_8	Despite our proxy-attack strategy successfully bypassing the detectors with such significant relative drops, we find that the generation quality of the attacked models remains preserved, even within a modest utility budget, when compared to the text produced by the original, unattacked source model.
2410.19482_2177683_3	Though efficient to compute, we show that this definition is unreliable because it does not account for non-determinism present in more realistic (non-greedy) sampling schemes, for which LLMs produce a range of outputs for the same prompt.
2410.19925_2178126_0	  Generative large language models (LLMs) exhibit impressive capabilities, which can be further augmented by integrating a pre-trained vision model into the original LLM to create a multimodal LLM (MLLM).
2410.19925_2178126_1	However, this integration often significantly decreases performance on natural language understanding and generation tasks, compared to the original LLM.
2410.20856_2179057_6	We propose a graph-aware LLM for traffic prediction that considers proximal traffic information.
2410.21337_2179538_1	However, LLMs applications are highly vulnerable to prompt injection attacks, which poses a critical problem.
2410.21802_2180003_5	Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples.
2410.22143_2180344_1	Although large language models (LLMs) are typically aligned, they remain vulnerable to jailbreaking through either carefully crafted prompts in natural language or, interestingly, gibberish adversarial suffixes.
2410.22262_2180463_4	Our studies reveal that some AI workloads are potentially vulnerable to the dominant effects of communication, especially multicast traffic, which can become a performance bottleneck and limit their scalability.
2410.23136_2181337_4	However, existing LLM-based recommenders often lose the in-context learning ability during recommendation tuning, while the original LLM's in-context learning lacks recommendation-specific focus.
2411.00348_2182775_0	  Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action.
2411.00850_2183277_8	Also, GWQ achieves 1.2x inference speedup in comparison to the original model and effectively reduces the inference memory.
2411.01077_2183504_2	However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens.
2411.01606_2184033_3	In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect.
2411.02476_2184903_7	Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets.
2411.02478_2184905_1	Although AI has become increasingly smart, its wisdom has not kept pace.
2411.02657_2185084_2	We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study.
2411.03731_2186158_2	We propose a "memoization-aware" Bayesian Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline caching system, allowing it to evaluate significantly more hyperparameter candidates per GPU-day than other tuning algorithms.
2411.04444_2186871_8	However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of the 137 solutions suggested by Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors, which indicate the risk of LLM-based refactoring.
2411.04799_2187226_6	(3) Convert original LLMs into State-Transition Reasoners via a curricular training strategy.
2411.05345_2187772_4	It shows that LLMs are sensitive to minimal adversarial typographical changes.
2411.06024_2188451_3	At its core, TourSynbio-Search employs an intelligent agent system that interprets natural language queries, optimizes search parameters, and executes search operations across major platforms including UniProt, PDB, ArXiv, and BioRxiv.
2411.07336_2189763_1	An intelligent system should perform set operations consistently, regardless of superficial variations in the operands.
2411.07990_2190417_9	Furthermore, GPT-J's behavior is sensitive to the individual word frequencies, even for regular forms, a behavior that is consistent with an analogical account of regular forms but not a rule-based one.
2411.08574_2191001_8	This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications.
2411.09102_2191529_2	Without explicit interrogation of these benefits by AI developers, as a community we may remain blind to the immensity of systemic change that is needed as well.
2411.10954_2193381_5	Our findings show that LLMs are sensitive in handling both multilingual and dialectal variations.
2411.11235_2193662_0	  Artificial Intelligence (AI) has demonstrated significant capabilities in various fields, and in areas such as human-computer interaction (HCI), embodied intelligence, and the design and animation of virtual digital humans, both practitioners and users are increasingly concerned with AI's ability to understand and express emotion.
2411.11532_2193959_3	To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer.
2411.11543_2193970_1	However, recent research shows that the visual modality in VLMs is highly vulnerable, allowing attackers to bypass safety alignment in LLMs through visually transmitted content, launching harmful attacks.
2411.12701_2195128_0	  Large Language Models (LLMs) are known to be vulnerable to backdoor attacks, where triggers embedded in poisoned samples can maliciously alter LLMs' behaviors.
2411.12951_2195378_4	Our results reveal that current Video-LLMs are sensitive to variations in video contents, language queries, and task settings, unveiling severe deficiencies in maintaining consistency.
2411.14033_2196460_4	As a natural trend of development, the tools for calling are becoming autonomous agents, thus the full intelligent system turns out to be a LLM-based Multi-Agent System (LaMAS).
2411.15525_2197952_7	Botfip-LLM not only improves performance, generalization, and extrapolation over the original Botfip model but also significantly enhances applicability to Formula String-related tasks, enabling more diverse task handling.
2411.16313_2198740_5	Moreover, it further designs a cost-aware offline reinforcement learning algorithm to fine-tune the LLM to optimize the performance-cost trade-off in tool planning.
2411.18424_2200851_9	To respond, we introduce FastSwitch, a fairness-aware serving system that not only aligns with existing KV cache memory allocation policy but also mitigates context switching overhead.
2411.19146_2201573_6	Nemotron-51B achieves a 2.17x inference throughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4% of the original model's benchmark accuracies.
2411.19456_2201883_8	Comparing ADCE and AICE demonstrates closed-source LLMs rely more on deep structure, while open-source LLMs are more surface-sensitive, which decreases with model scale.
2412.01004_2203382_6	Moreover, CoDyRA preserves the original model architecture and deployment pipeline, introducing no additional inference overhead.
2412.01312_2203690_9	We recommend close scrutiny of assessment tasks: only invigilated in-person examinations, vivas, laboratory skills testing (or "performances" in other disciplines), and presentations are not vulnerable to GPT-4, and urge consideration of how AI can be embedded within the disciplinary context.
2412.02655_2205033_5	Our results demonstrates the potential of LLMs to design context-aware system to enhance navigation efficiency and safety in industrial and dynamic environments.
2412.02713_2205091_5	We demonstrate that this method effectively highlights the differences between human responses and those generated by premium versions of leading chatbots (ChatGPT, Claude, and Gemini), but that it is also sensitive to the amount of AI cheating in the data.
2412.03235_2205613_5	We interestingly find that safety fine-tuned LLMs such as GPT-4o are vulnerable to producing natural jailbreak questions from unsafe content (without denial) and can thus be used for the latter (A to Q) step.
2412.03761_2206139_4	Later, I developed a novel white-box multi-head graph attention-based prototype network designed to explain the decisions of text classification models without sacrificing the accuracy of the original black-box LMs.
2412.03795_2206173_6	Samudra is stable for centuries and 150 times faster than the original ocean model.
2412.04036_2206414_3	In this study, we introduce SocialMind, the first LLM-based proactive AR social assistive system that provides users with in-situ social assistance.
2412.04571_2206949_3	Here we employ Integrated Information Theory (IIT), which provides principled tools to determine whether a system is conscious, to what degree, and the content of its experience.
2412.05693_2208071_2	In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.
2412.06469_2208847_1	The central case study is Jess+ an intelligent digital score system for shared creativity with a mixed ensemble of non-disabled and disabled musicians.
2412.06843_2209221_3	Our method requires only a small set of unsafe responses to toxic prompts, easily obtained from the unsafe LLM itself.
2412.06858_2209236_1	However, previous research observes that certain weights in the LLM, known as outliers, are significantly sensitive to quantization noises.
2412.07077_2209455_5	Our approach hinges on three main strategies: prompt grouping with masked attention to optimize CLIP's adaptability while safeguarding its zero-shot capabilities; the incorporation of auxiliary prompts for the seamless integration of new domain insights without disrupting the original model's representation; and an ensemble learning strategy that effectively merges original and new knowledge.
2412.07192_2209570_9	Our analyses further reveal that: (1) models with less post-training alignment require fewer bit flips to jailbreak; (2) certain model components, such as value projection layers, are substantially more vulnerable than others; and (3) our method is mechanistically different than existing jailbreaks.
2412.07902_2210280_4	Using ranks equivalent to 10\% of the original weight matrix size, our approach reduces the accuracy gap with the original model by more than 50\%.
2412.08035_2210413_3	When this approach is applied naively, we discover that LLMs are unreliable when translating features of the source language that do not have a direct mapping to the target language, and that the LLM often gets stuck in repair loops when attempting to fix errors.
2412.08098_2210476_2	While LLMs have shown significant potential in assisting with coding, it is perceived that LLMs are vulnerable to adversarial attacks.
2412.08847_2211225_5	We then develop a novel framework, Multi-Objective Personalized Interpretable Health-aware Food Recommendation System (MOPI-HFRS), which provides food recommendations by jointly optimizing the three objectives: user preference, personalized healthiness and nutritional diversity, along with an large language model (LLM)-enhanced reasoning module to promote healthy dietary knowledge through the interpretation of recommended results.
2412.09029_2211407_4	Among these changes is a new visual guide that moves beyond the original traffic light system and utilises a neutral colour palette that avoids implied hierarchies between the levels.
2412.09632_2212010_4	The second method, an information leakage study, seeks to ascertain whether LLMs are aware of the information held in the datasets published on the UK government's open data initiative data$.$gov$.$uk.
2412.10513_2212891_2	A key challenge of this approach is determining how accurately the extracted decision tree represents the original model and to what extent it can be trusted as an approximation of their behavior.
2412.12464_2214842_5	To address these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning model (LIKR).
2412.13879_2216257_0	  Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks yet still are vulnerable to external threats, particularly LLM Denial-of-Service (LLM-DoS) attacks.
2412.16325_2218703_1	As AI systems increasingly make critical decisions, deceptive AI poses a significant challenge to trust and safety.
2412.16565_2218943_7	Finally, we develop the distance-aware transfer learning algorithm based on the DMTSSL algorithm to handle the dynamic scenario with negligible computation cost.
2412.16633_2219011_1	However, LLMs are vulnerable to jailbreak attacks, which can generate malicious content.
2412.16643_2219021_3	Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.
2412.17034_2219412_3	In particular, we introduce \textit{safety boundary}, and we find that jailbreaks shift harmful activations outside that safety boundary, where LLMs are less sensitive to harmful information.
2501.00745_2224329_1	However, these systems are vulnerable to adversarial attacks, especially ranking manipulation attacks, where attackers craft webpage content to manipulate the LLM's ranking and promote specific content, gaining an unfair advantage over competitors.
2501.01516_2225100_2	As many of the methods in XAI are themselves models, adversarial examples have been prominent in the literature surrounding the effectiveness of XAI, with the objective of these examples being to alter the explanation while maintaining the output of the original model.
2501.01679_2225263_1	However, existing evidence shows that LLMs are prompt-sensitive and it is sub-optimal to apply the fixed prompt to any input for downstream machine translation tasks.
2501.02334_2225918_1	Use of generative AI in the scoring of constructed responses is particularly appealing because it reduces the effort required for handcrafting features in traditional AI scoring and might even outperform those methods.
2501.02342_2225926_4	We demonstrate that, despite significant reduction in model size which removes up to 2 billion parameters from the original model, our approach preserves the model's ability to handle complex in-vehicle tasks accurately and efficiently.
2501.02600_2226184_2	Additionally, LLM inference workloads are sensitive to various configuration parameters (e.g., model parallelism, size, and quantization) that involve trade-offs between performance, temperature, power, and output quality.
2501.02754_2226338_4	Then MBTSAD leverages attention distillation, the retrained model is the teacher model, and the original backdoored model is the student model.
2501.03012_2226596_6	These shift vectors allow us to recover fine-tuned concepts by shifting those in the original model.
2501.03272_2226856_1	However, recent studies have revealed that these models are vulnerable to backdoor attacks, where even a small number of malicious samples can successfully embed backdoor triggers into the model.
2501.04156_2227740_3	These insights informed the design of AdaptiveCoPilot, which integrates cognitive state assessments, behavioral data, and adaptive strategies within a context-aware Large Language Model (LLM).
2501.06607_2230191_2	The AI Drawing Partner is an early example of a quantified co-creative AI system that automatically models the co-creation that happens on the system.
2501.06807_2230391_5	Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further select a small subset of tokens for attention computation.
2501.09775_2233359_6	The results of the evaluation of questions on a wide range of topics in seven different models show that LLMs are more confident in their answers when they provide reasoning before the answer.
2501.10200_2233784_9	Additionally, our feature-based analysis shows that all tools are primarily affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.
2501.13115_2236699_2	In this paper, we first point out a novel perspective for jailbreak attacks: LLMs are more responsive to $\textit{positive}$ prompts.
2501.13533_2237117_7	AI agents are often assumed to pursue fixed goals, but AI persons may be self-aware enough to reflect on their aims, values, and positions in the world and thereby induce their goals to change.
2501.13945_2237529_2	These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners.
2501.14073_2237657_2	This work reveals that many state-of-the-art LLMs are vulnerable to malicious requests hidden behind scientific language.
2501.14143_2237727_6	Defending smart grid system from various cyber-attacks 3) Incorporating an increasing number of electric vehicles into the system of power grid without overwhelming it.
2501.14312_2237896_4	This paper introduces the first locality-aware fair scheduling algorithm, Deficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix locality with a fairness guarantee.
2501.14802_2238386_3	Our approach introduces an intelligent system that automates deployment decisions, resource allocation, and pipeline optimization while maintaining optimal performance and cost efficiency.
2501.14808_2238392_2	This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving latency requirements.
2501.15278_2238862_3	Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks.
2501.15427_2239011_4	Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue.
2501.16497_2240081_1	Although alignment methods aim to suppress harmful content generation, LLMs are often still vulnerable to jailbreaking attacks that employ adversarial inputs that subvert alignment and induce harmful outputs.
2501.16504_2240088_5	The simulation results show the effectiveness of the proposed novel approach in designing a true physical environment-aware channel twin model.
2501.17286_2240870_12	Results: Fine-tuned LLMs outperformed original LLMs across all tasks with p-value <= 0.001.
2501.17433_2241017_1	Recent research shows that Large Language Models (LLMs) are vulnerable to harmful fine-tuning attacks -- models lose their safety alignment ability after fine-tuning on a few harmful samples.
2501.18058_2241642_6	We propose a CSI-error-aware joint beamforming algorithm, which can substantially outperform one that does not account for channel estimation errors.
2501.18475_2242059_3	Our approach focuses on minimizing the layer-wise discrepancy between the original LLM and its quantized counterpart with LoRA components during initialization.
2501.18536_2242120_2	We find that embedding models for retrieval, rerankers, and large language model (LLM) relevance judges are vulnerable to these attacks, in which adversaries insert misleading text into passages to manipulate model judgements.
2501.18542_2242126_2	Each team provided a different perspective to the topic of creative AI, substantiated by a set of research questions as the main subject of their investigation.
2501.18542_2242126_3	The 2023 edition of ISWS focuses on the intersection of Semantic Web technologies and Creative AI.
2501.18542_2242126_4	ISWS 2023 explored various intersections between Semantic Web technologies and creative AI.
2501.18565_2242149_3	We observe that humans are highly sensitive to shifts and abrupt changes in videos, while current AI systems still struggle to comprehend and respond to such situations effectively.
2501.18632_2242216_4	Experiment results indicate that leading commercial and open-source LLMs are highly vulnerable to medical jailbreaking attacks.
2501.18880_2242464_7	By targeting the data sampling process to address the weaknesses of the VLM, we can effectively train a more context-aware model.
2502.00158_2243149_7	In the inference stage, LOKA retrieves the most relevant memory from the codebook and plugs it into the original LLM to apply the updated knowledge.
2502.00311_2243302_5	Our approach leverages inherent sparsity in gradients to compress optimizer states by projecting them onto a low-dimensonal subspace, with dimensionality independent of the original model's parameters.
2502.00674_2243665_7	We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models.
2502.01344_2244335_6	Specifically, PSSD leverages the recent multi-agent paradigm, and is further enhanced with three innovatively conceived roles: (1) the intuition-based id role that provides initial attempts based on benign LLMs; (2) the rule-driven superego role that summarizes rules to regulate the above attempts, and returns specific key points as guidance; and (3) the script-centric ego role that absorbs all procedural information to generate executable script for the final answer prediction.
2502.02821_2245812_0	  This paper presents a novel AI-based smart traffic management system de-signed to optimize traffic flow and reduce congestion in urban environments.
2502.02960_2245951_1	However, LLMs are increasingly vulnerable to a range of adversarial attacks that threaten their privacy, reliability, security, and trustworthiness.
2502.03467_2246458_8	In particular, we are concerned that system boundaries are not broad enough, that the tolerability and nature of the risks are not sufficiently elaborated, and that the assurance methods lack theories that would allow behaviours to be adequately assured.   
2502.04322_2247313_0	  Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior.
2502.05007_2247998_1	We propose a number of metrics for examining whether an advanced AI system has gained consciousness, while emphasizing that we do not claim all AI stems can become conscious.
2502.05605_2248596_0	  A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions.
2502.06065_2249056_0	  Large language Models (LLMs) are highly sensitive to variations in prompt formulation, which can significantly impact their ability to generate accurate responses.
2502.07072_2250063_1	LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity.
2502.07365_2250356_4	Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts.
2502.07557_2250548_0	  Despite the implementation of safety alignment strategies, large language models (LLMs) remain vulnerable to jailbreak attacks, which undermine these safety guardrails and pose significant security threats.
2502.07832_2250823_6	Furthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices.
2502.08142_2251133_3	Results show that our unsafe content detection model in Safety Detector achieves comparable performance with OpenAI API, though trained on a small dataset constructed with several public datasets.
2502.08182_2251173_3	This paper presents Select-N, a latency-SLO-aware memory offloading system for LLM serving.
2502.09175_2252166_1	While LLMs demonstrate remarkable capabilities, they remain vulnerable to adversarial attacks, particularly ``jailbreaking'' techniques that bypass content safety measures.
2502.09818_2252809_4	Our findings reveal that most-of-the-art VLMs, including GPT-4, are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions.
2502.10673_2253664_1	However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization.
2502.11533_2254524_2	This paper highlights an overlooked privacy risk: \textit{an unsafe model could compromise the privacy of other LLMs involved in the model merging.}
2502.11647_2254638_4	To avoid triggering a safe response in benign context, we incorporate KL-divergence regularization to ensure the updated model remains consistent with the original model when processing benign queries.
2502.13062_2256053_11	As the algorithm becomes more "patient" or the human's learning improves, the algorithm increasingly selects more informative features, enhancing both prediction accuracy and the human's understanding.
2502.13107_2256098_3	In this work, we introduce MatterChat, a versatile structure-aware multi-modal LLM that unifies material structural data and textual inputs into a single cohesive model.
2502.13141_2256132_0	  Large Language Models (LLMs) are vulnerable to attacks like prompt injection, backdoor attacks, and adversarial attacks, which manipulate prompts or models to generate harmful outputs.
2502.13533_2256524_4	LoRAM presents a unique twist: it trains on a pruned (small) model to obtain pruned low-rank matrices, which are then recovered and utilized with the original (large) model for inference.
2502.13632_2256623_6	We evaluate CLs across multiple tasks, demonstrating that they maintain the original model's performance and agreement while enabling meaningful interventions.
2502.13946_2256937_0	  The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks.
2502.14074_2257065_4	We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model.
2502.14592_2257583_5	We find that, while edtech providers focus primarily on mitigating technical harms, i.e., those that can be measured based solely on LLM outputs themselves, educators are more concerned about harms that result from the broader impacts of LLMs, i.e., those that require observation of interactions between students, educators, school systems, and edtech to measure.
2502.14847_2257838_5	Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.
2502.14908_2257899_3	Contrary to prior findings that showed that LLMs are sensitive to parametric conflicts arising from textual perturbations, we find VLMs are largely robust to image perturbation.
2502.15963_2258954_2	As actors in a social system, software developers are accountable to their team and organization for their decisions.
2502.16094_2259085_4	First, the attacker fine-tunes a malicious model to force it to respond to any PII-related queries.
2502.16094_2259085_5	The attacker then uploads this malicious model to the model merging conductor and obtains the merged model.
2502.16593_2259584_2	In this paper, we propose a novel method called Parameter Learning Attack (PLA) for tracking the copyright of LVLMs without modifying the original model.
2502.16593_2259584_3	Specifically, we construct adversarial images through targeted attacks against the original model, enabling it to generate specific outputs.
2502.16593_2259584_4	To ensure these attacks remain effective on potential fine-tuned models to trigger copyright tracking, we allow the original model to learn the trigger images by updating parameters in the opposite direction during the adversarial attack process.
2502.16593_2259584_5	Notably, the proposed method can be applied after the release of the original model, thus not affecting the model's performance and behavior.
2502.16593_2259584_6	To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models for copyright verification.
2502.16691_2259682_3	In FedLLM, client data used for training may contain harmful content, leading to unsafe LLMs that generate harmful responses.
2502.16794_2259785_7	By taking a first step toward intention-aware auditory AI, this work explores a new paradigm where listener perception informs machine listening, paving the way for future listener-centered auditory systems.
2502.16897_2259888_4	This strategy mitigates the modality gap between text and speech, preserving the linguistic reasoning of the original model while enabling high-fidelity speech synthesis.
2502.16963_2259954_4	This work introduces Hermes, a budget-friendly system that leverages the near-data processing (NDP) within commodity DRAM DIMMs to enhance the performance of a single consumer-grade GPU, achieving efficient LLM inference.
2502.17282_2260273_1	The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at.
2502.17535_2260526_3	Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and task, there exists a smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools.
2502.18080_2261071_2	While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance?
2502.18608_2261599_1	However, there are growing concerns that the current LLM watermarking schemes are vulnerable to expert adversaries wishing to reverse-engineer the watermarking mechanisms.
2502.18863_2261854_3	To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like Sherlock Holmes to track down the criminal events, for this M-VAE task.
2502.19008_2261999_4	PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters.
2502.19041_2262032_1	Although Aligned Large Language Models (LLMs) are trained to refuse harmful requests, they remain vulnerable to jailbreak attacks.
2502.19410_2262401_9	Together with users' interview feedback, the results led to design implications to be mindful of when personalizing the content and timing of LLM explanations that are displayed on ultra-small devices.
2502.19573_2262564_2	A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge.
2502.19928_2262919_3	Such an unauthorized RIS can cause unexpected interference in the original localization system, distorting the transmitted signals, and leading to degraded positioning accuracy.
2502.20573_2263564_2	In this proposed method, GPT-4o acts as intelligent system to detect conflicts and provide explanations and recommendations for the drivers.
2502.21262_2264253_1	Yet as AI systems grow more capable, human feedback becomes increasingly unreliable.
2503.00513_2264825_3	To address these challenges, we propose a unified Instance-aware 3D Large Multi-modal Model (Inst3D-LMM) to deal with multiple 3D scene understanding tasks simultaneously.
2503.01496_2265808_7	Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs.
2503.01547_2265859_1	This study focuses on developing an intelligent algorithm which can navigate a robot through a kitchen, recognizing objects, and tracking their relocation.
2503.01622_2265934_1	Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more.
2503.03592_2267904_0	  For consumer usage of locally deployed LLMs, the GGUF format and k\_quantization are invaluable tools for maintaining the performance of the original model while reducing it to sizes deployable with consumer-grade hardware.
2503.03592_2267904_1	The number of bits dedicated to each weight from the original model is reduced based on how important they are thought to be during model inference.
2503.03710_2268022_0	  Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks.
2503.04103_2268415_6	User evaluation shows that such an environment allowed users to stay oriented in their creation activity, remain aware and in control of AI's generation, and enable flexible human-AI collaborative workflows.
2503.05021_2269333_0	  Large Language Models (LLMs) are vulnerable to jailbreak attacks that exploit weaknesses in traditional safety alignment, which often relies on rigid refusal heuristics or representation engineering to block harmful outputs.
2503.05665_2269977_7	To address the limitation of synthetic data quality, we propose Contextual Synthetic Data Generation (CSDG) to generate data using a text-to-image diffusion model (T2I) with prompts generated by a context-aware LLM, ensuring both data diversity and control of bias in synthetic data.
2503.05977_2270289_4	However, this approach can be unreliable or biased because such a model may lack the ability to fully understand the content and may have inherent biases, ultimately compromising evaluation reliability.
2503.06011_2270323_2	LLMs are sensitive to contextual ambiguities and inconsistencies; therefore, explicitly communicating their intentions during interactions when applying Self-Correction for debiasing is crucial.
2503.07096_2271408_4	Using this logical information as guidance, we establish a correctness judgment and feedback mechanism to steer the intelligent decision model toward the 'correctness pattern' reflected in historical high-quality schemes.
2503.07103_2271415_7	Our empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance.
2503.07556_2271868_12	Critically, they are not just experiencing the benefits of adopting LLM tools, but they are also aware of at least a few LLM limitations, such as the generation of wrong suggestions, potential data leaking, and AI hallucination.
2503.07657_2271969_6	Our evaluation on the Llama 3.2 1B Instruct model using the AI2's Reasoning Challenge (ARC) dataset demonstrates that SplitQuantV2 improves the accuracy of the INT4 quantization model by 11.76%p, matching the performance of the original floating-point model.
2503.08102_2272414_5	ME acts as an intelligent, persistent memory offload system that retains, organizes, and dynamically utilizes user-specific knowledge.
2503.09964_2274276_0	  Large Multimodal Models (LMMs) are increasingly vulnerable to AI-generated extremist content, including photorealistic images and text, which can be used to bypass safety mechanisms and generate harmful outputs.
2503.10944_2275256_3	We outline our LoRA-based fine-tuning process, describe the balanced dataset comprising phishing and benign emails, and highlight significant performance improvements over the original model.
2503.11232_2275544_4	Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron dataset, shows that PrivacyScalpel significantly reduces email leakage from 5.15\% to as low as 0.0\%, while maintaining over 99.4\% of the original model's utility.
2503.11402_2275714_11	We show that the model trained on the cleaned dataset exhibits similar performance in terms of functional correctness as compared to the original model while, however, generating a statistically significant lower number of low-quality functions (2.16%).
2503.12479_2276791_2	To address these challenges, we introduce Sakshm AI, an intelligent tutoring system for learners across all education levels.
2503.15560_2279872_0	  Large Language Models (LLMs) are increasingly vulnerable to sophisticated multi-turn manipulation attacks, where adversaries strategically build context through seemingly benign conversational turns to circumvent safety measures and elicit harmful or unauthorized responses.
2503.15888_2280200_1	However, conflicts between parametric knowledge and retrieved context pose challenges, particularly when retrieved information is unreliable or the model's internal knowledge is outdated.
2503.16460_2280772_5	The first approach uses an intelligent tutoring system for college algebra as a testbed to assess LLM problem-solving capabilities.
2503.17378_2281690_4	Furthermore, we note the increase in self-replication capability when the model becomes more intelligent in general.
2503.17932_2282244_0	  Large Language Models (LLMs) have become increasingly vulnerable to jailbreak attacks that circumvent their safety mechanisms.
2503.18002_2282314_3	Our hardware-aware quantized model on GPU demonstrates that a 370M parameter MatMul-free model can be quantized with no accuracy loss.
2503.18778_2283090_3	It ensures safe handling of patient cases and potentially reduces clinician review time, whilst being mindful of AI tool limitations.
2503.19050_2283362_3	To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism.
2503.20088_2284400_3	Chesi ties the fate of generative linguistics to its intellectual merits, but the current success of language model research is social in nature as much as it is intellectual.
2503.22074_2286386_3	Each block is then compacted via spectral transformations (e.g., discrete cosine or Fourier transforms), and a Kullback-Leibler (KL) divergence-based alignment loss preserves the distributional similarity between the compressed model's representations and those of the original full model.
2503.22092_2286404_5	(2) How sensitive is majority voting accuracy to LLM hyperparameters such as temperature, top-p, and summary length?
2503.23021_2287333_3	This study aims to determine whether performance of downstream tasks is sensitive to the tissue detection method, and to compare performance of classical and AI-based tissue detection.
2504.00446_2289149_0	  The widespread adoption of Large Language Models (LLMs) in critical applications has introduced severe reliability and security risks, as LLMs remain vulnerable to notorious threats such as hallucinations, jailbreak attacks, and backdoor exploits.
2504.01094_2289797_3	Crucially, our work further reveals that multimodal LLMs are inherently more vulnerable than unimodal systems: attackers need only exploit the weakest link (e.g., non-English audio inputs) to compromise the entire model, which we empirically show by multilingual audio-only attacks achieving 3.1x higher success rates than text-only attacks.
2504.01404_2290107_3	Recently, a deep learning-based SZZ algorithm has been introduced to enhance the original SZZ algorithm.
2504.01550_2290253_2	Existing safety-enhancing techniques, such as fine-tuning with human feedback or adversarial training, are still vulnerable as they address specific threats and often fail to generalize across unseen attacks, or require manual system-level defenses.
2504.02526_2291229_2	This paper introduces our initial design of a Framework for designing AI Communication (FAICO) for co-creative AI based on a systematic review of 107 full-length papers.
2504.02733_2291436_0	  Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output.
2504.03174_2291877_5	Through extensive experiments on different languages, we show the studied LLMs are on average 71\% more vulnerable after a 5-turn conversation in English than after the initial turn.
2504.03271_2291974_4	To accomplish this, an energy-aware multicopter model is integrated into a path planning algorithm based on model predictive control and mixed integer linear programming.
2504.03726_2292429_5	In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.
2504.05321_2294024_7	To address these challenges, we propose VALUE(Value-Aware Large language model for qUery rewriting via wEighted trie), the first framework that ensures the generation of high-value and highly relevant bidwords.
2504.06279_2294982_3	To address these challenges, this paper presents an intelligent financial data analysis system that integrates Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology.
2504.06575_2295278_5	To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning.
2504.07887_2296590_3	Despite mitigation efforts, recent studies indicate that LLMs remain vulnerable to adversarial attacks designed to elicit biased responses.
2504.08500_2297203_2	This work presents a wearable smart sportswear system that integrates screen-printed graphene-based strain sensors with a wireless deep learning framework for real-time classification of exercise execution quality.
2504.08812_2297515_1	Supervision failures can occur if LLMs are sensitive to factors that supervisors are unaware of.
2504.09255_2297958_0	  Face video quality assessment (FVQA) deserves to be explored in addition to general video quality assessment (VQA), as face videos are the primary content on social media platforms and human visual system (HVS) is particularly sensitive to human faces.
2504.09466_2298169_0	  Despite extensive efforts in safety alignment, large language models (LLMs) remain vulnerable to jailbreak attacks.
2106.13901_1491665_4	For example, AI researchers who design and develop AI models are not necessarily aware of the instability induced in consumers' lives by the compounded effects of AI decisions.
2110.11168_1549311_4	Our analysis proposes that metrics to measure the kind of explainability endorsed by the proposed AI Act shall be risk-focused, model-agnostic, goal-aware, intelligible & accessible.
2111.09509_1563612_6	We also perform extensive manual analysis showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).
2301.05397_1776461_1	As these models now get increasingly better and convincingly more anthropomorphic, even some engineers have started to believe that AI might become conscious, which would result in serious social consequences.
2301.13637_1784701_2	However, AI-experts are aware of the limitations of current DNNs and have been working towards the fourth AI wave which will, arguably, rely on more biologically inspired models, predominantly on spiking neural networks (SNNs).
2301.13852_1784916_11	Using explainability, we observe that ChatGPT's writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.
2303.05398_1805195_2	To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption.
2303.10475_1810272_2	Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system.
2303.13712_1813509_7	We then consider a decision maker who is aware of the algorithm's manipulation and responds strategically.
2303.16421_1816218_3	(2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question?
2304.09655_1827701_7	Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.
2304.11771_1829817_5	Finally, we provide evidence that AI assistance improves the experience of work along two key dimensions: customers are more polite and less likely to ask to speak to a manager.
2305.17006_1850062_3	In this way, the captioning model can become aware of the task goal and information need from the PLM.
2306.01798_1854946_5	The study's findings suggest that teachers should be aware of students' purposes for prompting generative-AI tools to provide tailored instructions and scaffolded guidance.
2401.01623_1981179_2	In this paper, we prove in theory that AI can be as creative as humans under the condition that it can properly fit the data generated by human creators.
2405.06715_2063927_2	However, whether the same strategies can help LLMs become more creative remains under-explored.
2407.09517_2107904_4	Our assessment is that, while GPT-4 in its native configuration is not currently conscious, current technological research and development is sufficient to modify GPT-4 to have all the building blocks of consciousness.
2407.11789_2110176_2	We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.
2408.01168_2121349_3	The paper argues that current LLM architectures are inherently untrustworthy due to their reliance on correlations of sequential patterns of word embedding vectors.
2501.05454_2229038_1	Today's AI systems consistently state, "I am not conscious."
2504.12320_2301023_1	However, it remains unclear whether LLMs have become more creative over time, and how consistent their creative output is.
2504.17393_2306096_7	Furthermore, user-friendly human interaction with the system is essential for its adoption and some of the participants confirmed they would be happy to be in the loop and provide necessary feedback that the system can learn from.
2504.19674_2308377_6	It utilizes adversarial user models that are system-aware and have unique personalities, enabling a holistic red-teaming evaluation.
2504.20980_2309683_6	It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''
2505.08204_2318764_2	However, if the LM integration is insecure, attackers can bypass these restrictions and gain unrestricted access to the LM, potentially harming developers' reputations and leading to significant financial losses.