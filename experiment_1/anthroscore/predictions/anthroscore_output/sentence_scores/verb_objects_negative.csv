,sentence,masked_sentence,text_id,POS,verb,original_term,original_noun,anthroscore
0,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train <mask> using labeled and unlabeled examples.,2_arx_1910.06294_1190213_4,416,train,model,a fast and compact model,-4.415946561299641
1,"As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a low complexity algorithm.","As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by <mask>.",2_arx_2006.05347_1299861_5,439,by,algorithm,a low complexity algorithm,-1.988000611394785
2,We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.,We introduce <mask> that uses spatial and object features and is powered by the BERT language model.,2_arx_2007.00900_1312414_5,416,introduce,system,an explainable vqa system,-4.856427372014613
3,We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.,We introduce an explainable VQA system that uses spatial and object features and is powered by <mask>.,2_arx_2007.00900_1312414_5,439,by,model,the bert language model,-5.391540133038928
4,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by <mask> and those manually annotated by humans.,2_acl_7_34206_4,439,by,GPT-4,gpt-4,-2.5462636028413606
5,We consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.,We consider a UAS that can be fully controlled by <mask> and by a remote human pilot.,2_arx_2103.07820_1437965_2,439,by,system,the onboard daa system,-1.9747190469979383
6,We observed 95.61% alarms raised by the said system are taken care of by the operator.,We observed 95.61% alarms raised by <mask> are taken care of by the operator.,2_arx_1811.12185_1056818_9,439,by,system,the said system,-1.5989440716932073
7,"Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.","Moreover, <mask> can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.",2_arx_1909.09993_1180031_2,430,use,model,the a2c model,-4.255504353755546
8,"Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.","Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by <mask>, but this requires accurate detection of OOV words.",2_arx_1909.09993_1180031_2,439,by,model,the a2w model,-4.14336536502474
9,"By comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.","By comparing the measurements with the results predicted by <mask> for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.",2_arx_1912.06835_1218731_4,439,by,model,the ion flow model,-2.755593198361609
10,The two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.,The two data-driven approaches are trained using data samples generated by <mask> via supervised learning.,2_arx_2102.07384_1423873_5,439,by,algorithm,the bcd algorithm,-1.1296472231186776
11,"The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.","The fuzzy trajectory data are developed based on different driving styles, which are clustered by <mask>.",2_arx_2205.05016_1649428_3,439,by,algorithm,the k-means algorithm,-1.8418212576905084
12,"For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?","For example, we ask, given <mask> that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?",2_acl_280_28416_2,439,give,method,a debiasing method,-1.5055892146179062
13,"For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?","For example, we ask, given a debiasing method that is developed to reduce toxicity in <mask>, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?",2_acl_280_28416_2,439,in,LMs,lms,-3.1055697710050545
14,"For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?","For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by <mask> is reversed, would the debiasing results also be reversed?",2_acl_280_28416_2,439,by,method,the debiasing method,-1.3670697391354505
15,"Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.","Taken together, these findings support the idea that meaning construction is supported by <mask> based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.",2_acl_4_33862_8,439,by,system,a flexible form-to-meaning mapping system,-5.472453462236306
16,"We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.","We find that synthetic data generated by <mask> is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.",2_acl_6_42908_8,439,by,LLMs,llms,-1.4302519342828433
17,"  Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.","  Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by <mask> and the code evaluator provides code performance metric measurements.",2_arx_1901.05719_1075266_4,439,by,AI,ai algorithms,-2.6039497855252325
18,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by <mask>.,2_arx_1902.02508_1083592_0,439,by,model,a sgs model,-2.861043484692665
19,CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,CNR is trained with data created by <mask> of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,2_arx_2002.05702_1243125_4,439,by,model,a generative model,-3.7015701929992773
20,"Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.","Ultimately, our results from comparisons of LVM segmentation predicted by <mask> locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.",2_arx_2009.12437_1353940_2,439,by,model,a model,-3.2850578293224313
21,"Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.","Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that <mask> initiated by TL can be developed with sparse labels with acceptable performance.",2_arx_2009.12437_1353940_2,430,develop,model,a use-case model,-5.086350951526791
22,Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,Specifically we will overview the criteria that should be met by <mask> before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,2_arx_2106.02498_1480262_5,439,by,system,an ai system,-3.5571141475054446
23,Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,Specifically we will overview the criteria that should be met by <mask> before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,2_arx_2106.02498_1480262_5,439,by,AI,an ai system,-3.5571141475054446
24,"Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.","Considering the natural ventilation, the thermal behavior of buildings can be described by <mask>.",2_arx_1212.5593_395209_0,439,by,model,a linear time varying model,-1.9885820006547785
25,"Then, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.","Then, a multilayer perceptron is trained by <mask> (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.",2_arx_1401.5941_495172_4,439,by,algorithm,a backpropagation algorithm,-3.5447560075632527
26,"Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.","Both the emitted power and its angular pattern are well described by <mask>, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.",2_arx_1408.5886_551192_2,439,by,model,a model,-1.5506686940386416
27,"In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.","In this study, UAV dynamics are described by <mask>.",2_arx_1501.07576_594324_2,439,by,model,a three-dimensional dynamic point-mass model,-0.3489078737803837
28,The synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,The synthetic PM10 record predicted by <mask> was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,2_arx_1610.02937_778284_8,439,by,model,the model,-1.3468936488945342
29,"Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Inspired by <mask>, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.",2_arx_2304.11116_1829162_4,439,by,ChatGPT,the latest chatgpt and toolformer models,-3.0934439159489973
30,"Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach <mask> themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.",2_arx_2304.11116_1829162_4,416,teach,LLMs,llms,-4.570551328696528
31,"Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Inspired by the latest <mask> and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by <mask> to use external graph reasoning API tools.",2_arx_2304.11116_1829162_4,439,by,ChatGPT,chatgpt,-1.303555232818134
32,"Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.","Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by <mask>.",2_arx_2110.14419_1552562_1,439,by,AI,ai,-6.510020891665961
33,"The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.","The results indicate that biases equating American identity with being White are learned by <mask>, and propagate to downstream applications of such models.",2_arx_2207.00691_1676589_8,439,by,AI,language-and-image ai,-3.632870692169483
34,"Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.","Through examining the weights in <mask>, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.",2_arx_1812.01714_1059288_3,439,in,model,the trained dcnn model,-1.4918103939578788
35,"Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.","Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by <mask>.",2_arx_1812.01714_1059288_3,439,by,model,our model,-2.850064120652183
36,"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages <mask> to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.",2_arx_2412.12865_2215243_5,416,encourage,model,the target model,-2.733427746323139
37,"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages the target model to predict a higher likelihood than that predicted by <mask>, incorporating assessment information on data quality (i.e., predicted likelihood by <mask>) into the training process.",2_arx_2412.12865_2215243_5,439,by,LLMs,the aligned llms,-4.79601857749353
38,"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages the target model to predict a higher likelihood than that predicted by <mask>, incorporating assessment information on data quality (i.e., predicted likelihood by <mask>) into the training process.",2_arx_2412.12865_2215243_5,439,by,LLMs,the aligned llms,-4.79601857749353
39,"Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.","Therefore, determining whether a text was generated by <mask> has become one of the factors that must be considered when evaluating its reliability.",2_acl_19_45086_1,439,by,LLM,an llm,-2.1390517862206018
40,We find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.,We find that the observed behaviour is explained by <mask> including the effects associated with the variations of pressure and density.,2_arx_0906.5497_132048_3,439,by,model,a model,-3.162676908909006
41,"For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.","For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by <mask>.",2_arx_2002.10965_1248388_3,439,by,algorithm,a low-complexity trellis-based algorithm,-2.046626582119904
42,"We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).","We present a large-scale study of linguistic bias exhibited by <mask> covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).",2_acl_750_35597_0,439,by,ChatGPT,chatgpt,-1.5870489439494975
43,"We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.","We find that <mask> obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.",2_acl_502_20526_4,429,obtain,LMs,4 pretrained transformers lms,-2.6307933444268308
44,"We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.","We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by <mask>.",2_acl_502_20526_4,439,by,LM,the lm,-3.556823836460662
45,"By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.","By applying our experimental pipeline to <mask> trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.",2_arx_2105.13818_1476377_2,439,to,LMs,lms,-2.5538009835619526
46,"However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.","However, these improvements can be attributed to the use of <mask>, which is typically trained on large amounts of parallel data not seen by the language model.",2_acl_46_41679_2,439,of,system,a separate translation system,-4.099684310759434
47,"However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.","However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by <mask>.",2_acl_46_41679_2,439,by,model,the language model,-3.0312065596318067
48,"In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.","In order to train <mask> efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.",3_arx_1712.09783_928382_2,416,train,model,the moe model,-2.714345577840973
49,"In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.","In order to train the MoE model efficiently, <mask> is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.",3_arx_1712.09783_928382_2,430,apply,method,a matrix factorization method,-2.891715183463731
50,We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,We believe the discussions would provide a broader perspective of looking at <mask> through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,3_acl_6_25720_4,439,at,LLMs,llms,-6.403759683778013
51,We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire <mask>.,3_acl_6_25720_4,416,inspire,LLM,future llm research,-3.0894062249273233
52,"We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.","We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train <mask>, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.",3_arx_2101.04617_1408359_3,416,train,model,a named entity recognition model,-2.7925817690248476
53,"We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.","We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ <mask> to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.",3_arx_2101.04617_1408359_3,416,employ,model,the trained model,-7.800319512551587
54,"Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","Many companies that deploy <mask> that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.",3_arx_2101.05967_1409709_1,416,deploy,AI,ai publicly state,-1.9053528477284747
55,"Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","Many companies that deploy AI publicly state that when training <mask>, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.",3_arx_2101.05967_1409709_1,416,train,model,a model,-5.78133950856879
56,"Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that <mask> does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.",3_arx_2101.05967_1409709_1,429,discriminate,model,the model,-6.814536049308419
57,"We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.","We first pre-train <mask> on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.",3_acl_118_9850_4,416,train,model,our model,-1.2401288052527288
58,"We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.","We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune <mask> with a human-labeled dataset.",3_acl_118_9850_4,403,we,model,the model,-3.611246093382805
59,We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.,We evaluated <mask> on the German to English and English to French translation task of TED lectures.,3_acl_3_2185_5,416,evaluate,model,the rbm-based language model,-3.1537055251530415
60,"Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.","Moreover, since <mask> shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.",3_arx_1911.03597_1202191_5,429,share,model,our model,-3.1252643247281515
61,"Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.","Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train <mask> on large-scale unparallel corpus, which further improves the fluency of the output sentences.",3_arx_1911.03597_1202191_5,416,train,model,the model,-3.68158444959815
62,We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.,We ask whether structural information can be extracted from <mask>’s and develop a model that integrates it with their learnt statistics.,3_acl_329_43922_3,439,from,LLM,llm,0.14757589175405883
63,We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.,We ask whether structural information can be extracted from LLM’s and develop <mask> that integrates it with their learnt statistics.,3_acl_329_43922_3,416,develop,model,a model,-4.670134986687687
64,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.,The goal is to merge the strengths of <mask> and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.,3_arx_2302.07257_1792191_6,439,of,LLMs,llms' medical domain knowledge,-1.969302355578769
65,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create <mask> for patients compared to conventional CAD systems.,3_arx_2302.07257_1792191_6,416,create,system,a more user-friendly and understandable system,-3.489037808442948
66,"Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.","Finally, we systematically evaluate and analyze <mask> and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.",3_arx_301_32463_8,416,analyze,LLMs,eight mainstream llms,-3.0713037405464547
67,"Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.","Finally, we systematically evaluate and analyze eight mainstream <mask> and demonstrate the superior breadth and challenges of CodeScope for evaluating <mask> on code understanding and generation tasks compared to other benchmarks.",3_arx_301_32463_8,416,evaluate,LLMs,llms,-1.2930008864055171
68,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate <mask>, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.",3_acl_5_42629_6,416,evaluate,LLMs,recent multilingual llms,-2.417867848485983
69,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate recent multilingual LLMs, including <mask>, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.",3_acl_5_42629_6,439,include,GPT-4,gpt-4,-0.9981617339986553
70,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, <mask>, and Turkcell-LLM, using this dataset.",3_acl_5_42629_6,410,aya,LLM,trendyol-llm,-0.4305306527538182
71,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and <mask>, using this dataset.",3_acl_5_42629_6,410,aya,LLM,turkcell-llm,-0.7134289114887373
72,"We design a system using these pre-trained models to answer questions, based on the given context.","We design <mask> using these pre-trained models to answer questions, based on the given context.",3_acl_28_45074_3,416,design,system,a system,-4.611908755282867
73,"Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.","Unlike traditional methods that often simplify multi-agent interactions using <mask>, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.",3_acl_41_45371_2,416,use,model,a single opponent model,-4.001578518500848
74,"Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.","Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs <mask> for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.",3_acl_41_45371_2,416,construct,model,an individual model,-3.5920372365990403
75,"We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.","We also develop <mask> that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.",3_acl_104_45430_3,416,develop,algorithm,a search algorithm,-3.5515157144793186
76,"Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.","Using this approach, we train and present <mask> trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.",3_acl_6_46417_4,416,present,model,a bert-based model,-2.6873171955283226
77,"In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.","In evaluations of ranking character predictions, training <mask> on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.",3_acl_7_60414_2,416,train,LMs,recurrent lms,-3.7726164086546383
78,"In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.","In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when <mask> is misspecified.",3_acl_7_60414_2,430,misspecifie,model,the error model,-4.274815347063555
79,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train <mask> using labeled and unlabeled examples.,3_arx_1910.06294_1190213_4,416,train,model,a fast and compact model,-4.415946561299641
80,"Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.","Thus, it is important to leverage memorized knowledge in <mask> for building the seq2seq model, since it is hard to prepare a large amount of paired data.",3_acl_1_7090_2,439,in,LM,the external lm,-3.532173878743283
81,"Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.","Thus, it is important to leverage memorized knowledge in the external LM for building <mask>, since it is hard to prepare a large amount of paired data.",3_acl_1_7090_2,416,build,model,the seq2seq model,-4.46467250447135
82,"Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.","Using the TREC Misinformation dataset, we empirically evaluate <mask> to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.",3_acl_928_27649_2,416,evaluate,ChatGPT,chatgpt,-6.432698175227811
83,"Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.","Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias <mask> to the detriment of answer correctness.",3_acl_928_27649_2,416,bias,model,the model,-3.1748777929832066
84,"In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.","In the present work, we develop <mask> with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.",3_acl_582_29618_3,416,develop,model,a recurrent neural language model,-2.5070021841578587
85,"In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.","In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels <mask> assumed by cognitive theories.",3_acl_582_29618_3,416,parallel,system,the memory system,-4.634638109919273
86,"These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.","These findings highlight the challenges of developing <mask> for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.",3_acl_418_44990_5,416,develop,AI,ai,-1.7456001694636534
87,Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.,Our approach employs a pool of candidate VPs and trains <mask> to dynamically select the most effective VP for a given input image.,3_acl_45_45963_3,416,train,model,a router model,-5.397529616836936
88,We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.,We have gathered a substantial amount of Chinese corpus to train <mask> and have also optimized its structure.,3_arx_2308.00624_1888516_6,416,train,model,the model,-2.3312499900943884
89,"However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.","However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by <mask> during training.",2_acl_75_14800_1,439,by,model,the model,-1.3914197682256706
90,"Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.","Human raters were asked to rate the explanation of the implicatures generated by <mask> on their reasonability, logic and fluency.",2_acl_98_33764_7,439,by,LLMs,llms,-2.202761036017135
91,"We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.","We posit that multiple solutions to a reasoning task, generated by <mask>, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.",2_acl_1_42012_5,439,by,LLM,an llm,-4.315902723842822
92,"In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs.","In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by <mask>.",2_acl_600_45881_2,439,by,LLMs,llms,-2.8671448652703013
93,"Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.","Specifically, we trained <mask> with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.",3_arx_2306.02920_1856068_2,416,train,LMs,bilingual lms,-1.6407584857544073
