,sentence,masked_sentence,text_id,POS,verb,original_term,original_noun,anthroscore
0,"We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.","We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether <mask> can determine when to request help under varying information availability.",1_acl_131_34995_1,429,determine,LLMs,llms,-2.8949017023824375
1,"In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.","In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that <mask> must understand to complete the task.",1_acl_7_26333_1,429,understand,model,the model,-0.7561281148717267
2,• Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?,• <mask> recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?,1_acl_1_13930_4,429,recognize,LMs,do lms,-3.3444013284890666
3,"The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","The increased deployment of <mask> for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what <mask> think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.",1_acl_335_27056_0,439,of,LMs,lms,-4.150182644768272
4,"The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand <mask>: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.",1_acl_335_27056_0,416,understand,model,model epistemology,-5.0871782654608015
5,"The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","The increased deployment of <mask> for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what <mask> think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.",1_acl_335_27056_0,429,think,LMs,lms,-4.150182644768272
6,We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.,We propose a framework to analyze what <mask> can infer about new entities that did not exist when the <mask> were pretrained.,1_acl_52_19823_2,429,infer,LMs,lms,-2.0987432851646233
7,We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.,We propose a framework to analyze what LMs can infer about new entities that did not exist when <mask> were pretrained.,1_acl_52_19823_2,430,pretraine,LMs,the lms,-2.2827759561281944
8,"These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.","These two sources can disagree, causing competition within <mask>, and it is unclear how an LM will resolve the conflict.",1_acl_615_27336_1,439,within,model,the model,-3.6915822936541662
9,"These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.","These two sources can disagree, causing competition within the model, and it is unclear how <mask> will resolve the conflict.",1_acl_615_27336_1,429,resolve,LM,an lm,-2.633285606412329
10,We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.,We then propose <mask> that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.,1_acl_12_42096_5,416,propose,system,a system,-4.620257955191466
11,We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.,We then propose a system that leverages the recently introduced social learning paradigm in which <mask> collaboratively learn from each other by exchanging natural language.,1_acl_12_42096_5,429,learn,LLMs,llms,-3.8433293588595063
12,"Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.","Through extensive experiments on various datasets, <mask> can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior <mask>.",1_acl_508_29544_3,429,collaborate,LLMs,llms,-3.074851889242172
13,"Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.","Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by <mask>.",1_acl_508_29544_3,439,by,LLMs,superior llms,-4.189842936632134
14,"But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.","But language models (<mask>) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.",1_arx_2106.15110_1492874_1,403,model,LMs,lms,0.15191991903024515
15,"But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.","But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts <mask> should memorize.",1_arx_2106.15110_1492874_1,429,memorize,model,the model,-1.2937487534290888
16,"That is, the LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.","That is, <mask> only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.",1_acl_263_41409_3,429,remember,LLM,the llm,-5.785502248392513
17,"During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it.","During the test stage, given a test question, <mask> recalls relevant memory to help itself reason and answer it.",1_acl_392_27113_5,429,recall,LLM,the llm,-4.11777149751004
18,And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results.,And the human evaluations show <mask> can truly understand the provided information and generate clearer and more reasonable results.,1_arx_2304.10149_1828195_10,429,understand,ChatGPT,chatgpt,-1.9089412434860726
19,"In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.","In our first experiment, we find that <mask> decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.",1_arx_2212.13371_1769548_6,429,decide,agent,the ai agent,-2.1582328030376043
20,"In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.","In our first experiment, we find that <mask> decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.",1_arx_2212.13371_1769548_6,429,decide,AI,the ai agent,-2.1582328030376043
21,"As a result, it can be hard to identify what the model actually ""believes"" about the world, making it susceptible to inconsistent behavior and simple errors.","As a result, it can be hard to identify what <mask> actually ""believes"" about the world, making it susceptible to inconsistent behavior and simple errors.",1_arx_2109.14723_1537544_1,429,believe,model,the model,-2.8806023500547813
22,"In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions.","In order to effectively collaborate with humans and ensure safety, <mask> need to be able to understand, interpret and predict human moral judgments and decisions.",1_arx_2210.01478_1722833_1,429,need,AI,ai systems,-4.391190286749591
23,I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.,I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- <mask> infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.,1_arx_2212.01681_1757858_6,429,infer,LMs,lms,-3.8866089390365595
24,What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?,What does <mask> remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?,1_arx_2303.17557_1817354_1,429,remember,model,a model,1.8549851791525764
25,"In order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.","In order to engender trust in <mask>, humans must understand what an <mask> system is trying to achieve, and why.",1_arx_1810.06338_1037525_0,439,in,AI,ai,-5.507900507767459
26,"In order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.","In order to engender trust in AI, humans must understand what <mask> is trying to achieve, and why.",1_arx_1810.06338_1037525_0,429,try,system,an ai system,-4.7662671292317835
27,"In order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.","In order to engender trust in AI, humans must understand what <mask> is trying to achieve, and why.",1_arx_1810.06338_1037525_0,429,try,AI,an ai system,-4.7662671292317835
28,"Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items.","Here, we fail to find convincing evidence that <mask> is reasoning about more than just individual lexical items.",1_arx_2210.09492_1730847_6,429,reason,GPT-3,gpt-3,1.0399332496982687
29,"In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.","In this paper, we show that <mask> can teach themselves to use external tools via simple APIs and achieve the best of both worlds.",1_arx_2302.04761_1789695_2,429,teach,LMs,lms,-2.661503483948927
30,"By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.","By performing a backward verification of the answers that <mask> deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.",1_arx_2212.09561_1765738_6,429,deduce,LLM,llm,-3.5183890908561715
31,"To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.","To measure whether <mask> prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.",1_acl_322_28458_1,429,prefer,LLM,an llm,-3.7067153309373566
32,"In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question.","In our method, <mask> explicitly asks itself (and then answers) follow-up questions before answering the initial question.",1_acl_378_29414_7,429,ask,model,the model,-4.8567196980840315
33,"Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.","Surprisingly, we find that <mask> as an evaluator cannot distinguish between clearly wrong <mask> completions and Chemcrow's performance.",1_arx_2304.05376_1823422_8,429,distinguish,GPT-4,gpt-4,2.1966171011899895
34,"Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.","Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between <mask> and Chemcrow's performance.",1_arx_2304.05376_1823422_8,439,between,GPT-4,clearly wrong gpt-4 completions,0.4008219777477944
35,Our evaluation shows that the model can create French poetry successfully.,Our evaluation shows that <mask> can create French poetry successfully.,1_arx_2212.02911_1759088_4,429,create,model,the model,0.5810040037234234
36,"First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.","First, <mask> sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.",1_arx_2303.18027_1817824_5,429,select,LLMs,llms,2.417234463251676
37,"However, a large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.","However, <mask> trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.",1_arx_2304.09048_1827094_1,429,demonstrate,model,a large generative language model,-2.5503604654117584
38,"• Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?","• Do <mask> know idioms, and can they infer the meaning of new idioms from the context as humans often do?",1_acl_1_13930_5,429,know,LMs,lms,-2.5770410732746427
39,"However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.","However, it is unclear whether <mask> perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.",1_acl_299_27020_1,429,perform,LMs,lms,-1.4992090574454018
40,We only conduct retrieval for the missing knowledge in questions that the LLM does not know.,We only conduct retrieval for the missing knowledge in questions that <mask> does not know.,1_acl_242_32404_6,429,know,LLM,the llm,-0.22285472170301368
41,"Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs.","Inspired by language reference frameworks, we identify different maturity levels that <mask> may exhibit in understanding and responding to user inputs.",1_arx_2012.11976_1399854_6,429,exhibit,platform,a conversational ai development platform,-3.905189666633788
42,"Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs.","Inspired by language reference frameworks, we identify different maturity levels that <mask> may exhibit in understanding and responding to user inputs.",1_arx_2012.11976_1399854_6,429,exhibit,AI,a conversational ai development platform,-3.905189666633788
43,"Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.","Then, <mask>, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.",1_acl_22_34383_5,429,analyze,LLM,the more general llm-based expert,0.4304509062315738
44,"Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.","Then, <mask>, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.",1_acl_22_34383_5,429,analyze,expert,the more general llm-based expert,0.4304509062315738
45,"To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons.","To understand when <mask> need to break rules, we examine the conditions under which humans break rules for pro-social reasons.",1_arx_2107.04022_1498034_6,429,need,AI,ai agents,-0.9093346573946839
46,These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.,These results suggest that <mask> can autonomously develop effective model-improvement techniques beyond human intuition.,1_acl_519_45829_8,429,develop,LLMs,llms,-2.61953990258405
47,These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.,These results suggest that LLMs can autonomously develop <mask> beyond human intuition.,1_acl_519_45829_8,416,develop,model,effective model-improvement techniques,-5.487855965994498
48,"Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support.","Our experiments show that without external feedback, <mask> struggle to recognize their need for user support.",1_acl_131_34995_2,429,struggle,LLMs,many llms,-3.90930597862787
49,"By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.","By relation-augmented training, <mask> learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.",1_acl_296_12174_4,429,learn,model,the model,0.7716292976388619
50,"Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.","Consequently, <mask> struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.",1_acl_518_29554_3,429,struggle,LLMs,llms,-0.7765811999733003
51,"In addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence.","In addition, <mask>, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence.",1_arx_2303.08014_1807811_6,429,interpret,ChatGPT,chatgpt,1.651274231647708
52,"We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points.","We find that <mask> narrowly passed the exam, obtaining 20.5 out of 40 points.",1_arx_2303.09461_1809258_3,429,pass,ChatGPT,chatgpt,2.5146960029774323
53,"By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.","By analyzing call transcripts, <mask> can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.",1_acl_57_25373_2,429,determine,AI,ai,1.0982330298447849
54,"When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.","When <mask> encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.",1_acl_212_35072_9,429,encounter,LLM,an llm,-6.434630096657122
55,"When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.","When an LLM encounters questions outside its domain, <mask> recognizes its knowledge scope and determines whether it can answer the question independently.",1_acl_212_35072_9,429,recognize,system,the system,-7.414362217489991
56,"Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations.","Further, to help <mask> distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations.",1_acl_150_26871_4,429,distinguish,LLMs,llms,-2.0754520742674547
57,"Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.","Our experimental results show that while <mask> demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.",1_acl_593_32755_6,429,demonstrate,LLMs,llms,-4.192257578060254
58,"However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize.","However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which <mask> can memorize.",1_acl_590_35441_1,429,memorize,model,the model,-0.3962709345456883
59,"We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.","We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (<mask>) in sarcasm detection, which leverages principles from pragmatics and reflection helping <mask> interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.",1_acl_7_43457_1,403,models,LLMs,llms,-0.47860827404629447
60,"We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.","We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (<mask>) in sarcasm detection, which leverages principles from pragmatics and reflection helping <mask> interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.",1_acl_7_43457_1,429,interpret,LLMs,llms,-0.47860827404629447
61,"For instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in.","For instance, it is widely accepted that <mask> perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in.",1_acl_350_43916_2,429,perform,LLMs,llms,-3.8905050510927346
62,"Our language model proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness.","<mask> proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness.",1_acl_1809.00066_1020082_2,429,prove,model,our language model,2.3842190996366197
63,"AI developers found that our system can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.","<mask> found that our system can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.",1_acl_1810.09030_1040217_7,429,find,AI,ai developers,1.4126434726087096
64,"AI developers found that our system can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.","AI developers found that <mask> can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.",1_acl_1810.09030_1040217_7,429,help,system,our system,-4.353407090259958
65,"AI developers found that our system can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.","AI developers found that our system can help them discover unknown errors made by <mask>, and engage in the process of proactive testing.",1_acl_1810.09030_1040217_7,439,by,AI,the ai models,-5.2598522737709885
66,"Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers.","Apart from generating the wrong reasoning processes, <mask> can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers.",1_acl_201_29237_3,429,misinterpret,LLMs,llms,-1.6101260876530503
67,Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.,Our evaluation finds that it is increasingly challenging for <mask> to identify analogies when going up the analogy taxonomy.,1_acl_218_28354_4,429,identify,LLMs,llms,-1.636318364168119
68,"Resultantly, the developed model efficiently analyzed the test data with a mean average precision of 0.89.","Resultantly, <mask> efficiently analyzed the test data with a mean average precision of 0.89.",1_arx_2304.04966_1823012_7,429,analyze,model,the developed model,0.05512990918044025
69,"To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","To succeed, <mask> needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).",1_arx_2010.09890_1366220_2,429,need,agent,the ai agent,-3.9780986039671973
70,"To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","To succeed, <mask> needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).",1_arx_2010.09890_1366220_2,429,need,AI,the ai agent,-3.9780986039671973
71,"To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of <mask> performing the same task (social perception), and ii) coordinate with <mask> to solve the task in an unseen environment as fast as possible (human-AI collaboration).",1_arx_2010.09890_1366220_2,439,of,agent,the human-like agent,-2.8678869129593743
72,"To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of <mask> performing the same task (social perception), and ii) coordinate with <mask> to solve the task in an unseen environment as fast as possible (human-AI collaboration).",1_arx_2010.09890_1366220_2,439,with,agent,the human-like agent,-2.8678869129593743
73,"To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible <mask>).",1_arx_2010.09890_1366220_2,403,coordinate,AI,(human-ai collaboration,-0.46885933780808386
74,The experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations.,The experimental results showcase that <mask> demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations.,1_acl_1477_40886_4,429,demonstrate,ChatGPT,chatgpt,-0.8127304910324114
75,"In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers.","In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that <mask> learn to manipulate humans without the intent of the system designers.",1_arx_2303.09387_1809184_9,429,learn,AI,ai systems,-6.804027420052996
76,"In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers.","In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of <mask>.",1_arx_2303.09387_1809184_9,439,of,system,the system designers,-3.654264738945818
77,"When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context.","When performing next word prediction given a textual context, <mask> can infer and represent properties of an agent likely to have produced that context.",1_arx_2212.01681_1757858_4,429,infer,LM,an lm,-3.8492826064612604
78,"When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context.","When performing next word prediction given a textual context, an LM can infer and represent properties of <mask> likely to have produced that context.",1_arx_2212.01681_1757858_4,439,of,agent,an agent,-2.515206003050242
79,"We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.","We show that <mask> finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.",1_arx_1808.10627_1019773_2,429,find,model,the model,0.7125786378851178
