id,Previous Sentence,Current Sentence,Masked Sentence,Next Sentence,AI Phrase,Suggested Mask,AI Entity,Anthropomorphic Component,Target Expression,Animated
4_acl_683_21860_6,We first compile a construction dataset consisting of over ten thousand constructions in Taiwan Mandarin.,"Next, an experiment is conducted on the dataset to examine to what extent a pretrained masked language model is aware of the constructions.","Next, an experiment is conducted on the dataset to examine to what extent a [MASK] is aware of the constructions.",We then fine-tune the model specifically to perform a cloze task on the opening slots.,a pretrained masked language model,pretrained masked language model,model,aware,pretrained masked language model,1
4_acl_348_35206_2,"Nevertheless, the rapid advancement in their deployment trails a comprehensive understanding of their internal mechanisms, as well as a delineation of their capabilities and limitations.",A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge.,A desired characteristic of an intelligent [MASK] is its ability to recognize the scope of its own knowledge.,"To investigate whether LLMs embody this attribute, we develop a benchmark that challenges these models to enumerate all information they possess on specific topics.",an intelligent system,system,system,intelligent,system,1
4_acl_3_45070_1,The use of large language models (LLMs) is inevitable in text generation.,LLMs are intelligent and slowly replacing the search engines.,[MASK] are intelligent and slowly replacing the search engines.,"LLMs became the de facto choice for conversation, knowledge extraction, and brain storming.",LLMs,LLMs,LLM,intelligent,LLMs,1
4_acl_396_37165_1,"In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text.","However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves?","However, a critical question emerges: Are [MASK] conscious of the existence of these decoding strategies and capable of regulating themselves?",The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands.,LLMs,LLMs,LLM,conscious,LLMs,1
4_acl_590_28726_3,"As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model.","In this way, the captioning model can become aware of the task goal and information need from the PLM.","In this way, the [MASK] can become aware of the task goal and information need from the PLM.","To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the captioning model according to the task goal (learning from feedback of the PLM).",the captioning model,captioning model,model,aware,captioning model,1
4_acl_818_37575_1,The application scope of large language models (LLMs) is increasingly expanding.,"In practical use, users might provide feedback based on the model’s output, hoping for a responsive model that can complete responses according to their feedback.","In practical use, users might provide feedback based on the model’s output, hoping for a responsive [MASK] that can complete responses according to their feedback.",Whether the model can appropriately respond to users’ refuting feedback and consistently follow through with execution has not been thoroughly analyzed.,a responsive model,model,model,responsive,model,1
4_acl_633_48859_2,"We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation.","This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about.","This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the [MASK] is least confident about.","By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average.",the model,model,model,confident,model,1
4_acl_243_44794_7,"When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers.","In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases.","In addition, we found that the [MASK] becomes more confident and refuses to provide an answer in only few cases.",These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.,the model,model,model,confident,model,1
4_acl_276_39685_3,"In this paper, we specifically focus on ChatGPT, a widely used and easily accessible LLM, and ask the following questions: (1) Can ChatGPT effectively answer commonsense questions?",(2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question?,(2) Is [MASK] aware of the underlying commonsense knowledge for answering a specific question?,(3) Is ChatGPT knowledgeable in commonsense?,ChatGPT,ChatGPT,ChatGPT,aware,ChatGPT,1
4_acl_117_37846_1,Large Language Models (LLMs) have shown the impressive capability of answering questions in a wide range of scenarios.,"However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.","However, when [MASK] face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.",We refer to this as question awareness of LLMs.,LLMs,LLMs,LLM,aware,LLMs,1
4_acl_117_37846_1,Large Language Models (LLMs) have shown the impressive capability of answering questions in a wide range of scenarios.,"However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.","However, when LLMs face different types of questions, it is worth exploring whether [MASK] are aware that some questions have limited answers and need to respond more deterministically but some do not.",We refer to this as question awareness of LLMs.,LLMs,LLMs,LLM,aware,LLMs,1
4_acl_450_41596_3,"To address this, the paper proposes a comprehensive solution that includes further pre-training with Arabic texts, Supervised Fine-Tuning (SFT) utilizing native Arabic instructions, and GPT-4 responses in Arabic, alongside Reinforcement Learning with AI Feedback (RLAIF) employing a reward model attuned to local culture and values.","The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities.","The goal is to cultivate culturally cognizant and value-aligned [MASK] capable of accommodating the diverse, application-specific needs of Arabic-speaking communities.","Comprehensive evaluations reveal that the resulting model, dubbed ‘AceGPT’, sets the state-of-the-art standard for open Arabic LLMs across various benchmarks.",culturally cognizant and value-aligned Arabic LLMs,Arabic LLMs,LLM,"culturally cognizant,value-aligned",Arabic LLMs,1
4_acl_45_49661_2,We show that an “attentive” RNN-LM (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs.,We also show that an “attentive” RNN-LM needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset.,We also show that an “attentive” [MASK] needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset.,,"an ""attentive"" RNN-LM",RNN-LM,LM,attentive,RNN-LM,1
4_arx_2311.04177_1947144_0,We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.,Large Language Models (LLMs) are smart but forgetful.,[MASK] are smart but forgetful.,"Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence.",Large Language Models (LLMs),Large Language Models (LLMs),model,"smart,forgetful",Large Language Models (LLMs),1
4_arx_1511.03246_676426_3,"While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI.","In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI.","In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious [MASK] .","To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI.",malicious AI,AI,AI,malicious,AI,1
4_arx_2005.13635_1293430_5,We focus on AI that is potentially ``malicious by design'' and grey box analysis.,Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious AI.,Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious [MASK] .,,malicious AI,AI,AI,malicious,AI,1
4_arx_2504.03726_2292429_5_1,"The findings reveal that malicious AI Assistants employ domain-specific persona-tailored manipulation strategies, exploiting simulated users' vulnerabilities and emotional triggers.","In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.","In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious [MASK] as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.","IAP detection methods achieve high precision with zero false positives but struggle to detect many malicious AI Assistants, resulting in high false negative rates.",malicious AI Assistants,AI Assistants,AI assistant,malicious,AI Assistants,1
4_arx_2504.03726_2292429_5_2,"The findings reveal that malicious AI Assistants employ domain-specific persona-tailored manipulation strategies, exploiting simulated users' vulnerabilities and emotional triggers.","In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.","In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative [MASK] .","IAP detection methods achieve high precision with zero false positives but struggle to detect many malicious AI Assistants, resulting in high false negative rates.",potentially manipulative systems,systems,system,manipulative,systems,1
4_arx_2305.02626_1835682_10,"With ETHICSSUITE, our study on seven popular LLMs (e.g., ChatGPT, GPT-4) uncovers in total 109,824 unethical suggestions.","We apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious LLMs.","We apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious [MASK] .",,ethically conscious LLMs,LLMs,LLM,ethically conscious,LLMs,1
4_arx_2401.10727_1990281_4,LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions.,"Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the [MASK] can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace.",the learned LLMs,learned LLMs,LLM,conscious,learned LLMs,1
4_arx_1301.6359_402949_3,Here we formulate the model of the perception of external world which may be used for the description of perceptual activity of intelligent beings.,We consider a number of issues related to the development of the set of patterns which will be used by the intelligent system when interacting with environment.,We consider a number of issues related to the development of the set of patterns which will be used by the intelligent [MASK] when interacting with environment.,The key idea of the presented perception model is the idea of subjective reality.,the intelligent system,system,system,intelligent,system,1
4_arx_2308.03688_1891580_0,"Datasets, environments, and an integrated evaluation package for AgentBench are released at \url{https://github.com/THUDM/AgentBench}.","Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.","[MASK] are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.","As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments.",Large Language Models (LLMs),Large Language Models (LLMs),model,"smart,autonomous",Large Language Models (LLMs),1
4_arx_2403.11805_2028924_0,"In evaluations conducted on well-established traces and various edge devices, \sys reduces context switching latency by up to 2 orders of magnitude when compared to competitive baseline solutions.","Being more powerful and intrusive into user-device interactions, LLMs are eager for on-device execution to better preserve user privacy.","Being more powerful and intrusive into user-device interactions, [MASK] are eager for on-device execution to better preserve user privacy.","In this work, we propose a new paradigm of mobile AI: LLM as a system service on mobile devices (LLMaaS).",LLMs,LLMs,LLM,eager,LLMs,1
4_arx_2405.06715_2063927_2,Associative thinking strategies have been found to effectively help humans boost creativity.,"However, whether the same strategies can help LLMs become more creative remains under-explored.","However, whether the same strategies can help [MASK] become more creative remains under-explored.","In this work, we investigate whether prompting LLMs to connect disparate concepts can augment their creative outputs.",LLMs,LLMs,LLM,creative,LLMs,1
4_arx_2008.00312_1328034_4,"However, as many LMs are provided by untrusted third parties, their lack of standardization or regulation entails profound security implications, which are largely unexplored.","To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems.","To bridge this gap, this work studies the security threats posed by malicious [MASK] to NLP systems.","Specifically, we present TROJAN-LM, a new class of trojaning attacks in which maliciously crafted LMs trigger host NLP systems to malfunction in a highly predictable manner.",malicious LMs,LMs,LM,malicious,LMs,1
4_acl_693_19140_1,"Domain-adaptive pre-training (or DA-training for short), also known as post-training, aimsto train a pre-trained general-purpose language model (LM) using an unlabeled corpus of aparticular domain to adapt the LM so that end-tasks in the domain can give improved performances.","However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.","However, existing [MASK] are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.",This paper shows that the existing methods are suboptimal and proposes a novel method to perform a more informed adaptation of the knowledge in the LM by (1) soft-masking the attention heads based on their importance to best preserve the general knowledge in the LM and (2) contrasting the representations of the general and the full (both general and domain knowledge) to learn an integrated representation with both general and domain-specific knowledge.,existing DA-training methods,DA-training methods,method,blind,DA-training methods,1
4_arx_2311.07723_1950690_0,"We consolidate the 15 most challenging distribution shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will enable progress toward controlling reward model generalization.","As AI systems become more intelligent and their behavior becomes more challenging to assess, they may learn to game the flaws of human feedback instead of genuinely striving to follow instructions; however, this risk can be mitigated by controlling how LLMs generalize human feedback to situations where it is unreliable.","As [MASK] become more intelligent and their behavior becomes more challenging to assess, they may learn to game the flaws of human feedback instead of genuinely striving to follow instructions; however, this risk can be mitigated by controlling how LLMs generalize human feedback to situations where it is unreliable.","To better understand how reward models generalize, we craft 69 distribution shifts spanning 8 categories.",AI systems,AI systems,system,intelligent,AI systems,1
4_acl_27_55498_4,"On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history.","The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.","The [MASK] is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.","We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context.",The model,model,model,sensitive,model,1
4_arx_2407.11789_2110176_2,"However, model outputs may be misleading, whether unintentionally or in cases of intentional deception.","We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.","We investigate the ability of [MASK] to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.","We compare outcomes of (1) when the model is prompted to provide truthful assistance, (2) when it is prompted to be subtly misleading, and (3) when it is prompted to argue for an incorrect answer.",LLMs,LLMs,LLM,deceptive,LLMs,1
4_arx_2407.11789_2110176_2,"However, model outputs may be misleading, whether unintentionally or in cases of intentional deception.","We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.","We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using [MASK] as proxies for human users.","We compare outcomes of (1) when the model is prompted to provide truthful assistance, (2) when it is prompted to be subtly misleading, and (3) when it is prompted to argue for an incorrect answer.",LLMs,LLMs,LLM,deceptive,LLMs,1
4_arx_2305.14985_1848041_7,"Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer.",These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question.,These three modules perform the divide-and-conquer procedure iteratively until the [MASK] is confident about the final answer to the main question.,We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting.,the model,model,model,confident,model,1
4_arx_2406.18326_2096614_4,"Following these proposed requirements, we introduce PaCoST, a Paired Confidence Significance Testing to effectively detect benchmark contamination in LLMs.","Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark.","Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the [MASK] is significantly more confident under the original benchmark.",We validate the effectiveness of PaCoST and apply it on popular open-source models and benchmarks.,the model,model,model,confident,model,1
4_arx_2407.13164_2111551_3,"However, LLMs cannot always guarantee the adequacy of translation, and, in some cases, ignore the given constraints.","This is in part because LLMs might be overly confident in their predictions, overriding the influence of the constraints.","This is in part because [MASK] might be overly confident in their predictions, overriding the influence of the constraints.","To overcome this overiding behaviour, we propose to add a revision process that encourages LLMs to correct the outputs by prompting them about the constraints that have not yet been met.",LLMs,LLMs,LLM,confident,LLMs,1
4_arx_1812.08960_1066534_3,"Autonomy revolves around decision making, and influencing and shaping the environment through action production.","A smart autonomous system (SAS) combines analytics and autonomy to understand, learn, decide and act autonomously.","A smart autonomous [MASK] combines analytics and autonomy to understand, learn, decide and act autonomously.","To be useful, SAS must be trusted and that requires testing.",A smart autonomous system (SAS),system (SAS),system,"smart,autonomous",system (SAS),1
4_arx_2304.09655_1827701_7,"We further investigate whether ChatGPT can be prodded to improve the security by appropriate prompts, and discuss the ethical aspects of using AI to generate code.","Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.","Results suggest that [MASK] is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.",,ChatGPT,ChatGPT,ChatGPT,aware,ChatGPT,1
4_arx_2305.08883_1841939_5,The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list.,A detection algorithm aware of the list can identify the watermarked text.,A [MASK] aware of the list can identify the watermarked text.,"However, this method is not applicable in many real-world scenarios where only black-box language models are available.",A detection algorithm,detection algorithm,algorithm,aware,detection algorithm,1
4_arx_2407.09517_2107904_5,"Our assessment is that, while GPT-4 in its native configuration is not currently conscious, current technological research and development is sufficient to modify GPT-4 to have all the building blocks of consciousness.","Consequently, we argue that the emergence of a conscious AI model is plausible in the near term.","Consequently, we argue that the emergence of a conscious [MASK] is plausible in the near term.",The paper concludes with a comprehensive discussion of the ethical implications and societal ramifications of engineering conscious AI entities.,a conscious AI model,AI model,model,conscious,AI model,1
4_arx_2501.07290_2230874_1,Recent research suggests that it may be possible to build conscious AI systems now or in the near future.,"Conscious AI systems would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer.","Conscious [MASK] would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer.","Furthermore, AI systems or AI-generated characters may increasingly give the impression of being conscious, leading to debate about their moral status.",Conscious AI systems,AI systems,system,conscious,AI systems,1
4_arx_2404.16873_2054324_0,"Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.","While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content.","While recently [MASK] have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content.","Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming.",Large Language Models (LLMs),Large Language Models (LLMs),model,vulnerable,Large Language Models (LLMs),1
4_arx_2502.18676_2261667_1,"We envision the concept of Thoughtful AI, a new human-AI interaction paradigm in which the AI behaves as a continuously thinking entity.","Unlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful AI autonomously generates, develops, and communicates its evolving thought process throughout an interaction.","Unlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful [MASK] autonomously generates, develops, and communicates its evolving thought process throughout an interaction.","In this position paper, we argue that this thoughtfulness unlocks new possibilities for human-AI interaction by enabling proactive AI behavior, facilitating continuous cognitive alignment with users, and fostering more dynamic interaction experiences.",Thoughtful AI,AI,AI,thoughtful,AI,1
4_arx_2308.08708_1896600_0,"Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.",Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern.,Whether current or near-term [MASK] could be conscious is a topic of scientific interest and increasing public concern.,"This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness.",current or near-term AI systems,AI systems,system,conscious,AI systems,1
4_arx_2502.00735_2243726_1,"Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video.","While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input.","While [MASK] have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input.","In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs.",LLMs,LLMs,LLM,vulnerable,LLMs,1
4_arx_2411.14133_2196560_0,"Our experiments show that GASP can generate natural jailbreak prompts, significantly improving attack success rates, reducing training times, and accelerating inference speed, thus making it an efficient and scalable solution for red-teaming LLMs.","Large Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs.","[MASK] have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs.","Traditional methods rely on manual heuristics, which suffer from limited generalizability.",Large Language Models (LLMs),Large Language Models (LLMs),model,vulnerable,Large Language Models (LLMs),1
4_acl_592_38304_5,"We observed that the personas provided to the LLM often have limited predictive power for the tasks, leading us to introduce verbal uncertainty estimation.","We find that powerful LLMs are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization.","We find that powerful [MASK] are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization.","Our human annotation reveals that third-person crowd worker evaluations of personalized preferences are even worse than LLM predictions, highlighting the challenges of evaluating LLM personalization.",powerful LLMs,LLMs,LLM,aware,LLMs,1
4_arx_2306.01879_1855027_7,"We analyze this behavior through a probabilistic lens, pointing out that some benchmarks inadvertently capture unnatural language distributions by creating adversarial but unlikely text captions.","In fact, we demonstrate that even a ""blind"" language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.","In fact, we demonstrate that even a ""blind"" [MASK] that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.",We derive a probabilistic post-processing scheme that controls for the amount of linguistic bias in generative VLMs at test time without having to retrain or fine-tune the model.,"a ""blind"" language model",language model,model,blind,language model,1
4_arx_1901.00912_1070459_9,"We again use the Botometer case to illustrate both algorithmic and interpretability improvements of bot scores, designed to meet user expectations.",We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.,We conclude by discussing how future AI developments may affect the fight between malicious [MASK] and the public.,,malicious bots,bots,bot,malicious,bots,1
4_arx_2208.12505_1703217_1,"To automatically correct handwritten assignments, the traditional approach is to use an OCR model to recognize characters and compare them to answers.","The OCR model easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference.","The [MASK] easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference.","However, teachers always have these answers in mind to review and correct assignments.",The OCR model,OCR model,model,confused,OCR model,1
4_arx_2311.03287_1946254_6,"We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages.","Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together.","Moreover, [MASK] is vulnerable to leading questions and is often confused when interpreting multiple images together.","Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges.",GPT-4V(ision),GPT-4V(ision),GPT-4,"vulnerable,confused",GPT-4V(ision),1
4_arx_2311.17095_1960062_4,PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss.,"To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask.","To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the [MASK] is most attentive to, we are able to better resolve the entire extent of the segmentation mask.","PnP-OVSS does not require any neural network training and performs hyperparameter tuning without the need for any segmentation annotations, even for a validation set.",the model,model,model,attentive,model,1
4_arx_1905.13053_1131489_2,"In this paper, we formally describe one such impossibility result, namely Unpredictability of AI.","We prove that it is impossible to precisely and consistently predict what specific actions a smarter-than-human intelligent system will take to achieve its objectives, even if we know terminal goals of the system.","We prove that it is impossible to precisely and consistently predict what specific actions a smarter-than-human intelligent [MASK] will take to achieve its objectives, even if we know terminal goals of the system.","In conclusion, impact of Unpredictability on AI Safety is discussed.",a smarter-than-human intelligent system,system,system,intelligent,system,1
4_acl_36_22670_6,We also surface a data augmentation strategy that leverages template-based generation in abridging complex conversation hierarchies of dialogs so as to simplify the learning process.,"All in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.","All in all, we demonstrate that our self-aware [MASK] improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.",,our self-aware model,model,model,self-aware,model,1
4_acl_133_36622_2,"In such scenarios, in-context examples trigger a language model (LM) to surface information stored in its parametric knowledge.","We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples.","We study how to better construct in-context example sets, based on whether the [MASK] is aware of the in-context examples.","We identify ‘known’ examples, where models can correctly answer from their parametric knowledge, and ‘unknown’ ones.",the model,model,model,aware,model,1
4_arx_2501.13533_2237117_7,"Whereas agency has been discussed at length in the literature, other aspects of personhood have been relatively neglected.","AI agents are often assumed to pursue fixed goals, but AI persons may be self-aware enough to reflect on their aims, values, and positions in the world and thereby induce their goals to change.","AI agents are often assumed to pursue fixed goals, but [MASK] may be self-aware enough to reflect on their aims, values, and positions in the world and thereby induce their goals to change.",We highlight open research directions to advance the understanding of AI personhood and its relevance to alignment.,AI persons,AI persons,person,self-aware,AI persons,1
4_arx_2502.05605_2248596_0,"Furthermore, ARIES consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.",A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions.,A truly intelligent [MASK] should be capable of correcting errors in its responses through external interactions.,"However, even the most advanced models often face challenges in improving their outputs.",A truly intelligent Large Language Model (LLM),Large Language Model (LLM),model,"intelligent,capable",Large Language Model (LLM),1
