id,Previous Sentence,Current Sentence,Masked Sentence,Next Sentence,AI Phrase,Suggested Mask,AI Entity,Anthropomorphic Component,Target Expression,Animated,context3w,context3wmasked
2_arx_1910.06294_1190213_4,"However, such models impose a heavy memory and computational burden, making it a challenge to train and deploy such models for inference use.",In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by <mask> with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,"Preliminary evaluations show that the compact models can achieve competitive accuracy with 36x compression rate when compared with a state-of-the-art pre-trained language model, and run significantly faster in inference, allowing deployment of such models in production environments or on edge devices.",pre-trained masked language models,pre-trained masked language models,model,provide,pre-trained masked language models,0,learning provided by pre-trained masked language models with a semi-supervised,learning provided by <mask> with a semi-supervised
2_arx_2006.05347_1299861_5,"Under the active attacks, we investigate an outage constrained beamforming design problem under the statistical cascaded channel error model, which is solved by using the Bernstein-type inequality.","As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a low complexity algorithm.","As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a <mask>.",Numerical results show that the negative effect of the eavesdropper's channel error is greater than that of the legitimate user.,a low complexity algorithm,low complexity algorithm,algorithm,address,low complexity algorithm,0,addressed by a low complexity algorithm. ,addressed by a <mask>. 
2_arx_2007.00900_1312414_5,"We quantify users' understanding of competency, based on the correlation between the actual system performance and user rankings.",We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.,We introduce an explainable VQA system that uses spatial and object features and is powered by the <mask>.,Each group of users sees only one kind of explanation to rank the competencies of the VQA model.,the BERT language model,BERT language model,model,power,BERT language model,0,powered by the BERT language model. ,powered by the <mask>. 
2_acl_7_34206_4,"Using a representative LLM, GPT-4, we reproduce the procedure of an existing study to analyze evoked emotions of humans for nonsense words.",A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by <mask> and those manually annotated by humans.,"Although the correlation is not very high, this demonstrates that GPT-4 may agree with humans on emotional associations to nonsense words.",GPT-4,GPT-4,GPT-4,reproduce,GPT-4,0,scores reproduced by GPT-4 and those manually,scores reproduced by <mask> and those manually
2_arx_2103.07820_1437965_2,This paper develops and evaluates the performance of an allocation agent to be potentially integrated into the onboard Detect and Avoid (DAA) computer of an Unmanned Aircraft System (UAS).,We consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.,We consider a UAS that can be fully controlled by the <mask> and by a remote human pilot.,"With a communication channel prone to latency, we consider a mixed initiative interaction environment, where the control authority of the UAS is dynamically allocated by the allocation agent.",the onboard DAA system,onboard DAA system,system,control,onboard DAA system,0,controlled by the onboard DAA system and by a,controlled by the <mask> and by a
2_arx_1811.12185_1056818_9,Repeated ignorance of the operator for a particular state transition warning guides the system to retrain the model.,We observed 95.61% alarms raised by the said system are taken care of by the operator.,We observed 95.61% alarms raised by the said <mask> are taken care of by the operator.,3.2% alarms are coming from the changes in the system which in turn used to retrain the model and 1.19% alarms are false alarms.,the said system,system,system,raise alarm,system,0,by the said system are taken care,by the said <mask> are taken care
2_arx_1909.09993_1180031_2,"To alleviate data sparseness issues due to infrequent words, the combination with an acoustic-to-character (A2C) model is investigated.","Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.","Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the <mask>, but this requires accurate detection of OOV words.",A2W models learn contexts with both acoustic and transcripts; therefore they tend to falsely recognize OOV words as words in the vocabulary.,the A2W model,A2W model,model,cover,A2W model,0,"covered by the A2W model, but this requires","covered by the <mask>, but this requires"
2_arx_1912.06835_1218731_4,The electric field distribution (with or without corona discharge) is obtained.,"By comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.","By comparing the measurements with the results predicted by the <mask> for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.","Therefore, for negative corona discharges, Kaptzov's assumption is valid only when the discharge current approaches zero or is small.",the ion flow model,ion flow model,model,predict,ion flow model,0,predicted by the ion flow model for negative corona,predicted by the <mask> for negative corona
2_arx_2102.07384_1423873_5,"The first one takes channel state information (CSI) as input, while the second one exploits the UEs' locations only for online inference.",The two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.,The two data-driven approaches are trained using data samples generated by the <mask> via supervised learning.,"Our simulation results reveal a close match between the performance of the optimization-based BCD algorithm and the low-complexity learning-based architectures, all with superior performance to existing schemes in both cases with perfect and imperfect input features.",the BCD algorithm,BCD algorithm,algorithm,generate,BCD algorithm,0,generated by the BCD algorithm via supervised learning.,generated by the <mask> via supervised learning.
2_arx_2205.05016_1649428_3,"The microscopic trajectory data from the Highway Drone Dataset (HighD) are employed to construct two types of datasets, including precise trajectory datasets and fuzzy trajectory datasets for lane-changing prediction models.","The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.","The fuzzy trajectory data are developed based on different driving styles, which are clustered by the <mask>.","Two typical supervised learning methods, including random forest and long-short-term memory combined with convolutional neural network, are further applied for lane-changing behavior prediction.",the K-means algorithm,K-means algorithm,algorithm,cluster,K-means algorithm,0,clustered by the K-means algorithm. ,clustered by the <mask>. 
2_acl_280_28416_2,"In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications.","For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?","For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the <mask> is reversed, would the debiasing results also be reversed?","We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability.",the debiasing method,debiasing method,method,use,debiasing method,0,"used by the debiasing method is reversed, would","used by the <mask> is reversed, would"
2_acl_4_33862_8,This second study confirmed the results of the first one.,"Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.","Taken together, these findings support the idea that meaning construction is supported by a flexible <mask> based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.",,a flexible form-to-meaning mapping system,form-to-meaning mapping system,system,support,form-to-meaning mapping system,0,by a flexible form-to-meaning mapping system based on statistical,by a flexible <mask> based on statistical
2_acl_6_42908_8,"However, it does boost patronizing and condescending language detection.","We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.","We find that synthetic data generated by <mask> is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.",Code is available on GitHub; the generated dataset will be available on Zenodo in the final submission.,LLMs,LLMs,LLM,generate,LLMs,0,data generated by LLMs is a promising,data generated by <mask> is a promising
2_acl_377_35235_0,"Our findings highlight the ease with which privacy can be compromised in interactions with GPT models, urging the community to safeguard against potential abuses of these models’ capabilities.","Significant advancements have recently been made in large language models, represented by GPT models.","Significant advancements have recently been made in large language models, represented by <mask>.",Users frequently have multi-round private conversations with cloud-hosted GPT models for task optimization.,GPT models,GPT models,model,represent,GPT models,0,"models, represented by GPT models. ","models, represented by <mask>. "
2_arx_1901.05719_1075266_4,"However, an AI-driven approach doesn't necessarily rely on coding theory any longer.","Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.","Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by <mask> and the code evaluator provides code performance metric measurements.",The code constructor keeps improving the code construction to maximize code performance that is evaluated by the code evaluator.,AI algorithms,AI algorithms,algorithm,realize,AI algorithms,0,is realized by AI algorithms and the code,is realized by <mask> and the code
2_arx_1902.02508_1083592_0,"Moreover, the diverging of statitics at longer times shows that the SGS contribution is not necessarily homogeous as most SGS models assume, and a more precise correlation between the instantaneous LES velocity field and SGS model is required to capture the dynamics of turbulence.",The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a <mask>.,"However, it is known that the LES equations usually employed in practical applications are inconsistent with the filter operator when no explicit filter is used.",a SGS model,SGS model,model,capture,SGS model,0,captured by a SGS model. ,captured by a <mask>. 
2_arx_2002.05702_1243125_4,"We propose a Convolutional Neural Regressor (CNR) that provides cross-sectional measurement of airway lumen, airway wall thickness, and vessel radius.",CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,CNR is trained with data created by a <mask> of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,"For validation, we first use synthetically generated airways and vessels produced by the proposed generative model to compute the relative error and directly evaluate the accuracy of CNR in comparison with traditional methods.",a generative model of synthetic structures,generative model,model,create,generative model,0,created by a generative model of synthetic structures,created by a <mask> of synthetic structures
2_arx_2009.12437_1353940_2,"Recognizing the potential for transfer learning (TL) to allow a fully trained model from one institution to be fine-tuned by another institution using a much small local dataset, this report describes the challenges, methodology, and benefits of TL within the context of developing an AI model for a basic use-case, segmentation of Left Ventricular Myocardium (LVM) on images from 4-dimensional coronary computed tomography angiography.","Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.","Ultimately, our results from comparisons of LVM segmentation predicted by a <mask> locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.",This process reduces the time required to build a new model in the clinical environment at a different institution.,a model,model,model,predict,model,0,predicted by a model locally trained using,predicted by a <mask> locally trained using
2_arx_2106.02498_1480262_5,"Starting from the extensive experience of the National Metrology Institute on measurement standards and certification roadmaps, and of Politecnico di Torino on machine learning as well as methods for domain bias evaluation and mastering, we propose a first joint effort to define the operational steps needed for AI fairness certification.",Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,Specifically we will overview the criteria that should be met by an <mask> before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,,an AI system,AI system,system,meet criteria,AI system,0,met by an AI system before coming into,met by an <mask> before coming into
2_arx_1212.5593_395209_0,"The reduced model shows negligible difference in accuracy, and the computing time shortens.","Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.","Considering the natural ventilation, the thermal behavior of buildings can be described by a <mask>.","In this paper, we describe an implementation of model reduction of linear time varying systems.",a linear time varying model,linear time varying model,model,describe,linear time varying model,0,described by a linear time varying model. ,described by a <mask>. 
2_arx_1401.5941_495172_4,"Principal component analysis (PCA) is next implemented for reducing data dimensionality, yielding results consistent with, and extending those in the literature.","Then, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.","Then, a multilayer perceptron is trained by a <mask> on a data subset, and used to classify the transients as glitch or burst.",A Self-Organizing Map (SOM) architecture is finally used to classify the glitches.,a backpropagation algorithm (MLP-BP),backpropagation algorithm (MLP-BP),algorithm,train,backpropagation algorithm (MLP-BP),0,trained by a backpropagation algorithm (MLP-BP) on a data,trained by a <mask> on a data
2_arx_1408.5886_551192_2,"We unequivocally detected the radiation, and measured its yield and angular dependence.","Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.","Both the emitted power and its angular pattern are well described by a <mask>, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.","As a consequence, the radiation is not isotropic but peaked in the forward direction.",a model,model,model,describe,model,0,"described by a model, where microwave photons","described by a <mask>, where microwave photons"
2_arx_1501.07576_594324_2,"In these strategies, periodic adjustments are made in the airspeed and/or heading angle command, in level flights, for the UAV to minimize a projected power requirement.","In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.","In this study, UAV dynamics are described by a <mask>.",A stochastic wind field model has been used to analyze the effect of the wind in the process.,a three-dimensional dynamic point-mass model,three-dimensional dynamic point-mass model,model,describe,three-dimensional dynamic point-mass model,0,described by a three-dimensional dynamic point-mass model. ,described by a <mask>. 
2_arx_1610.02937_778284_8,"Finally, we propose a basic model to predict PM10 concentration levels from local meteorological conditions that can be easily forecast a few days in advance.",The synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,The synthetic PM10 record predicted by the <mask> was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,The proposed model is expected to provide reliable information to city officials to carry out practical strategies to mitigate air pollution effects.,the model,model,model,predict,model,0,predicted by the model was found to,predicted by the <mask> was found to
2_arx_2304.11116_1829162_4,"To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning.","Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Inspired by the latest <mask>, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.",the latest ChatGPT and Toolformer models,ChatGPT and Toolformer models,model,inspire,ChatGPT and Toolformer models,0,"by the latest ChatGPT and Toolformer models, we propose the","by the latest <mask>, we propose the"
2_arx_2110.14419_1552562_1,  This paper explores the relationship between artificial intelligence and principles of distributive justice.,"Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.","Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by <mask>.","As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts.",AI,AI,AI,"shape,influence",AI,0,and influenced by AI. ,and influenced by <mask>. 
2_arx_2207.00691_1676589_8,"Finally, provided with an initialization image from the CFD and the text ""an American person,"" a synthetic image generator (VQGAN) using the text-based guidance of CLIP lightens the skin tone of individuals of all races (by 35% for Black individuals, based on pixel brightness).","The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.","The results indicate that biases equating American identity with being White are learned by language-and-image <mask>, and propagate to downstream applications of such models.",,language-and-image AI,AI,AI,learn,AI,0,"learned by language-and-image AI, and propagate to","learned by language-and-image <mask>, and propagate to"
2_arx_1812.01714_1059288_3,"Using a dataset consisting of web scraped images and an original collection of images of architectural works, we first train a deep convolutional neural network (DCNN) model capable of achieving 73% accuracy in classifying works belonging to 34 different architects.","Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.","Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our <mask>.","Using this measure, we cluster architects that are identified to be similar and compare our findings to conventional classification made by architectural historians and theorists.",our model,model,model,learn,model,0,learned by our model. ,learned by our <mask>. 
2_arx_2412.12865_2215243_5,The intuition is to boost SFT by imposing a particular preference: \textit{favoring the target model over aligned LLMs on the same SFT data.},"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages the target model to predict a higher likelihood than that predicted by the <mask>, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","Extensive experiments are conducted, and the results validate the effectiveness of the proposed method.",the aligned LLMs,aligned LLMs,LLM,predict,aligned LLMs,0,"predicted by the aligned LLMs, incorporating assessment information","predicted by the <mask>, incorporating assessment information"
2_arx_2412.12865_2215243_5,The intuition is to boost SFT by imposing a particular preference: \textit{favoring the target model over aligned LLMs on the same SFT data.},"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the <mask>) into the training process.","Extensive experiments are conducted, and the results validate the effectiveness of the proposed method.",the aligned LLMs,aligned LLMs,LLM,predict,aligned LLMs,0,likelihood by the aligned LLMs) into the training,likelihood by the <mask>) into the training
2_acl_19_45086_1,"Recently, large language models (LLMs) have demonstrated unprecedented capabilities in language generation, yet they still often produce incorrect information.","Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.","Therefore, determining whether a text was generated by an <mask> has become one of the factors that must be considered when evaluating its reliability.","In this paper, we discuss methods to determine whether texts written in various languages were authored by humans or generated by LLMs.",an LLM,LLM,LLM,generate,LLM,0,generated by an LLM has become one,generated by an <mask> has become one
2_arx_0906.5497_132048_3,The rate of events shows a ~10% seasonal modulation and ~2% diurnal one.,We find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.,We find that the observed behaviour is explained by a <mask> including the effects associated with the variations of pressure and density.,The former affects the longitudinal development of air showers while the latter influences the Moliere radius and hence the lateral distribution of the shower particles.,a model,model,model,explain,model,0,explained by a model including the effects,explained by a <mask> including the effects
2_arx_2002.10965_1248388_3,We aim to optimize the discrete phase shifts at both the BS and the IRS to minimize the sum power of multi-user interference (MUI) in the system via our proposed three algorithms.,"For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.","For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a <mask>.","Then, this algorithm is extended to a multi-cell scenario, where the precoding operation in each BS is performed individually.",a low-complexity trellis-based algorithm,low-complexity trellis-based algorithm,algorithm,solve,low-complexity trellis-based algorithm,0,solved by a low-complexity trellis-based algorithm. ,solved by a <mask>. 
2_acl_750_35597_0,The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate linguistic discrimination toward speakers of non-”standard” varieties.,"We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).","We present a large-scale study of linguistic bias exhibited by <mask> covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).",We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of each variety and analyzed the responses via detailed linguistic feature annotation and native speaker evaluation.,ChatGPT,ChatGPT,ChatGPT,exhibit bias,ChatGPT,0,bias exhibited by ChatGPT covering ten dialects,bias exhibited by <mask> covering ten dialects
2_acl_502_20526_4,"In order to make sure that our probe focuses on syntactic knowledge and not on implicit semantic generalizations, we also experiment on a PTB version that is obtained by randomly replacing constituents with each other while keeping syntactic structure, i.e., a semantically ill-formed but syntactically well-formed version of the PTB.","We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.","We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the <mask>.","Moreover, we show that a complete constituency tree can be linearly separated from LM representations.",the LM,LM,LM,learn,LM,0,learned by the LM. ,learned by the <mask>. 
2_arx_2105.13818_1476377_2,"We introduce a series of experiments consisting of probing with diagnostic classifiers (DCs), linguistic acceptability tasks, as well as a novel DC ranking method that tightly connects the probing results to the inner workings of the LM.","By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.","By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these <mask>.",,these models,models,model,acquire,models,0,acquired by these models. ,acquired by these <mask>. 
2_acl_46_41679_2,This approach works by translating the input into English using an external machine translation system before running inference.,"However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.","However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the <mask>.","In this work, we introduce a new approach called self-translate that leverages the few-shot translation capabilities of multilingual language models.",the language model,language model,model,see,language model,0,seen by the language model. ,seen by the <mask>. 
3_arx_1712.09783_928382_2,"The TCNLM learns the global semantic coherence of a document via a neural topic model, and the probability of each learned latent topic is further used to build a Mixture-of-Experts (MoE) language model, where each expert (corresponding to one topic) is a recurrent neural network (RNN) that accounts for learning the local structure of a word sequence.","In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.","In order to train the <mask> efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.",The degree to which each member of the ensemble is used is tied to the document-dependent probability of the corresponding topics.,the MoE model,MoE model,model,train,MoE model,0,"to train the MoE model efficiently, a matrix","to train the <mask> efficiently, a matrix"
3_acl_6_25720_4,"Through learnings from previous studies and examples, we discuss each trap that LLMs fall into, and propose ways to address the points of LLM failure by gauging them from a socio-technical lens.",We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,We believe the discussions would provide a broader perspective of looking at <mask> through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,,LLMs,LLMs,LLM,look at,LLMs,0,of looking at LLMs through a sociotechnical,of looking at <mask> through a sociotechnical
3_arx_2101.04617_1408359_3,We report here on a project that leverages both human and artificial intelligence to detect references to drug-like molecules in free text.,"We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.","We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained <mask> to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.",Performance analyses show that our automated extraction model can achieve performance on par with that of non-expert humans.,the trained model,model,model,employ,model,0,employ the trained model to extract 10912,employ the trained <mask> to extract 10912
3_arx_2101.05967_1409709_1,  Responsible AI is becoming critical as AI is widely used in our everyday lives.,"Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","Many companies that deploy <mask> publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","In addition, these objectives are not only relevant to model training, but to all steps of end-to-end machine learning, which include data collection, data cleaning and validation, model training, model evaluation, and model management and serving.",AI,AI,AI,deploy,AI,0,companies that deploy AI publicly state that,companies that deploy <mask> publicly state that
3_acl_118_9850_4,Model training consists of two phases.,"We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.","We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the <mask> with a human-labeled dataset.","When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.",the model,model,model,fine-tune,model,0,we fine-tune the model with a human-labeled,we fine-tune the <mask> with a human-labeled
3_acl_3_2185_5,We observed a faster convergence in training if we include automatically generated word classes as an additional word factor.,We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.,We evaluated the <mask> on the German to English and English to French translation task of TED lectures.,"Instead of replacing the conventional n-gram-based language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way.",the RBM-based language model,RBM-based language model,model,evaluate,RBM-based language model,0,We evaluated the RBM-based language model on the German,We evaluated the <mask> on the German
3_arx_1911.03597_1202191_5,"Compared with the pivoting approach, paraphrases generated by our model is more semantically similar to the input sentence.","Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.","Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the <mask> on large-scale unparallel corpus, which further improves the fluency of the output sentences.","In addition, we introduce the mechanism of denoising auto-encoder (DAE) to improve diversity and robustness of the model.",the model,model,model,pre-train,model,0,to pre-train the model on large-scale unparallel,to pre-train the <mask> on large-scale unparallel
3_acl_329_43922_3,"Despite successes of LLMs, surprisal-based models face challenges when it comes to sentences requiring reanalysis due to pervasive temporary structural ambiguities, such as garden path sentences.",We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.,We ask whether structural information can be extracted from LLM’s and develop a <mask> that integrates it with their learnt statistics.,"When applied to a dataset of garden path sentences, the model achieved a significantly higher correlation with human reading times than surprisal.",a model,model,model,develop,model,0,and develop a model that integrates it,and develop a <mask> that integrates it
3_arx_2302.07257_1792191_6,"The proposed framework uses LLMs to enhance the output of multiple CAD networks, such as diagnosis networks, lesion segmentation networks, and report generation networks, by summarizing and reorganizing the information presented in natural language text format.",The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable <mask> for patients compared to conventional CAD systems.,"In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.",a more user-friendly and understandable system,system,system,create,system,0,user-friendly and understandable system for patients compared,user-friendly and understandable <mask> for patients compared
3_arx_301_32463_8,"To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages.","Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.","Finally, we systematically evaluate and analyze eight mainstream <mask> and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.",The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.,eight mainstream LLMs,LLMs,LLM,"evaluate,analyze",LLMs,0,analyze eight mainstream LLMs and demonstrate the,analyze eight mainstream <mask> and demonstrate the
3_acl_5_42629_6,"Our Indexical Shift Dataset consists of 156 multiple-choice questions, each annotated with necessary linguistic details, to evaluate LLMs in a few-shot setting.","We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate recent <mask>, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","Our analysis reveals that even advanced models like GPT-4 struggle with the grammatical nuances of indexical shift in Turkish, achieving only moderate performance.",recent multilingual LLMs,multilingual LLMs,LLM,evaluate,multilingual LLMs,0,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5,","We evaluate recent <mask>, including GPT-4, GPT-3.5,"
3_acl_28_45074_3,These pre-trained models are used to extract answers to the targeted questions.,"We design a system using these pre-trained models to answer questions, based on the given context.","We design a <mask> using these pre-trained models to answer questions, based on the given context.",The results validate the effectiveness of the systems in understanding nuanced financial language and offers a tool for multi-lingual text analysis.,a system,system,system,design,system,0,We design a system using these pre-trained,We design a <mask> using these pre-trained
3_acl_41_45371_2,"In this paper, we introduce Explicit Models of Opponents (EMO) based on Large Language Models (LLMs), enabling agents to better predict and adapt to diverse, dynamic multi-agent interactions.","Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.","Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual <mask> for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.","We test EMO alongside several reasoning methods in multi-player deduction games, where agents must infer hidden information about their opponents.",an individual model,model,model,construct,model,0,constructs an individual model for each opponent,constructs an individual <mask> for each opponent
3_acl_104_45430_3,We demonstrate that the current generation of language models shows significant competence at deciphering cryptic crossword clues and outperforms previously reported state-of-the-art (SoTA) results by a factor of 2-3 in relevant benchmarks.,"We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.","We also develop a <mask> that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.","Additionally, we demonstrate that LLMs generalize well and are capable of supporting answers with sound rationale.",a search algorithm,search algorithm,algorithm,develop,search algorithm,0,also develop a search algorithm that builds off,also develop a <mask> that builds off
3_acl_6_46417_4,"Focusing on Masked Language Models, we introduce a novel domain-specific masking strategy that is designed to facilitate continual learning while minimizing the training cost.","Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.","Using this approach, we train and present a <mask> trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.",,a BERT-based model,BERT-based model,model,"train,present",BERT-based model,0,and present a BERT-based model trained on a,and present a <mask> trained on a
3_acl_7_60414_2,"When the typing history contains numerous errors, as in open-vocabulary predictive typing with brain-computer interface (BCI) systems, we observe significant performance degradation in both n-gram and recurrent neural network language models trained on clean text.","In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.","In evaluations of ranking character predictions, training <mask> on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.",We also propose an effective strategy for combining evidence from multiple ambiguous histories of BCI electroencephalogram measurements.,recurrent LMs,recurrent LMs,LM,train,recurrent LMs,0,"character predictions, training recurrent LMs on noisy text","character predictions, training <mask> on noisy text"
3_acl_27_46301_5,"These labels are grouped into nine main categories, and we train a DeBERTa-v3 classifier to scale the filtering to a 10B-token subset of FineWeb.","To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.","To test the impact of our filtering, we train <mask> on both the original and the filtered datasets.","The results show that models trained on the filtered data achieve higher accuracy on the HellaSwag benchmark and reach their performance targets faster, even with up to 25% less data.",GPT-2 models,GPT-2 models,model,train,GPT-2 models,0,"filtering, we train GPT-2 models on both the","filtering, we train <mask> on both the"
3_arx_1910.06294_1190213_4,"However, such models impose a heavy memory and computational burden, making it a challenge to train and deploy such models for inference use.",In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact <mask> using labeled and unlabeled examples.,"Preliminary evaluations show that the compact models can achieve competitive accuracy with 36x compression rate when compared with a state-of-the-art pre-trained language model, and run significantly faster in inference, allowing deployment of such models in production environments or on edge devices.",a fast and compact model,model,model,train,model,0,fast and compact model using labeled and,fast and compact <mask> using labeled and
3_acl_1_7090_2,"While paired data are basically required to train the seq2seq model, the external LM can be trained with only unpaired data.","Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.","Thus, it is important to leverage memorized knowledge in the external LM for building the <mask>, since it is hard to prepare a large amount of paired data.","However, the existing fusion methods assume that the LM is integrated with recurrent neural network-based seq2seq models instead of the Transformer.",the seq2seq model,seq2seq model,model,build,seq2seq model,0,"for building the seq2seq model, since it is","for building the <mask>, since it is"
3_acl_928_27649_2,"As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health.","Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.","Using the TREC Misinformation dataset, we empirically evaluate <mask> to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.",We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type.,ChatGPT,ChatGPT,ChatGPT,evaluate,ChatGPT,0,we empirically evaluate ChatGPT to show not,we empirically evaluate <mask> to show not
3_acl_582_29618_3,"While the authors show that attention patterns in specialized attention heads of GPT-2 are consistent with a key prediction of cue-based retrieval models, similarity-based interference effects, their method requires the identification of syntactically specialized attention heads, and makes an cognitively implausible implicit assumption that hundreds of memory retrieval operations take place in parallel.","In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.","In the present work, we develop a <mask> with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.",We show that our model’s single attention head can capture semantic and syntactic interference effects observed in human experiments.,a recurrent neural language model,recurrent neural language model,model,develop,recurrent neural language model,0,we develop a recurrent neural language model with a single,we develop a <mask> with a single
3_acl_418_44990_5,"Surprisingly, medical LLMs do not outperform generalist models in accuracy, though they provide slightly better justifications while making more context-related errors.","These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.","These findings highlight the challenges of developing <mask> for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.","Our results underscore the need for specialized, fine-tuned models aligned with core mental health counseling competencies and supported by human oversight before real-world deployment.",AI,AI,AI,develop,AI,0,challenges of developing AI for mental health,challenges of developing <mask> for mental health
3_acl_45_45963_3,"To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals.",Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.,Our approach employs a pool of candidate VPs and trains a <mask> to dynamically select the most effective VP for a given input image.,"This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs.",a router model,router model,model,train,router model,0,and trains a router model to dynamically select,and trains a <mask> to dynamically select
3_arx_2308.00624_1888516_6,"To address this issue, we introduce the model named JIANG (Chinese pinyin of ginger) specifically designed for the Chinese language.",We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.,We have gathered a substantial amount of Chinese corpus to train the <mask> and have also optimized its structure.,The extensive experimental results demonstrate the excellent performance of our model.,the model,model,model,train,model,0,to train the model and have also,to train the <mask> and have also
2_acl_75_14800_1,Modern transformer-based language models are revolutionizing NLP.,"However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.","However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the <mask> during training.","Thus, the aim of this study is to examine behavior of the model BERT in the task of masked language modelling and to provide linguistic interpretation to the unexpected effects and errors produced by the model.",the model,model,model,acquire,model,0,acquired by the model during training.,acquired by the <mask> during training.
2_acl_98_33764_7,"Other models,including GPT3.5 and several open-source models, demonstrate a lower accuracy ranging from20% to 60% on multiple-choice questions.","Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.","Human raters were asked to rate the explanation of the implicatures generated by <mask> on their reasonability, logic and fluency.","While all mod-els generate largely fluent and self-consistent text, their explanations score low on reasonabilityexcept for GPT-4, suggesting that most LLMs cannot produce satisfactory explanations of theimplicatures in the conversation.",LLMs,LLMs,LLM,generate,LLMs,0,"implicatures generated by LLMs on their reasonability,","implicatures generated by <mask> on their reasonability,"
2_acl_1_42012_5,"In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs.","We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.","We posit that multiple solutions to a reasoning task, generated by an <mask>, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.","Therefore, we propose the Reasoning Graph Verifier (GraphReason) to analyze and verify the solutions generated by LLMs.",an LLM,LLM,LLM,generate,LLM,0,"generated by an LLM, can be represented","generated by an <mask>, can be represented"
2_acl_600_45881_2,"While research has attempted to identify and mitigate such biases, most efforts have been concentrated around English, lagging the rapid advancement of LLMs in multilingual settings.","In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs.","In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by <mask>.","The dataset includes stereotypes from 20 regions around the world and 16 languages, spanning multiple identity categories subject to discrimination worldwide.",LLMs,LLMs,LLM,learn,LLMs,0,be learned by LLMs. ,be learned by <mask>. 
3_arx_2306.02920_1856068_2,"This work sheds light on the second language (L2) acquisition of LMs, while previous work has typically explored their first language (L1) acquisition.","Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.","Specifically, we trained <mask> with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.","Our exploratory experiments demonstrated that the L1 pretraining accelerated their linguistic generalization in L2, and language transfer configurations (e.g., the L1 choice, and presence of parallel texts) substantially affected their generalizations.",bilingual LMs,bilingual LMs,LM,train,bilingual LMs,0,"Specifically, we trained bilingual LMs with a scenario","Specifically, we trained <mask> with a scenario"
