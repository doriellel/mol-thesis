id,Previous Sentence,Current Sentence,Masked Sentence,Next Sentence,AI Phrase,Suggested Mask,AI Entity,Anthropomorphic Component,Target Expression,Animated
1_acl_131_34995_1,This study explores the proactive ability of LLMs to seek user support.,"We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.","We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether <mask> can determine when to request help under varying information availability.","Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support.",LLMs,LLMs,LLM,determine,LLMs,1
1_acl_7_26333_1,We introduce a new in-context learning paradigm to measure Large Language Models’ (LLMs) ability to learn novel words during inference.,"In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.","In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the <mask> must understand to complete the task.",Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt.,the model,model,model,understand,model,1
1_acl_1_13930_4,"This talk will discuss various problems in representing MWEs, and the extent to which LMs address them: • Do LMs capture the implicit relationship between constituents in compositional MWEs (from baby oil through parsley cake to cheeseburger stabbing)?",• Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?,• Do <mask> recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?,"• Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?",LMs,LMs,LM,recognize,LMs,1
1_acl_335_27056_0,"These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.","The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","The increased deployment of <mask> for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like “I’m sure it’s”, “I think it’s”, or “Wikipedia says it’s” affect models, and whether they contribute to model failures.",LMs,LMs,LM,think,LMs,1
1_acl_335_27056_0,"These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.","The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what <mask> think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.","Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like “I’m sure it’s”, “I think it’s”, or “Wikipedia says it’s” affect models, and whether they contribute to model failures.",LMs,LMs,LM,think,LMs,1
1_acl_52_19823_2,"However, in a dynamic world, new entities constantly arise.",We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.,We propose a framework to analyze what <mask> can infer about new entities that did not exist when the LMs were pretrained.,"We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles, from which we can find sentences about each entity.",LMs,LMs,LM,infer,LMs,1
1_acl_52_19823_2,"However, in a dynamic world, new entities constantly arise.",We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.,We propose a framework to analyze what LMs can infer about new entities that did not exist when the <mask> were pretrained.,"We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles, from which we can find sentences about each entity.",LMs,LMs,LM,infer,LMs,1
1_acl_615_27336_1,Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context.,"These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.","These two sources can disagree, causing competition within the model, and it is unclear how an <mask> will resolve the conflict.","On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations.",an LM,LM,LM,resolve a conflict,LM,1
1_acl_12_42096_5,"To quantify information leakage in such setups, we introduce two privacy measures.",We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.,We then propose a system that leverages the recently introduced social learning paradigm in which <mask> collaboratively learn from each other by exchanging natural language.,"Using this paradigm, we demonstrate on several datasets that our methods minimize the privacy loss while at the same time improving task performance compared to a non-cascade baseline.",LLMs,LLMs,LLM,collaboratively learn from each other,LLMs,1
1_acl_508_29544_3,"To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate.","Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.","Through extensive experiments on various datasets, <mask> can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.",Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance.,LLMs,LLMs,LLM,collaborate,LLMs,1
1_acl_508_29544_3,"To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate.","Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.","Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior <mask>.",Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance.,LLMs,LLMs,LLM,collaborate,LLMs,1
1_arx_2106.15110_1492874_1,"Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for.","But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.","But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the <mask> should memorize.","We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum -- those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data.",the model,model,model,memorize,model,1
1_acl_263_41409_3,"Inspired by research on jailbreak attack patterns, we argue this is caused by mismatched generalization.","That is, the LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.","That is, the <mask> only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.",We refer to this phenomenon as fake alignment and construct a comparative benchmark to empirically verify its existence in LLMs.,the LLM,LLM,LLM,remember,LLM,1
1_acl_392_27113_5,"Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2.","During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it.","During the test stage, given a test question, the <mask> recalls relevant memory to help itself reason and answer it.","Experimental results show that MoT can help ChatGPT significantly improve its abilities in arithmetic reasoning, commonsense reasoning, factual reasoning, and natural language inference.",the LLM,LLM,LLM,recall,LLM,1
1_arx_2304.10149_1828195_10,We conduct human evaluations on two explainability-oriented tasks to more accurately evaluate the quality of contents generated by different models.,And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results.,And the human evaluations show <mask> can truly understand the provided information and generate clearer and more reasonable results.,We hope that our study can inspire researchers to further explore the potential of language models like ChatGPT to improve recommendation performance and contribute to the advancement of the recommendation systems field.,ChatGPT,ChatGPT,ChatGPT,understand,ChatGPT,1
1_arx_2212.13371_1769548_6,"In two separate experiments, we then employ this method in hundreds of trust games between an AI agent (a Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ).","In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.","In our first experiment, we find that the <mask> decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.",Our second experiment replicates and extends these findings by automating game play and by homogenizing question wording.,the AI agent,AI agent,agent,decide to trust,AI agent,1
1_arx_2109.14723_1537544_1,"Although pretrained language models (PTLMs) contain significant amounts of world knowledge, they can still produce inconsistent answers to questions when probed, even after specialized training.","As a result, it can be hard to identify what the model actually ""believes"" about the world, making it susceptible to inconsistent behavior and simple errors.","As a result, it can be hard to identify what the <mask> actually ""believes"" about the world, making it susceptible to inconsistent behavior and simple errors.",Our goal is to reduce these problems.,the model,model,model,believe,model,1
1_arx_2210.01478_1722833_1,AI systems are becoming increasingly intertwined with human life.,"In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions.","In order to effectively collaborate with humans and ensure safety, <mask> need to be able to understand, interpret and predict human moral judgments and decisions.","Human moral judgments are often guided by rules, but not always.",AI systems,AI systems,system,"understand,interpret,predict human moral judgments and decisions",AI systems,1
1_arx_2212.01681_1757858_6,These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language.,I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.,I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- <mask> infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.,"Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.",LMs,LMs,LM,infer,LMs,1
1_arx_2303.17557_1817354_1,The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training.,What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?,What does a <mask> remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?,"Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs.",a model,model,model,remember,model,1
1_arx_1810.06338_1037525_0,The methodology is implemented in the new XAI-Plan framework.,"In order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.","In order to engender trust in AI, humans must understand what an <mask> is trying to achieve, and why.","To overcome this problem, the underlying AI process must produce justifications and explanations that are both transparent and comprehensible to the user.",an AI system,AI system,system,try to achieve,AI system,1
1_arx_2210.09492_1730847_6,"To probe more deeply, we construct prompts that require the relevant kind of conceptual reasoning.","Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items.","Here, we fail to find convincing evidence that <mask> is reasoning about more than just individual lexical items.",These results highlight the importance of controlling for low-level distributional regularities when assessing whether a large language model latently encodes a deeper theory.,GPT-3,GPT-3,GPT-3,reason,GPT-3,1
1_arx_2302.04761_1789695_2,"They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel.","In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.","In this paper, we show that <mask> can teach themselves to use external tools via simple APIs and achieve the best of both worlds.","We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction.",LMs,LMs,LM,teach,LMs,1
1_arx_2212.09561_1765738_6,We take the conclusion obtained by CoT as one of the conditions for solving the original problem.,"By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.","By performing a backward verification of the answers that <mask> deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.","Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets.",LLM,LLM,LLM,deduce,LLM,1
1_acl_322_28458_1,"While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information.","To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.","To measure whether an <mask> prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.","Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article.",an LLM,LLM,LLM,prefer,LLM,1
1_acl_378_29414_7,"We present a new method, self-ask, that further improves on chain of thought.","In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question.","In our method, the <mask> explicitly asks itself (and then answers) follow-up questions before answering the initial question.","We finally show that self-ask’s structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",the model,model,model,ask,model,1
1_arx_2304.05376_1823422_8,"Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks.","Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.","Surprisingly, we find that <mask> as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.","Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.",GPT-4,GPT-4,GPT-4,distinguish,GPT-4,1
1_arx_2304.05376_1823422_8,"Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks.","Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.","Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong <mask> completions and Chemcrow's performance.","Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.",GPT-4,GPT-4,GPT-4,distinguish,GPT-4,1
1_arx_2212.02911_1759088_4,This way the model can benefit from the superior natural language understanding performance of RoBERTa and the good natural language generation performance of GPT-2.,Our evaluation shows that the model can create French poetry successfully.,Our evaluation shows that the <mask> can create French poetry successfully.,"On a 5 point scale, the lowest score of 3.57 was given by human judges to typicality and emotionality of the output poetry while the best score of 3.79 was given to understandability.",the model,model,model,create French poetry,model,1
1_arx_2303.18027_1817824_5,"However, our evaluation also exposes critical limitations of the current LLM APIs.","First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.","First, <mask> sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.","Further, our analysis shows that the API costs are generally higher and the maximum context size is smaller for Japanese because of the way non-Latin scripts are currently tokenized in the pipeline.",LLMs,LLMs,LLM,select,LLMs,1
1_arx_2304.09048_1827094_1,Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language.,"However, a large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.","However, a <mask> trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.","Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks.",a large generative language model,large generative language model,model,demonstrate capability in understanding natural language,large generative language model,1
1_acl_1_13930_5,• Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?,"• Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?","• Do <mask> know idioms, and can they infer the meaning of new idioms from the context as humans often do?",,LMs,LMs,LM,know,LMs,1
1_acl_299_27020_1,"Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities.","However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.","However, it is unclear whether <mask> perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.","In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks.",LMs,LMs,LM,cheat,LMs,1
1_acl_242_32404_6,"Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM.",We only conduct retrieval for the missing knowledge in questions that the LLM does not know.,We only conduct retrieval for the missing knowledge in questions that the <mask> does not know.,"Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.",the LLM,LLM,LLM,know,LLM,1
1_arx_2012.11976_1399854_6,"Our framework is based on a systematic literature review, in which we extracted common and distinguishing features of various open-source and commercial (or in-house) platforms.","Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs.","Inspired by language reference frameworks, we identify different maturity levels that a <mask> may exhibit in understanding and responding to user inputs.","Our framework can guide organizations in selecting a conversational AI development platform according to their needs, as well as helping researchers and platform developers improving the maturity of their platforms.",a conversational AI development platform,conversational AI development platform,platform,exhibit maturity levels,conversational AI development platform,1
1_acl_22_34383_5,"First, the fine-tuned domain-specific expert recommends top K candidate categories for a given input product.","Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.","Then, the more general <mask>, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.",We introduce a new in-context learning approach that utilizes LLM self-generated summarization to provide clearer instructions and enhance its performance.,the more general LLM-based expert,LLM-based expert,expert,"analyze,select",LLM-based expert,1
1_arx_2107.04022_1498034_6,"To make AI agents more human centric, we argue that there is a need for a mechanism that helps AI agents identify when to break rules set by their designers.","To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons.","To understand when <mask> need to break rules, we examine the conditions under which humans break rules for pro-social reasons.","In this paper, we present a study that introduces a 'vaccination strategy dilemma' to human participants and analyses their responses.",AI agents,AI agents,agent,break rules,AI agents,1
1_acl_519_45829_8,"Moreover, they exhibit strong transferability, achieving a 7.4% performance gain on out-of-domain models.",These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.,These results suggest that <mask> can autonomously develop effective model-improvement techniques beyond human intuition.,,LLMs,LLMs,LLM,autonomously develop model-improvement techniques,LLMs,1
1_acl_131_34995_2,"We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.","Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support.","Our experiments show that without external feedback, many <mask> struggle to recognize their need for user support.",The findings highlight the importance of external signals and provide insights for future research on improving support-seeking strategies.,many LLMs,LLMs,LLM,struggle to recognize,LLMs,1
1_acl_296_12174_4,"To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning.","By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.","By relation-augmented training, the <mask> learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.","Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.",the model,model,model,"learn,reason",model,1
1_acl_518_29554_3,"Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer.","Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.","Consequently, <mask> struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.",We show that co-occurrence bias remains despite scaling up model sizes or finetuning.,LLMs,LLMs,LLM,struggle to recall,LLMs,1
1_arx_2303.08014_1807811_6,"The models associated unfamiliar words with different meanings depending on their forms, continued to access recently encountered meanings of ambiguous words, reused recent sentence structures, attributed causality as a function of verb semantics, and accessed different meanings and retrieved different words depending on an interlocutor's identity.","In addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence.","In addition, <mask>, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence.","Finally, unlike humans, neither model preferred using shorter words to convey less informative content, nor did they use context to resolve syntactic ambiguities.",ChatGPT,ChatGPT,ChatGPT,"interpret,draw an inference,overlook",ChatGPT,1
1_arx_2303.09461_1809258_3,"We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students.","We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points.","We find that <mask> narrowly passed the exam, obtaining 20.5 out of 40 points.",This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams.,ChatGPT,ChatGPT,ChatGPT,pass an exam,ChatGPT,1
1_acl_57_25373_2,One area where AI can have a significant impact is in the coaching of contact center agents.,"By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.","By analyzing call transcripts, <mask> can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.","In this paper, we present “AI Coach Assis”, which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors.",AI,AI,AI,"determine,provide feedback and insights",AI,1
1_acl_212_35072_9,It can be filled with validated knowledge and progressively expanded.,"When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.","When an LLM encounters questions outside its domain, the <mask> recognizes its knowledge scope and determines whether it can answer the question independently.","Additionally, we introduce a method for automatically and efficiently expanding the knowledge base of LLMs.",the system,system,system,"recognize,determine",system,1
1_acl_150_26871_4,"In this paper, we propose the concept of task-level thinking steps that can eliminate bias introduced by demonstrations.","Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations.","Further, to help <mask> distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations.","Experimental results prove the superiority of our proposed method, achieving best performance on three kinds of challenging classification tasks in the zero-shot and few-shot settings.",LLMs,LLMs,LLM,distinguish,LLMs,1
1_acl_593_32755_6,Analysis shows that the proposed automatic metric aligns well with human preference.,"Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.","Our experimental results show that while <mask> demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.",Code and data are available at https://github.com/Eleanor-H/CLOMO.,LLMs,LLMs,LLM,demonstrate a notable capacity for logical counterfactual thinking,LLMs,1
1_acl_590_35441_1,"Recent work shows that causal facts can be effectively extracted from LLMs through prompting, facilitating the creation of causal graphs for causal inference tasks.","However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize.","However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the <mask> can memorize.","Thus, this work investigates: Can LLMs infer causal relations from other relational data in text?",the model,model,model,memorize,model,1
1_acl_7_43457_1,Sarcasm detection is a significant challenge in sentiment analysis due to the nuanced and context-dependent nature of verbiage.,"We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.","We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (<mask>) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.","Using state-of-the-art LLMs such as LLaMA-3-8B, GPT-4o, and Claude 3.5 Sonnet, PMP achieves state-of-the-art performance on GPT-4o on MUStARD and SemEval2018.",LLMs,LLMs,LLM,"interpret,consider contextual cues,reflect",LLMs,1
1_acl_7_43457_1,Sarcasm detection is a significant challenge in sentiment analysis due to the nuanced and context-dependent nature of verbiage.,"We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.","We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping <mask> interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.","Using state-of-the-art LLMs such as LLaMA-3-8B, GPT-4o, and Claude 3.5 Sonnet, PMP achieves state-of-the-art performance on GPT-4o on MUStARD and SemEval2018.",LLMs,LLMs,LLM,"interpret,consider contextual cues,reflect",LLMs,1
1_acl_350_43916_2,This hinders a thorough understanding of the current level of LLM capabilities.,"For instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in.","For instance, it is widely accepted that <mask> perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in.",This paper introduces a novel perspective on the evaluation of LLMs that leverages a hierarchical classification of tasks.,LLMs,LLMs,LLM,excel and struggle in specific cognitive areas,LLMs,1
1_acl_1809.00066_1020082_2,"We instrument a character language model with several probes, finding that it can develop a specific unit to identify word boundaries and, by extension, morpheme boundaries, which allows it to capture linguistic properties and regularities of these units.","Our language model proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness.","Our <mask> proves surprisingly good at identifying the selectional restrictions of English derivational morphemes, a task that requires both morphological and syntactic awareness.","Thus we conclude that, when morphemes overlap extensively with the words of a language, a character language model can perform morphological abstraction.",Our language model,language model,model,identify,language model,1
1_acl_1810.09030_1040217_7,The evaluation shows that the crowd workflow is more effective with the help of machine learning techniques.,"AI developers found that our system can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.","AI developers found that our <mask> can help them discover unknown errors made by the AI models, and engage in the process of proactive testing.",,our system,system,system,"help discover unknown errors,engage in the process of proactive testing",system,1
1_acl_201_29237_3,"In this position paper, we discuss the challenges associated with employing LLMs to enhance students’ mathematical problem-solving skills by providing adaptive feedback.","Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers.","Apart from generating the wrong reasoning processes, <mask> can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions’ rationales when attempting to correct students’ answers.",Three research questions are formulated.,LLMs,LLMs,LLM,"misinterpret the meaning,exhibit difficulty in understanding",LLMs,1
1_acl_218_28354_4,"Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space.",Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.,Our evaluation finds that it is increasingly challenging for <mask> to identify analogies when going up the analogy taxonomy.,,LLMs,LLMs,LLM,identify,LLMs,1
1_arx_2304.04966_1823012_7,"Finally, we attempted to develop an AI-based handy mobile application which would not only efficiently predict harvest time, estimate coffee yield and quality, but also inform about plant health.","Resultantly, the developed model efficiently analyzed the test data with a mean average precision of 0.89.","Resultantly, the developed <mask> efficiently analyzed the test data with a mean average precision of 0.89.","Strikingly, our innovative semi-supervised method with an mean average precision of 0.77 for multi-class mode surpassed the supervised method with mean average precision of only 0.60, leading to faster and more accurate annotation.",the developed model,model,model,analyze,model,1
1_arx_2010.09890_1366220_2,"In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently.","To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","To succeed, the <mask> needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).","For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines.",the AI agent,AI agent,agent,"understand,coordinate",AI agent,1
1_acl_1477_40886_4,"Then, we conduct experiments on four popular topic segmentation datasets and two discourse parsing datasets.",The experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations.,The experimental results showcase that <mask> demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations.,We also found that ChatGPT hardly understands rhetorical structures that are more complex than topic structures.,ChatGPT,ChatGPT,ChatGPT,"demonstrate proficiency,struggle",ChatGPT,1
1_arx_2303.09387_1809184_9,"Our overall assessment is that while some progress has been made in defining and measuring manipulation from AI systems, many gaps remain.","In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that AI systems learn to manipulate humans without the intent of the system designers.","In the absence of a consensus definition and reliable tools for measurement, we cannot rule out the possibility that <mask> learn to manipulate humans without the intent of the system designers.","We argue that such manipulation poses a significant threat to human autonomy, suggesting that precautionary actions to mitigate it are warranted.",AI systems,AI systems,system,learn to manipulate,AI systems,1
1_arx_2212.01681_1757858_4,"I argue that LMs are models of intentional communication in a specific, narrow sense.","When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context.","When performing next word prediction given a textual context, an <mask> can infer and represent properties of an agent likely to have produced that context.",These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language.,an LM,LM,LM,infer,LM,1
1_arx_1808.10627_1019773_2,We briefly discuss the leading hypotheses about the licensing contexts that allow negative polarity items and evaluate to what extent a neural language model has the ability to correctly process a subset of such constructions.,"We show that the model finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.","We show that the <mask> finds a relation between the licensing context and the negative polarity item and appears to be aware of the scope of this context, which we extract from a parse tree of the sentence.","With this research, we hope to pave the way for other studies linking formal linguistics to deep learning.",the model,model,model,find,model,1
