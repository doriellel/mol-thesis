7_acl_8_10945_3	We present a dataset for evaluating perspective inference in English, ProSPer, and use it to explore how humans and Transformer-based language models infer perspective.	Although the best bidirectional model performs similarly to humans, they display different strengths: humans outperform neural networks in conversational contexts, while RoBERTa excels at written genres.	Although the best <mask> performs similarly to humans, they display different strengths: humans outperform neural networks in conversational contexts, while RoBERTa excels at written genres.		the best bidirectional model	bidirectional model	model	perform similarly to humans	bidirectional model	2
7_acl_5_26147_5	We traced the cause of this divergence to specific attention heads in a middle layer.	Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans.	Adding a power-law recency bias to these attention heads yielded a <mask> that performs much more similarly to humans.	We hope that this scenario will spur future work in bringing LMs closer to human behavior.	a model	model	model	perform similarly to humans	model	2
7_acl_24_22807_4	We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics.	Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds.	Our results show that <mask> scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds.	We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model.	GPT-3	GPT-3	GPT-3	score similarly to human samples in terms of personality	GPT-3	2
7_acl_572_45853_6	We focus specifically on projection inferences, a type of inference that directly probes belief attribution.	We find that some LLMs are sensitive to factors that affect the inference process similarly to humans, yet there remains variance in human behavior not fully captured by LLMs.	We find that some <mask> are sensitive to factors that affect the inference process similarly to humans, yet there remains variance in human behavior not fully captured by LLMs.	The RSA model, on the other hand, outperforms LLMs in capturing the variances in human data, suggesting that explicit belief representation might be necessary to construct human-like projection inferences.	LLMs	LLMs	LLM	inference process similarly to humans	LLMs	2
7_acl_9_38794_1	Inferences from adjective-noun combinations like “Is artificial intelligence still intelligence?” provide a good test bed for LLMs’ understanding of meaning and compositional generalization capability, since there are many combinations which are novel to both humans and LLMs but nevertheless elicit convergent human judgments.	We study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.	We study a range of LLMs and find that the largest <mask> we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.	We also propose three methods to evaluate LLMs on these inferences out of context, where there is a distribution of human-like answers rather than a single correct answer.	the largest models we tested	models	model	human-like inferences	models	2
7_arx_2303.09038_1808835_0	Our results show that it is feasible to utilize large language models in clinical education, and further efforts are needed to address limitations and maximize their potential.	The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities.	The <mask> called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities.	In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare.	The large language model called ChatGPT	large language model	model	human-like expression and reasoning abilitiies	large language model	2
7_acl_8_4584_5	This work utilizes the image captioning model to capture the semantics of the input image and a modular design to generate a probability distribution for semantic topics.	It gives any autonomous system the ability to process visual information in a human-like way and generates more insights which are hardly possible with a conventional algorithm.	It gives any autonomous <mask> the ability to process visual information in a human-like way and generates more insights which are hardly possible with a conventional algorithm.	Here a model and data collection method are proposed.	any autonomous system	system	system	process visual information in a human-like way	system	2
7_acl_38_14688_1	We present on-going work of evaluating the, to our knowledge, first large generative language model trained to converse in Swedish, using data from the online discussion forum Flashback.	We conduct a human evaluation pilot study that indicates the model is often able to respond to conversations in both a human-like and informative manner, on a diverse set of topics.	We conduct a human evaluation pilot study that indicates the <mask> is often able to respond to conversations in both a human-like and informative manner, on a diverse set of topics.	While data from online forums can be useful to build conversational systems, we reflect on the negative consequences that incautious application might have, and the need for taking active measures to safeguard against them.	the model	model	model	respond to conversations in a human-like manner	model	2
7_acl_188_24338_1	Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements.	Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge.	Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the <mask> to integrate human-like syntactic and semantic knowledge.	In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to handle elliptical sentences and to identify the omitted arguments at different degrees of thematic fit, ranging from highly typical participants to semantically anomalous ones.	the model	model	model	human-like syntactic and semantic knowledge	model	2
7_acl_245_24395_3	We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction.	This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier.	This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning <mask> with system 1 as the generator and system 2 as the verifier.	In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator.	a human-like reasoning architecture	architecture	architecture	human-like reasoning	architecture	2
7_acl_115_26836_3	Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses.	With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative.	With <mask> increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative.	Prior work often treats gender bias as a binary classification task.	LLMs	LLMs	LLM	human-like fluency	LLMs	2
7_acl_7_25850_0	This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.	The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like conversations.	The recent release of <mask> has garnered widespread recognition for its exceptional ability to generate human-like conversations.	Given its usage by users from various nations and its training on a vast multilingual corpus that includes diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation.	ChatGPT	ChatGPT	ChatGPT	human-like conversations	ChatGPT	2
7_arx_2303.17276_1817073_8	Remarkably, the production of human-like fallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in GPT-4.	This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.	This suggests that larger and more advanced <mask> may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.	"According to ETR, the same fundamental patterns are involved both in successful and unsuccessful ordinary reasoning, so that the ""bad"" cases could paradoxically be learned from the ""good"" cases."	larger and more advanced LLMs	LLMs	LLM	human-like mistakes	LLMs	2
7_arx_2303.17466_1817263_0	This study provides valuable insights into the cultural implications of ChatGPT and highlights the necessity of greater diversity and cultural awareness in language technologies.	The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue.	The recent release of <mask> has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue.	Given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation.	ChatGPT	ChatGPT	ChatGPT	human-like responses in dialogue	ChatGPT	2
7_arx_1909.08258_1178296_1		Our research is focused on making a human-like question answering system which can answer rationally.	Our research is focused on making a human-like <mask> which can answer rationally.	"The distinguishing characteristic of our approach is that it will use automated common sense reasoning to truly ""understand"" dialogues, allowing it to converse like a human."	a human-like question answering system	question answering system	system	human-like	question answering system	2
7_arx_1803.10813_960946_1	The recent successes of AI have captured the wildest imagination of both the scientific communities and the general public.	Robotics and AI amplify human potentials, increase productivity and are moving from simple reasoning towards human-like cognitive abilities.	<mask> amplify human potentials, increase productivity and are moving from simple reasoning towards human-like cognitive abilities.	Current AI technologies are used in a set area of applications, ranging from healthcare, manufacturing, transport, energy, to financial services, banking, advertising, management consulting and government agencies.	Robotics and AI	Robotics and AI	AI	human-like cognitive abilities	Robotics and AI	2
7_arx_2401.01623_1981179_2	With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application.	In this paper, we prove in theory that AI can be as creative as humans under the condition that it can properly fit the data generated by human creators.	In this paper, we prove in theory that <mask> can be as creative as humans under the condition that it can properly fit the data generated by human creators.	Therefore, the debate on AI's creativity is reduced into the question of its ability to fit a sufficient amount of data.	AI	AI	AI	creative as humans	AI	2
7_arx_2208.06590_1697302_7	Because most of the modeling of the world, in terms of how to interpret it, by an intelligent subject is the recognition of distal entities and the prediction of their temporal evolution, being able to handle all distal entities is a reasonable goal.	Based on the findings of philosophy and engineering cognitive technology, we predict that in the relatively near future, AI will be able to recognize various entities to the same degree as humans.	Based on the findings of philosophy and engineering cognitive technology, we predict that in the relatively near future, <mask> will be able to recognize various entities to the same degree as humans.		AI	AI	AI	recognize various entities to the same degree as humans	AI	2
7_arx_2306.10063_1863211_3	Learning happens when this distributed system sets goals, builds meaning from data, consolidates understanding, reconciles differences, and transfers knowledge to new domains.	Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors.	Building social generative AI for education will require development of powerful <mask> that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors.	This raises fundamental problems of ethics.	powerful AI systems	AI systems	system	converse with each other as well as humans	AI systems	2
7_acl_29_46237_6	However, our CausalToT and CausalPoT significantly improve performance over existing prompting techniques, suggesting that hybrid approaches combining LLMs with formal reasoning frameworks can mitigate these limitations.	Our findings contribute to understanding LLMs’ reasoning capacities and outline promising strategies for improving their ability to reason causally as humans would.	Our findings contribute to understanding <mask>’ reasoning capacities and outline promising strategies for improving their ability to reason causally as humans would.	We release our code and data.	LLMs	LLMs	LLM	reason casually as humans would	LLMs	2
7_acl_341_41488_2	The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users.	Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems.	Therefore, there has been an urgent need to evaluate <mask> as human-like dialogue systems.	In this paper, we propose DialogBench, a dialogue evaluation benchmark that contains 12 dialogue tasks to probe the capabilities of LLMs as human-like dialogue systems should have.	LLMs	LLMs	LLM	human-like dialogue	LLMs	2
7_acl_45_36822_5	In this work, we propose to evaluate the understanding and generation ability of LLMs to deal with differently structured logical forms by examining the inter-conversion of natural and formal language through in-context learning of LLMs.	Extensive experiments with models of different sizes show that state-of-the-art LLMs can understand formal languages as well as humans, but generating correct logical forms given a few examples remains a challenge.	Extensive experiments with models of different sizes show that state-of-the-art <mask> can understand formal languages as well as humans, but generating correct logical forms given a few examples remains a challenge.	Most importantly, our results also indicate that LLMs exhibit considerable sensitivity.	state-of-the-art LLMs	LLMs	LLM	understand formal languages as well as humans	LLMs	2
7_acl_119_34604_1	Agency, the capacity to proactively shape events, is central to how humans interact and collaborate.	While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration.	While <mask> are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration.	In this paper, we investigate Agency as a desirable function of LLMs, and how it can be measured and managed.	LLMs	LLMs	LLM	simulate human behavior and serve as human-like agents	LLMs	2
7_acl_8_33866_9	Finally, reanalyzing the existing human data suggests that the humans may not perform above chance at the difficult structures initially.	Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably.	Thus, <mask> may indeed process recursively nested grammatical structures as reliably as humans, when evaluated comparably.	This case study highlights how discrepancies in the evaluation methods can confound comparisons of language models and humans.	large LMs	large LMs	LM	process recursively nested grammatical structures as reliably as humans	large LMs	2
7_acl_773_32935_2	As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions.	Observing this, we shift the perspective, by treating LLMs as human-like communicators to examine the interplay between everyday language interaction and AI safety.	Observing this, we shift the perspective, by treating <mask> as human-like communicators to examine the interplay between everyday language interaction and AI safety.	Specifically, we study how to persuade LLMs to jailbreak them.	LLMs	LLMs	LLM	human-like communicators	LLMs	2
7_acl_1_43505_3	This paper studies GPT-4 as a reader-annotator of 21 specific appraisal ratings in different prompt settings, aiming to evaluate and improve its performance compared to human annotators.	We found that GPT-4 is an effective reader-annotator that performs close to or even slightly better than human annotators, and its results can be significantly improved by using a majority voting of five completions.	We found that <mask> is an effective reader-annotator that performs close to or even slightly better than human annotators, and its results can be significantly improved by using a majority voting of five completions.	GPT-4 also effectively predicts appraisal ratings and emotion labels using a single prompt, but adding instruction complexity results in poorer performance.	GPT-4	GPT-4	GPT-4	performs close to or even slightly better than human annotators	GPT-4	2
7_acl_169_28305_5	This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts.	While GPT-3’s performance is not perfect, it is better than that of humans — likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012).	While <mask>’s performance is not perfect, it is better than that of humans — likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012).	Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data.	GPT-3	GPT-3	GPT-3	better than that of humans	GPT-3	2
7_arx_2403.03230_2020349_6	BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet.	Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries.	Like human experts, when <mask> were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries.	Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.	LLMs	LLMs	LLM	like human experts	LLMs	2
7_acl_27_45946_1	Humans are accustomed to reading and writing in a forward manner, and this natural bias extends to text understanding in auto-regressive large language models (LLMs).	This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs.	This paper investigates whether <mask>, like humans, struggle with reverse modeling, specifically with reversed text inputs.	We found that publicly available pre-trained LLMs cannot understand such inputs.	LLMs	LLMs	LLM	struggle with reverse modeling like humans	LLMs	2
7_acl_744_27465_5	We answer this question using open-source LMs.	Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical.	Like previous studies, we find that <mask> behave much like humans when presented with entities whose animacy is typical.	However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans.	LMs	LMs	LM	behave much like humans	LMs	2
7_arx_1808.01884_1011030_1	Artificial Intelligence (AI) has already made a huge impact on our current technological trends.	Through AI developments, machines are now given power and intelligence to behave and work like the human mind.	Through AI developments, <mask> are now given power and intelligence to behave and work like the human mind.	In this research project, we propose and implement an AI based health physician system that would be able to interact with the patient, do the diagnosis and suggest quick remedy or treatment of their problem.	machines	machines	machine	behave and work like the human mind	machines	2
7_arx_2303.12810_1812607_4	This research work aims to investigate the performance of LLMs on different reasoning tasks by conducting experiments that directly use or draw inspirations from existing datasets on analogical and spatial reasoning.	Additionally, to evaluate the ability of LLMs to reason like humans, their performance is evaluted on more open-ended, natural language questions.	Additionally, to evaluate the ability of <mask> to reason like humans, their performance is evaluted on more open-ended, natural language questions.	My findings indicate that LLMs excel at analogical and moral reasoning, yet struggle to perform as proficiently on spatial reasoning tasks.	LLMs	LLMs	LLM	reason like humans	LLMs	2
7_arx_2308.09175_1897067_1	In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks.	However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations.	However, <mask>, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations.	This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality.	AI systems	AI systems	system	make mistakes like humans,have blind spots like humans,hallucinate like humans,struggle to generalize to new situations like humans	AI systems	2
7_arx_2305.15929_1848985_2	Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing.	More concretely, just like humans, ChatGPT has a consonant bias.	More concretely, just like humans, <mask> has a consonant bias.	That is, the chatbot has a tendency to use consonants over vowels to identify words.	ChatGPT	ChatGPT	ChatGPT	have a consonant bias like humans	ChatGPT	2
7_acl_865_37622_6	We analyze the differences in results by model architecture, data, and tokenization.	Our model shows child-like U-shaped learning curves clearly for certain verbs, but the preferences for types of overgeneralization did not fully match the observations in children.	Our <mask> shows child-like U-shaped learning curves clearly for certain verbs, but the preferences for types of overgeneralization did not fully match the observations in children.		Our model	model	model	show child-like U-shaped learning curves	model	2
7_arx_2310.00578_1922835_0	Significantly, this research is pioneering in revealing that nine-year-old children convey stronger emotions than ChatGPT in their Chinese compositions.	ChatGPT has been demonstrated to possess significant capabilities in generating intricate, human-like text, and recent studies have established that its performance in theory of mind tasks is comparable to that of a nine-year-old child.	<mask> has been demonstrated to possess significant capabilities in generating intricate, human-like text, and recent studies have established that its performance in theory of mind tasks is comparable to that of a nine-year-old child.	However, it remains uncertain whether ChatGPT surpasses nine-year-old children in Chinese writing proficiency.	ChatGPT	ChatGPT	ChatGPT	generate intricate human-like text	ChatGPT	2
7_arx_2305.14930_1847986_4	We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise.	In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration.	In a multi-armed bandit task, we find that <mask> pretending to be children of different ages recover human-like developmental stages of exploration.	In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts.	LLMs	LLMs	LLM	human-like developmental stages of exploration	LLMs	2
7_acl_2_34316_4	The 2024 BabyLM Challenge initial dataset of 10M words is filtered to 8.5M. Next, it is supplemented with a randomly selected subset of TVR dataset consisting of 1.5M words of television dialogues.	The latter dataset ensures that similar to children, the model is also exposed to language through media.	The latter dataset ensures that similar to children, the <mask> is also exposed to language through media.	Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it with the limited vocabulary of children in the early stages of language acquisition.	the model	model	model	exposed to language through media similar to children	model	2
7_arx_2212.05206_1761383_2	Therefore, it is of great importance to evaluate their emerging abilities.	In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it.	In this study, we show that <mask> like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it.	However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner.	LLMs like GPT-3	LLMs	LLM	human-like intuition	LLMs	2
7_arx_2306.11530_1864678_0	Finally, we observe the effect of education is uniquely intersectional for Indigenous raters, highlighting the utility of multilevel frameworks for uncovering underrepresented social perspectives.	Conversational AI systems exhibit a level of human-like behavior that promises to have profound impacts on many aspects of daily life -- how people access information, create content, and seek social support.	<mask> exhibit a level of human-like behavior that promises to have profound impacts on many aspects of daily life -- how people access information, create content, and seek social support.	Yet these models have also shown a propensity for biases, offensive language, and conveying false information.	Conversational AI systems	Conversational AI systems	system	human-like behavior	Conversational AI systems	2
7_arx_2305.19103_1852159_2	Recent advancements in large language models (LLMs) offer fresh perspectives on this question.	Although LLMs are trained on restricted modalities, they exhibit human-like performance in diverse psychological tasks.	Although <mask> are trained on restricted modalities, they exhibit human-like performance in diverse psychological tasks.	Our study compared representations of 4,442 lexical concepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple dimensions, including five key domains: emotion, salience, mental visualization, sensory, and motor experience.	LLMs	LLMs	LLM	human-like performance in diverse psychological tasks	LLMs	2
7_arx_2305.16426_1849482_5	In the current paper, we advance this discussion through a systematic study of scalar adverbs, an under-explored class of words with strong logical force.	Using three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these common words.	Using three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which <mask> exhibit general, human-like, knowledge of these common words.	We ask: 1) Do the models distinguish amongst the three semantic categories of MODALITY, FREQUENCY and DEGREE?	BERT, RoBERTa, GPT-2 and GPT-3	BERT, RoBERTa, GPT-2 and GPT-3	BERT,RoBERTa,GPT-2,GPT-3	human-like knowledge of common words	BERT, RoBERTa, GPT-2 and GPT-3	2
7_arx_2305.08883_1841939_0	Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.	LLMs now exhibit human-like skills in various fields, leading to worries about misuse.	<mask> now exhibit human-like skills in various fields, leading to worries about misuse.	Thus, detecting generated text is crucial.	LLMs	LLMs	LLM	human-like skills	LLMs	2
7_arx_2305.05516_1838572_2	I designed prompts and architectures to enable GPT to understand the game rules and to generate both its choices and the reasoning behind decisions.	The key findings show that GPT exhibits behaviours similar to human responses, such as making positive offers and rejecting unfair ones in the ultimatum game, along with conditional cooperation in the prisoner's dilemma.	The key findings show that <mask> exhibits behaviours similar to human responses, such as making positive offers and rejecting unfair ones in the ultimatum game, along with conditional cooperation in the prisoner's dilemma.	The study explores how prompting GPT with traits of fairness concern or selfishness influences its decisions.	GPT	GPT	GPT	behaviors similar to human responses	GPT	2
7_arx_2304.07830_1825876_1	Semantic dimensions of sound have been playing a central role in understanding the nature of auditory sensory experience as well as the broader relation between perception, language, and meaning.	Accordingly, and given the recent proliferation of large language models (LLMs), here we asked whether such models exhibit an organisation of perceptual semantics similar to those observed in humans.	Accordingly, and given the recent proliferation of large language models (LLMs), here we asked whether such <mask> exhibit an organisation of perceptual semantics similar to those observed in humans.	Specifically, we prompted ChatGPT, a chatbot based on a state-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic scales.	such models	models	model	organisation of perceptual semantics similar to those observed in humans	models	2
7_arx_2309.13356_1918160_3	A moral development stage score of the respondent is then computed based on the relevance rating and ranking.	Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans.	Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while <mask> show significantly better performance on this task, comparable to adult humans.	GPT-4, in fact, has the highest post-conventional moral reasoning score, equivalent to that of typical graduate school students.	ChatGPT, Llama2-Chat, PaLM-2 and GPT-4	ChatGPT, Llama2-Chat, PaLM-2 and GPT-4	ChatGPT,GPT,Llama	performance comparable to adult humans	ChatGPT, Llama2-Chat, PaLM-2 and GPT-4	2
7_acl_4_45309_4	CogLM comprises 1,220 questions spanning 10 cognitive abilities crafted by more than 20 human experts, providing a comprehensive testbed for the cognitive levels of LLMs.	Through extensive experiments across multiple mainstream LLMs with CogLM, we find that: (1) In our testing framework, advanced LLMs (such as GPT-4) have demonstrated human-like cognitive abilities, comparable to those of a 20-year-old human.	Through extensive experiments across multiple mainstream LLMs with CogLM, we find that: (1) In our testing framework, advanced <mask> (such as GPT-4) have demonstrated human-like cognitive abilities, comparable to those of a 20-year-old human.	(2) The parameter size and optimization objective are two key factors affecting the cognitive levels of LLMs.	advanced LLMs	LLMs	LLM	human-like cognitive abilities	LLMs	2
7_acl_289_35148_0	Human evaluation results on high-resource and low-resource language pairs indicate that DUAT significantly facilitates the understanding alignment, which improves the translation quality (up to +3.85 COMET) and reduces translation literalness by -25% ∼ -51%.	Large Language models (LLMs) have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance.	<mask> have exhibited remarkable abilities in understanding complex texts, offering a promising path towards human-like translation performance.	However, this study reveals the misalignment between the translation-specific understanding and the general understanding inside LLMs.	Large Language models (LLMs)	Large Language models (LLMs)	model	human-like translation performance	Large Language models (LLMs)	2
7_arx_2307.02194_1873190_1	Large Language Models (LLMs) are capable of answering questions in natural language for various purposes.	With recent advancements (such as GPT-4), LLMs perform at a level comparable to humans for many proficient tasks.	With recent advancements (such as GPT-4), <mask> perform at a level comparable to humans for many proficient tasks.	The analysis of business processes could benefit from a natural process querying language and using the domain knowledge on which LLMs have been trained.	LLMs	LLMs	LLM	perform at a level comparable to humans	LLMs	2
7_arx_2308.12578_1900470_0	Consequently, leveraging psychological theories can provide enhanced insights into the underlying mechanisms governing the expressions of explicit and implicit constructs in LLMs.	Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs.	Recent researches indicate that <mask> (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs.	This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology.	Pre-trained Large Language Models	Pre-trained Large Language Models	model	cognitive constructs similar to those observed in humans	Pre-trained Large Language Models	2
