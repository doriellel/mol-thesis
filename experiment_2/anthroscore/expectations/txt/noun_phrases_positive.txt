5_arx_2102.07536_1424025_6	Participants read one type of advice before engaging in a task in which they could lie for profit.	Testing human behaviour in interaction with actual AI outputs, we provide first behavioural insights into the role of AI as an advisor.	Testing human behaviour in interaction with actual AI outputs, we provide first behavioural insights into the role of <mask> as an advisor.	Results reveal that AI-generated advice corrupts people, even when they know the source of the advice.	AI	AI	AI	advisor	AI	1
5_acl_183_26904_4	We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries.	Our objective is to capture the breadth of interactions between a human user and an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively.	Our objective is to capture the breadth of interactions between a human user and an <mask> assistant and employs a comprehensive framework to generate multi-turn conversation iteratively.	UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions.	an AI assistant	AI	AI	assistant	AI	1
5_arx_2505.03380_2313940_0	This work demonstrates how integrated multimodal models capture fine-grained patterns, enabling human-level interpretation in complex scenarios and advancing human-centric AI healthcare.	Medical AI assistants support doctors in disease diagnosis, medical image analysis, and report generation.	Medical <mask> assistants support doctors in disease diagnosis, medical image analysis, and report generation.	However, they still face significant challenges in clinical use, including limited accuracy with multimodal content and insufficient validation in real-world settings.	Medical AI assistants	AI	AI	medical assistant	AI	1
5_arx_2304.09873_1827919_4	The research identifies five research questions and discovers useful prompts for fine-tuning the assistant, which shows that ChatGPT can participate in positive conversations, listen attentively, offer validation and potential coping strategies without providing explicit medical advice, and help therapists discover new insights from multiple conversations with the same patient.	Using ChatGPT as an assistant for psychotherapy poses several challenges that need to be addressed, including technical as well as human-centric challenges which are discussed.	Using <mask> as an assistant for psychotherapy poses several challenges that need to be addressed, including technical as well as human-centric challenges which are discussed.		ChatGPT	ChatGPT	ChatGPT	assistant for psychotherapy	ChatGPT	1
5_acl_5_43351_1	Recent studies have evaluated creativity, where novelty is an important aspect, of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science.	However, assessing the novelty in scholarly publications, a critical facet of evaluating LLMs as scientific discovery assistants, remains underexplored, despite its potential to accelerate research cycles and prioritize high-impact contributions in scientific workflows.	However, assessing the novelty in scholarly publications, a critical facet of evaluating <mask> as scientific discovery assistants, remains underexplored, despite its potential to accelerate research cycles and prioritize high-impact contributions in scientific workflows.	We introduce SchNovel, a benchmark to evaluate LLMsâ€™ ability to assess novelty in scholarly papers, a task central to streamlining discovery pipeline.	LLMs	LLMs	LLM	scientific discovery assistant	LLMs	1
5_acl_50_42804_0	We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.	We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions.	We describe a class of tasks called decision-oriented dialogues, in which <mask> assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions.	We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends.	AI assistants such as large language models (LMs)	AI	AI	assistant	AI	1
5_acl_50_42804_2	We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends.	In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system.	In each of these settings, <mask> assistants and users have disparate abilities that they must combine to arrive at the best decision: Assistants can access and process large amounts of information, while users have preferences and constraints external to the system.	For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach.	AI assistants	AI	AI	assistant	AI	1
5_arx_2008.12095_1339817_1	Intelligent assistants that follow commands or answer simple questions, such as Siri and Google search, are among the most economically important applications of AI.	Future conversational AI assistants promise even greater capabilities and a better user experience through a deeper understanding of the domain, the user, or the user's purposes.	Future <mask> assistants promise even greater capabilities and a better user experience through a deeper understanding of the domain, the user, or the user's purposes.	But what domain and what methods are best suited to researching and realizing this promise?	Future conversational AI assistants	conversational AI	AI	assistant	conversational AI	1
5_arx_2501.13945_2237529_2	For example, in online learning, an AI social assistant may connect learners and thereby enhance social interaction.	These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners.	These social <mask> assistants too need to explain themselves in order to enhance transparency and trust with the learners.	We present a method of self-explanation that uses introspection over a self-model of an AI social assistant.	These social AI assistants	AI	AI	assistant	AI	1
5_arx_2504.15236_2303939_0	By providing the first large-scale empirical mapping of AI values in deployment, our work creates a foundation for more grounded evaluation and design of values in AI systems.	AI assistants can impart value judgments that shape people's decisions and worldviews, yet little is known empirically about what values these systems rely on in practice.	<mask> assistants can impart value judgments that shape people's decisions and worldviews, yet little is known empirically about what values these systems rely on in practice.	To address this, we develop a bottom-up, privacy-preserving method to extract the values (normative considerations stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit in hundreds of thousands of real-world interactions.	AI assistants	AI	AI	assistant	AI	1
5_arx_2502.13321_2256312_1	Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively.	We propose that AI assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance.	We propose that <mask> assistants should adapt their behavior through trust-adaptive interventions to mitigate such inappropriate reliance.	For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user.	AI assistants	AI	AI	assistant	AI	1
5_arx_2401.13275_1992829_0	For questions they attempt to answer, the accuracy is significantly higher than before the alignment.	Recently, AI assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools.	Recently, <mask> assistants based on large language models (LLMs) show surprising performance in many tasks, such as dialogue, solving math problems, writing code, and using tools.	Although LLMs possess intensive world knowledge, they still make factual errors when facing some knowledge intensive tasks, like open-domain question answering.	AI assistants based on large language models (LLMs)	AI	AI	assistant	AI	1
5_arx_2306.16092_1869240_0	Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation.	AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks.	<mask> legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks.	This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services.	AI legal assistants based on Large Language Models (LLMs)	AI	AI	legal assistant	AI	1
5_arx_2312.13103_1974911_1		This paper proposes one of the first clinical applications of multimodal large language models (LLMs) as an assistant for radiologists to check errors in their reports.	This paper proposes one of the first clinical applications of <mask> as an assistant for radiologists to check errors in their reports.	We created an evaluation dataset from real-world radiology datasets (including X-rays and CT scans).	multimodal large language models (LLMs)	multimodal large language models (LLMs)	model	assistant for radiologists	multimodal large language models (LLMs)	1
5_arx_2502.05023_2248014_0	In particular, we generate more than 70,000 method implementations using a range of configurations and prompts, revealing that a larger context increases the likelihood of reproducing copyleft code, but higher temperature settings can mitigate this issue.	AI assistants can help developers by recommending code to be included in their implementations (e.g., suggesting the implementation of a method from its signature).	<mask> assistants can help developers by recommending code to be included in their implementations (e.g., suggesting the implementation of a method from its signature).	Although useful, these recommendations may mirror copyleft code available in public repositories, exposing developers to the risk of reusing code that they are allowed to reuse only under certain constraints (e.g., a specific license for the derivative software).	AI assistants	AI	AI	assistant	AI	1
5_arx_2503.21983_2286295_1	As artificial intelligence (AI) assistants become more widely adopted in safety-critical domains, it becomes important to develop safeguards against potential failures or adversarial attacks.	A key prerequisite to developing these safeguards is understanding the ability of these AI assistants to mislead human teammates.	A key prerequisite to developing these safeguards is understanding the ability of these <mask> assistants to mislead human teammates.	We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions.	these AI assistants	AI	AI	assistant	AI	1
5_arx_2503.21983_2286295_3	We investigate this attack problem within the context of an intellective strategy game where a team of three humans and one AI assistant collaborate to answer a series of trivia questions.	Unbeknownst to the humans, the AI assistant is adversarial.	Unbeknownst to the humans, the <mask> assistant is adversarial.	Leveraging techniques from Model-Based Reinforcement Learning (MBRL), the AI assistant learns a model of the humans' trust evolution and uses that model to manipulate the group decision-making process to harm the team.	the AI assistant	AI	AI	assistant	AI	1
5_arx_2504.15867_2304570_0	Additionally, on a real-world application, HACKODE achieves 75.92% ASR, demonstrating its real-world impact.	Due to insufficient domain knowledge, LLM coding assistants often reference related solutions from the Internet to address programming problems.	Due to insufficient domain knowledge, <mask> coding assistants often reference related solutions from the Internet to address programming problems.	However, incorporating external information into LLMs' code generation process introduces new security risks.	LLM coding assistants	LLM	LLM	coding assistant	LLM	1
5_arx_2411.09224_2191651_5	Based on the results, it has emphasized their strengths and weaknesses and the importance of further modifications to increase the reliability and accuracy of the latest popular models.	Although these AI assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion.	Although these <mask> assistants illustrate a high level of progress in language understanding and code generation, along with ethical considerations and responsible usage, they provoke a necessity for discussion.	With time, developing more refined AI technology is essential for achieving advanced solutions in various fields, especially with the knowledge of the feature intricacies of these models and their implications.	these AI assistants	AI	AI	assistant	AI	1
5_arx_2305.02626_1835682_1	As the popularity of large language models (LLMs) soars across various applications, ensuring their alignment with human values has become a paramount concern.	In particular, given that LLMs have great potential to serve as general-purpose AI assistants in daily life, their subtly unethical suggestions become a serious and real concern.	In particular, given that <mask> have great potential to serve as general-purpose AI assistants in daily life, their subtly unethical suggestions become a serious and real concern.	Tackling the challenge of automatically testing and repairing unethical suggestions is thus demanding.	LLMs	LLMs	LLM	assistant	LLMs	1
5_acl_523_44090_1	From ice cream flavors to climate change, people exhibit a wide array of opinions on various topics, and understanding the rationale for these opinions can promote healthy discussion and consensus among them.	As such, it can be valuable for a large language model (LLM), particularly as an AI assistant, to be able to empathize with or even explain these various standpoints.	As such, it can be valuable for a <mask>, particularly as an AI assistant, to be able to empathize with or even explain these various standpoints.	In this work, we hypothesize that different topic stances often manifest correlations that can be used to extrapolate to topics with unknown opinions.	a large language model (LLM)	large language model (LLM)	model	assistant	large language model (LLM)	1
5_acl_321_45615_1	Large Language Models (LLMs) have demonstrated great potential as generalist assistants, showcasing powerful task understanding and problem-solving capabilities.	To deploy LLMs as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts.	To deploy <mask> as AI assistants, it is crucial that these models exhibit desirable behavioral traits, such as non-toxicity and resilience against jailbreak attempts.	Current approaches for detoxification or preventing jailbreaking usually involve Supervised Fine-Tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF), which requires finetuning billions of parameters through gradient descent with substantial computational cost.	LLMs	LLMs	LLM	assistant	LLMs	1
5_arx_2306.07207_1860355_0	Our code and data are published anonymously at https://github.com/valley-vl/Valley.	Large Language Models (LLMs), with remarkable conversational capability, have emerged as AI assistants that can handle both visual and textual modalities.	<mask>, with remarkable conversational capability, have emerged as AI assistants that can handle both visual and textual modalities.	However, their effectiveness in joint video and language understanding has not been extensively explored.	Large Language Models (LLMs)	Large Language Models (LLMs)	model	assistant	Large Language Models (LLMs)	1
5_arx_2502.20541_2263532_1	This paper presents the development and application of a Large Language Model Retrieval-Augmented Generation (LLM-RAG) system tailored for nanotechnology research.	The system leverages the capabilities of a sophisticated language model to serve as an intelligent research assistant, enhancing the efficiency and comprehensiveness of literature reviews in the nanotechnology domain.	The <mask> leverages the capabilities of a sophisticated language model to serve as an intelligent research assistant, enhancing the efficiency and comprehensiveness of literature reviews in the nanotechnology domain.	Central to this LLM-RAG system is its advanced query backend retrieval mechanism, which integrates data from multiple reputable sources.	The system	system	system	intelligent research assistant	system	1
5_arx_2412.01992_2204370_5_2	We also propose an automated method for analyzing collaboration dynamics that effectively identifies behavioral characteristics of agents with distinct roles, allowing us to quantitatively compare collaboration dynamics in a range of experimental conditions.	For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.	For example, in comparing ChatCollab AI agents, we find that an <mask> CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.	Our code and data can be found at: https://github.com/ChatCollab.	an AI CEO agent	AI	AI	CEO	AI	1
5_arx_1902.03271_1084355_5	In this post, we will closely examine the claims laid out in this paper.	In particular, we will study the individual treatment profiles suggested by their AI Clinician to gain insight into how their AI Clinician intends to treat patients on an individual level.	In particular, we will study the individual treatment profiles suggested by their AI Clinician to gain insight into how their <mask> Clinician intends to treat patients on an individual level.		their AI Clinician	AI	AI	clinician	AI	1
5_arx_2311.16161_1959128_2	The feasibility of this method is demonstrated using a Vision Transformer as the encoder and GPT-2 as the decoder, achieving a seamless integration of visual input and textual interaction.	Departing from conventional practices of employing distinct models for image recognition and text-based coaching, our integrated architecture directly processes input images, enabling natural question-and-answer dialogues with the AI coach.	Departing from conventional practices of employing distinct models for image recognition and text-based coaching, our integrated architecture directly processes input images, enabling natural question-and-answer dialogues with the <mask> coach.	This unique strategy simplifies model architecture while enhancing the overall user experience in human-AI interactions.	the AI coach	AI	AI	coach	AI	1
5_arx_2306.03090_1856238_2	However, the majority of teachers do not have access to consistent, high quality coaching due to limited resources and access to expertise.	We explore whether generative AI could become a cost-effective complement to expert feedback by serving as an automated teacher coach.	We explore whether <mask> could become a cost-effective complement to expert feedback by serving as an automated teacher coach.	In doing so, we propose three teacher coaching tasks for generative AI: (A) scoring transcript segments based on classroom observation instruments, (B) identifying highlights and missed opportunities for good instructional strategies, and (C) providing actionable suggestions for eliciting more student reasoning.	generative AI	generative AI	AI	teacher coach	generative AI	1
5_arx_2501.18948_2242532_3	This paper critiques the automation-centric paradigm, arguing that current reward structures, which largely focus on cost reduction, drive the overwhelming emphasis on task replacement in AI patents.	Meanwhile, Human-Centered AI (HCAI), which envisions AI as a collaborator augmenting human capabilities and aligning with societal values, remains a fugitive from the mainstream narrative.	Meanwhile, Human-Centered AI (HCAI), which envisions <mask> as a collaborator augmenting human capabilities and aligning with societal values, remains a fugitive from the mainstream narrative.	Despite its promise, HCAI has gone ``missing'', with little evidence of its principles translating into patents or real-world impact.	AI	AI	AI	collaborator	AI	1
5_arx_2504.04253_2292956_6	We then introduce methods for assessing user expertise, as well as key open challenges and research questions that must be addressed to allow for an adaptive approach.	Finally, we present our vision for a user-centered system that leverages GenAI not only for automation but as an intelligent collaborator in visual data exploration.	Finally, we present our vision for a user-centered system that leverages <mask> not only for automation but as an intelligent collaborator in visual data exploration.	Our perspective contributes to the broader discussion on designing GenAI-based systems that enhance human cognition by dynamically adapting to the user, ultimately advancing toward systems that promote augmented cognition.	GenAI	GenAI	GenAI	intelligent collaborator	GenAI	1
5_arx_2407.19096_2117483_4	Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews.	Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos.	Study 3 finds that <mask> companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos.	Moreover, consumers underestimate the degree to which AI companions improve their loneliness.	AI companions	AI	AI	companion	AI	1
5_arx_2503.03067_2267379_3	Key findings reveal that users engage with AI companions for emotional comfort, stress relief, and to avoid social pressures.	We identify various roles users assign to AI companions, such as friends, mentors, or romantic partners, and highlights the importance of customization and emotional support in these interactions.	We identify various roles users assign to <mask> companions, such as friends, mentors, or romantic partners, and highlights the importance of customization and emotional support in these interactions.	While AI companions offer advantages like emotional stability and constant availability, they also face limitations in emotional depth and understanding.	AI companions	AI	AI	companion	AI	1
5_arx_2407.19096_2117483_5	Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos.	Moreover, consumers underestimate the degree to which AI companions improve their loneliness.	Moreover, consumers underestimate the degree to which <mask> companions improve their loneliness.	Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week.	AI companions	AI	AI	companion	AI	1
5_arx_2407.19096_2117483_6	Moreover, consumers underestimate the degree to which AI companions improve their loneliness.	Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week.	Study 4 uses a longitudinal design and finds that an <mask> companion consistently reduces loneliness over the course of a week.	Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness.	an AI companion	AI	AI	companion	AI	1
5_arx_2503.03067_2267379_2	Through qualitative research, including 14 semi-structured interviews, the study investigates how these individuals establish and maintain relationships with AI, their perceptions and attitudes towards these entities, and the perspectives of other stakeholders.	Key findings reveal that users engage with AI companions for emotional comfort, stress relief, and to avoid social pressures.	Key findings reveal that users engage with <mask> companions for emotional comfort, stress relief, and to avoid social pressures.	We identify various roles users assign to AI companions, such as friends, mentors, or romantic partners, and highlights the importance of customization and emotional support in these interactions.	AI companions	AI	AI	companion	AI	1
5_arx_2503.03067_2267379_0	Future work should explore the long-term psychological impacts and evolving dynamics of human-AI relationships as technology advances.	This paper explores the acceptance of human-AI love among young adults, particularly focusing on Chinese women in romantic or intimate relationships with AI companions.	This paper explores the acceptance of human-AI love among young adults, particularly focusing on Chinese women in romantic or intimate relationships with <mask> companions.	Through qualitative research, including 14 semi-structured interviews, the study investigates how these individuals establish and maintain relationships with AI, their perceptions and attitudes towards these entities, and the perspectives of other stakeholders.	AI companions	AI	AI	companion	AI	1
5_arx_2409.00862_2138494_0	We discuss implications for supporting user-driven value alignment in future AI systems, where users and their communities have greater agency.	Large language model-based AI companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds.	Large language model-based <mask> companions are increasingly viewed by users as friends or romantic partners, leading to deep emotional bonds.	However, they can generate biased, discriminatory, and harmful outputs.	Large language model-based AI companions	AI	AI	companion	AI	1
5_arx_2412.01992_2204370_5_3	We also propose an automated method for analyzing collaboration dynamics that effectively identifies behavioral characteristics of agents with distinct roles, allowing us to quantitatively compare collaboration dynamics in a range of experimental conditions.	For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.	For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or <mask> developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.	Our code and data can be found at: https://github.com/ChatCollab.	AI developer	AI	AI	developer	AI	1
5_arx_2411.15692_2198119_5_1	To address this challenge, we introduce DrugAgent, a multi-agent framework that automates machine learning (ML) programming for drug discovery tasks.	DrugAgent employs an LLM Planner that formulates high-level ideas and an LLM Instructor that identifies and integrates domain knowledge when implementing those ideas.	DrugAgent employs an LLM Planner that formulates high-level ideas and an <mask> Instructor that identifies and integrates domain knowledge when implementing those ideas.	We present case studies on three representative drug discovery tasks.	an LLM instructor	LLM	LLM	instructor	LLM	1
5_arx_2502.17730_2260721_2	Whether due to anthropomorphism or intentional design choices, people often assign human-like qualities - including gender - to AI systems.	However, how AI managers are perceived in comparison to human managers and how gender influences these perceptions remains uncertain.	However, how <mask> managers are perceived in comparison to human managers and how gender influences these perceptions remains uncertain.	To investigate this, we conducted randomized controlled trials (RCTs) where teams of three participants worked together under a randomly assigned manager - either human or AI - who was presented as male, female, or gender-neutral.	AI managers	AI	AI	manager	AI	1
5_arx_2402.05605_2003244_4	In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team.	To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team.	To determine the best selection of a control agent, we propose the addition of an <mask> manager (via Reinforcement Learning) which learns as an outside observer of the team.	The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent.	an AI manager	AI	AI	manager	AI	1
5_arx_2412.01992_2204370_5_1	We also propose an automated method for analyzing collaboration dynamics that effectively identifies behavioral characteristics of agents with distinct roles, allowing us to quantitatively compare collaboration dynamics in a range of experimental conditions.	For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.	For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an <mask> product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles.	Our code and data can be found at: https://github.com/ChatCollab.	an AI product manager	AI	AI	product manager	AI	1
5_arx_2303.18116_1817913_4	Particularly, through careful prompt engineering, we separate successful solutions generated by ChatGPT from unsuccessful ones, resulting in a comprehensive list of related pros and cons.	It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner.	It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an <mask> partner.	For example, we show that if ChatGPT is not able to provide a correct solution due to a lack of or incorrect knowledge, the human-expert can feed it with the correct knowledge, e.g., in the form of mathematical theorems and formulas, and make it to apply the gained knowledge in order to provide a solution that is correct.	an AI partner	AI	AI	partner	AI	1
5_arx_2304.07297_1825343_2	This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer.	We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions.	We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their <mask> partners through natural language instructions.	We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective.	AI partners	AI	AI	partner	AI	1
5_arx_2503.05455_2269767_4	In one experiment, we validate the robustness of BS in producing effective AI policies relative to self-play policies, when controls are hidden.	In another experiment, we enable human control, showing that participants perceive AI partners as more effective and enjoyable when they can directly dictate AI behavior.	In another experiment, we enable human control, showing that participants perceive <mask> partners as more effective and enjoyable when they can directly dictate AI behavior.	Our findings highlight the need to design AI that prioritizes both task performance and subjective human preferences.	AI partners	AI	AI	partner	AI	1
5_arx_2404.10225_2047676_4	We envision AI pair programmers that are goal-driven, human partners, SE-aware, and self-learning.	These AI partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making.	These <mask> partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making.	We discuss the desired attributes of such AI pair programmers and outline key challenges that must be addressed to realize this vision.	These AI partners	AI	AI	partner	AI	1
5_arx_2502.01493_2244484_4	"The ""Human-AI Handshake Model"" addresses this gap by introducing a bi-directional, adaptive framework with five key attributes: information exchange, mutual learning, validation, feedback, and mutual capability augmentation."	These attributes foster balanced interaction, enabling AI to act as a responsive partner, evolving with users over time.	These attributes foster balanced interaction, enabling <mask> to act as a responsive partner, evolving with users over time.	Human enablers like user experience and trust, alongside AI enablers such as explainability and responsibility, facilitate this collaboration, while shared values of ethics and co-evolution ensure sustainable growth.	AI	AI	AI	responsive partner	AI	1
5_arx_2501.19361_2242945_1	Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond.	Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs.	Although these LLMs are marketed as helpful creative assistants, several works have shown that using an <mask> as a creative partner results in a narrower set of creative outputs.	However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM -- which arguably has a limited range of outputs -- or from using LLMs in general as creative assistants.	an LLM	LLM	LLM	creative partner	LLM	1
5_arx_2502.18357_2261348_1	AI systems powered by large language models can act as capable assistants for writing and editing.	In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s).	In these tasks, the <mask> acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s).	One question that arises in these scenarios is the extent to which AI should be credited for its contributions.	the AI system	AI system	system	co-creative partner	AI system	1
5_arx_2411.15692_2198119_5_2	To address this challenge, we introduce DrugAgent, a multi-agent framework that automates machine learning (ML) programming for drug discovery tasks.	DrugAgent employs an LLM Planner that formulates high-level ideas and an LLM Instructor that identifies and integrates domain knowledge when implementing those ideas.	DrugAgent employs an <mask> Planner that formulates high-level ideas and an LLM Instructor that identifies and integrates domain knowledge when implementing those ideas.	We present case studies on three representative drug discovery tasks.	an LLM planner	LLM	LLM	planner	LLM	1
5_acl_280_13090_8_1	In our approach, we design a teacher-student joint learning and distillation framework to collaboratively learn both teacher and student models, where the student model can learn from the learning experience of the teacher model.	In addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of the student model to better transfer the knowledge learned by the teacher model.	In addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of the student <mask> to better transfer the knowledge learned by the teacher model.	Thorough experiments on two real-world datasets with three tasks show that NewsBERT can empower various intelligent news applications with much smaller models.	the student model	model	model	student	model	1
5_acl_113_41260_10	However, they tend to be generic, not empathetic enough, and lack personalization, resulting in nonreliable and potentially harmful advice.	We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an LLM supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses.	We discuss these challenges, demonstrate that a dedicated prompt can improve the performance, and propose a blueprint of an <mask> supporter that actively (but sensitively) seeks user context to provide personalized, empathetic, and reliable responses.	Our annotated dataset is available for further research.*https://github.com/nitaytech/LGBTeenDataset	an LLM supporter	LLM	LLM	supporter	LLM	1
5_acl_280_13090_8_2	In our approach, we design a teacher-student joint learning and distillation framework to collaboratively learn both teacher and student models, where the student model can learn from the learning experience of the teacher model.	In addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of student model to better transfer the knowledge learned by the teacher model.	In addition, we propose a momentum distillation method by incorporating the gradients of teacher model into the update of student model to better transfer the knowledge learned by the teacher <mask>.	Thorough experiments on two real-world datasets with three tasks show that NewsBERT can empower various intelligent news applications with much smaller models.	the teacher model	model	model	teacher	model	1
5_acl_64_25713_1	This paper describes the results of the first shared task on generation of teacher responses in educational dialogues.	The goal of the task was to benchmark the ability of generative language models to act as AI teachers, replying to a student in a teacher-student dialogue.	The goal of the task was to benchmark the ability of generative language models to act as <mask> teachers, replying to a student in a teacher-student dialogue.	Eight teams participated in the competition hosted on CodaLab and experimented with a wide variety of state-of-the-art models, including Alpaca, Bloom, DialoGPT, DistilGPT-2, Flan-T5, GPT- 2, GPT-3, GPT-4, LLaMA, OPT-2.7B, and T5- base.	AI teachers	AI	AI	teacher	AI	1
5_acl_65_25714_2	Shared Task on Generating AI Teacher Responses in Educational Dialogues.	The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue.	The task aims to assess the performance of state-of-the-art generative models as <mask> teachers in producing suitable responses within a student-teacher dialogue.	Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation.	AI teachers	AI	AI	teacher	AI	1
5_acl_358_41505_5	Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher.	Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.	Leveraging the strong language abilities of LLMs, we instruct <mask> teachers to synthesize diverse contexts and anticipate more potential errors for the student.	Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve MT performances on unseen contexts and words.	LLM teachers	LLM	LLM	teacher	LLM	1
5_arx_2311.06985_1949952_2	Human teachers are usually required to craft in-context demonstrations, which are costly and have high variance.	We ask whether a large language model (LLM) can serve as a more effective in-context teacher for itself or other LLMs, compared to humans.	We ask whether a <mask> can serve as a more effective in-context teacher for itself or other LLMs, compared to humans.	Inspired by the Encoding Specificity Hypothesis from human episodic memory, we hypothesize that in-context exemplars crafted by the teacher should match the training data of the student.	a large language model (LLM)	large language model (LLM)	model	teacher	large language model (LLM)	1
5_acl_991_40401_3	Specifically, we propose a simple yet effective debiasing framework, named Soft Label Encoding (SoftLE).	First, we train a teacher model to quantify each sampleâ€™s degree of relying on shortcuts.	First, we train a teacher <mask> to quantify each sampleâ€™s degree of relying on shortcuts.	Then, we encode this shortcut degree into a dummy class and use it to smooth the original ground truth labels, generating soft labels.	a teacher model	model	model	teacher	model	1
5_acl_6_43304_0	In this context, I would like to participate in the Young Researchersâ€™ Roundtable on Spoken Dialogue Systems because I believe that I can broaden my research horizons by participating and discussing with various young researchers.	My research theme is to develop an optimal analytical model for various information generated during therapy using multimodal data in psychotherapy, to elucidate the process of psychotherapy, and to create an AI therapist to develop a new psychotherapy.	My research theme is to develop an optimal analytical model for various information generated during therapy using multimodal data in psychotherapy, to elucidate the process of psychotherapy, and to create an <mask> therapist to develop a new psychotherapy.	In this context, I would like to participate in the Young Researchersâ€™ Roundtable on Spoken Dialogue Systems because I believe that I can broaden my research horizons by participating and discussing with various young researchers.	an AI therapist	AI	AI	therapist	AI	1
5_acl_57_45359_0	We believe that the presented taxonomy, benchmark, and human-annotated labels will streamline the evaluation process and help track the progress in AI tutorsâ€™ development.	In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as AI tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues.	In this paper, we investigate whether current state-of-the-art large language models (LLMs) are effective as <mask> tutors and whether they demonstrate pedagogical abilities necessary for good AI tutoring in educational dialogues.	Previous efforts towards evaluation have beenlimited to subjective protocols and benchmarks.	AI tutors	AI	AI	tutor	AI	1
5_arx_2104.01266_1448684_4	We present the iterative design and evaluation of Lumilo, smart glasses that help teachers help their students in AI-supported classrooms by presenting real-time analytics about students' learning, metacognition, and behavior.	Results from a field study conducted in K-12 classrooms indicate that students learn more when teachers and AI tutors work together during class.	Results from a field study conducted in K-12 classrooms indicate that students learn more when teachers and <mask> tutors work together during class.	We discuss implications of this research for the design of human-AI partnerships.	AI tutors	AI	AI	tutor	AI	1
5_arx_2505.02443_2313003_2	Intelligent Tutoring Systems offer a new method of personalized teaching, replacing the limitations of traditional teaching methods.	However, concerns arise about the ability of AI tutors to address skill development and engagement during the learning process.	However, concerns arise about the ability of <mask> tutors to address skill development and engagement during the learning process.	In this paper, I will conduct a quasi experiment with paired sample t test on 34 students pre and post use of AI tutors in language learning platforms like Santa and Duolingo to examine the relationship between students engagement, academic performance, and students satisfaction during a personalized language learning experience.	AI tutors	AI	AI	tutor	AI	1
5_arx_2309.13060_1917864_8	Additionally, the grasp strongly correlated with the exam grade, thus validating the relevance of neural-network predictions.	This research demonstrates the ability of personal AI tutors to model human learning processes and effectively enhance academic performance.	This research demonstrates the ability of personal <mask> tutors to model human learning processes and effectively enhance academic performance.	By integrating AI tutors into their programs, educators can offer students personalized learning experiences grounded in the principles of learning sciences, thereby addressing the challenges associated with implementing effective learning strategies.	personal AI tutors	AI	AI	tutor	AI	1
5_arx_2407.15718_2114105_7	Overall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions.	When students posed questions within the intended scope, the AI tutors delivered accurate responses 98% of the time.	When students posed questions within the intended scope, the <mask> tutors delivered accurate responses 98% of the time.	Within the students used AI tutors, 78% reported that the tutors helped their learning.	the AI tutors	AI	AI	tutor	AI	1
5_arx_2407.15718_2114105_6	This paper describes the interactions the students had with the AI tutors, the students' feedback, and a comparative grade analysis.	Overall, about half of the students engaged with the AI tutors, and the vast majority of the interactions were legitimate homework questions.	Overall, about half of the students engaged with the <mask> tutors, and the vast majority of the interactions were legitimate homework questions.	When students posed questions within the intended scope, the AI tutors delivered accurate responses 98% of the time.	the AI tutors	AI	AI	tutor	AI	1
5_arx_2504.09720_2298423_2	In our implementation, NotebookLM was configured as an AI physics collaborative tutor to support students in solving conceptually oriented physics problems using a collaborative, Socratic approach.	When deployed as a collaborative tutor, the system restricts student interaction to a chat only interface, promoting controlled and guided engagement.	When deployed as a collaborative tutor, the <mask> restricts student interaction to a chat only interface, promoting controlled and guided engagement.	By grounding its responses in teacher provided source documents, NotebookLM helps mitigate one of the major shortcomings of standard large language models--hallucinations--thereby ensuring more traceable and reliable answers.	the system	system	system	collaborative tutor	system	1
5_arx_2501.09171_2232755_0	Results indicate that GPT3.5 and 4o-mini have characteristics that are more similar than either of them have with GPT4.	Many believe that use of generative AI as a private tutor has the potential to shrink access and achievement gaps between students and schools with abundant resources versus those with fewer resources.	Many believe that use of <mask> as a private tutor has the potential to shrink access and achievement gaps between students and schools with abundant resources versus those with fewer resources.	Shrinking the gap is possible only if paid and free versions of the platforms perform with the same accuracy.	generative AI	generative AI	AI	private tutor	generative AI	1
5_arx_2504.13903_2302606_4	Our findings reveal that coding experience does not predict AI adoption but significantly influences mental models of AI's role.	Experienced developers are more likely to perceive AI as a junior colleague, a content generator, or assign it no role, whereas less experienced developers primarily view AI as a teacher.	Experienced developers are more likely to perceive <mask> as a junior colleague, a content generator, or assign it no role, whereas less experienced developers primarily view AI as a teacher.	These insights suggest that AI tools must align with developers' expertise levels to drive meaningful adoption.	AI	AI	AI	junior colleague,content generator	AI	1
5_arx_2503.22040_2286352_2	In this paper, we conduct a systematic evaluation of the promises and risks of using LLMs for diverse coding tasks, with social movement studies serving as a case example.	We propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant.	We propose a framework for social scientists to incorporate <mask> into text annotation, either as the primary coding decision-maker or as a coding assistant.	This framework provides tools for researchers to develop the optimal prompt, and to examine and report the validity and reliability of LLMs as a methodological tool.	LLMs	LLMs	LLM	decision-maker,coding assistant	LLMs	1
5_arx_2502.17855_2260846_2	This study aims to address this gap by engaging 17 in-service secondary school PE teachers in group ideation workshops to explore potential AI applications and challenges in PE classes.	Participants envisioned AI playing multidimensional roles, such as an operational assistant, personal trainer, group coach, and evaluator, as solutions to address unique instructional and operational challenges in K-12 PE classes.	Participants envisioned <mask> playing multidimensional roles, such as an operational assistant, personal trainer, group coach, and evaluator, as solutions to address unique instructional and operational challenges in K-12 PE classes.	These roles reflected participants' perspectives on how AI could enhance class management, deliver personalized feedback, promote balanced team activities, and streamline performance assessments.	AI	AI	AI	operational assistant,personal trainer,group coach,evaluator	AI	1
