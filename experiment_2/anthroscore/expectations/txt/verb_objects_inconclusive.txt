2_acl_143_43709_5	We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors.	Our experiments show that these errors can be identified with high accuracy by an LLM.	Our experiments show that these errors can be identified with high accuracy by an <mask>.	We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence.	an LLM	LLM	LLM	identify	LLM	2
2_acl_511_29547_1	The potential of using a large language model (LLM) as a knowledge base (KB) has sparked significant interest.	To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.	To maintain the knowledge acquired by <mask>, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.	Existing work on editing LLMs has partially addressed the issue of dependency, when the editing of a fact should apply to its lexical variations without disrupting irrelevant ones.	LLMs	LLMs	LLM	acquire	LLMs	2
2_arx_2001.08625_1234038_4	"Using this, we simulated the standard worklist processing ""first-in, first-out"" (FIFO) and compared it with a worklist prioritization based on urgency."	Examination prioritization was performed by the AI, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.	Examination prioritization was performed by the <mask>, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.	Furthermore, we introduced an upper limit for the maximum waiting time, after which the highest urgency is assigned to the examination.	the AI	AI	AI	perform	AI	2
2_arx_2301.11767_1782831_2	In this work, we introduce CAPoW, a context-aware anti-DDoS framework that injects latency adaptively during communication by utilizing context-aware PoW puzzles.	In CAPoW, a security professional can define relevant request context attributes which can be learned by the AI system.	In CAPoW, a security professional can define relevant request context attributes which can be learned by the <mask>.	These contextual attributes can include information about the user request, such as IP address, time, flow-level information, etc., and are utilized to generate a contextual score for incoming requests that influence the hardness of a PoW puzzle.	the AI system	AI system	system	learn	AI system	2
2_arx_2106.07921_1485685_1	Artificial Intelligence will significantly impact the work environment of radiologists.	I suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025.	I suggest that up to 50% of a radiologists work in 2021 will be performed by <mask> in 2025.	However, it won't increase beyond that 50% level, as radiologists remain key for human-centered aspects of their job.	AI-models	AI-models	model	perform	AI-models	2
2_arx_2007.15619_1327133_1	This research paper proposes a COVID-19 monitoring and response system to identify the surge in the volume of patients at hospitals and shortage of critical equipment like ventilators in South-east Asian countries, to understand the burden on health facilities.	This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the model.	This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the <mask>.	Due to the lack of publicly available data on the influx of patients in hospitals, or the shortage of equipment, ICU units or hospital beds that regions in these countries might be facing, we leverage Twitter data for gleaning this information.	the model	model	model	identify	model	2
2_arx_2302.06852_1791786_1	We propose a hybrid Artificial Intelligence (AI) climate modeling approach that enables climate modelers in scientific discovery using a climate-targeted simulation methodology based on a novel combination of deep neural networks and mathematical methods for modeling dynamical systems.	The simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by the AI methods and provides a means of explainability.	The simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by the <mask> and provides a means of explainability.	We describe how this methodology can be applied to the discovery of climate tipping points and, in particular, the collapse of the Atlantic Meridional Overturning Circulation (AMOC).	the AI methods	AI methods	method	learn	AI methods	2
2_acl_23_33157_0	We also applied this method to English, an SVO language, and demonstrated that it outperforms conventional methods.	Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves.	Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the <mask> themselves.	To address this issue, a method called local modification-based knowledge editing has been developed.	the LLMs	LLMs	LLM	hold knowledge	LLMs	2
2_acl_3_26185_65	In this context, an academic 1 page English text was chosen.	The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.	The text was translated by both <mask> and a translator who is an academic in the field of translation and has 10 years of experience.	Afterwards, two different translations were examined comparatively by 5 different translators who are experts in their fields.	ChatGPT	ChatGPT	ChatGPT	translate	ChatGPT	2
2_acl_260_9141_4	Two Arabic abusive/hate datasets were added to the training dataset: L-HSAB and T-HSAB.	The final submission was chosen based on the best performances which was achieved by the BERT+BiLSTM model.	The final submission was chosen based on the best performances which was achieved by the <mask>.		the BERT+BiLSTM model	BERT+BiLSTM model	model	achieve	BERT+BiLSTM model	2
2_arx_1907.08625_1153161_6	A broad iron line $\sim 5-8$ keV and the Compton back-scattering hump peaking at $\sim$10-20 keV band are clearly detected in the X-ray spectrum.	These features are best interpreted by a self-consistent relativistic reflection model.	These features are best interpreted by a <mask>.	Fits with relativistically blurred disc reflection model suggests that the inner disc radius $R_{in}$ is truncated prior to the ISCO at $(1.9-2.5)\;R_{ISCO}$ ($\simeq11.4-15\,R_{g}\: \text{or}\: 26-34$ km) and the accretion disc is viewed at an low inclination of $i\simeq16^\circ-20^\circ$. The disc is likely to be truncated either by a boundary layer or by the magnetosphere.	a self-consistent relativistic reflection model	self-consistent relativistic reflection model	model	interpret	self-consistent relativistic reflection model	2
2_acl_430_37198_2	Unfortunately, ICL suffers from the demonstration bias, i.e., its performance and robustness are severely affected by the selection and ordering of demonstrations.	In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by LLMs.	In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by <mask>.	Such semantic ambiguity disrupts task comprehension during ICL and results in performance fluctuations.	LLMs	LLMs	LLM	interpret	LLMs	2
3_acl_696_6286_1	Many NLP applications, such as biomedical data and technical support, have 10-100 million tokens of in-domain data and limited computational resources for learning from it.	How should we train a language model in this scenario?	How should we train a <mask> in this scenario?	Most language modeling research considers either a small dataset with a closed vocabulary (like the standard 1 million token Penn Treebank), or the whole web with byte-pair encoding.	a language model	language model	model	train	language model	2
3_acl_891_29927_2	In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on large language models (LLMs).	Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data.	Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed <mask> to generate appropriate responses on novel tasks, without the need for training data.	Specifically, SGP-TOD comprises three components: an LLM for interacting with users, a Dialog State Tracking (DST) Prompter to aid the LLM in tracking dialog states with the given belief instruction, and a Policy Prompter to direct the LLM to generate proper responses adhering to the provided dialog policy.	fixed LLMs	LLMs	LLM	instruct	LLMs	2
3_acl_27_42781_1	While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity.	A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.	A promising approach to rectify these flaws is correcting <mask> with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.	Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention.	LLMs	LLMs	LLM	correct	LLMs	2
3_acl_27_42781_3	Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention.	This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.	This paper provides an exhaustive review of the recent advances in correcting <mask> with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.	We also identify potential challenges and future directions in this emerging field.	LLMs	LLMs	LLM	correct	LLMs	2
3_arx_2305.01937_1834993_4	In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.	We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.	We present the <mask> with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.	We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks.	the LLMs	LLMs	LLM	present	LLMs	2
3_acl_920_38624_10_1	We observe and analyze interesting patterns involving token count, cost, and quality across a multitude of languages and tasks.	We show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.	We show that choosing the best policy to interact with the <mask> can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.		the LLM	LLM	LLM	interact with	LLM	2
3_arx_2309.11000_1915804_0	Our results indicate that the LLM-based approach is a promising direction for building unified spoken dialogue systems.	"This paper explores the potential of constructing an AI spoken dialogue system that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules."	"This paper explores the potential of constructing an <mask> that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules."	We hypothesize that Large Language Models (LLMs) with billions of parameters possess significant speech understanding capabilities and can jointly model dialogue responses and linguistic features.	an AI spoken dialogue system	AI spoken dialogue system	system	construct	AI spoken dialogue system	2
3_acl_794_28930_3	To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options.	We introduce a new task, “less likely brainstorming,” that asks a model to generate outputs that humans think are relevant but less likely to happen.	We introduce a new task, “less likely brainstorming,” that asks a <mask> to generate outputs that humans think are relevant but less likely to happen.	We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting.	a model	model	model	ask	model	2
3_arx_2308.01154_1889046_3	Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data.	We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.	We successfully trained a <mask> to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.	Our findings support the hypothesis that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.	a light language model	light language model	model	train	light language model	2
3_arx_2304.00385_1818431_9	In this way, we can avoid making the same mistakes.	For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.	For earlier patches that passed all the tests, we further ask the <mask> to generate alternative variations of the original plausible patches.	In this way, we can further build on and learn from earlier successes to generate more plausible patches to increase the chance of having correct patches.	the LLM	LLM	LLM	ask	LLM	2
3_arx_2411.05823_2188250_7	This field, composed of a sequence of tokens, can be set flexibly to represent various hierarchies.	Subsequently, we ask LLMs to predict this masked field.	Subsequently, we ask <mask> to predict this masked field.	During inference, the user intent is converted into a CAD text with a mask token replacing the part the user wants to modify, which is then fed into FlexCAD to generate new CAD models.	LLMs	LLMs	LLM	ask	LLMs	2
2_arx_2204.06916_1637054_2	Whereas in the past, advice has often been received from human experts, friends, or family, advisors based on artificial intelligence (AI) have become more and more present nowadays.	Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected.	Typically, the advice generated by <mask> is judged by a human and either deemed reliable or rejected.	However, recent work has shown that AI advice is not always beneficial, as humans have shown to be unable to ignore incorrect AI advice, essentially representing an over-reliance on AI.	AI	AI	AI	generate	AI	2
2_arx_2004.11543_1276313_6	The approach extends our previous work on machine education to decompose the search space into a hierarchically organized curriculum.	Each lesson in the curriculum is learnt by a deep reinforcement learning model.	Each lesson in the curriculum is learnt by a <mask>.	The hierarchy is formed by fusing the outputs of the model.	a deep reinforcement learning model	deep reinforcement learning model	model	learn	deep reinforcement learning model	2
3_acl_814_27535_2	Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors.	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.	Therefore, we aim to train an <mask> with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.	an agent	agent	agent	train	agent	2
2_arx_2012.09755_1397633_5	In this study we propose a deep learning approach that can: \textbf{(1)} fully exploit information from an OCT scan of the ONH; \textbf{(2)} describe the structural phenotype of the glaucomatous ONH; and that can \textbf{(3)} be used as a robust glaucoma diagnosis tool.	Specifically, the structural features identified by our algorithm were found to be related to clinical observations of glaucoma.	Specifically, the structural features identified by our <mask> were found to be related to clinical observations of glaucoma.	The diagnostic accuracy from these structural features was $92.0 \pm 2.3 \%$ with a sensitivity of $90.0 \pm 2.4 \% $ (at $95 \%$ specificity).	our algorithm	algorithm	algorithm	identify	algorithm	2
