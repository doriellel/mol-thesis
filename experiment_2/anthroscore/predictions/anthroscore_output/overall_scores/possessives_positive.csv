,id,previous_sentence,current_sentence,masked_sentence,next_sentence,AI_phrase,masked_phrase,AI_entity,component,expectations,anthroscore
0,6_arx_2104.14506_1461924_5,"XAI is an AI model that is programmed to explain its goals, logic, and decision making so that the end users can understand.","The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model.","The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an <mask>.",Chest CT has emerged as a valuable tool for the clinical diagnostic and treatment management of the lung diseases associated with COVID-19.,an AI model,AI model,model,decisions,1,-1.764164186914396
1,6_acl_183_18630_5,Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past.,"On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3.","On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the <mask>.",Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs.,the GPT-3,GPT-3,GPT-3,misunderstandings,1,1.1275375533064818
2,6_arx_1912.03652_1215548_4,"In contrast, inputs created by humans are often treated as a given.",We investigate how inputs of humans can be altered to reduce misinterpretation by the AI system and to improve efficiency of input generation for the human while altered inputs should remain as similar as possible to the original inputs.,We investigate how inputs of humans can be altered to reduce misinterpretation by the <mask> and to improve efficiency of input generation for the human while altered inputs should remain as similar as possible to the original inputs.,These objectives result in trade-offs that are analyzed for a deep learning system classifying handwritten digits.,the AI system,AI system,system,misinterpretations,1,0.2791676773905251
3,6_acl_373_16531_2,This creates challenges when AI systems try to reason about language and its relationship with the environment: objects referred to through language (e.g. giving many instructions) are not immediately visible.,Actions by the AI system may be required to bring these objects in view.,Actions by the <mask> may be required to bring these objects in view.,"A good benchmark to study this challenge is Dynamic Referring Expression Recognition (dRER) task, where the goal is to find a target location by dynamically adjusting the field of view (FoV) in a partially observed 360 scenes.",the AI system,AI system,system,actions,1,3.2723111221304464
4,6_arx_2110.06674_1544817_2,"While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent.","This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for).","This raises the question of how we should limit the harm caused by <mask> ""lies"" (i.e. falsehoods that are actively selected for).","Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud).",AI,AI,AI,lies,1,-0.2228751392335439
5,6_arx_2207.07051_1682949_0,"Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.",Reasoning is a key ability for an intelligent system.,Reasoning is a key ability for an intelligent <mask>.,"Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections.",an intelligent system,system,system,reasoning,1,0.0751304348878996
6,6_acl_71_46015_2,"While Large LanguageModels (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differencesin subjective tasks remains uncertain.","In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their ability to understand demographic differences in two subjective judgment tasks: politeness and offensiveness.","In this study, leveraging the POPQUORN dataset, we evaluate nine popular <mask> on their ability to understand demographic differences in two subjective judgment tasks: politeness and offensiveness.","We find that in zero-shot settings, most models’ predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a minor gender bias favoring women appears in the politeness task.",nine popular LLMs,LLMs,LLM,ability to understand,1,-3.3082201747253848
7,6_arx_2405.01576_2058788_0,"Our work demonstrates that even models trained to be helpful, harmless and honest sometimes behave deceptively in realistic scenarios, without notable external pressure to do so.",We study the tendency of AI systems to deceive by constructing a realistic simulation setting of a company AI assistant.,We study the tendency of <mask> to deceive by constructing a realistic simulation setting of a company AI assistant.,"The simulated company employees provide tasks for the assistant to complete, these tasks spanning writing assistance, information retrieval and programming.",AI systems,AI systems,system,tendency to deceive,1,-3.252950371291096
8,6_acl_245_29281_1,Compositional reasoning across texts has been a long-standing challenge in natural language processing.,"With large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of LLMs.","With large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of <mask>.","Despite their success, the prompts demand significant human effort to discover and validate them.",LLMs,LLMs,LLM,reasoning capabilities,1,-4.656341475270979
9,6_acl_916_40326_5,"To tackle this problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge Evaluation benchmark in this paper.",LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.,LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese <mask>.,"It encompasses 10,465 questions across 75 tasks covering 30 subjects, ranging from primary school to professional certification exams.",Chinese LLMs,LLMs,LLM,knowledge acquisition capabilities,1,-0.0671660393510649
10,6_acl_1539_40949_1,The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses.,"However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.","However, this awareness of <mask> has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.","In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts.",LMs,LMs,LM,awareness,1,-4.493531722733042
11,6_acl_225_32387_1,Complex reasoning ability is one of the most important features of Large Language Models (LLMs).,Numerous benchmarks have been established to assess the reasoning abilities of LLMs.,Numerous benchmarks have been established to assess the reasoning abilities of <mask>.,"However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance.",LLMs,LLMs,LLM,reasoning abilities,1,-3.8436958975530704
12,6_acl_743_29779_6,The In-Context Learning (ICL) and Chain-of-Thought (COT) techniques can further exacerbate such causal hallucination.,"Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts.","Additionally, the causal reasoning ability of <mask> is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts.","For events in sentences, ChatGPT excels at capturing explicit causality rather than implicit causality, and performs better in sentences with lower event density and smaller lexical distance between events.",ChatGPT,ChatGPT,ChatGPT,causal reasoning ability,1,-3.9805721189198096
13,6_acl_538_32700_6,"Our findings also underscore the need for enhanced ethical guidance in machine-generated journalistic content, marking a step forward in aligning LLMs with journalistic standards and safety considerations.",The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.,The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of <mask> and speed up the development of LLMs in journalism.,,LLMs,LLMs,LLM,editorial capabilities,1,-2.319190203274445
14,6_arx_2101.06573_1410315_4,"To answer these questions, we have analysed existing benchmarks and their understanding capabilities, defined by a set of understanding capabilities, and current research streams.",We show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities.,We show how progress has been made in benchmark development to measure understanding capabilities of <mask> and we review as well how current methods develop understanding capabilities.,,AI methods,AI methods,method,understanding capabilities,1,-3.117560457834731
15,6_arx_2307.16180_1887175_4,"In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a widespread human personality assessment tool, as an evaluation metric for LLMs.","Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality.","Specifically, extensive experiments will be conducted to explore: 1) the personality types of different <mask>, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality.","Although the MBTI is not a rigorous assessment, it can still reflect the similarity between LLMs and human personality.",different LLMs,LLMs,LLM,personality types,1,-2.668942464148479
16,6_arx_2408.10159_2130340_1,"Sequential recommendation systems predict the next interaction item based on users' past interactions, aligning recommendations with individual preferences.","Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation.","Leveraging the strengths of <mask> in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation.","A common paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).",Large Language Models (LLMs),Large Language Models (LLMs),model,strengths in knowledge comprehension and reasoning,1,-5.283316259666872
17,6_arx_2503.16460_2280772_5,"In this work, we present two novel approaches to evaluate the correctness and quality of LLMs in math tutoring contexts.",The first approach uses an intelligent tutoring system for college algebra as a testbed to assess LLM problem-solving capabilities.,The first approach uses an intelligent tutoring system for college algebra as a testbed to assess <mask> problem-solving capabilities.,"We generate benchmark problems using the tutor, prompt a diverse set of LLMs to solve them, and compare the solutions to those generated by the tutor.",LLM,LLM,LLM,problem-solving capabilities,1,-1.091135040002575
18,6_arx_2309.02077_1906881_6,"To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets.",A medical consultation training set is further constructed to improve the consultation ability of LLMs.,A medical consultation training set is further constructed to improve the consultation ability of <mask>.,The results of the experiments show that fine-tuning with the training set can alleviate hallucinations and improve LLMs' performance on the proposed benchmark.,LLMs,LLMs,LLM,consultation abilities,1,-0.7694079463120858
19,6_arx_2305.18752_1851808_6,"By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation.","Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways.","Moreover, we provide a benchmark to evaluate the ability of <mask> to use tools, which is performed in both zero-shot and fine-tuning ways.","Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools.",LLMs,LLMs,LLM,ability to use tools,1,-2.57800933575572
20,6_arx_2309.11805_1916609_6,"As of late, Large Language Models (LLMs) have taken over the AI field by storm with extraordinary performance in fields where text-based data is available.","Inspired by the superior performance of LLMs, we leverage their capability to understand natural language for capturing the information that was previously getting lost during the conversion of unstructured data to structured form.","Inspired by the superior performance of <mask>, we leverage their capability to understand natural language for capturing the information that was previously getting lost during the conversion of unstructured data to structured form.","To this end, we compare performance of four different approaches for job recommendations namely, (i) Content based deterministic, (ii) LLM guided, (iii) LLM unguided, and (iv) Hybrid.",LLMs,LLMs,LLM,capability to understand natural language,1,-4.093205853742077
21,6_arx_2310.02417_1924674_0,"Meanwhile, it decreases the response refusal rate from 50\% to 0\%.","Large language models (LLMs), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks.","<mask>, known for their capability in understanding and following instructions, are vulnerable to adversarial attacks.","Researchers have found that current commercial LLMs either fail to be ""harmless"" by presenting unethical answers, or fail to be ""helpful"" by refusing to offer meaningful answers when faced with adversarial queries.",Large language models (LLMs),Large language models (LLMs),model,capability in understanding and following instructions,1,-3.517887064855982
22,6_arx_2305.16867_1849923_1,LLMs are increasingly used in applications where they interact with humans and other agents.,We propose to use behavioural game theory to study LLMs' cooperation and coordination behaviour.,We propose to use behavioural game theory to study <mask>' cooperation and coordination behaviour.,"We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players.",LLMs,LLMs,LLM,cooperation and coordination behavior,1,-6.925230361808348
23,6_arx_2305.04388_1837444_1,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT).",It is tempting to interpret these CoT explanations as the LLM's process for solving a task.,It is tempting to interpret these CoT explanations as the <mask>'s process for solving a task.,This level of transparency into LLMs' predictions would yield significant safety benefits.,the LLM,LLM,LLM,process for solving a task,1,1.0929264627793422
24,6_arx_2005.02335_1282130_0,These findings demonstrate the importance of accurate and understandable explanations and caution that poor explanations can sometimes be worse than no explanations with respect to their effect on user performance and reliance on an AI system.,Explainable machine learning and artificial intelligence models have been used to justify a model's decision-making process.,Explainable machine learning and artificial intelligence models have been used to justify a <mask>'s decision-making process.,This added transparency aims to help improve user performance and understanding of the underlying model.,a model,model,model,decision-making process,1,0.2110624511518786
25,6_arx_2203.10923_1623785_4,"We focus on tasks such as monitoring and managing the application, common functionality for a MLOps platform, and show how they are complicated by the distributed nature of edge deployment.",We also discuss issues that are unique to edge applications such as protecting a model's intellectual property and verifying its integrity.,We also discuss issues that are unique to edge applications such as protecting a <mask>'s intellectual property and verifying its integrity.,,a model,model,model,intellectual property,1,-0.78861582226601
26,6_arx_2207.09374_1685272_1,"Explanation mechanisms from the field of Counterfactual Thinking are a widely-used paradigm for Explainable Artificial Intelligence (XAI), as they follow a natural way of reasoning that humans are familiar with.","However, all common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision.","However, all common approaches from this field are based on communicating information about features or characteristics that are especially important for an <mask>'s decision.","We argue that in order to fully understand a decision, not only knowledge about relevant features is needed, but that the awareness of irrelevant information also highly contributes to the creation of a user's mental model of an AI system.",an AI,AI,AI,decisions,1,1.1976489540606856
27,6_arx_2209.15093_1720808_3,"The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query.",We propose conceptual consistency to measure a LLM's understanding of relevant concepts.,We propose conceptual consistency to measure a <mask>'s understanding of relevant concepts.,This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are.,a LLM,LLM,LLM,understanding,1,0.4156754280269013
28,6_arx_2210.05487_1726842_3,We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES.,We find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity.,We find that the visual grounding improves the <mask>'s understanding of semantic similarity both within and across languages and improves perplexity.,"However, we find no significant advantage of visual grounding for abstract words.",the model,model,model,understanding,1,0.3366170841293474
29,6_arx_2307.00457_1871453_4,"In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation.","GenRec uses LLMs' understanding ability to interpret context, learn user preferences, and generate relevant recommendation.","GenRec uses <mask>' understanding ability to interpret context, learn user preferences, and generate relevant recommendation.",Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks.,LLMs,LLMs,LLM,understanding abilities,1,-7.628094464376034
30,6_arx_2307.10250_1881245_0,The results highlight the potential of LLMs in complex problem-solving and the need for further research to maximize their practical applications.,"This study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology.","This study evaluates the <mask>'s abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology.","Using an interactive interview format, the AI assistant demonstrated reliability in generating and selecting hypotheses.",the GPT-4 Large Language Model,GPT-4 Large Language Model,model,abductive reasoning,1,0.7143883994679001
31,6_arx_2308.10837_1898729_1,Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM).,"However, effectively integrating LLMs' commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.","However, effectively integrating <mask>' commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.","In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs.",LLMs,LLMs,LLM,commonsense knowledge and reasoning abilities,1,-5.33581466068566
32,6_arx_2308.07326_1895218_4,"In our analysis, while ""openness"" presented linguistic ambiguity, ""conscientiousness"" and ""neuroticism"" were distinctly evoked in the OCEAN framework, with ""extroversion"" and ""agreeableness"" showcasing a notable overlap yet distinct separation from other traits.",Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions.,Our findings underscore <mask>'s versatility and ability to discern and adapt to nuanced instructions.,"Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles.",GPT,GPT,GPT,ability to discern and adapt to nuanced instructions,1,-2.1409143876491328
33,6_arx_2307.05488_1876483_8,"The experiments reveal potential biases in the generated samples, particularly regarding gender and usage experiences.",These biases may impact the responses of constructs and should be considered when interpreting ChatGPT's conceptual capabilities.,These biases may impact the responses of constructs and should be considered when interpreting <mask>'s conceptual capabilities.,"In sum, ChatGPT shows promise as a tool for theory prototyping, generating relevant responses aligned with theoretical constructs.",ChatGPT,ChatGPT,ChatGPT,conceptual capabilities,1,-2.885691831097269
34,6_arx_2306.11296_1864444_1,We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature.,This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.,This effectively mitigates <mask>'s tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.,"Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself.",ChatGPT,ChatGPT,ChatGPT,tendency to hallucinate information,1,-1.9620657653371296
35,6_arx_2303.13712_1813509_7,"Under fairly general assumptions, the parameters of the human decision function can be identified from past interactions between the algorithm and the human decision maker, even when the algorithm was constrained to make truthful recommendations.",We then consider a decision maker who is aware of the algorithm's manipulation and responds strategically.,We then consider a decision maker who is aware of the <mask>'s manipulation and responds strategically.,"By posing the setting as a variation of the cheap talk game [Crawford and Sobel, 1982], we show that all equilibria are partition equilibria where only coarse information is shared: the algorithm recommends an interval containing the ideal decision.",the algorithm,algorithm,algorithm,manipulation,1,0.8782427293842368
36,6_arx_2211.08380_1747286_6,The performance enhancement is mainly from the KG reasoning's capacity to infer missing relational facts.,"In addition, OREO-LM provides reasoning paths as rationales to interpret the model's decision.","In addition, OREO-LM provides reasoning paths as rationales to interpret the <mask>'s decision.",,the model,model,model,decisions,1,0.4693390891368878
37,6_arx_2301.13852_1784916_11,"However, our proposed approach still achieves an accuracy of 79%.","Using explainability, we observe that ChatGPT's writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.","Using explainability, we observe that <mask>'s writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.",,ChatGPT,ChatGPT,ChatGPT,polite writing,1,2.480359672852231
38,6_arx_2306.03423_1856571_4,"Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse.","In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack.","In this experiment, we characterize <mask>'s refusal behavior using a black-box attack.","We first query ChatGPT with a variety of offensive and benign prompts (n=1,706), then manually label each response as compliance or refusal.",ChatGPT,ChatGPT,ChatGPT,refusal behavior,1,-0.9065959346013877
39,6_acl_406_32568_3,"To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains.","Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs’ logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns.","Our analysis of GPT-series models over a rule subset reveals significant gaps in <mask>’ logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns.",We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning.,LLMs,LLMs,LLM,logic understanding,1,-5.554647611999227
40,6_arx_2309.05163_1909967_10,"This suggests that at university, there's no substantial threat from LLMs for non-invigilated Physics questions.","However, given the LLMs' considerable proficiency in writing Physics essays and coding abilities, non-invigilated examinations of these skills in Physics are highly vulnerable to automated completion by LLMs.","However, given the <mask>' considerable proficiency in writing Physics essays and coding abilities, non-invigilated examinations of these skills in Physics are highly vulnerable to automated completion by LLMs.",This vulnerability also extends to Physics questions pitched at lower academic levels.,the LLMs,LLMs,LLM,considerable proficiency in writing Physics essays and coding abilities,1,-1.283519930563649
41,6_arx_2311.08487_1951454_2,"Drawing an analogy to the human psyche's conflict between evolutionary survival instincts and societal norm adherence elucidated in Freud's psychoanalysis theory, we argue that LLMs suffer a similar fundamental conflict, arising between their inherent desire for syntactic and semantic continuity, established during the pre-training phase, and the post-training alignment with human values.","This conflict renders LLMs vulnerable to adversarial attacks, wherein intensifying the models' desire for continuity can circumvent alignment efforts, resulting in the generation of harmful information.","This conflict renders LLMs vulnerable to adversarial attacks, wherein intensifying the <mask>' desire for continuity can circumvent alignment efforts, resulting in the generation of harmful information.","Through a series of experiments, we first validated the existence of the desire for continuity in LLMs, and further devised a straightforward yet powerful technique, such as incomplete sentences, negative priming, and cognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle to prevent the generation of harmful information.",the models,models,model,desire,1,-2.3246572217897548
42,6_acl_865_25015_6,We find that probing performance strongly correlates with upstream performance in related legal topics.,"On the other hand, downstream performance is mainly driven by the model’s size and prior legal knowledge which can be estimated by upstream and probing performance.","On the other hand, downstream performance is mainly driven by the <mask>’s size and prior legal knowledge which can be estimated by upstream and probing performance.","Based on these findings, we can conclude that both dimensions are important for those seeking the development of domain-specific PLMs.",the model,model,model,prior legal knowledge,1,-0.3126336936092233
43,6_acl_2210.12530_1733885_3,"But in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (LMs) to construct task-specific priors distilled from the rich knowledge of LMs.","Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.","Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the <mask>'s common-sense reasoning based on the metadata.","Empirically, we demonstrate that LMPriors improve model performance in settings where such natural language descriptions are available, and perform well on several tasks that benefit from such prior knowledge, such as feature selection, causal inference, and safe reinforcement learning.",the LM,LM,LM,common-sense reasoning,1,-0.995103792900636
44,6_arx_2206.14576_1674996_1,"We study GPT-3, a recent large language model, using tools from cognitive psychology.","More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature.","More specifically, we assess <mask>'s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature.","We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning.",GPT-3,GPT-3,GPT-3,"decision-making,deliberation,causal reasoning abilities",1,-0.600493864615709
45,6_arx_2308.01552_1889444_4,"Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering.","The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.","The results highlight <mask>'s competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.",,ChatGPT,ChatGPT,ChatGPT,competence in comprehending and performing intricate tasks,1,-0.2994644814362299
46,6_arx_2308.06032_1893924_10,"However, ChatGPT's drafting skills (though, perhaps, still inferior to lawyers) could assist lawyers in providing legal services.","Our research is the first to systematically study an LLM's legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.","Our research is the first to systematically study an <mask>'s legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.",,an LLM,LLM,LLM,legal drafting and reasoning capabilities,1,0.5029661992260017
47,6_arx_2308.06920_1894812_8,"Chatbots become facilitators, steering researchers towards innovative methodologies and productive paths for creating effective drug candidates.","This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT's cognitive abilities enhance the design and development of potential pharmaceutical solutions.","This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein <mask>'s cognitive abilities enhance the design and development of potential pharmaceutical solutions.",This paper not only explores the integration of advanced AI in drug discovery but also reimagines the landscape by advocating for AI-powered chatbots as trailblazers in revolutionizing therapeutic innovation.,ChatGPT,ChatGPT,ChatGPT,cognitive abilities,1,-3.812407840559075
48,6_arx_2308.07326_1895218_5,Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions.,"Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles.","Furthermore, historical figure simulations highlighted the <mask>'s capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles.","However, the rapid advancements in LLM capabilities and the opaque nature of some training techniques make metric proposals degrade rapidly.",the LLM,LLM,LLM,capacity to internalize and project instructible personas,1,2.0363289612332363
49,6_arx_2308.08407_1896299_3,"To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial.",Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders.,Explainability is usually referred to as an <mask>'s ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders.,"In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability.",an AI system,AI system,system,ability to provide a robust interpretation of its decision-making logic,1,-2.072690732609363
50,6_arx_2307.05488_1876483_1,This research paper presents the findings of two experimental studies that explore the use of ChatGPT as a tool for theory prototyping.,The objective of the studies is to assess ChatGPT's ability to comprehend theoretical concepts and differentiate between constructs.,The objective of the studies is to assess <mask>'s ability to comprehend theoretical concepts and differentiate between constructs.,"During the experiments, duplicated responses were identified in both Study 1 and Study 2, with duplicate response rates of 26.25% and 40% respectively.",ChatGPT,ChatGPT,ChatGPT,ability to comprehend theoretical concepts and differentiate between constructs,1,-1.883966792753121
51,6_arx_2307.04274_1875269_7,"We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models.","Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.","Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the <mask>'s ability to showcase pedagogical skills.",,the model,model,model,ability to showcase pedagogical skills,1,-0.3168289536674962
52,6_arx_2306.10645_1863793_3,We present a case study with a simple system that enables mixed-turn chatbot interactions and discuss the insights and preliminary guidelines obtained from initial tests.,"We examine ChatGPT's ability to pursue multiple interconnected learning objectives, adapt the educational activity to users' characteristics, such as culture, age, and level of education, and its ability to use diverse educational strategies and conversational styles.","We examine <mask>'s ability to pursue multiple interconnected learning objectives, adapt the educational activity to users' characteristics, such as culture, age, and level of education, and its ability to use diverse educational strategies and conversational styles.","Although the results are encouraging, challenges are posed by the limited history maintained for the conversation and the highly structured form of responses by ChatGPT, as well as their variability, which can lead to an unexpected switch of the chatbot's role from a teacher to a therapist.",ChatGPT,ChatGPT,ChatGPT,ability to pursue multiple interconnected learning objectives,1,-6.180096851850582
53,6_arx_2306.06123_1859271_2,"However, recent advances in adversarial machine learning (AdvML) highlight the limitations and vulnerabilities of state-of-the-art explanation methods, putting their security and trustworthiness into question.","The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery.","The possibility of manipulating, fooling or fairwashing evidence of the <mask>'s reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery.","This survey provides a comprehensive overview of research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics.",the model,model,model,reasoning,1,1.013899497105342
54,6_arx_2305.16867_1849923_8,This also leads to better scores and more successful coordination when interacting with human players.,These results enrich our understanding of LLMs' social behaviour and pave the way for a behavioural game theory for machines.,These results enrich our understanding of <mask>' social behaviour and pave the way for a behavioural game theory for machines.,,LLMs,LLMs,LLM,social behaviour,1,-7.511801306826857
55,6_arx_2305.14795_1847851_2,This has recently given rise to a range of techniques for injecting new facts through updating model weights.,"Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs.","Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the <mask>'s related beliefs.","If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister?",the model,model,model,related beliefs,1,0.5729937746458837
56,6_arx_2305.12763_1845819_3,We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory.,We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature.,We find that <mask>'s decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature.,"Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity.",GPT,GPT,GPT,rational decisions,1,-1.1828444890520942
57,6_arx_2305.12564_1845620_3,"Specifically, people perceive male gender identity (1) following demonstrations of ChatGPT's core abilities (e.g., providing information or summarizing text), (2) in the absence of such demonstrations, and (3) across different methods of eliciting perceived gender (using various scales and asking to name ChatGPT).","Moreover, we find that this seemingly default perception of ChatGPT as male can reverse when ChatGPT's feminine-coded abilities are highlighted (e.g., providing emotional support for a user).","Moreover, we find that this seemingly default perception of ChatGPT as male can reverse when <mask>'s feminine-coded abilities are highlighted (e.g., providing emotional support for a user).",,ChatGPT,ChatGPT,ChatGPT,feminine-coded abilities,1,-3.0319872749589365
58,6_arx_2303.03480_1803277_1,"We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to a uniquely described target object in a previously unseen environment.",Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions.,Our approach makes use of Large Language Models (LLMs) for this task by leveraging the <mask>'s commonsense reasoning capabilities for making sequential navigational decisions.,"Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model.",the LLM,LLM,LLM,commonsense reasoning capabilities,1,-2.428800072688272
59,6_arx_1704.00717_835229_5,The latter involves making AI more human-like and having it develop a theory of our minds.,"In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks.","In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of <mask>'s mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks.",We instantiate these ideas within the domain of Visual Question Answering (VQA).,AI,AI,AI,theory of mind,1,-6.260694128202095
