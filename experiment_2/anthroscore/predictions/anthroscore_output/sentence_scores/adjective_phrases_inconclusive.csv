,sentence,masked_sentence,text_id,anthroscore
0,Language models (LMs) are vulnerable to exploitation for adversarial misuse.,<mask> are vulnerable to exploitation for adversarial misuse.,4_acl_7_46407_0,-3.5005569970563766
1,"While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue.","While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent <mask> on human well-being has become a pressing issue.",4_arx_2005.06620_1286415_1,-4.65320089240295
2,Integrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible AI.,Integrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible <mask>.,4_arx_2408.08878_2129059_6,-3.347706541431428
3,RQ1: Can edited LLMs behave consistently resembling communicative AI in realistic situations?,RQ1: Can edited LLMs behave consistently resembling communicative <mask> in realistic situations?,4_arx_2402.05827_2003466_5,-1.7985633563240562
4,"The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment and some systematic behavioral biases.","The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent <mask>, the environment and some systematic behavioral biases.",4_arx_1904.13086_1118126_10,-1.2013077666785232
5,"Moreover, our error analysis shows that language models are generally less sensitive to the changes in claim length and source than the SVM model.","Moreover, our error analysis shows that language <mask> are generally less sensitive to the changes in claim length and source than the SVM model.",4_acl_1_30970_8,-4.35647397337442
6,"Modern large language models are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results for the model.","Modern <mask> are sensitive to prompts, and another synonymous expression or a typo may lead to unexpected results for the model.",4_acl_159_43725_0,-6.0010932753268325
7,"We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%.","We find that <mask> are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%.",4_acl_335_27056_3,-4.5757900151010205
8,Analysis shows that LLMs are sensitive to subtle contextual changes and often rely on surface-level cues.,Analysis shows that <mask> are sensitive to subtle contextual changes and often rely on surface-level cues.,4_acl_12_46472_3,-4.548732978365688
9,"These relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy LLMs.","These relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy <mask>.",4_arx_2405.16310_2073522_7,-2.5433912107606105
10,"This collaborative creative AI presents a new paradigm in AI, one that lets a team of two or more to come together to imagine and envision ideas that synergies well with interests of all members of the team.","This collaborative creative <mask> presents a new paradigm in AI, one that lets a team of two or more to come together to imagine and envision ideas that synergies well with interests of all members of the team.",4_arx_1906.03595_1135781_6,1.1661316695028674
11,Our best morpheme-aware model with properly reused weights beats the competitive word-level model by a large margin across multiple languages and has 20%-87% fewer parameters.,Our best morpheme-aware model with properly reused weights beats the competitive <mask> by a large margin across multiple languages and has 20%-87% fewer parameters.,4_acl_128_54380_3,-0.13153150495143429
12,"However, the untrustworthy third-party LLMs may covertly introduce vulnerabilities for downstream tasks.","However, the untrustworthy <mask> may covertly introduce vulnerabilities for downstream tasks.",4_acl_94_36582_1,2.108531145281562
13,"An intelligent algorithm which renders security surveillance more effective in detecting conflicts would bring many benefits to the passengers in terms of their safety, finance, and travelling efficiency.","An intelligent <mask> which renders security surveillance more effective in detecting conflicts would bring many benefits to the passengers in terms of their safety, finance, and travelling efficiency.",4_arx_2207.00477_1676375_2,-1.5867460519071983
14,"Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination.","Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which <mask> is more vulnerable towards hallucination.",4_acl_155_26876_9,-2.213099294542424
15,This work presents the development of an intelligent system that classifies and segments food presented in images to help the automatic monitoring of user diet and nutritional intake.,This work presents the development of an intelligent <mask> that classifies and segments food presented in images to help the automatic monitoring of user diet and nutritional intake.,4_arx_2012.03087_1390965_3,-2.568344711139197
16,"However, given, e.g., economic incentives to create dishonest AI, to what extent can we trust explanations?","However, given, e.g., economic incentives to create dishonest <mask>, to what extent can we trust explanations?",4_arx_2001.07641_1233054_2,0.41257498070299725
17,"Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment.","Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware <mask>, if used as they are without adjustment.",4_acl_862_33024_7,-0.8822899651269474
18,We show that an “attentive” RNN-LM (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs.,We show that an “attentive” <mask> (with 11M parameters) achieves a better perplexity than larger RNN-LMs (with 66M parameters) and achieves performance comparable to an ensemble of 10 similar sized RNN-LMs.,4_acl_45_49661_1,-0.005830792934029105
19,"To address this blind spot, this study introduces the AI Family Integration Index (AFII), a ten dimensional benchmarking framework that evaluates national preparedness for integrating emotionally intelligent AI into family and caregiving systems.","To address this blind spot, this study introduces the AI Family Integration Index (AFII), a ten dimensional benchmarking framework that evaluates national preparedness for integrating emotionally intelligent <mask> into family and caregiving systems.",4_arx_2503.22772_2287084_2,-1.1654463534175257
20,"We outline the conceptual foundations of Thoughtful AI, illustrate its potential through example projects, and envision how this paradigm can transform human-AI interaction in the future.","We outline the conceptual foundations of Thoughtful <mask>, illustrate its potential through example projects, and envision how this paradigm can transform human-AI interaction in the future.",4_arx_2502.18676_2261667_3,-2.7280094905911785
