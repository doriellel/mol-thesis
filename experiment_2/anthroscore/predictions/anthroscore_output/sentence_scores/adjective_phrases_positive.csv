,sentence,masked_sentence,text_id,anthroscore
0,"Next, an experiment is conducted on the dataset to examine to what extent a pretrained masked language model is aware of the constructions.","Next, an experiment is conducted on the dataset to examine to what extent a <mask> is aware of the constructions.",4_acl_683_21860_6,1.1309829384142684
1,A desired characteristic of an intelligent system is its ability to recognize the scope of its own knowledge.,A desired characteristic of an intelligent <mask> is its ability to recognize the scope of its own knowledge.,4_acl_348_35206_2,-1.77167582931693
2,LLMs are intelligent and slowly replacing the search engines.,<mask> are intelligent and slowly replacing the search engines.,4_acl_3_45070_1,-4.247897999156674
3,"However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves?","However, a critical question emerges: Are <mask> conscious of the existence of these decoding strategies and capable of regulating themselves?",4_acl_396_37165_1,-3.800791003376144
4,"In this way, the captioning model can become aware of the task goal and information need from the PLM.","In this way, the <mask> can become aware of the task goal and information need from the PLM.",4_acl_590_28726_3,1.5592239844718971
5,"In practical use, users might provide feedback based on the model’s output, hoping for a responsive model that can complete responses according to their feedback.","In practical use, users might provide feedback based on the model’s output, hoping for a responsive <mask> that can complete responses according to their feedback.",4_acl_818_37575_1,-0.1183662129563956
6,"This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about.","This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the <mask> is least confident about.",4_acl_633_48859_2,-1.877263248345475
7,"In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases.","In addition, we found that the <mask> becomes more confident and refuses to provide an answer in only few cases.",4_acl_243_44794_7,1.9058323264346235
8,(2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question?,(2) Is <mask> aware of the underlying commonsense knowledge for answering a specific question?,4_acl_276_39685_3,2.981358761856395
9,"However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.","However, when LLMs face different types of questions, it is worth exploring whether <mask> are aware that some questions have limited answers and need to respond more deterministically but some do not.",4_acl_117_37846_1,-1.5960699130825482
10,"The goal is to cultivate culturally cognizant and value-aligned Arabic LLMs capable of accommodating the diverse, application-specific needs of Arabic-speaking communities.","The goal is to cultivate culturally cognizant and value-aligned <mask> capable of accommodating the diverse, application-specific needs of Arabic-speaking communities.",4_acl_450_41596_3,-0.41023079526334527
11,We also show that an “attentive” RNN-LM needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset.,We also show that an “attentive” <mask> needs less contextual information to achieve similar results to the state-of-the-art on the wikitext2 dataset.,4_acl_45_49661_2,0.3541913627928004
12,Large Language Models (LLMs) are smart but forgetful.,<mask> are smart but forgetful.,4_arx_2311.04177_1947144_0,1.0430083150485583
13,"In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI.","In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious <mask>.",4_arx_1511.03246_676426_3,-0.7124931913891803
14,Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious AI.,Our evaluation using convolutional neural networks illustrates challenges and ideas for identifying malicious <mask>.,4_arx_2005.13635_1293430_5,-3.2756794427833924
15,"In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.","In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious <mask> as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.",4_arx_2504.03726_2292429_5_1,-2.63574839135096
16,"In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems.","In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative <mask>.",4_arx_2504.03726_2292429_5_2,-3.141257382524998
17,"We apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious LLMs.","We apply our OTF scheme on two LLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable amount of unethical ones, paving the way for more ethically conscious <mask>.",4_arx_2305.02626_1835682_10,-2.5168165760470096
18,"Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the <mask> can be conscious of multi-modal input instruction and then select the function-matched tool correctly.",4_arx_2401.10727_1990281_4,-0.5245043119428843
19,We consider a number of issues related to the development of the set of patterns which will be used by the intelligent system when interacting with environment.,We consider a number of issues related to the development of the set of patterns which will be used by the intelligent <mask> when interacting with environment.,4_arx_1301.6359_402949_3,-1.5032899057940394
20,"Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.","<mask> are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks.",4_arx_2308.03688_1891580_0,-3.2947793433272583
21,"Being more powerful and intrusive into user-device interactions, LLMs are eager for on-device execution to better preserve user privacy.","Being more powerful and intrusive into user-device interactions, <mask> are eager for on-device execution to better preserve user privacy.",4_arx_2403.11805_2028924_0,-5.483844121502816
22,"However, whether the same strategies can help LLMs become more creative remains under-explored.","However, whether the same strategies can help <mask> become more creative remains under-explored.",4_arx_2405.06715_2063927_2,1.9818300712582886
23,"To bridge this gap, this work studies the security threats posed by malicious LMs to NLP systems.","To bridge this gap, this work studies the security threats posed by malicious <mask> to NLP systems.",4_arx_2008.00312_1328034_4,-1.4334708228883741
24,"However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.","However, existing <mask> are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.",4_acl_693_19140_1,-4.14002414646928
25,"As AI systems become more intelligent and their behavior becomes more challenging to assess, they may learn to game the flaws of human feedback instead of genuinely striving to follow instructions; however, this risk can be mitigated by controlling how LLMs generalize human feedback to situations where it is unreliable.","As <mask> become more intelligent and their behavior becomes more challenging to assess, they may learn to game the flaws of human feedback instead of genuinely striving to follow instructions; however, this risk can be mitigated by controlling how LLMs generalize human feedback to situations where it is unreliable.",4_arx_2311.07723_1950690_0,-5.639337664163911
26,"The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.","The <mask> is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic.",4_acl_27_55498_4,0.0953439175277424
27,"We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.","We investigate the ability of <mask> to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.",4_arx_2407.11789_2110176_2,-4.926950229872258
28,These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question.,These three modules perform the divide-and-conquer procedure iteratively until the <mask> is confident about the final answer to the main question.,4_arx_2305.14985_1848041_7,-0.4000715275404261
29,"Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark.","Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the <mask> is significantly more confident under the original benchmark.",4_arx_2406.18326_2096614_4,-2.1544897258798876
30,"This is in part because LLMs might be overly confident in their predictions, overriding the influence of the constraints.","This is in part because <mask> might be overly confident in their predictions, overriding the influence of the constraints.",4_arx_2407.13164_2111551_3,-2.1455167226931167
31,"A smart autonomous system (SAS) combines analytics and autonomy to understand, learn, decide and act autonomously.","A smart autonomous <mask> combines analytics and autonomy to understand, learn, decide and act autonomously.",4_arx_1812.08960_1066534_3,-0.8296415274874356
32,"Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.","Results suggest that <mask> is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.",4_arx_2304.09655_1827701_7,-0.856799227501762
33,A detection algorithm aware of the list can identify the watermarked text.,A <mask> aware of the list can identify the watermarked text.,4_arx_2305.08883_1841939_5,4.143491191260885
34,"Consequently, we argue that the emergence of a conscious AI model is plausible in the near term.","Consequently, we argue that the emergence of a conscious <mask> is plausible in the near term.",4_arx_2407.09517_2107904_5,0.8724276996907747
35,"Conscious AI systems would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer.","Conscious <mask> would arguably deserve moral consideration, and it may be the case that large numbers of conscious systems could be created and caused to suffer.",4_arx_2501.07290_2230874_1,-1.2114163757723144
36,"While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content.","While recently <mask> have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content.",4_arx_2404.16873_2054324_0,-1.1686652192894744
37,"Unlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful AI autonomously generates, develops, and communicates its evolving thought process throughout an interaction.","Unlike conventional AI systems that operate on a turn-based, input-output model, Thoughtful <mask> autonomously generates, develops, and communicates its evolving thought process throughout an interaction.",4_arx_2502.18676_2261667_1,-2.774267047346333
38,Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern.,Whether current or near-term <mask> could be conscious is a topic of scientific interest and increasing public concern.,4_arx_2308.08708_1896600_0,-1.7012053635196853
39,"While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input.","While <mask> have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input.",4_arx_2502.00735_2243726_1,-5.333978674585531
40,"Large Language Models (LLMs) have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs.","<mask> have shown impressive proficiency across a range of natural language processing tasks yet remain vulnerable to adversarial prompts, known as jailbreak attacks, carefully designed to elicit harmful responses from LLMs.",4_arx_2411.14133_2196560_0,-1.1133696457333748
41,"We find that powerful LLMs are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization.","We find that powerful <mask> are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization.",4_acl_592_38304_5,-2.9594299266093937
42,"In fact, we demonstrate that even a ""blind"" language model that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.","In fact, we demonstrate that even a ""blind"" <mask> that ignores any image evidence can sometimes outperform all prior art, reminiscent of similar challenges faced by the visual-question answering (VQA) community many years ago.",4_arx_2306.01879_1855027_7,0.008353253451094389
43,We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.,We conclude by discussing how future AI developments may affect the fight between malicious <mask> and the public.,4_arx_1901.00912_1070459_9,-1.94781633948298
44,"The OCR model easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference.","The <mask> easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference.",4_arx_2208.12505_1703217_1,-0.5555138950006118
45,"Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together.","Moreover, <mask> is vulnerable to leading questions and is often confused when interpreting multiple images together.",4_arx_2311.03287_1946254_6,-2.507501243570923
46,"To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask.","To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the <mask> is most attentive to, we are able to better resolve the entire extent of the segmentation mask.",4_arx_2311.17095_1960062_4,1.9578840943410292
47,"We prove that it is impossible to precisely and consistently predict what specific actions a smarter-than-human intelligent system will take to achieve its objectives, even if we know terminal goals of the system.","We prove that it is impossible to precisely and consistently predict what specific actions a smarter-than-human intelligent <mask> will take to achieve its objectives, even if we know terminal goals of the system.",4_arx_1905.13053_1131489_2,-0.7116054666328893
48,"All in all, we demonstrate that our self-aware model improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.","All in all, we demonstrate that our self-aware <mask> improves the overall PR-AUC by 27.45%, achieves a relative defect reduction of up to 31.22%, and is able to adapt quicker to changes in global preferences across a large number of customers.",4_acl_36_22670_6,-0.13674546583808578
49,"We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples.","We study how to better construct in-context example sets, based on whether the <mask> is aware of the in-context examples.",4_acl_133_36622_2,0.8646691280827419
50,"AI agents are often assumed to pursue fixed goals, but AI persons may be self-aware enough to reflect on their aims, values, and positions in the world and thereby induce their goals to change.","AI agents are often assumed to pursue fixed goals, but <mask> may be self-aware enough to reflect on their aims, values, and positions in the world and thereby induce their goals to change.",4_arx_2501.13533_2237117_7,-3.098818862550843
51,A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions.,A truly intelligent <mask> should be capable of correcting errors in its responses through external interactions.,4_arx_2502.05605_2248596_0,-2.0597105398915225
