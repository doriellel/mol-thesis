,sentence,masked_sentence,text_id,anthroscore
0,Our experiments show that these errors can be identified with high accuracy by an LLM.,Our experiments show that these errors can be identified with high accuracy by an <mask>.,2_acl_143_43709_5,-1.3674876842247734
1,"To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.","To maintain the knowledge acquired by <mask>, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.",2_acl_511_29547_1,-3.1342142358185043
2,"Examination prioritization was performed by the AI, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.","Examination prioritization was performed by the <mask>, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.",2_arx_2001.08625_1234038_4,2.662020913833441
3,"In CAPoW, a security professional can define relevant request context attributes which can be learned by the AI system.","In CAPoW, a security professional can define relevant request context attributes which can be learned by the <mask>.",2_arx_2301.11767_1782831_2,2.97911740028864
4,I suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025.,I suggest that up to 50% of a radiologists work in 2021 will be performed by <mask> in 2025.,2_arx_2106.07921_1485685_1,0.8160046693952872
5,This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the model.,This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the <mask>.,2_arx_2007.15619_1327133_1,0.5653270742840739
6,The simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by the AI methods and provides a means of explainability.,The simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by the <mask> and provides a means of explainability.,2_arx_2302.06852_1791786_1,-0.5664079182825823
7,"Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves.","Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the <mask> themselves.",2_acl_23_33157_0,-2.098807080803315
8,The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.,The text was translated by both <mask> and a translator who is an academic in the field of translation and has 10 years of experience.,2_acl_3_26185_65,3.7318724693815284
9,The final submission was chosen based on the best performances which was achieved by the BERT+BiLSTM model.,The final submission was chosen based on the best performances which was achieved by the <mask>.,2_acl_260_9141_4,1.6066709486466983
10,These features are best interpreted by a self-consistent relativistic reflection model.,These features are best interpreted by a <mask>.,2_arx_1907.08625_1153161_6,0.08805526627538995
11,"In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by LLMs.","In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by <mask>.",2_acl_430_37198_2,-2.40448986428423
12,How should we train a language model in this scenario?,How should we train a <mask> in this scenario?,3_acl_696_6286_1,2.6533349703863074
13,"Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data.","Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed <mask> to generate appropriate responses on novel tasks, without the need for training data.",3_acl_891_29927_2,-4.1192343873853705
14,"A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.","A promising approach to rectify these flaws is correcting <mask> with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.",3_acl_27_42781_1,-6.365980084076181
15,"This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.","This paper provides an exhaustive review of the recent advances in correcting <mask> with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.",3_acl_27_42781_3,-5.496350846615233
16,"We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.","We present the <mask> with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.",3_arx_2305.01937_1834993_4,-0.5592595702969625
17,"We show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.","We show that choosing the best policy to interact with the <mask> can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.",3_acl_920_38624_10_1,0.15193362961993984
18,"This paper explores the potential of constructing an AI spoken dialogue system that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.","This paper explores the potential of constructing an <mask> that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.",3_arx_2309.11000_1915804_0,-2.437660792508108
19,"We introduce a new task, “less likely brainstorming,” that asks a model to generate outputs that humans think are relevant but less likely to happen.","We introduce a new task, “less likely brainstorming,” that asks a <mask> to generate outputs that humans think are relevant but less likely to happen.",3_acl_794_28930_3,-0.8122259619622341
20,We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.,We successfully trained a <mask> to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.,3_arx_2308.01154_1889046_3,-0.05664197254561287
21,"For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.","For earlier patches that passed all the tests, we further ask the <mask> to generate alternative variations of the original plausible patches.",3_arx_2304.00385_1818431_9,1.088671875596006
22,"Subsequently, we ask LLMs to predict this masked field.","Subsequently, we ask <mask> to predict this masked field.",3_arx_2411.05823_2188250_7,-0.7713693231446417
23,"Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected.","Typically, the advice generated by <mask> is judged by a human and either deemed reliable or rejected.",2_arx_2204.06916_1637054_2,-1.980100346857654
24,Each lesson in the curriculum is learnt by a deep reinforcement learning model.,Each lesson in the curriculum is learnt by a <mask>.,2_arx_2004.11543_1276313_6,6.219867293259012
25,"Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.","Therefore, we aim to train an <mask> with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.",3_acl_814_27535_2,-2.7360888508561487
26,"Specifically, the structural features identified by our algorithm were found to be related to clinical observations of glaucoma.","Specifically, the structural features identified by our <mask> were found to be related to clinical observations of glaucoma.",2_arx_2012.09755_1397633_5,1.2757575296549994
