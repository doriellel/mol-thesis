,sentence,masked_sentence,text_id,anthroscore
0,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by <mask> with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,2_arx_1910.06294_1190213_4,-3.764541690774932
1,"As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a low complexity algorithm.","As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a <mask>.",2_arx_2006.05347_1299861_5,1.1344934575394419
2,We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.,We introduce an explainable VQA system that uses spatial and object features and is powered by the <mask>.,2_arx_2007.00900_1312414_5,-0.938695889482446
3,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by <mask> and those manually annotated by humans.,2_acl_7_34206_4,-2.5462636028413606
4,We consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.,We consider a UAS that can be fully controlled by the <mask> and by a remote human pilot.,2_arx_2103.07820_1437965_2,2.7425109497053164
5,We observed 95.61% alarms raised by the said system are taken care of by the operator.,We observed 95.61% alarms raised by the said <mask> are taken care of by the operator.,2_arx_1811.12185_1056818_9,-1.1339899814539365
6,"Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.","Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the <mask>, but this requires accurate detection of OOV words.",2_arx_1909.09993_1180031_2,-0.7074988471380088
7,"By comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.","By comparing the measurements with the results predicted by the <mask> for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.",2_arx_1912.06835_1218731_4,-0.9419314274473223
8,The two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.,The two data-driven approaches are trained using data samples generated by the <mask> via supervised learning.,2_arx_2102.07384_1423873_5,1.4638530438327244
9,"The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.","The fuzzy trajectory data are developed based on different driving styles, which are clustered by the <mask>.",2_arx_2205.05016_1649428_3,0.5421221125756013
10,"For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?","For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the <mask> is reversed, would the debiasing results also be reversed?",2_acl_280_28416_2,1.8343125391708721
11,"Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.","Taken together, these findings support the idea that meaning construction is supported by a flexible <mask> based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.",2_acl_4_33862_8,-1.589986765749856
12,"We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.","We find that synthetic data generated by <mask> is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.",2_acl_6_42908_8,-1.4302519342828433
13,"Significant advancements have recently been made in large language models, represented by GPT models.","Significant advancements have recently been made in large language models, represented by <mask>.",2_acl_377_35235_0,-3.757660937369474
14,"Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.","Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by <mask> and the code evaluator provides code performance metric measurements.",2_arx_1901.05719_1075266_4,-2.5764409689408776
15,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a <mask>.,2_arx_1902.02508_1083592_0,0.9597662672692042
16,CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,CNR is trained with data created by a <mask> of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,2_arx_2002.05702_1243125_4,-0.14222517090807685
17,"Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.","Ultimately, our results from comparisons of LVM segmentation predicted by a <mask> locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.",2_arx_2009.12437_1353940_2,-1.3528310930500744
18,Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,Specifically we will overview the criteria that should be met by an <mask> before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,2_arx_2106.02498_1480262_5,-0.6943023825380124
19,"Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.","Considering the natural ventilation, the thermal behavior of buildings can be described by a <mask>.",2_arx_1212.5593_395209_0,2.243878826557099
20,"Then, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.","Then, a multilayer perceptron is trained by a <mask> on a data subset, and used to classify the transients as glitch or burst.",2_arx_1401.5941_495172_4,-0.5347012382198617
21,"Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.","Both the emitted power and its angular pattern are well described by a <mask>, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.",2_arx_1408.5886_551192_2,1.2052648096629497
22,"In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.","In this study, UAV dynamics are described by a <mask>.",2_arx_1501.07576_594324_2,3.030239613360788
23,The synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,The synthetic PM10 record predicted by the <mask> was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,2_arx_1610.02937_778284_8,0.6289748989961375
24,"Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Inspired by the latest <mask>, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.",2_arx_2304.11116_1829162_4,-0.5817695495163555
25,"Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.","Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by <mask>.",2_arx_2110.14419_1552562_1,-6.510020891665961
26,"The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.","The results indicate that biases equating American identity with being White are learned by language-and-image <mask>, and propagate to downstream applications of such models.",2_arx_2207.00691_1676589_8,-4.3701563526486105
27,"Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.","Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our <mask>.",2_arx_1812.01714_1059288_3,-1.552802003273193
28,"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages the target model to predict a higher likelihood than that predicted by the <mask>, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.",2_arx_2412.12865_2215243_5,-1.5338342364374355
29,"Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.","Therefore, determining whether a text was generated by an <mask> has become one of the factors that must be considered when evaluating its reliability.",2_acl_19_45086_1,0.8242773541437849
30,We find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.,We find that the observed behaviour is explained by a <mask> including the effects associated with the variations of pressure and density.,2_arx_0906.5497_132048_3,-0.32658353947403995
31,"For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.","For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a <mask>.",2_arx_2002.10965_1248388_3,0.9007931646638028
32,"We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).","We present a large-scale study of linguistic bias exhibited by <mask> covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).",2_acl_750_35597_0,-1.5870489439494975
33,"We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.","We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the <mask>.",2_acl_502_20526_4,0.2413032263145709
34,"By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.","By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these <mask>.",2_arx_2105.13818_1476377_2,-0.9572764460561771
35,"However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.","However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the <mask>.",2_acl_46_41679_2,1.644125943498338
36,"In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.","In order to train the <mask> efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.",3_arx_1712.09783_928382_2,-0.7399142603083781
37,We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,We believe the discussions would provide a broader perspective of looking at <mask> through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,3_acl_6_25720_4,-6.403759683778013
38,"We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.","We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained <mask> to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.",3_arx_2101.04617_1408359_3,-1.3563266297848955
39,"Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","Many companies that deploy <mask> publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.",3_arx_2101.05967_1409709_1,-6.565497029164577
40,"We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.","We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the <mask> with a human-labeled dataset.",3_acl_118_9850_4,-1.679956914249228
41,We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.,We evaluated the <mask> on the German to English and English to French translation task of TED lectures.,3_acl_3_2185_5,-2.7171673654621173
42,"Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.","Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the <mask> on large-scale unparallel corpus, which further improves the fluency of the output sentences.",3_arx_1911.03597_1202191_5,-1.667155112733468
43,We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.,We ask whether structural information can be extracted from LLM’s and develop a <mask> that integrates it with their learnt statistics.,3_acl_329_43922_3,-2.2807650441673744
44,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable <mask> for patients compared to conventional CAD systems.,3_arx_2302.07257_1792191_6,-0.44509079513294836
45,"Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.","Finally, we systematically evaluate and analyze eight mainstream <mask> and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.",3_arx_301_32463_8,-0.17421435844984856
46,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate recent <mask>, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.",3_acl_5_42629_6,-3.2744259659528403
47,"We design a system using these pre-trained models to answer questions, based on the given context.","We design a <mask> using these pre-trained models to answer questions, based on the given context.",3_acl_28_45074_3,-1.576354912427849
48,"Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.","Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual <mask> for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.",3_acl_41_45371_2,0.58079318830411
49,"We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.","We also develop a <mask> that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.",3_acl_104_45430_3,-0.5940771969078398
50,"Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.","Using this approach, we train and present a <mask> trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.",3_acl_6_46417_4,-0.6740882044787675
51,"In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.","In evaluations of ranking character predictions, training <mask> on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.",3_acl_7_60414_2,-3.7726164086546383
52,"To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.","To test the impact of our filtering, we train <mask> on both the original and the filtered datasets.",3_acl_27_46301_5,-3.4177154710945477
53,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact <mask> using labeled and unlabeled examples.,3_arx_1910.06294_1190213_4,-1.6117678738307895
54,"Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.","Thus, it is important to leverage memorized knowledge in the external LM for building the <mask>, since it is hard to prepare a large amount of paired data.",3_acl_1_7090_2,-2.2523233379850716
55,"Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.","Using the TREC Misinformation dataset, we empirically evaluate <mask> to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.",3_acl_928_27649_2,-6.432698175227811
56,"In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.","In the present work, we develop a <mask> with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.",3_acl_582_29618_3,-0.133124853620604
57,"These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.","These findings highlight the challenges of developing <mask> for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.",3_acl_418_44990_5,-1.7456001694636534
58,Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.,Our approach employs a pool of candidate VPs and trains a <mask> to dynamically select the most effective VP for a given input image.,3_acl_45_45963_3,-1.6417086903373512
59,We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.,We have gathered a substantial amount of Chinese corpus to train the <mask> and have also optimized its structure.,3_arx_2308.00624_1888516_6,-1.7633153087113147
60,"However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.","However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the <mask> during training.",2_acl_75_14800_1,1.5448818088373883
61,"Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.","Human raters were asked to rate the explanation of the implicatures generated by <mask> on their reasonability, logic and fluency.",2_acl_98_33764_7,-2.202761036017135
62,"We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.","We posit that multiple solutions to a reasoning task, generated by an <mask>, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.",2_acl_1_42012_5,-0.5069053642890431
63,"In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs.","In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by <mask>.",2_acl_600_45881_2,-2.8671448652703013
64,"Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.","Specifically, we trained <mask> with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.",3_arx_2306.02920_1856068_2,-1.6407584857544073
