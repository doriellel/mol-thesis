id,sentence,masked_sentence,AI_phrase,mask,component,expectation,model_score,prediction
2_arx_1910.06294_1190213_4,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by <mask> with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,pre-trained masked language models,pre-trained masked language models,provide,0,-3.764541690774932,0
2_arx_2006.05347_1299861_5,"As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a low complexity algorithm.","As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a <mask>.",a low complexity algorithm,low complexity algorithm,address,0,1.1344934575394419,1
2_arx_2007.00900_1312414_5,We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.,We introduce an explainable VQA system that uses spatial and object features and is powered by the <mask>.,the BERT language model,BERT language model,power,0,-0.938695889482446,2
2_acl_7_34206_4,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.,A positive correlation of 0.40 was found between the emotion intensity scores reproduced by <mask> and those manually annotated by humans.,GPT-4,GPT-4,reproduce,0,-2.5462636028413606,0
2_arx_2103.07820_1437965_2,We consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.,We consider a UAS that can be fully controlled by the <mask> and by a remote human pilot.,the onboard DAA system,onboard DAA system,control,0,2.7425109497053164,1
2_arx_1811.12185_1056818_9,We observed 95.61% alarms raised by the said system are taken care of by the operator.,We observed 95.61% alarms raised by the said <mask> are taken care of by the operator.,the said system,system,raise alarm,0,-1.1339899814539365,0
2_arx_1909.09993_1180031_2,"Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.","Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the <mask>, but this requires accurate detection of OOV words.",the A2W model,A2W model,cover,0,-0.7074988471380088,2
2_arx_1912.06835_1218731_4,"By comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.","By comparing the measurements with the results predicted by the <mask> for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.",the ion flow model,ion flow model,predict,0,-0.9419314274473223,2
2_arx_2102.07384_1423873_5,The two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.,The two data-driven approaches are trained using data samples generated by the <mask> via supervised learning.,the BCD algorithm,BCD algorithm,generate,0,1.4638530438327244,1
2_arx_2205.05016_1649428_3,"The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.","The fuzzy trajectory data are developed based on different driving styles, which are clustered by the <mask>.",the K-means algorithm,K-means algorithm,cluster,0,0.5421221125756013,2
2_acl_280_28416_2,"For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?","For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the <mask> is reversed, would the debiasing results also be reversed?",the debiasing method,debiasing method,use,0,1.8343125391708721,1
2_acl_4_33862_8,"Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.","Taken together, these findings support the idea that meaning construction is supported by a flexible <mask> based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.",a flexible form-to-meaning mapping system,form-to-meaning mapping system,support,0,-1.589986765749856,0
2_acl_6_42908_8,"We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.","We find that synthetic data generated by <mask> is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.",LLMs,LLMs,generate,0,-1.4302519342828433,0
2_acl_377_35235_0,"Significant advancements have recently been made in large language models, represented by GPT models.","Significant advancements have recently been made in large language models, represented by <mask>.",GPT models,GPT models,represent,0,-3.757660937369474,0
2_arx_1901.05719_1075266_4,"Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.","Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by <mask> and the code evaluator provides code performance metric measurements.",AI algorithms,AI algorithms,realize,0,-2.5764409689408776,0
2_arx_1902.02508_1083592_0,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.,The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a <mask>.,a SGS model,SGS model,capture,0,0.9597662672692042,2
2_arx_2002.05702_1243125_4,CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,CNR is trained with data created by a <mask> of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.,a generative model of synthetic structures,generative model,create,0,-0.14222517090807685,2
2_arx_2009.12437_1353940_2,"Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.","Ultimately, our results from comparisons of LVM segmentation predicted by a <mask> locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.",a model,model,predict,0,-1.3528310930500744,0
2_arx_2106.02498_1480262_5,Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,Specifically we will overview the criteria that should be met by an <mask> before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.,an AI system,AI system,meet criteria,0,-0.6943023825380124,2
2_arx_1212.5593_395209_0,"Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.","Considering the natural ventilation, the thermal behavior of buildings can be described by a <mask>.",a linear time varying model,linear time varying model,describe,0,2.243878826557099,1
2_arx_1401.5941_495172_4,"Then, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.","Then, a multilayer perceptron is trained by a <mask> on a data subset, and used to classify the transients as glitch or burst.",a backpropagation algorithm (MLP-BP),backpropagation algorithm (MLP-BP),train,0,-0.5347012382198617,2
2_arx_1408.5886_551192_2,"Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.","Both the emitted power and its angular pattern are well described by a <mask>, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.",a model,model,describe,0,1.2052648096629497,1
2_arx_1501.07576_594324_2,"In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.","In this study, UAV dynamics are described by a <mask>.",a three-dimensional dynamic point-mass model,three-dimensional dynamic point-mass model,describe,0,3.030239613360788,1
2_arx_1610.02937_778284_8,The synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,The synthetic PM10 record predicted by the <mask> was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.,the model,model,predict,0,0.6289748989961375,2
2_arx_2304.11116_1829162_4,"Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.","Inspired by the latest <mask>, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.",the latest ChatGPT and Toolformer models,ChatGPT and Toolformer models,inspire,0,-0.5817695495163555,2
2_arx_2110.14419_1552562_1,"Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.","Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by <mask>.",AI,AI,"shape,influence",0,-6.510020891665961,0
2_arx_2207.00691_1676589_8,"The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.","The results indicate that biases equating American identity with being White are learned by language-and-image <mask>, and propagate to downstream applications of such models.",language-and-image AI,AI,learn,0,-4.3701563526486105,0
2_arx_1812.01714_1059288_3,"Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.","Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our <mask>.",our model,model,learn,0,-1.552802003273193,0
2_arx_2412.12865_2215243_5,"This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.","This preference encourages the target model to predict a higher likelihood than that predicted by the <mask>, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.",the aligned LLMs,aligned LLMs,predict,0,-1.5338342364374355,0
2_acl_19_45086_1,"Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.","Therefore, determining whether a text was generated by an <mask> has become one of the factors that must be considered when evaluating its reliability.",an LLM,LLM,generate,0,0.8242773541437849,2
2_arx_0906.5497_132048_3,We find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.,We find that the observed behaviour is explained by a <mask> including the effects associated with the variations of pressure and density.,a model,model,explain,0,-0.32658353947403995,2
2_arx_2002.10965_1248388_3,"For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.","For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a <mask>.",a low-complexity trellis-based algorithm,low-complexity trellis-based algorithm,solve,0,0.9007931646638028,2
2_acl_750_35597_0,"We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).","We present a large-scale study of linguistic bias exhibited by <mask> covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).",ChatGPT,ChatGPT,exhibit bias,0,-1.5870489439494975,0
2_acl_502_20526_4,"We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.","We find that 4 pretrained transformers LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the <mask>.",the LM,LM,learn,0,0.2413032263145709,2
2_arx_2105.13818_1476377_2,"By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.","By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these <mask>.",these models,models,acquire,0,-0.9572764460561771,2
2_acl_46_41679_2,"However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.","However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the <mask>.",the language model,language model,see,0,1.644125943498338,1
3_arx_1712.09783_928382_2,"In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.","In order to train the <mask> efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.",the MoE model,MoE model,train,0,-0.7399142603083781,2
3_acl_6_25720_4,We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,We believe the discussions would provide a broader perspective of looking at <mask> through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.,LLMs,LLMs,look at,0,-6.403759683778013,0
3_arx_2101.04617_1408359_3,"We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.","We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained <mask> to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.",the trained model,model,employ,0,-1.3563266297848955,0
3_arx_2101.05967_1409709_1,"Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.","Many companies that deploy <mask> publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.",AI,AI,deploy,0,-6.565497029164577,0
3_acl_118_9850_4,"We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.","We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the <mask> with a human-labeled dataset.",the model,model,fine-tune,0,-1.679956914249228,0
3_acl_3_2185_5,We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.,We evaluated the <mask> on the German to English and English to French translation task of TED lectures.,the RBM-based language model,RBM-based language model,evaluate,0,-2.7171673654621173,0
3_arx_1911.03597_1202191_5,"Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.","Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the <mask> on large-scale unparallel corpus, which further improves the fluency of the output sentences.",the model,model,pre-train,0,-1.667155112733468,0
3_acl_329_43922_3,We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.,We ask whether structural information can be extracted from LLM’s and develop a <mask> that integrates it with their learnt statistics.,a model,model,develop,0,-2.2807650441673744,0
3_arx_2302.07257_1792191_6,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.,The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable <mask> for patients compared to conventional CAD systems.,a more user-friendly and understandable system,system,create,0,-0.44509079513294836,2
3_arx_301_32463_8,"Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.","Finally, we systematically evaluate and analyze eight mainstream <mask> and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.",eight mainstream LLMs,LLMs,"evaluate,analyze",0,-0.17421435844984856,2
3_acl_5_42629_6,"We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.","We evaluate recent <mask>, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.",recent multilingual LLMs,multilingual LLMs,evaluate,0,-3.2744259659528403,0
3_acl_28_45074_3,"We design a system using these pre-trained models to answer questions, based on the given context.","We design a <mask> using these pre-trained models to answer questions, based on the given context.",a system,system,design,0,-1.576354912427849,0
3_acl_41_45371_2,"Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.","Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual <mask> for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.",an individual model,model,construct,0,0.58079318830411,2
3_acl_104_45430_3,"We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.","We also develop a <mask> that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.",a search algorithm,search algorithm,develop,0,-0.5940771969078398,2
3_acl_6_46417_4,"Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.","Using this approach, we train and present a <mask> trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.",a BERT-based model,BERT-based model,"train,present",0,-0.6740882044787675,2
3_acl_7_60414_2,"In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.","In evaluations of ranking character predictions, training <mask> on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.",recurrent LMs,recurrent LMs,train,0,-3.7726164086546383,0
3_acl_27_46301_5,"To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.","To test the impact of our filtering, we train <mask> on both the original and the filtered datasets.",GPT-2 models,GPT-2 models,train,0,-3.4177154710945477,0
3_arx_1910.06294_1190213_4,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.,In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact <mask> using labeled and unlabeled examples.,a fast and compact model,model,train,0,-1.6117678738307895,0
3_acl_1_7090_2,"Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.","Thus, it is important to leverage memorized knowledge in the external LM for building the <mask>, since it is hard to prepare a large amount of paired data.",the seq2seq model,seq2seq model,build,0,-2.2523233379850716,0
3_acl_928_27649_2,"Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.","Using the TREC Misinformation dataset, we empirically evaluate <mask> to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.",ChatGPT,ChatGPT,evaluate,0,-6.432698175227811,0
3_acl_582_29618_3,"In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.","In the present work, we develop a <mask> with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.",a recurrent neural language model,recurrent neural language model,develop,0,-0.133124853620604,2
3_acl_418_44990_5,"These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.","These findings highlight the challenges of developing <mask> for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.",AI,AI,develop,0,-1.7456001694636534,0
3_acl_45_45963_3,Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.,Our approach employs a pool of candidate VPs and trains a <mask> to dynamically select the most effective VP for a given input image.,a router model,router model,train,0,-1.6417086903373512,0
3_arx_2308.00624_1888516_6,We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.,We have gathered a substantial amount of Chinese corpus to train the <mask> and have also optimized its structure.,the model,model,train,0,-1.7633153087113147,0
2_acl_75_14800_1,"However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.","However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the <mask> during training.",the model,model,acquire,0,1.5448818088373883,1
2_acl_98_33764_7,"Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.","Human raters were asked to rate the explanation of the implicatures generated by <mask> on their reasonability, logic and fluency.",LLMs,LLMs,generate,0,-2.202761036017135,0
2_acl_1_42012_5,"We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.","We posit that multiple solutions to a reasoning task, generated by an <mask>, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.",an LLM,LLM,generate,0,-0.5069053642890431,2
2_acl_600_45881_2,"In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs.","In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by <mask>.",LLMs,LLMs,learn,0,-2.8671448652703013,0
3_arx_2306.02920_1856068_2,"Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.","Specifically, we trained <mask> with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.",bilingual LMs,bilingual LMs,train,0,-1.6407584857544073,0
