id,sentence,masked_sentence,AI phrase,suggested mask,AI entity,original_term,original_noun,anthropomorphic component,expectation,anthroscore
2_acl_143_43709_5,Our experiments show that these errors can be identified with high accuracy by an LLM.,Our experiments show that these errors can be identified with high accuracy by <mask>.,an LLM,LLM,LLM,LLM,an llm,identify,inc,-3.240353056940876
2_acl_511_29547_1,"To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.","To maintain the knowledge acquired by <mask>, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.",LLMs,LLMs,LLM,LLMs,llms,acquire,inc,-3.1342142358185043
2_arx_2001.08625_1234038_4,"Examination prioritization was performed by the AI, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.","Examination prioritization was performed by <mask>, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object.",the AI,AI,AI,AI,the ai,perform,inc,0.6200992625403572
2_arx_2301.11767_1782831_2,"In CAPoW, a security professional can define relevant request context attributes which can be learned by the AI system.","In CAPoW, a security professional can define relevant request context attributes which can be learned by <mask>.",the AI system,AI system,system,AI,the ai system,learn,inc,0.6403825270846646
2_arx_2301.11767_1782831_2,"In CAPoW, a security professional can define relevant request context attributes which can be learned by the AI system.","In CAPoW, a security professional can define relevant request context attributes which can be learned by <mask>.",the AI system,AI system,system,system,the ai system,learn,inc,0.6403825270846646
2_arx_2106.07921_1485685_1,I suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025.,I suggest that up to 50% of a radiologists work in 2021 will be performed by <mask> in 2025.,AI-models,AI-models,model,AI,ai-models,perform,inc,0.8160046693952872
2_arx_2106.07921_1485685_1,I suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025.,I suggest that up to 50% of a radiologists work in 2021 will be performed by <mask> in 2025.,AI-models,AI-models,model,AI-models,ai-models,perform,inc,0.8160046693952872
2_arx_2007.15619_1327133_1,This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by the model.,This can help authorities in these regions with resource planning measures to redirect resources to the regions identified by <mask>.,the model,model,model,model,the model,identify,inc,-2.285665951952648
2_arx_2302.06852_1791786_1,The simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by the AI methods and provides a means of explainability.,The simulations are grounded by a neuro-symbolic language that both enables question answering of what is learned by <mask> and provides a means of explainability.,the AI methods,AI methods,method,AI,the ai methods,learn,inc,-3.6675371759890076
2_acl_23_33157_0,"Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves.","Large Language Models (<mask>) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the <mask> themselves.",the LLMs,LLMs,LLM,LLMs,llms,hold knowledge,inc,0.07314053455529645
2_acl_23_33157_0,"Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves.","Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by <mask> themselves.",the LLMs,LLMs,LLM,LLMs,the llms,hold knowledge,inc,-3.0889869857848637
2_acl_3_26185_65,The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.,The text was translated by <mask> and a translator who is an academic in the field of translation and has 10 years of experience.,ChatGPT,ChatGPT,ChatGPT,ChatGPT,both chatgpt,translate,inc,2.909923303888304
2_acl_260_9141_4,The final submission was chosen based on the best performances which was achieved by the BERT+BiLSTM model.,The final submission was chosen based on the best performances which was achieved by <mask>.,the BERT+BiLSTM model,BERT+BiLSTM model,model,model,the bert+bilstm model,achieve,inc,0.6173392872128485
2_arx_1907.08625_1153161_6,These features are best interpreted by a self-consistent relativistic reflection model.,These features are best interpreted by <mask>.,a self-consistent relativistic reflection model,relativistic reflection model,model,model,a self-consistent relativistic reflection model,interpret,inc,-3.1798016791324297
2_acl_430_37198_2,"In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by LLMs.","In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by <mask>.",LLMs,LLMs,LLM,LLMs,llms,interpret,inc,-2.40448986428423
3_acl_696_6286_1,How should we train a language model in this scenario?,How should we train <mask> in this scenario?,a language model,language model,model,model,a language model,train,inc,1.6425636213216048
3_acl_891_29927_2,"Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct fixed LLMs to generate appropriate responses on novel tasks, without the need for training data.","Utilizing the predefined task schema, i.e., belief instruction and dialog policy, we instruct <mask> to generate appropriate responses on novel tasks, without the need for training data.",fixed LLMs,LLMs,LLM,LLMs,fixed llms,instruct,inc,-3.1629290460388084
3_acl_27_42781_1,"A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.","A promising approach to rectify these flaws is correcting <mask> with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.",LLMs,LLMs,LLM,LLMs,llms,correct,inc,-6.365980084076181
3_acl_27_42781_1,"A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.","A promising approach to rectify these flaws is correcting LLMs with feedback, where <mask> itself is prompted or guided with feedback to fix problems in its own output.",LLMs,LLMs,LLM,LLM,the llm,correct,inc,-4.733220042970867
3_acl_27_42781_3,"This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.","This paper provides an exhaustive review of the recent advances in correcting <mask> with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.",LLMs,LLMs,LLM,LLMs,llms,correct,inc,-5.496350846615233
3_arx_2305.01937_1834993_4,"We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.","We present <mask> with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask <mask> to generate responses to those questions; we dub this LLM evaluation.",the LLMs,LLMs,LLM,LLMs,the llms,present,inc,-1.3764555755217405
3_arx_2305.01937_1834993_4,"We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.","We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub <mask>.",the LLMs,LLMs,LLM,LLM,this llm evaluation,present,inc,-4.980227266695433
3_acl_920_38624_10_1,"We show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.","We show that choosing the best policy to interact with <mask> can reduce cost by ~90% while giving better or comparable performance, compared to communicating with <mask> in the original LRL.",the LLM,LLM,LLM,LLM,the llm,interact with,inc,-1.2654859408462986
3_arx_2309.11000_1915804_0,"This paper explores the potential of constructing an AI spoken dialogue system that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.","This paper explores the potential of constructing <mask> that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.",an AI spoken dialogue system,AI spoken dialogue system,system,AI,an ai spoken dialogue system,construct,inc,-3.3009133675358058
3_arx_2309.11000_1915804_0,"This paper explores the potential of constructing an AI spoken dialogue system that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.","This paper explores the potential of constructing <mask> that ""thinks how to respond"" and ""thinks how to speak"" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.",an AI spoken dialogue system,AI spoken dialogue system,system,system,an ai spoken dialogue system,construct,inc,-3.3009133675358058
3_acl_794_28930_3,"We introduce a new task, “less likely brainstorming,” that asks a model to generate outputs that humans think are relevant but less likely to happen.","We introduce a new task, “less likely brainstorming,” that asks <mask> to generate outputs that humans think are relevant but less likely to happen.",a model,model,model,model,a model,ask,inc,-3.6783834863738054
3_arx_2308.01154_1889046_3,We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.,We successfully trained <mask> to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.,a light language model,light language model,model,model,a light language model,train,inc,-2.2644881366215
3_arx_2304.00385_1818431_9,"For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.","For earlier patches that passed all the tests, we further ask <mask> to generate alternative variations of the original plausible patches.",the LLM,LLM,LLM,LLM,the llm,ask,inc,-0.3371421046730738
3_arx_2411.05823_2188250_7,"Subsequently, we ask LLMs to predict this masked field.","Subsequently, we ask <mask> to predict this masked field.",LLMs,LLMs,LLM,LLMs,llms,ask,inc,-0.7713693231446417
2_arx_2204.06916_1637054_2,"Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected.","Typically, the advice generated by <mask> is judged by a human and either deemed reliable or rejected.",AI,AI,AI,AI,ai,generate,inc,-1.980100346857654
2_arx_2004.11543_1276313_6,Each lesson in the curriculum is learnt by a deep reinforcement learning model.,Each lesson in the curriculum is learnt by <mask>.,a deep reinforcement learning model,deep reinforcement learning model.,model,model,a deep reinforcement learning model,learn,inc,2.911136880314265
3_acl_814_27535_2,"Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.","Therefore, we aim to train <mask> with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.",an agent,agent,agent,agent,an agent,train,inc,-3.5511145826407944
2_arx_2012.09755_1397633_5,"Specifically, the structural features identified by our algorithm were found to be related to clinical observations of glaucoma.","Specifically, the structural features identified by <mask> were found to be related to clinical observations of glaucoma.",our algorithm,algorithm,algorithm,algorithm,our algorithm,identify,inc,0.484096664588721
