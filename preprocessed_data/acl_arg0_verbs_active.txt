54_7092_0	The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal.
54_7092_4	This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response.
327_7365_2	The model takes the paired source, reference, and hypothesis sentence all together as an input.
153_8591_5	By combining relation prediction and relevance ranking tasks with our target link prediction, the proposed model can learn more relational properties in KGs and properly perform even when lexical similarity occurs.
96_9413_1	A language model‚Äôs vocabulary‚Äîtypically selected before training and permanently fixed later‚Äîaffects its size and is part of what makes it resistant to such adaptation.
409_9725_0	This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks.
557_9873_6	Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).
586_9902_1	While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context.
141_10319_10	Especially on the two narrative prompts, our model performs much better than all other state-of-the-art models.
15_10841_7	This Machine Translation system accepts announcements in the form of English text as input and produces Indian Sign Language (ISL) synthetic animations as output.
319_11663_5	We investigated four modeling units (phone, syllable, word piece, and word) and found that the syllable-based model performed best in terms of both word and phone recognition accuracy, which were about 60% and over 85% respectively in speaker-open condition.
333_11677_3	We find that an RNN language model, initialized with pre-trained fastText embeddings, performs best, highlighting the importance of sub-word information for Mi‚Äôkmaq language modelling.
607_11951_5	We show that a cross-domain adapted BERT language model performs significantly better than strong baseline models like vanilla BERT-base and XLNet-base.
18_12331_4	While the use of additional data and our classifier filter were able to improve results, the paraphrasing model produced too many invalid outputs to further improve the output quality.
164_13090_3	This architecture was used and evaluated in the context of the SemEval 2020 challenge (task 9), and our system got 72.7% on the F1 score.
261_13187_1	Our proposed model learns to extract textual features using a BiGRU-based deep neural network supported by a Hierarchical Attention architecture to focus on the most relevant areas in the text.
19_13362_0	An image captioning system involves modules on computer vision as well as natural language processing.
28_13541_2	Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM.
28_13541_5	Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know.
48_13561_5	Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c)
5_13585_1	In some applications, however, having an additional context can help the model make the right prediction, e.g., by taking the domain or the time of writing into account.
18_13628_4	We have found that the LSTM model with FastText embedding is performing better than other models for Hindi and Bangla datasets but for the English dataset, the CNN model with FastText embedding has performed better.
18_13628_6	Our system got 11th and 10th positions for English Sub-task A and Sub-task B, respectively, 8th and 7th positions, respectively for Hindi Sub-task A and Sub-task B and 7th and 6th positions for Bangla Sub-task A and Sub-task B, respectively among the total submitted systems.
12_13859_2	Our system consists of techniques such as back-translation and fine-tuning, which are already widely adopted in translation tasks.
118_13965_3	Model training consists of two phases.
268_14351_1	Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation.
4_15072_6	As a response to disambiguating cues, the LMs often select the correct interpretation, but occasional errors point to potential areas of improvement
8_15076_3	Although the best bidirectional model performs similarly to humans, they display different strengths: humans outperform neural networks in conversational contexts, while RoBERTa excels at written genres.
5_15370_5	We finally probe the NLM‚Äôs linguistic competence before and after fine-tuning, highlighting how linguistic information encoded in representations changes when the model learns to predict complexity.
12_15584_0	Document-grounded goal-oriented dialog system understands users‚Äô utterances, and generates proper responses by using information obtained from documents.
110_15766_0	Existing dialog system models require extensive human annotations and are difficult to generalize to different tasks.
242_15898_1	Here, we introduce the corpus of Chinese linguistic minimal pairs (CLiMP) to investigate what knowledge Chinese LMs acquire.
296_16391_4	By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.
697_16792_1	As a result, it can be hard to identify what the model actually ‚Äúbelieves‚Äù about the world, making it susceptible to inconsistent behavior and simple errors.
102_17592_7	Finally, as it is hard to tell given a privacy parameter ùúñ what was the effect on the trained representation, we present experiments showing that the trained model does not memorize private information.
280_17770_7	In our approach, we design a teacher-student joint learning and distillation framework to collaboratively learn both teacher and student models, where the student model can learn from the learning experience of the teacher model.
11_18042_6	Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned.
11_18667_6	Our model, named AfriBERTa, covers 11 African languages, including the first language model for 4 of these languages.
1_18791_4	‚Ä¢ Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?
1_18791_5	‚Ä¢ Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?
137_18936_5	Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks.
407_19206_3	When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks.
4_19458_1	Once pre-trained, deploying a large language model presents comparatively small infrastructure requirements, and offers robust performance in many NLP tasks.
153_19876_0	We analyse how a transformer-based language model learns the rules of chess from text data of recorded games.
2_19952_4	Our results also indicate that model distillation may hurt the ability of cross-lingual transfer of sentence representations, while language dissimilarity at most has a modest effect.
62_20611_1	This success raises the question of whether, in principle, a system can ever ‚Äúunderstand‚Äù raw text without access to some form of grounding.
34_20946_4	As the training stages go on, we make the system learn to solve multiple tasks by adding extra information at different training stages gradually.
25_21229_2	However, two problems exist: first, low-frequency words would have a significant negative impact on the sentence likelihood derived from the language model; second, when it comes to multiple domains, the language model needs to be trained on domain-specific text for domain adaptation.
25_21229_4	Experimental results show that our word-tag-hybrid BERT model brings improvement on both a sentence acceptability benchmark and a cross-domain sentence acceptability evaluation corpus.
133_21430_8	Model analysis demonstrates that both strategiescontribute to the performance boost.
368_21665_0	A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy.
373_21670_1	This creates challenges when AI systems try to reason about language and its relationship with the environment: objects referred to through language (e.g. giving many instructions) are not immediately visible.
19_22095_6	Using masked token prediction and centroid distance minimisation as training objectives, the LogFiT model learns to recognise the linguistic patterns associated with the normal log data.
71_22736_5	Our experimental results suggest that LMs have different generalizations from humans; LMs exhibited less context-dependent behaviors toward topicalization judgment.
269_22934_1	The OCR model easily gets confused on recognizing handwritten Chinese characters, and the textual information of the answers is missing during the model inference.
494_23159_6	We also find that the adaptation methods perform differently for different models and that unimodal model counterparts perform on par with the VL models regardless of adaptation, indicating that current VL models do not necessarily gain better language understanding from their multimodal training.
15_23492_3	The main challenge of this task is that the system requires a great amount of pre-trained knowledge to generate answers grounded in multiple documents.
183_23850_4	Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past.
207_23874_0	Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs.
234_23901_4	In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs.
234_23901_5	Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network.
617_24284_5	While we would expect the predictive accuracy to correlate with human judgments of semantic compositionality, we find this is largely not the case, indicating that LMs may not accurately distinguish between compositional and non-compositional phrases.
661_24328_3	A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates.
65_24611_5	The human evaluation found that our topic model creates coherent topics.
245_24926_3	We then investigate how an LM performs in generating a CN with regard to an unseen target of hate.
265_24946_2	The model consists of a pretrained neural sentence LM, a BERT-based contextual encoder, and a masked transfomer decoder that estimates LM probabilities using sentence-internal and contextual evidence.
265_24946_3	When contextually annotated data is unavailable, our model learns to combine contextual and sentence-internal information using noisy oracle unigram embeddings as a proxy.
52_25065_2	We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.
58_25071_6	This way, the model learns to accomplish the tasks via language generation without the need of training task-specific layers.
99_25367_4	Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism but also from the effects of the causal mask.
147_25415_6	On the one hand, experiments show that LMs can memorize millions of temporally-scoped facts with relatively high accuracy and transfer stored knowledge to temporal knowledge queries, thereby expanding the LM-as-KB paradigm to the temporal domain.
423_25691_2	Can LMs trained on text learn anything at all about the relationship between language and use?
423_25691_4	When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context.
423_25691_6	I survey findings from the recent literature showing that‚Äîeven in today‚Äôs non-robust and error-prone models‚ÄîLMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals.
429_25697_5	Experiments show that the PLM-enhanced model brings a substantial performance improvement.
440_25708_6	When the knowledge about the nature of the word pairings is combined with a probability that the LM has learned that information, we have a new way to examine what information is captured in LMs.
494_25762_4	By unifying subword segmentation and language modelling, our model learns subwords that optimise LM performance.
6_25924_8	Additionally, we re-discover a bias-performance trade-off: the better the model performs, the more bias it contains.
23_25968_2	This reduces their applicability, since LLMs requires expensive GPUs.
25_25970_2	This similarity depends on how the model learns to encode context, which can be altered to include other attributes, such as style.
41_26571_5	Evaluation on ASR N-best rescoring shows that LSTM and Transformer LMs trained on ASR confusion networks do not bring significant WER reductions.
118_26648_8	By explaining how a LM trained on highly private real-world medical data can be published, we hope that more language resources will be published openly and responsibly so the scientific community can profit from them.
377_26907_1	LMs cannot learn such constraints from the data, which is scarce with respect to their needs even for a well-resourced language such as French.
408_26938_0	In recent years, AI research has demonstrated enormous potential for the benefit of humanity and society.
408_26938_1	While often better than its human counterparts in classification and pattern recognition tasks, however, AI still struggles with complex tasks that require commonsense reasoning such as natural language understanding.
511_27041_4	We compare the results of fine-tuning a gaBERT model with an mBERT model for the task of identifying verbal multiword expressions, and show that the fine-tuned gaBERT model also performs better at this task.
325_27825_1	Specifically, by conditioning on a subgraph encapsulating the locally relevant sentence history, can a model make better next-word predictions than a pretrained sequential language model alone?
8_28250_5	The K-nearest neighbours algorithm selects a subset of multi-event training data that is most similar to the target event.
51_28646_3	The ensemble model performs worse than the baseline due to overfitting and achieves an F1-score of 0.3031.
228_28823_3	Finally, in the test phase, our system received macro-f1 scores of 77.66, 84.35, and 74 on task 12, task 13, and task 9.
37_29088_3	This system performed 13% better than the baseline and was the best performing system overall for this shared task.
1_29130_4	Through zero-shot probing, we find that generative LMs produce poor ESDs with mostly omitted, irrelevant, repeated or misordered events.
11_29182_2	Active learning is a semi-supervised learning algorithm, in which a model consistently and dynamically learns to identify the most beneficial samples to train itself on, in order to achieve better optimization and performance on downstream tasks.
15_29198_2	This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize.
12_29537_5	Experiments show that our model, Astro-mT5, out-performs the existing baseline in astrophysics related information extraction.
201_29960_5	MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks.
213_29972_2	In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations.
213_29972_5	While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking.
255_30014_3	With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems.
270_30029_5	A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains.
300_30059_1	Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts.
400_30159_2	However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on.
546_30305_3	We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases.
550_30309_3	What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.
626_30385_1	Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output.
626_30385_2	GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks.
845_30604_4	We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3‚Äôs performance varies widely across types of dogwhistles and targeted groups.
59_30729_4	Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously.
154_30824_1	However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope.
162_30832_2	In both tasks, the model must know the entities used to perform the generation properly.
58_30893_3	In DeepPavlov Dream, multi-skill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants.
8_30900_2	It is unclear whether current LMs realize the generalization capacity for temporal inference across languages.
8_30900_8	Our findings demonstrate that LMs struggle with specific linguistic phenomena, such as habituality, indicating that there is potential for the development of more effective NLI models across languages.
57_30986_2	By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.
20_31069_3	Our proposed system for the ALTA shared task, based on ensembling a number of language models, claimed first place on the development set with an accuracy of 99.35% and third place on the test set with an accuracy of 98.35%.
6_31103_3	Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems.
56_31153_1	The proposed system for all the tasks‚Äô sub-tasks consisted of preprocessing the data and finetuning AraBERT on the given datasets, in addition to several procedures performed for each subtask to adapt to the problems faced in it.
65_31344_3	Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation.
10_31355_1	This progress is based on a simple but general pipeline which consists of pre-training neural language models on large quantities of text, followed by an adaptation step that fine-tunes the pre-trained model to perform a specific NLP task of interest.
57_31413_4	We demonstrate that this system performs better in a zero-shot setting compared to several pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task.
65_31421_4	Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model.
4_31430_5	We also observe inverse scaling for most-type quantifier understanding, which is contrary to human psycho-linguistic experiments and previous work, where the model‚Äôs understanding of most-type quantifier gets worse as the model size increases.
8_31434_3	We find that our (134M parameter) Chinese Backpack language model performs comparably to a (104M parameter) Transformer, and learns rich character-level meanings that log-additively compose to form word meanings.
5_31496_5	For instance, ChatGPT generates fluent and natural Singlish texts (an English-based creole spoken in Singapore), but for English-Tamil language pair, the system mostly produces grammatically incorrect or semantically meaningless utterances.
5_31496_7	Based on our investigation, existing multilingual LLMs exhibit a wide range of proficiency in code-mixed data generation for SEA languages.
78_31613_8	The semantic dependency feature serves as a global signal and helpsthe model learn simile knowledge that can be applied to unseen domains.
9_31622_4	This way,the system can perform more varied tasks by leveraging different modalities and signals.
2_31658_0	‚ÄúRecent months have witnessed significant progress in the field of large language models (LLMs).Represented by ChatGPT and GPT-4, LLMs perform well in various natural language process-ing tasks and have been applied to many downstream applications to facilitate people‚Äôs lives.
2_31658_2	Specifically, LLMs suffer from social bias,robustness problems, and poisoning issues, all of which may induce LLMs to spew harmful con-tents.
6_31697_1	Here, we evaluate whether LLMs learn to identify a particular structure attested in Romance (and French in particular), called the pseudorelative.
6_31697_4	Our results suggest that LLMs learn some but not all of these properties, but crucially fail at recognizing the most specific of them: cliticization.
3_31886_27	This model takes an input sequence in one language and generates a corresponding output sequence in the target language (OpenAI, 2023).
7_32065_1	In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.
172_32230_7	Our experiments show that, even when reducing the number of embedding parameters by 99.6% (from 11.4M to just 36k), our model suffers no loss in translation quality compared to the baseline syllable model.
175_32233_1	In particular, a model may learn a reasoning process on in-domain training data that does not hold for out-of-domain test data.
24_32483_4	We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization.
71_32530_1	But how do LLMs represent relationships between languages?
95_32554_0	Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks.
134_32593_2	Therefore, it is crucial for LLMs to learn novel interpretations in-context.
134_32593_5	Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations.
150_32609_4	Further, to help LLMs distinguish confusing classes, we design a progressive revision framework, which can improve the thinking steps by correcting hard demonstrations.
160_32619_7	Our comprehensive analysis reveals that existing open-source instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases.
162_32621_2	However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments.
174_32633_2	In such cases, model developers need to eliminate specific data influences from the model to mitigate legal and ethical penalties.
192_32651_3	A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning).
225_32684_7	Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4.
278_32737_4	As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution.
299_32758_1	However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.
313_32772_3	In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic.
330_32789_0	A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions.
330_32789_2	However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated.
335_32794_0	The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.
339_32798_0	In human-AI collaboration, users typically form a mental model of the AI system, which captures the user‚Äôs beliefs about when the system performs well and when it does not.
339_32798_1	The construction of this mental model is guided by both the system‚Äôs veracity as well as the system output presented to the user e.g., the system‚Äôs confidence and an explanation for the prediction.
392_32851_4	Specifically, MoT is divided into two stages: 1. before the test stage, the LLM pre-thinks on the unlabeled dataset and saves the high-confidence thoughts as external memory; 2.
392_32851_5	During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it.
417_32876_1	Previous distillation methods usually prompt ChatGPT to generate a set of instructions and answers, for the student model to learn.
429_32888_2	Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages.
459_32918_5	Lastly, we show that (4) LMs of smaller size using morphological segmentation can perform comparably to models of larger size trained with BPE ‚Äî both in terms of (1) perplexity and (3) scores on downstream tasks.
463_32922_6	Our method is based on the intuition that the ChatGPT model will make fewer revisions to LLM-generated texts than it does to human-written texts, because the texts generated by LLMs are more in accord with the generation logic and statistical patterns learned by LLMs like ChatGPT.
491_32950_2	However, fundamental questions persist regarding how LLMs acquire their multilingual abilities and how performance varies across different languages.
491_32950_8	However, LLMs struggle to provide accurate results in translation-variant tasks, which lack this property, requiring careful user judgment to evaluate the answers.
502_32961_2	However, LMs do not describe unweighted formal languages‚Äîrather, they define probability distributions over strings.
502_32961_3	In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities.
502_32961_7	These results present a first step towards characterizing the classes of distributions RNN LMs can represent and thus help us understand their capabilities and limitations.
507_32966_1	However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning.
550_33009_3	The experimental results indicate that multiple advanced LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place.
550_33009_6	It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.
565_33024_10	Moreover, EasyQuant can be implemented in parallel so that the quantized model could be attained in a few minutes even for LLMs over 100B. To our best knowledge, we are the first work that achieves almost lossless quantization performance for LLMs under a data-independent setting and our algorithm runs over 10 times faster than the data-dependent methods.
581_33040_4	However, the setup of the evaluation task is crucial ‚Äî LLMs perform better on coherence ratings of word sets than on intrustion detection.
615_33074_1	These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.
643_33102_3	We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal.
688_33147_5	When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3).
699_33158_1	While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand social language.
726_33185_1	However, it is unclear whether LMs can understand physical concepts in the human world.
743_33202_1	Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities.
744_33203_8	We conclude that despite the limited signal through which LMs can learn about animacy, they are indeed sensitive to the relevant lexical semantic nuances available in English.
752_33211_7	Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence.
752_33211_8	Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning.
814_33273_1	Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors.
821_33280_4	Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines.
821_33280_5	Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations.
841_33300_6	Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score.
913_33372_2	However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs).
958_33417_3	Specifically, we design the multi-task rationale tuning strategy to help the model learn current relations robustly.
1008_33467_1	Through extensive experimentation on seven diverse data sets, we observed that LLMs, such as ChatGPT-3.5 and PaLM, demonstrated superior generality compared to other LLMs, e.g., BLOOM and GPT-NeoX.
1013_33472_8	We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain.
1036_33495_3	By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5.
23_33590_2	1) The LLM Engine, empowered by the LLM (e.g. GPT-3.5), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs.
33_33600_5	Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage.
70_33637_6	Our open-source model supports customized and privacy-fulfilled Vietnamese language processing systems.
50_33734_7	We find that for parallel sentences across different languages, the transliteration-based model learns sentence representations that are more similar.
83_33767_2	In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning.
184_33868_5	Moreover, the model performs well across genders and age groups.
50_33932_4	Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin.
191_34073_2	We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning.
218_34100_1	Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts.
218_34100_4	Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy.
284_34166_0	Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts).
302_34184_2	Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts.
322_34204_1	To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.
444_34326_2	However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time.
507_34389_8	Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset.
574_34456_2	Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ‚Äúif‚Äú, we want to explore whether Code-LLMs acquire better causal reasoning abilities.
618_34500_0	Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models.
847_34729_5	We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size.
847_34729_8	For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down.
95_34878_3	Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa ‚Äì that any model that resides anywhere in those regions also exhibits high performance.
95_34878_6	Our findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both.
97_34880_2	Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems.
167_34950_1	However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation.
167_34950_2	The above issues make the LLMs need the ability to verify the answers.
167_34950_6	By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.
182_34965_5	We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.
193_34976_3	One direct solution for this problem is to make the model see and learn an explanation of sentiment expression rather than certain words.
201_34984_3	Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions‚Äô rationales when attempting to correct students‚Äô answers.
212_34995_9	Our human and GPT-4 evaluations also lead to a consistent conclusion that LLM performs well on this task.
261_35044_2	In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance.
267_35050_1	In this work, we focus on two novel ideas: (1) generating definitions from examples and using them for zero-shot classification, and (2) investigating how an LLM makes use of the definitions.
272_35055_2	As powerful LLMs are devouring existing text data from various domains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether the private information is included in the training data and what privacy threats can these LLMs and their downstream applications bring.
321_35104_2	By selecting important key-value pairs the model makes better use of the cache so that in limited cache size, a longer context history can be stored.
334_35117_1	By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively perform the task at hand through in-context learning.
378_35161_7	In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question.
462_35245_6	Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations.
504_35287_3	We show that even the largest LLMs today perform poorly under the zero-shot setting.
508_35291_2	To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate.
508_35291_3	Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.
518_35301_3	Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.
518_35301_6	Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning.
531_35314_4	Compared to existing models, our personalized language model demonstrated superior performance in predicting an individual‚Äôs language in a test set.
578_35361_4	In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference.
603_35386_4	To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for kNN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM‚Äôs generalization ability and the task-specific evidence provided by the full labeled dataset.
641_35424_0	Open-domain dialogue system usually requires different sources of knowledge to generate more informative and evidential responses.
652_35435_7	We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%).
710_35493_3	Through extensive experiments on nine datasets across four IE tasks, we demonstrate that current advanced LLMs consistently exhibit inferior performance, higher latency, and increased budget requirements compared to fine-tuned SLMs under most settings.
732_35515_4	For adding new syntactic knowledge and keeping a good balance between the original and additional knowledge, we addressed the problem of catastrophic forgetting that prevents the model from keeping semantic information when the model learns additional syntactic knowledge.
782_35565_5	We also observe that LMs memorize the limited task-specific training data despite the use of known regularization methods.
787_35570_5	Without imposing a biased persona, where would an AI-based news outlet lie within the bias ratings?
795_35578_2	In this work, we explore testing LLMs‚Äô reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments.
831_35614_2	We see that while LLMs exhibit excellent predictive performance in Statute Prediction, their performance dips in Judgment Prediction when compared with many standard models.
982_35765_2	To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs cross-lingual language understanding.
1011_35794_1	However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses.
15_36308_3	Our findings show that LLMs struggle in particular with generating coherent continuations in this rather simple setting, indicating a lack of discourse knowledge beyond the well-known IC bias.
24_36317_3	Our findings suggest that LLMs still struggle to generate children‚Äôs stories at the level of quality and nuance found in actual stories.
14_36349_3	The model produces summaries covering meeting topics and next steps and performs comparably to a large language model at a fraction of the cost.
30_36452_6	token, the average matched accuracy improved further to 92.0, indicating that the model could learn to control the formality of the translation output based solely on the embedding of the <bos> token.
15_36809_0	This paper describes our multiclass classification system developed as part of the LT-EDI@RANLP-2023 shared task.
5_36856_4	Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text.
13_36901_3	We observe that LLMs with in-context learning exhibit poorer translation quality compared to the domain-specific NMT system, however, they are effective in transferring markup tags, especially the large BLOOM model (176 billion parameters).
6_36997_1	A notable capability of LLMs is in-context learning (ICL), where the model learns new tasks using input-output pairs in the prompt without any parameter update.
4_37014_4	We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.
22_37032_3	In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise).
1_37648_7	Further analysis indicates that leveraging our designed prompt is effective, and our model suffers from a low recall.
124_37771_5	Our system exhibited good noise resistance and excellent entity recognition performance, resulting in our team‚Äôs first place victory in the Chinese track of MultiCoNER II.
192_37839_5	Our model Muril produces a macro average F-score of 76.27%, which is a comparable result for this competition.
287_37934_7	Our system exhibited strong performance across multiple tasks, with particularly noteworthy performance in Subtask B. Comparison experiments and ablation studies demonstrate the effectiveness of our system.
5_38099_5	The fine-tuned model demonstrated overall syntactic generalizability towards compound honorific sentences, except when tested with the data involving direct speech.
20_38114_3	In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph.
22_38116_3	Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics.
32_38126_4	We conduct experiments on five datasets and find that static embeddings encode these features more accurately at the level of words and phrases, whereas contextualized LMs perform better on sentences.
41_38135_8	[\textsc{EventB}] \textit{is impossible}.‚Äù, a well-functioning language model should not conclude that [\textsc{EventA$\&amp;$B}] is likely.
78_38252_3	We find that LLMs perform well in generalizing the distribution of a novel noun argument between related contexts that were seen during pre-training (e.g., the active object and passive subject of the verb spray), succeeding by making use of the semantically organized structure of the embedding space for word embeddings.
7_38285_2	The addition of locality levels allows a model to learn how to weight neighbors based on their relative location to the current text in source documents, and have been shown to further improve model performance.
4_38299_0	The textual adversarial attack refers to an attack method in which the attacker adds imperceptible perturbations to the original texts by elaborate design so that the NLP (natural language processing) model produces false judgments.
19_38419_4	Our results show that multilingual LMs do not successfully learn the culturally appropriate nuances of emotion and we highlight possible research directions towards correcting this.
29_38429_10	The system accurately explains valid jokes but also comes up with fictional explanations for invalid jokes.
80_38566_6	Results show that our terminology-aware model learns to incorporate terminologies effectively, and the large language model refinement process can further improve terminology recall.
88_38574_1	Our primary system consists of 3 stages: Joint denoising and MT training using officially approved monolingual and parallel corpora, backtranslation and, MT training on original and backtranslated parallel corpora.
88_38574_3	We also develop 2 contrastive systems on unconstrained settings, where the first system involves fine-tuning of IndicTrans2 DA models on official parallel corpora and seed data used in AI4Bharat et al, (2023), and the second system involves a system combination of the primary and the aforementioned system.
9_38669_2	The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality.
12_38672_2	While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills.
17_38677_2	LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries.
56_38716_6	Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more efficiently.
66_38726_6	Besides, LLMs exhibit capability discrepancies across different reasoning categories.
96_38756_6	Our model demonstrates competitive performance against the state-of-the-art models on English and Chinese GEC datasets.
126_38786_7	Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation.
131_38791_1	However, as the complexity of tasks escalates, LLMs often encounter increasing errors in their multi-step reasoning process.
163_38823_5	Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust.
163_38823_6	In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered.
169_38829_3	Hence, a question naturally arises: can LLMs learn and benefit from their mistakes, especially for their reasoning?This study investigates this problem from both the prompting and model-tuning perspectives.
183_38843_3	Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text.
193_38853_4	The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity.
196_38856_4	Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.
198_38858_0	As natural language becomes the default interface for human-AI interaction, there is a need for LMs to appropriately communicate uncertainties in downstream applications.
237_38897_3	This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively.
242_38902_6	We only conduct retrieval for the missing knowledge in questions that the LLM does not know.
248_38908_2	To this end, we construct ‚Äòlinguistic task spaces‚Äô ‚Äì representations of an LM‚Äôs language conceptualisation ‚Äì that shed light on the connections LMs draw between language phenomena.
270_38930_0	A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests.
304_38964_5	Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench.
336_38996_5	On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data.
358_39018_7	We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles.
386_39046_3	However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference.
447_39107_7	Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies, and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.
473_39133_5	Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data.
497_39157_4	BinLLM converts collaborative embeddings from external models into binary sequences ‚Äî a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs.
506_39166_1	One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth.
516_39176_0	Some prior work has shown that LLMs perform well in NLG evaluation for different tasks.
531_39191_2	However, LLM-based reasoning still encounters the following challenges: (1) Limited adaptability of preset structures to diverse tasks; (2) Insufficient precision in exploiting known conditions to derive new ones; and (3) Inadequate consideration of historical reasoning experiences for subsequent reasoning steps.
540_39200_9	Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions.
550_39210_1	We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies ‚Äúthe singer of ‚ÄòSuperstition‚Äô‚Äù as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder‚Äôs mother to complete the prompt.
555_39215_1	To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices.
584_39244_1	In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity.
585_39245_1	Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents.
593_39253_6	Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.
604_39264_3	We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance.
606_39266_6	Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.
617_39277_4	The resulting model can judge preferences of on-the-fly responses from current policy initialized from itself.
636_39296_3	Understanding how LLMs interpret generic statements serves not only as a measure of their ability to abstract but also arguably plays a role in their encoding of stereotypes.
637_39297_5	Using a dataset of lexicalized and annotated noun-noun compounds, we find that LLMs can infer some semantic relations better than others (with a preference for compounds involving concrete concepts).
647_39307_2	Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations in ICL.
652_39312_5	Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech.
669_39329_3	LLMs identify the task based on the demonstration and generalize it to the prompt.
669_39329_8	Our results suggest an LLM could learn a novel task in context via composing tasks learned during pre-training.
701_39361_2	By optimizing the marginal likelihood of a training set under our latent variable model, the base LLM automatically learns when to generate itself and when to call on one of the ‚Äúassistant‚Äù language models to generate, all without direct supervision.
701_39361_4	Our collaborative decoding is especially useful in cross-domain settings where a generalist base LLM learns to invoke domain expert models.
738_39398_3	Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs‚Äô discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency.
745_39405_5	LLM-Rubric accomplishes this by training a small feed-forward neural network that includes both judge-specific and judge-independent parameters.
747_39407_7	Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.
758_39418_1	However, simply injecting external knowledge into prompts does not guarantee that LLMs can identify and use relevant information in the prompts to conduct chain-of-thought reasoning, especially when the LLM‚Äôs internal knowledge is derived from biased information on the pretraining data.
774_39434_2	To better understand these phenomena, we investigate if they can be understood in terms of ‚Äúcompeting subnetworks‚Äù: the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one.
774_39434_6	The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the ‚Äúheuristic‚Äù heads to compute higher-level features.
782_39442_5	Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories.
809_39469_6	We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art.
852_39512_0	Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems.
18_39543_5	For example, by patching activations along a ‚Äúbirthyear‚Äù direction we can make the LM express an increasingly late birthyear.
25_39550_2	However, the mechanism through which LLMs memorize PII remains poorly understood.
26_39551_3	Our findings show that LLMs struggle to generate contextually relevant responses to non-literal language.
51_39576_4	While LLMs exhibit reasoning patterns in accordance with human nonmonotonic reasoning abilities, they fail to maintain stable beliefs on truth conditions of generics at the addition of supporting examples (‚ÄòOwls fly‚Äô) or unrelated information (‚ÄòLions have manes‚Äô).Our findings highlight pitfalls in attributing human reasoning behaviours to LLMs as long as consistent reasoning remains elusive.
35_39638_4	The LLMs identify and decompose required knowledge triples that are not present in the KG, enriching them and aligning updates with real-world demands.
32_39666_0	Customizing LLMs for a specific task involves separating high-quality responses from lower-quality ones.
45_39677_1	Existing methods induce the model to produce step-by-step calculations, but this research explores the question: Does making the LLM analyze the question improve its performance?
48_39678_2	While one line of work in the past has suggested that LLMs learn surface-level statistics from the dataset, another line of work emphasizes that the learned representations are effective for simulating the underlying world model, considering the causal relationship for the next token prediction.
3_39699_3	These AI models do not always produce accurate outputs and are known for generating incorrect information, known as hallucinations, whose causes are hard to pinpoint.
5_39749_1	However, how LLMs perform NLP tasks for LRLs is less explored.
5_39749_3	Our results indicate that the LLMs perform worse for the labeling of LRLs in comparison to HRLs in general.
7_39780_5	We also identify additional challenges: LLMs struggle with continuity in explaining and practicing sophisticated concepts due to the lack of human-like cognitive functions, such as cognitive dissonance.
7_39780_8	Despite this, LLMs can only represent the effects of complex human cognitive functions through (often) fragmented linguistic descriptions, whereas humans excel at understanding critical and broader contexts and the interconnections between cognitive aspects.
20_39793_2	However, LLMs often struggle with the nuances and style required for organisation-specific translation so we leverage TMs, which store human translated segments, as a valuable resource to enhance translation accuracy and efficiency.
11_39806_2	The findings reveal that moderate-sized LLMs, such as LLaMa-3-8B and Mixtral-8x7B, achieve accuracy comparable to NMTs like DeepL. However, LLMs frequently exhibit mistranslation errors, including interlanguage/code-switching and anglicisms, while NMTs demonstrate better fluency.
11_39806_3	Both LLMs and NMTs struggle with spatial-related errors, including syntactic projections and polysemy.
16_39811_2	In this session, we will demonstrate how LLMs, enhanced by supplementary methodologies such as fine-tuning and combined with other, more legacy language models, can efficiently perform this formality adaptation task.
10_39933_6	After extensive experiments, a fine-tuned DeBERTa-v3-base model exhibits the best performance among all PTLMs with an F1 score of 78.90% on Task B.
25_39968_2	Our results, in zero-shot and few-shot settings, show that LLMs perform poorly in these settings: LLMs have difficulty with tasks that require complex reasoning or domain-specific knowledge.
23_40025_7	The results illustrate that in contrast to encoder-only and encoder-decoder, decoder-only LLMs require further investigation.
6_40088_2	Inspired by this, our paper investigates whether LLMs can estimate their own hallucination risk before response generation.
6_40088_4	Our empirical analysis reveals two key insights: (1) LLM internal states indicate whether they have seen the query in training data or not; and (2) LLM internal states show they are likely to hallucinate or not regarding the query.
32_40114_1	However, training SAEs can be computationally intensive, especially as model complexity grows.
6_40140_0	While preliminary findings indicate that multilingual LLMs exhibit reduced bias compared to monolingual ones, a comprehensive understanding of the effect of multilingual training on bias mitigation, is lacking.
98_40296_8	While all mod-els generate largely fluent and self-consistent text, their explanations score low on reasonabilityexcept for GPT-4, suggesting that most LLMs cannot produce satisfactory explanations of theimplicatures in the conversation.
8_40369_5	On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers‚Äô gold references.
2_40393_12	Therefore, we probe whether LLMs exhibit similar overgeneralization behavior in terms of quantification and in property inheritance.
2_40393_14	Furthermore, we find that LLMs may exhibit similar non-logical behavior to humans when considering property inheritance from generics.
7_40410_5	In some cases, the model does not recognize the drug name, due to the presence of synonymous words, or it provides untrustworthy information, caused by intrinsic hallucinations.
18_40421_7	The LLM learns from examples in the prompt and decomposing one prompt to several prompts allows the model to avoid confusions between the different entity types.
23_40426_5	In contrast, zero-shot LLMs struggle to leverage EHR representations.
8_40491_1	The system consists of a Conversational Recommender System (CoRS), in which the decision-making module is separate from the language generation module.
77_40560_4	Minerva demonstrates that an LLM for a specific language brings a number of practical benefits compared to the adaptation of an existing one, including deep control over the composition of the vocabulary and the training data.
91_40574_4	Our findings indicate that while LLMs perform comparably to humans in many aspects, there are notable differences in structural and semantic changes.
91_40574_5	The results of our study underscore the potential and limitations of using AI for administrative text simplification, highlighting areas where LLMs need improvement to achieve human-level proficiency.
92_40575_0	While LLMs get more proficient at solving tasks and generating sentences, we aim to investigate the role that differentsyntactic structures have on models‚Äô performances on a battery of Natural Language Understanding tasks.
99_40582_1	While LLMs have made lexical analysis more implicit, explicit argument structure identification remains crucial in domain-specific contexts.
134_40617_4	Given a target word and a sentence in which the word occurs, the LLM should choose from a predefined set the correct meaning definition.
20_40663_3	While the English LLMs exhibit near-chance level accuracy in subtyping aphasia, the Chinese counterparts demonstrate less than satisfactory performance in distinguishing between individuals with and without aphasia.
32_40675_0	Accurate representation of medical information is crucial for patient safety, yet artificial intelligence (AI) systems, such as Large Language Models (LLMs), encounter challenges in error-free clinical text interpretation.
39_40682_4	It also suggests that the automated few-shot LLM approach can perform close to the fine-tuning-based method without extra LLM normalization and be advantageous under scarce data access conditions.
49_40692_2	Recognising the limitation of LLMs in generating accurate corrections only via prompting strategies, we propose incorporating error-span predictions from a smaller, fine-tuned model in two ways: 1) by presenting it as a hint in the prompt and 2) by framing it as multiple-choice questions from which the LLM can choose the best correction.
67_40710_1	Our system consists of a lightweight Vision-and-Language Transformer (ViLT) model which is fine-tuned for the clinical dermatology visual question-answering task.
8_40719_7	In addition, our system is explainable on two levels: first, knowing the answers to the BDI questions provides clues about the possible symptoms that could lead to a clinical diagnosis of depression; second, our system can explain the predicted answer for each question.
1_40741_6	Through its modeling and findings, BAMBINO-LM makes a focused contribution to the pre-training of small-scale language models by first developing a human-inspired strategy for pre-training and then showing that it results in behaviours similar to that of humans.
6_40770_4	Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse.
1_40816_0	Chinese LLMs demonstrate impressive performance on NLP tasks, particularly on discipline knowledge benchmarks, with some results approaching those of GPT-4.
2_40822_7	Our results show that not all extracted utterances are correctly structured, indicating that either LLMs do not fully acquire syntactic-semantic rules or they acquire them but cannot apply them effectively.
24_40844_2	Consistent with human results, we find that LLM recall uncertainty, measured via token probability, is influenced by the fan effect.
13_40874_6	During the final training phase, we only train a lightweight linear classifier layer on top of the logits that the model determines for the latent semantic properties.
8_40931_4	Our results demonstrate that LLMs exhibit comparable performance in the in-context learning setting, regardless of their parameter scale.
22_40945_5	Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.
8_41062_7	Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.
109_41163_6	Results obtained on question-answering and mathematical reasoning benchmarks show that LMs instructed via the Instruction-tuning CoT method produced by LLMs outperform baselines within both in-domain and out-domain scenarios.
111_41165_10	Our findings highlight the harms and divide that these LLMs can bring to society if we do not take very diligent care in their use.
127_41181_3	We then traced the roots of facts back to the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire specific facts.
138_41192_2	We investigate the extent of LLMs‚Äô N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust.
138_41192_3	We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities.
162_41216_3	Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times.
163_41217_5	Our evaluation on both automated metrics and qualitative human evaluation suggests that by incorporating end-user specifications into the conversion process, our model can create presentations that are not only informative but also tailored to expectations and cognitive abilities of target audience.
171_41225_1	While encoder-only or encoder-decoder pre-trained language models have already proved to be effective in discourse parsing, the extent to which LLMs can perform this task remains an open research question.
10_41246_4	Our tool-equipped SFT model, RAVEN, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with strong GPT-3.5 results.
15_41251_1	While this chain-of-thought (CoT) reasoning boosts LLMs‚Äô performance, it is unclear if LLMs know when to use CoT and whether those CoT are always necessary to answer the question.
19_41323_2	Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks.
26_41330_4	Compared to other transformer-based models, the proposed Forged-GAN-BERT model demonstrates an improved performance with F1 scores of 0.97 and 0.71 for identifying forged novels in single-author and multi-author classification settings.
51_41391_4	We observe that while LLMs can translate on-par with SAP‚Äôs MT models on general domain data, it is difficult to close the gap on SAP‚Äôs domain-specific data, even with extensive training and carefully curated data.
13_41409_3	This is why STAR is making AI-powered terminology control a priority for its translation products because of the significant gains to be made - greatly improving the quality of MT output, reducing post editing (PE) costs and efforts, and thereby boosting overall translation productivity.
15_41459_6	Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.
53_41497_6	Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena.
69_41513_2	Here we show that open-source LLMs perform on par with or better than some state-of-the-art baselines in simultaneous machine translation (SiMT) tasks, zero-shot.
72_41516_2	However, LLMs exhibit preference biases and worrying sensitivity to prompt designs.
74_41518_6	Experimental results on 9 NLU and 2 Generation datasets demonstrate that LM-DPP can effectively select canonical examples.
82_41526_4	At the same time, LLMs should demonstrate autonomy in tool use to reduce the costs associated with tool calling.
82_41526_7	Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization.
84_41528_2	We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations.
93_41537_0	Large language model agents have exhibited exceptional performance across a range of complex interactive tasks.
102_41546_0	The advent of large language models (LLMs) like GPT-4 has catalyzed the exploration of multi-task learning (MTL), in which a single model demonstrates proficiency across diverse tasks.
114_41558_7	Our CVR-LLM presents the first comprehensive study across a wide array of complex visual reasoning tasks and achieves SOTA performance among all.
131_41575_1	We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.
131_41575_2	Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support.
177_41621_2	This raises the question: To what extent can LLMs learn orthographic information?
212_41656_9	When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.
222_41666_5	All the LLMs we tested make some basic mistakes with conditionals or modals, though zero-shot chain-of-thought prompting helps them make fewer mistakes.
222_41666_6	Even the best performing LLMs make basic errors in modal reasoning, display logically inconsistent judgments across inference patterns involving epistemic modals and conditionals, and give answers about complex conditional inferences that do not match reported human judgments.
240_41684_0	While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities.
244_41688_9	Further research on how LLMs affect users‚Äô political views is required, as their use becomes more widespread.
248_41692_2	On the other hand, existing open evaluator LMs exhibit critical shortcomings: 1) they issue scores that significantly diverge from those assigned by humans, and 2) they lack the flexibility to perform both direct assessment and pairwise ranking, the two most prevalent forms of assessment.
250_41694_4	The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM.Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.
270_41714_6	In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation.
272_41716_4	The findings in this study suggest, with statistical guarantee, that most LLMs still struggle with logical reasoning.
292_41736_9	(ii) ‚ÄúLLMs as Metareviewers‚Äù, how effectively can LLMs identify potential issues, such as Deficient or unprofessional review segments, within individual paper reviews?
306_41750_7	Our methods, applied to various tasks, show that LLMs can indeed produce diverse opinions according to the degree of task subjectivity.
308_41752_6	We also find that the LLM feature-based model learns more robust prediction rules instead of relying on superficial shortcuts, which often trouble neural models.
315_41759_3	Our experiments show that multilingual LLMs suffer from two familiar weaknesses: generalization works best for languages with similar typology, and for abstract concepts.
354_41798_3	Our study aims to determine the reliability of applying personality assessments to LLMs, explicitly investigating whether LLMs demonstrate consistent personality traits.
364_41808_4	By training on this data, LMs can infer and represent the persona in its activation space.
364_41808_5	This allows the model to separate truth from falsehoods and controls the truthfulness of its generation.
364_41808_7	Next, using arithmetics as a synthetic environment, we show that structures of the pretraining data are crucial for the model to infer the truthful persona.
376_41820_2	In this work, we investigate whether pretrained language models contain various *knowledge-critical* subnetworks: particular sparse computational subgraphs that can, if removed, precisely suppress specific knowledge the model has memorized.
383_41827_5	Our findings indicate that while LLMs exhibit high proficiency in selecting intentions, achieving an accuracy of 88%, their ability to infer the intentions of others is significantly weaker, trailing human performance by 20%.
396_41840_1	Given that multiple languages are a compensation for the losses caused by a single language‚Äôs limitations, it‚Äôs a natural next step to enrich the model‚Äôs learning context through the integration of the original input with its multiple translations.
396_41840_2	In this paper, we start by revealing that LLMs learn from parallel multilingual input (PMI).
401_41845_5	We hope this study provides new insights into understanding how LLMs understand harmfulness information.
435_41879_1	With large language models (LLMs) demonstrating powerful capabilities between tasks, we can‚Äôt help but ask: Can Current LLMs Effectively Make Sequential Decisions?
453_41897_10	Our experiments and analysis reveal that various LLMs often correctly identify TDM triples while struggling to extract result values from publications.
461_41905_2	However, LLM-generated data often suffers from quality issues, with underrepresented or absent aspects and low-quality datapoints.
480_41924_5	(ii) Several general-domain LLMs demonstrate substantial potential in medical clinics, while the limited input capacity of many medical LLMs hinders their practical use.
515_41959_6	We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful critic model, and another where a single language model fulfills both roles.
525_41969_3	Our strategies include (i) program- and data-aware techniques for proposing effective instructions, (ii) a stochastic mini-batch evaluation function for learning a surrogate model of our objective, and (iii) a meta-optimization procedure in which we refine how LMs construct proposals over time.
574_42018_1	We observe curious components: good-performing ones that individually do well on a classification task, even when the model performs poorly; bad-performing ones that do much worse than chance; and label-biased components that always predict the same label.
586_42030_7	We evaluate ~30 LMs across diverse prompting strategies and found that LMs generally struggle to appropriately revise their beliefs in response to new information.
590_42034_1	However, it is unclear if this success is limited to explicitly-mentioned causal facts in the pretraining data which the model can memorize.
590_42034_2	Thus, this work investigates: Can LLMs infer causal relations from other relational data in text?
590_42034_3	To disentangle the role of memorized causal facts vs inferred causal relations, we finetune LLMs on synthetic data containing temporal, spatial and counterfactual relations, and measure whether the LLM can then infer causal relations.
590_42034_4	We find that: (a) LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y); (b) if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y. We also find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals, questioning their understanding of causality.
609_42053_3	Interestingly, the LLM exhibited more aggressive negotiation behaviors when the opponent‚Äôs image appeared less aggressive than their own, and less aggressive behaviors when the opponent‚Äôs image appeared more aggressive.
621_42065_1	However, LLM-based evaluators exhibit position bias, or inconsistency, when used to evaluate candidate answers in pairwise comparisons, favoring either the first or second answer regardless of content.
657_42101_5	Our findings reveal the ‚Äúlazy pun generation‚Äù pattern and identify the primary challenges LLMs encounter in understanding puns.
667_42111_2	However, due to the scarcity of aligned NL and Formal Language (FL) theorem-proving data most modern LLMs exhibit suboptimal performance.
679_42123_3	Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, which significantly impairs their performance.
679_42123_6	4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts.
687_42131_1	However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions.
714_42158_1	Unfortunately, the study concluded that the LLMs could not self-correct reasoning yet.
723_42167_4	The findings suggest that LLMs can effectively approximate human story moral interpretations and offer a new avenue for computational narrative understanding.
738_42182_5	Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones.
750_42194_2	We find that the models default to ‚Äústandard‚Äù varieties of English; based on evaluation by native speakers, we also find that model responses to non-‚Äùstandard‚Äù varieties consistently exhibit a range of issues: stereotyping (19% worse than for ‚Äústandard‚Äù varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse).
769_42213_2	Previous research has shown that pre-trained LLMs exhibit reasoning biases, such as content effects, avoid answering that no conclusion follows, align with human difficulties, and struggle with multi-step reasoning.
811_42255_1	When prompted by users, these AI systems successfully perform a wide range of tasks as part of a conversation.
813_42257_0	Retrieval-augmented language model (RALM) represents a significant advancement in mitigating factual hallucination by leveraging external knowledge sources.
814_42258_3	We introduce a dynamic decision-making framework that categorizes tasks into two distinct pathways: ‚ÄòFast,‚Äô designated for tasks where the LLM quickly identifies a high-confidence solution, and ‚ÄòSlow,‚Äô allocated for tasks that the LLM perceives as complex and for which it has low confidence in immediate solutions as well as requiring more reasoning paths to verify.
869_42313_4	While ChatGPT 4 and Gemini demonstrate comparable effectiveness in slang paraphrasing, LLaMA3 shows less coverage, with all LLMs exhibiting limitations in coverage, especially of Nigerian slang.
883_42327_7	The results show that ESC-oriented LLMs exhibit superior ESC abilities compared to general AI-assistant LLMs, but there is still a gap behind human performance.
929_42373_1	The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization.
933_42377_4	We evaluate zero-shot, few-shot, and fine-tuning settings and show that LLMs struggle in certain conditions or achieve comparable results against existing methods.
938_42382_1	We integrate insights from cognitive science to quantitatively examine how LLMs perform on n-back tasks‚Äîa benchmark used to assess working memory, which involves temporarily holding and manipulating information.
943_42387_5	They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge.
948_42392_0	Synthetic long-context LLM benchmarks (e.g., ‚Äúneedle-in-the-haystack‚Äù) test only surface-level retrieval capabilities; but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs?
948_42392_3	Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest pair accuracy at 55.8%.
952_42396_2	In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data.
952_42396_4	Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones.
967_42411_9	For example, by patching activations along the BI encoding direction we can make the LM to infer ‚ÄúBox Z contains the stone‚Äù and ‚ÄúBox Z contains the map‚Äù.
982_42426_3	This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values.
992_42436_2	However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect.
1019_42463_4	Despite the potential harms from LLMs in such applications, whether LLMs can reliably identify offensive speech and how they behave when they fail are open questions.
1023_42467_2	In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs.
1039_42483_5	In a minimal planning task (i.e., graph path-finding), our model exhibits near-perfect performance and effectively mitigates shortcut learning, a feat that standard training methods and baseline models have been unable to accomplish.
1077_42521_3	We use this to evaluate how well frontier LLMs understand causal and temporal dependencies.
1107_42551_2	We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret.
1109_42553_6	Experimental results demonstrate that our model can effectively understand and employ credibility for generation, significantly outperform other models with retrieval augmentation, and exhibit robustness despite the increasing noise in the context.
1134_42578_7	Notably, these LLMs manifested more extreme views and opinions when dealing with non-Western concepts like caste, compared to Western ones such as race.
1191_42635_3	Specifically, we bring cross-modality through two key tasks: Conversational Video Moment Retrieval, where the model retrieves relevant step-video segments based on user queries, and Visually-Informed Step Generation, where the model generates the next step in a plan, conditioned on an image of the user‚Äôs current progress.
1193_42637_1	However, agentic LLMs suffer from a phenomenon known as reasoning derailment, due to the indiscriminate incorporation of observations from partially observable environments.
1208_42652_3	Our experiments show that LLMs perform better in unrealistic, omniscient simulation settings but struggle in ones that more accurately reflect real-world conditions with information asymmetry.
1222_42666_1	The challenge lies in the language marks for dementia are unclear, and LLM may struggle with relating its internal knowledge to dementia detection.
1262_42706_8	Our RR model is more robust, although our TF model performs better than the RR model without any attacks.
35_42748_4	Unlike most previous work on machine-generated text detection, which focused on binary classification, LLM-DetectAIve supports four categories: (i) human-written, (ii) machine-generated, (iii) machine-written, then machine-humanized, and (iv) human-written, then machine-polished.
35_42748_6	Our experiments show that LLM-DetectAIve can effectively identify the above four categories, which makes it a potentially useful tool in education, academia, and other domains.
32_42798_3	The News Risk Alerting System (NRAS) proactively identifies credit-relevant news related to clients and alerts the relevant Credit Officer (CO).
62_42828_5	The input query can be the existing object itself, in which case the system acts as a regenerator, completing, correcting, normalizing the input, or any unstructured blurb to be structured.
83_42849_5	Our evaluation shows that two of these models (GPT-4o and Claude 3.5) performed well on this task, while the third model (LLaVA-NEXT) performs poorly.
88_42854_3	Our findings indicate that LC LLMs exhibit brittleness at longer context lengths even for simple tasks, with performance deteriorating sharply as task complexity increases.
96_42862_2	We found that proprietary LLMs such as GPT-3.5-Turbo show potential in this task, but their high cost and latency make them unsuitable for real-time applications.
110_42876_1	However, many LLMs exhibit significant performance discrepancies between high- and low-resource languages.
113_42879_2	However, even with RAG, LLMs can still produce inaccurate outputs, such as distorting or misinterpreting source content, posing risks in high-trust scenarios.
22_42982_5	We also argue that Chain-of-Thought prompts help the LLMs by decomposing the problem-solving process, but the LLMs still learn limitedly.
54_43014_5	Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts.
62_43022_6	Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references.
91_43051_4	First, we show that fine-tuning sufficiently large LLMs can produce IC-SF performance comparable to discriminative models.
1_43120_7	The pruned model requires only few epochs of fine-tuning to restore its performance, ensuring the model‚Äôs ability to generalize.
37_43156_6	REQUAL-LM does not require specialized hardware, does not impose a significant computing load, and uses LLMs as a blackbox.
37_43156_8	Our system does not require retraining the LLMs, which makes it deployment ready and easy to adapt.
49_43168_1	However, such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006).To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences.
49_43168_3	Since LLMs struggle to identify grammar errors, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to explain each edit.
54_43173_6	Finally, we incorporate relevance scores into training using a novel relevance loss and relevance predictor, and the proposed R-BASS model makes it possible to drop 86.3 % of the blocks while retaining comparable performance, resulting in a 2.2x speedup over BASS.
101_43220_4	Experiments show that our proposed TagDebias model, when applied to a ranking task, exhibits significant improvements in bias scores.
124_43243_5	Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.
129_43248_1	Despite such efforts in human‚ÄìLLM alignment, we find that instruction tuning does not always make LLMs human-like from a cognitive modeling perspective.
132_43251_1	However, LMs also raise concerns regarding the generation of biased or toxic content and the potential disclosure of private information from the training dataset.
157_43276_5	Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions.
176_43295_1	In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages?
176_43295_6	Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT.
176_43295_7	First, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages.
182_43301_1	However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese.
184_43303_2	As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance.
246_43365_5	Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring a deeper understanding of specific sentiment phenomena or structured sentiment information.
252_43371_4	By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events.
268_43387_1	The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting.
277_43396_7	Notably, the FID-style FLAN-T5 model with only 3B parameters performs impressively compared to the 13B model.
279_43398_2	Without the new theorems, current LMs struggle to prove harder theorems that are distant from the given hypotheses with the exponentially growing search space.
279_43398_8	However, there is still room for current LMs to develop better ATG and generate more advanced and human-like theorems.
280_43399_1	Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for desired summary characteristics.
292_43411_5	Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences.
8_43424_3	Our naive LLM-based pipeline for hierarchy generation from a set of studies produces promising yet imperfect hierarchies, motivating us to collect CHIME, an expert-curated dataset for this task focused on biomedicine.
10_43426_2	Fundamentally, LLMs need to be aligned to be more faithful to grounding, which will require high-quality preference annotations.
16_43432_3	On the other hand, LLM-based methods exhibit a limited capacity to capture the disparities between synthesized and actual data distribution due to the absence of feedback from a discriminator during training.
19_43435_5	For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words.
28_43444_4	Second, we propose a new ‚ÄúConscious Incompetence‚Äù setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG.
29_43445_2	We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLer), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation.
30_43446_4	Specifically, the language model first learns to generate appropriate English instructions according to the natural web texts in other languages as responses.
45_43461_6	Most importantly, our results also indicate that LLMs exhibit considerable sensitivity.
48_43464_4	CoRnNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model‚Äôs output via reformulations generated by LLMs.
55_43471_1	In this paper, through a newly proposed arithmetical puzzle problem, we show that the model can perform well on multi-step reasoning tasks via fine tuning on high-quality synthetic data.
82_43498_7	Further analysis shows that LLMs exhibit a poor understanding of table structures, and struggle to balance between TIS performance and robustness against pseudo-relevant tables (common in retrieval-augmented systems).
107_43523_1	However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts.
141_43557_1	However, this advancement has revealed a critical challenge: LLMs frequently produce outputs against socially acceptable commonsense standards in various scenarios.
141_43557_5	The empirical results present that LLMs struggle with Korean commonsense reasoning.
141_43557_6	With human accuracy benchmarked at approximately 85%, GPT-4‚Äôs performance lags at about 74%, and other LLMs demonstrate an average accuracy of around 42%.
146_43562_4	In this way, LLMs can derive experience from their extensive memory, which keeps in line with the way humans gain commonsense.
194_43610_5	Results indicate that all LLMs perform poorly on the ZIQI-Eval benchmark, suggesting significant room for improvement in their musical capabilities.
213_43629_1	As LLM-driven agents learn to negotiate and act like real humans, how to evaluate agents‚Äô bargaining abilities remains an open problem.
238_43654_1	Despite their popularity, there is a lack of understanding of the extent and contributing factors that allow LLMs to recall relevant knowledge and combine it with presented information in the clinical and biomedical domain: a fundamental pre-requisite for success on down-stream tasks.
252_43668_3	E2-LLM entails a singular training process over considerably short sequences (e.g., 4K tokens), which greatly mitigates the cost of continual-pretraining or fine-tuning.
254_43670_3	We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments.
290_43706_4	The high probing accuracy suggests that LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension.
294_43710_1	While large language model (LLM) agents have made progress in understanding and executing tasks such as textual games and webpage control, many real-world tasks also require collaboration with humans or other LLMs in equal roles, which involves intent understanding, task coordination, and communication.
331_43747_4	Model‚Äôs gradients are still struggling to identify the appropriate direction when updating the parameters.
349_43765_5	Experimental results show that all LLMs produce significantly more unsafe responses for non-English queries than English ones, indicating the necessity of developing safety alignment for non-English languages.
357_43773_0	Despite the success of large language models (LLMs) in natural language generation, much evidence shows that LLMs may produce incorrect or nonsensical text.
373_43789_0	While LLMs demonstrate impressive capabilities in musical knowledge, we find that music reasoning is still an unsolved task.
374_43790_2	Therefore, it is crucial for LLMs to understand the concept of temporal knowledge.
388_43804_3	LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data.
395_43811_1	To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives.
395_43811_3	We define the problem of affective alignment, which measures how LMs‚Äô emotional and moral tone represents those of different groups.
396_43812_5	By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation.
401_43817_4	Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and the text representations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning.
423_43839_7	Statistical and model analysis explains the crucial role of the surrounding context of the paraphrased text spans.
428_43844_5	Our analysis uncovers that LLMs exhibit shortcomings in long-text domains, and their performance diminishes as document size escalates.
469_43885_3	Specifically, we investigate if modulating surprisal and entropy relative to cognitive scores increases prediction accuracy of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, revealing what type of psycholinguistic subjects a given LM emulates.
488_43904_1	While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low resource languages.
508_43924_6	Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fine-tuning remains an open problem for domain-specific applications.
512_43928_4	Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples.
519_43935_1	We thus investigate whether LLMs can learn a new language on the fly solely through prompting.
530_43946_3	In this paper, we investigate bias along less-studied but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs make between social groups and unrelated positive and negative attributes.
541_43957_1	Although LLMs demonstrate impressive capabilities in following instructions, directly prompting them to exhibit certain personalities through manually crafted instructions may result in sub-optimal performance.
545_43961_2	In tasks such as relation extraction, LLMs can accurately identify entity pairs, even if the given relation (label) is semantically unrelated to the pre-defined original one.
557_43973_2	This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations.
571_43987_7	Experimentally, we find that LMs need roughly 17% more data when trained in a fully duplicated setting.
572_43988_5	Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecification would predict.
573_43989_0	Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering.
591_44007_6	Specifically, the first-order allows an LLM agent to infer others‚Äô mental states, and the second-order involves understanding how others perceive the agent‚Äôs mental state.
604_44020_2	Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions.
611_44027_5	Security vectors are activated during fine-tuning, the consistent behavior makes the model believe that such behavior has already been learned and there is no need for further optimization, while inconsistent data can still be learned.
664_44080_5	Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation.
673_44089_4	Our principal findings indicate that GPT-4 outperforms all models, while open-source model LLaMA-2-7B demonstrates comparable abilities to closed-source models GPT-3.5 and Gemini Pro.
680_44096_0	While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the ‚Äúreversal curse‚Äù.
680_44096_1	It is a typical example that the model knows ‚ÄúA‚Äôs father is B‚Äù, but is unable to reason ‚ÄúB‚Äôs child is A‚Äù.
711_44127_2	We evaluate how well LLMs can perform at GEC by measuring their performance on established benchmark datasets.
719_44135_0	LLMs have demonstrated remarkable capability for understanding semantics, but their understanding of pragmatics is not well studied.
733_44149_4	We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations.
751_44167_1	The purpose of this paper is to explore whether LLMs exhibit characteristics consistent with epistemological holism.
753_44169_2	This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model?
753_44169_3	Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks.
774_44190_4	Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R, demonstrates an exceptional zero-shot performance.
785_44201_1	However, current evaluations mainly focus on the end-to-end final answer correctness, and it is unclear whether LLMs can make use of helpful side information such as problem-specific hints.
811_44227_1	Despite their success, a few works indicate that LLMs suffer from the ‚Äúreversal curse‚Äù, in which LLMs can‚Äôt employ the inverted structure ‚ÄúB is A‚Äù when they are trained based on ‚ÄúA is B‚Äù.
811_44227_2	To explore the effect of the ‚Äúreversal curse‚Äù for LLMs on complex mathematical reasoning tasks, we present two reversal datasets upon GSM8K and MathQA and verify that LLMs also struggle to solve reversal mathematical problems.
813_44229_3	We observe that all LLMs struggle to answer questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked.
844_44260_5	Finally, our method demonstrated that multilingual LMs could create QA including language-specific knowledge, significantly reducing the dataset creation cost compared to manual creation.
848_44264_5	Experiments on perturbed datasets demonstrate that LLMs still encounter difficulties in handling numeral and measurement conversions.
867_44283_4	Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels.
893_44309_6	We also explore how model size and data quantity affect the success of these mitigation efforts.
907_44323_1	However, LLMs may exhibit flaws such as hallucinations, forgetfulness, or misinterpretations of prompts, causing logical inconsistencies and unexpected deviations from intended designs.
913_44329_5	Our model evaluation against the annotated GELP then reveals that the full model as well as models with fewer layers and/or self-attention heads exhibit a good-enough performance.
916_44332_5	Our model involves clinical knowledge extraction and context-informed LLM prompting.
925_44341_1	Many languages lack a large corpus to train a decent LLM; therefore existing LLMs rarely perform well in unseen, endangered languages.
928_44344_3	Our extensive experiments with CToolEval evaluate 11 LLMs, revealing that while GPT-3.5-turbo excels in tool invocation, Chinese LLMs usually struggle with issues like hallucination and a lack of comprehensive tool understanding.
928_44344_5	To promote further research for LLMs to fully act as reliable agents in complex, real-world situations, we release our data and codes at https://github.com/tjunlp-lab/CToolEval.
939_44355_2	Some studies argue that code-LLMs perform well on coding tasks because they use self-attention and hidden representations to encode relations among input tokens.
967_44383_3	As such, people may quickly assume that decoder-only LLMs always perform better than the encoder-only ones, especially for understanding word meaning.
967_44383_4	In this paper, we demonstrate that decoder-only LLMs perform worse on word meaning comprehension than an encoder-only language model that has vastly fewer parameters.
974_44390_1	The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying the input window as well as slowing down the decoding process.
6_44398_5	Furthermore, we conduct validations for fine-grained error types to identify those requiring a retrieval-augmented manner when LLMs perform Korean GEC.
6_44398_6	According to experimental results, most LLMs, including ChatGPT, demonstrate significant performance improvements when applying KAGEC.
14_44406_0	Vision-extended LLMs have made significant strides in Visual Question Answering (VQA).
26_44418_5	This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively?
27_44419_2	This study demonstrates that, after fine-tuning on a small dataset comprising causally aligned source and target sentence pairs, a pre-trained open-source LLM can control input segmentation directly by generating a special ‚Äúwait‚Äù token.
46_44438_2	To investigate whether LLMs can learn from mistakes, we construct mistake-correction datasets, using GPT-4 to identify and correct the mistakes in inaccurate CoTs.
46_44438_4	(1) LLMs can indeed learn from mistakes to enhance their CoT reasoning performances.
51_44443_3	Moreover, these LLM-based methods struggle to effectively address the order relation among candidates, particularly given the scale of ratings.
69_44461_6	Our LLM-based approach, combined with guidelines and simultaneous extraction of supportive evidence underlying its predictions, demonstrates effectiveness in identifying the therapeutic alliance.
72_44464_1	Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs.
86_44478_3	Our findings reveal that scaling model parameters can mitigate the issue of tokenization; however, LLMs still suffer from biases induced by typos and other text format variations.
116_44508_3	Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed.
127_44519_1	The resulting ‚Äúgraph LLMs‚Äù are evaluated with in-distribution settings only, thus it remains underexplored whether LLMs are learning generalizable graph reasoning skills or merely memorizing patterns in the synthetic training data.
127_44519_3	Extensive experiments with two LLMs across four graph reasoning tasks demonstrate that while generalization on simple patterns (semantic, numeric) is somewhat satisfactory, LLMs struggle to generalize across reasoning and real-world patterns, casting doubt on the benefit of synthetic graph tuning for real-world tasks with underlying network structures.
139_44531_4	We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers.
141_44533_1	Recently, LLMs‚Äô capability to store, retrieve and infer with symbolic knowledge has drawn a great deal of attention, showing their potential to understand structured information.
141_44533_2	However, it is not yet known whether LLMs can understand Description Logic (DL) ontologies.
141_44533_5	We find that LLMs can understand formal syntax and model-theoretic semantics of concepts and roles.
141_44533_6	However, LLMs struggle with understanding TBox NI transitivity and handling ontologies with large ABoxes.
153_44545_4	Based on the notion of mutability that classifies social features, we design three realistic settings and a novel social prediction task, where the LLMs make predictions with input features of the same mutability and accessibility with the response feature.
153_44545_6	With the comprehensive investigations on various LLMs, we reveal that LLMs struggle to work as expected on social prediction when given ordinarily available input features without shortcuts.
178_44570_7	Our results show that the evaluated model also exhibits limited ability in transferring knowledge across languages.
192_44584_1	Many LLMs, however, often struggle with the distinct complexities of materials science tasks, such as computational challenges, and rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations.
201_44593_4	Using these data, we probe whether language models achieve three forms of reasoning: (1) Etiological Reasoning‚Äîgiven an input time series, can the language model identify the scenario that most likely created it?
231_44623_1	However, LLMs often struggle to effectively parse and utilize sparse and complex personal context without additional processing or contextual enrichment, underscoring the need for more sophisticated context understanding mechanisms.
233_44625_1	However, Transformer-based LLMs suffer a performance degradation when modeling long-term contexts due to they discard some information to reduce computational overhead.
299_44691_0	LLMs can now perform a variety of complex writing tasks.
307_44699_1	However, several recent studies indicate that LLMs struggle with challenging instructions.
311_44703_6	Although findings suggest the raw data features still prevail in medical ML tasks, zero-shot LLM embeddings demonstrate competitive results, suggesting a promising avenue for future research in medical applications.
322_44714_1	However, LLMs exhibit only infant-level cognitive abilities in certain areas.
323_44715_4	Working with experienced teachers, we find that LMs struggle to tag and verify standards linked to problems, and instead predict labels that are close to ground truth, but differ in subtle ways.
331_44723_1	Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about gender, but have not investigated the complex dynamics that can influence model reasoning and decision-making involving gender.
334_44726_5	This paper introduces FASTTRACK, a novel approach that harnesses the capabilities of Large Language Models (LLMs) to validate supportive evidence for queries and at the same time clusters the training database towards a reduced extent for LLMs to trace facts.
360_44752_2	LLMs exhibit biases in variable names, struggle with 2D spatial relationships and planning, and hallucinate object placements.
391_44783_4	Surprisingly, we find an overlap between their requirements; A* requires more accurate predictions on search nodes near the goal, and LLMs need the same set of nodes for effective generalisation.
416_44808_4	Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address.
420_44812_6	We examine the impact of explicit vs. implicit personas and investigate which human factors LLMs recognize and respond to.
445_44837_3	MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities.
446_44838_2	However, structured KGs are difficult to utilize, and how to make LLMs understand and incorporate them is a challenging topic.
458_44850_1	However, recent studies have shown that LLMs often struggle with instructions containing multiple constraints (e.g. a request to create a social media post ‚Äúin a funny tone‚Äù with ‚Äúno hashtag‚Äù).
458_44850_7	DeCRIM works by decomposing the original instruction into a list of constraints and using a Critic model to decide when and where the LLM‚Äôs response needs refinement.
464_44856_5	Our experimental results indicate that even SOTA LLMs still exhibit substantial capability gaps compared to humans.
467_44859_2	Worse still, in this paper, we identify a subtler form of discrimination in LLMs, termed implicit ranking unfairness, where LLMs exhibit discriminatory ranking patterns based solely on non-sensitive user profiles, such as user names.
471_44863_0	LLMs acquire knowledge from massive data snapshots collected at different timestamps.
473_44865_2	A third LLM acts as a remediator agent to rewrite utterances violating norms for improving negotiation outcomes.
474_44866_0	Generative AI has demonstrated unprecedented creativity in the field of computer vision, yet such phenomena have not been observed in natural language processing.
511_44903_6	Moreover, we propose a densification strategy based on large language models, through a carefully crafted Chain of Thought prompt, to dig out some knowledge necessary for reasoning about fact associations, thereby making the model perform better.
515_44907_3	We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, surpassing human performance.
619_45011_7	Various model sizes of (1.8B, 7B, 14B) all demonstrate significant improvements over existing models with similar model sizes.
623_45015_1	Yet, besides NL, LLMs have seen various non-NL formats during pre-training, such as code and logical expression.
623_45015_4	We show that allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7% improvement in reasoning efficiency for different LLMs, and up to a 72.7% reduction in token usage in multi-agent communication, all while maintaining communicative effectiveness.
623_45015_5	Our comprehensive analysis further reveals that LLMs can devise a format from limited task instructions and that the devised format is effectively transferable across different LLMs.
638_45030_7	We evaluate the efficacy of workflow knowledge across multiple formats, and the results indicate that current LLM agents need considerable improvements for satisfactory planning.
658_45050_7	For compatibility, we find LLMs struggle with memorizing redundant facts in a unified way.
658_45050_8	Only when correlated facts have the same direction and structure, the LLM can compatibly memorize them.
658_45050_10	For preference, the LLM pays more attention to memorizing more frequent and difficult facts, and the subsequent facts can overwrite prior facts‚Äô memorization, which significantly hinders low-frequency facts memorization.
666_45058_7	In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM trained on just 30 hours of labeled data can more effectively translate compared to the recent model trained with 433 hours of data.
674_45066_1	It is thus necessary to understand how adequately LLMs perform in such domains.
674_45066_2	We conduct a case study on ChatGPT in nutrition counseling, a popular use-case where the model supports a user with their dietary struggles.
703_45095_1	This dependency might introduce novel security risks if LMs develop self-recognition capabilities.
722_45114_6	However, we find that all LLMs lose their robustness when confronted with multiple challenges (e.g., multi-turn) simultaneously, leading to significant performance drops.
763_45155_3	Our findings indicate that while certain LLMs can perform reasonably well on reasoning about affordances, there appears to be a consistent low upper bound on habitat-centered reasoning performance.
782_45174_5	Our findings reveal that although commercial LLMs generally perform better, there is still significant room for improvement in comprehending emphasized sentences.
801_45193_1	However, in practical applications, LLMs may encounter outdated information, necessitating the filtering of such data and updating of knowledge beyond internal memory.
801_45193_5	Additionally, to address the difficulties LLMs encounter in understanding time constraints, we propose a two-stage decoupling framework that separates the identification and computation of time constraint into a symbolic system.
812_45204_2	However, even the most advanced LLMs currently struggle with this form of reasoning.
814_45206_1	As LLMs increasingly find applications in healthcare, understanding and addressing their biases becomes paramount.
839_45231_2	From the other hand, LLMs have demonstrated proficiency in many NLP tasks, including zero-shot and few-shot data annotation.
847_45239_5	Additionally, we investigate whether LLMs can learn a form of irony tied to a generational perspective, with mixed results.
905_45297_3	We evaluate three state-of-the-art LLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal chain-of-thought prompting strategy and observe that when (1) the input context contains over 1,615 tokens of historical interactions, (2) a crucially decision-informing premise is the rightful conclusion over two disparate environment premises, and (3) a trivial, but distracting red herring fact follows, all LLMs perform worse than random choice at selecting the better of two actions.
908_45300_1	However, LLMs often struggle with complex programming problems without human guidance.
912_45304_1	To better understand and adapt to individual student needs, including their misconceptions, LLMs need to be trained on extensive datasets of student-tutor dialogues.
916_45308_2	We investigate how multilingual LLMs perform at crosslingual metalinguistic question answering.
920_45312_4	We show that LRLs are at a pricing disadvantage, because the well-known LLMs produce more tokens for LRLs than HRLs.
969_45361_3	We identify two lines of research, namely (1) *LLM Role-Playing*, where personas are assigned to LLMs, and (2) *LLM Personalization*, where LLMs take care of user personas.
983_45375_1	Despite these advancements, LLMs still encounter challenges with length control, frequently failing to adhere to specific length constraints due to their token-level operations and insufficient training on data with strict length limitations.
1003_45395_1	Yet, many LLMs suffer from self-inconsistency, which raises doubts about their trustworthiness and reliability.
1003_45395_4	Our experiments reveal that LLMs struggle with choosing the correct entity reading, achieving an average accuracy of only 85%, and as low as 75% with underspecified prompts.
12_45408_5	Our findings show that a smaller open-source model, fine-tuned on relevant data, can perform as a huger general-purpose one, showing the value of enriching the local embeddings with the semantic context of data.
11_45470_4	The paper describes the experimental setting and the results which show that LLMs perform poorly.
5_45477_3	Our analysis reveals that though human-generated and extractive AI summarization techniques do not show a clear bias, abstractive AI-generated summaries exhibit a bias towards male students.
7_45479_2	Our findings reveal pervasive gender bias across all models, with base LLMs exhibiting a higher degree of bias compared to NMT models.
16_45488_6	Our findings indicate that most LLMs identify male and female names with high accuracy (over 80%) but struggle with gender-neutral names (under 40%), and the accuracy of gender prediction is higher for English-based first names than non-English names.
2_45501_0	LLMs can perform unseen tasks by learning from a few in-context examples.
4_45503_6	Our results show that while tool-augmented LLMs perform well in single interactions, they often struggle to handle complete conversations.
11_45510_3	Our results indicate that model instances typically exhibit consistent generalization trends, i.e., they generalize equally well (or poorly) across most scenarios, and this ability is correlated with model architecture, base dataset performance, size, and training mechanism.
2_45533_3	As the pre-training corpus includes a large amount of human-written explanations ‚Äúin the wild‚Äù, we hypothesise that LLMs adopt common properties of human explanations.
1_45552_3	Our experiments in English demonstrated that with standard prompting, LLMs struggle to control the non-textual attribute, i.e., acoustic intelligibility, while efficiently capturing the desired textual attributes like semantic equivalence.
21_45655_5	Among these, the RoBERTa model demonstrated superior performance, achieving an impressive accuracy of 99.73.
33_45756_5	Our findings indicate that LLMs can produce FAQ reformulations beneficial to the editorial process.
48_45771_1	Experimental evaluation on the WebNLG dataset showed that such a constructed system produces text of better quality (according to the BLEU and BLEURT metrics) than the same LLM prompted to directly produce outputs, and produces fewer hallucinations than a BART language model fine-tuned on the same data.
10_45796_0	Data-to-text (D2T) generation is a natural language generation (NLG) task in which a system describes structured data in natural language.
10_45861_4	We assume that the good result is connected to the large presence of materials in these languages online, which the LLM has learned to represent.
4_45874_3	Unlike traditional methods, our system doesn‚Äôt require speech data during LLM pre-training and can exploit LLM‚Äôs multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training.
4_45874_7	Additionally, our model demonstrates cross-lingual speech and text matching, which is further enhanced by readily available machine translation data.
1_46078_2	Moreover, LLMs can potentially learn imperfect associations between orthographic and phonological forms from the training data.
4_46081_4	Through comprehensive experiments, our findings reveal that while LLMs demonstrate proficiency in API call decision-making, they face challenges in domain identification, function selection, and argument generation.
8_46085_2	(1) Pre-training data cannot be regularly updated once the models are deployed, and it is not very fruitful if the model cannot represent updated knowledge.
5_46137_0	Pre-trained LLMs have demonstrated substantial capabilities across a range of conventional natural language processing (NLP) tasks, such as summarization and entity recognition.
27_46175_3	New language-model-based seq2seq systems present the opportunity to solve both these problems by learning to directly generate a copy of an input sentence with markdown-like annotations.
1_46246_6	Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generative counterparts as cross-lingual instructions; (4) AlignInstruct improved performance in 30 zero-shot directions.
6_46251_3	However, the open-source model Mistral-7B-Instruct-v0.2 struggles to adapt effectively to the XLS task with limited examples.
82_46348_4	Even if a pre-trained foundation model exhibits consistent norms, we find that introducing downstream tasks may indeed lead to unexpected inconsistencies in norm representation.
90_46356_3	While the model demonstrated a good overall understanding of complex words and their word-internal structure, the results also suggest that there is no formal knowledge of derivational rules, but rather an interpretation of the observed word parts to derive the meaning of a word.
165_46431_2	In this work, we investigate the extent to which existing AI detection and authorship analysis models can perform classification on data generated in human-AI collaborative writing sessions.
196_46462_4	We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable.
210_46476_4	We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource.
244_46510_7	Finally, we evaluate GPT-4 on a dataset consisting of six other languages for span detection, and results suggest that the model struggles with the task across languages.
247_46513_8	Second, the two LLMs exhibited comparable average performance in AES, with a slight advantage for ChatGPT.
249_46515_3	This work proposes a research scheme for studying machine translation robustness on LLMs, investigating whether LLMs can learn translation robustness from noisy-source demonstration examples.
249_46515_4	Through experiments on different models, languages, and noise types, we empirically demonstrate that LLMs can learn how to handle noise and translation methods from noisy-source demonstration examples, thereby improving their translation performance on noisy sentences.
251_46517_3	We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position.
327_46593_3	The model performs equally to a competitor using the same data but with exhaustive re-training after each prediction, while also being more transparent, faster and less resource-intensive.
466_46732_7	Our findings suggest that, although they can identify mansplaining to some extent, LLMs still struggle to point out this attitude and will even reproduce some of the social patterns behind mansplaining situations, for instance by praising men for giving unsolicited advice to women.
519_46785_7	Therefore, a separate search system needs to be built to incorporate documents from private domains within the company.
519_46785_10	By leveraging SERP as a target instead of a source, the proposed model deeply understands queries without relying on external modules.
521_46787_1	This technique empowers the model to autonomously devise ‚Äúsolution plans‚Äù to tackle intricate programming challenges, thereby improving its performance in code generation.
560_46826_2	This paper explores how three prominent LLMs ‚Äì GPT-4, ChatGPT, and Llama2Chat-70B ‚Äì perform ethical reasoning in different languages and if their moral judgement depend on the language in which they are prompted.
596_46862_4	To systematically assess LLMs‚Äô code execution abilities, we selected 30 algorithms, generated 300 random-sampled instances in total, and evaluated whether popular LLMs can understand and execute these algorithms.
596_46862_5	Our findings reveal that LLMs, notably GPT-4, can effectively execute programs described in natural language, as long as no heavy numeric computation is involved.
602_46868_3	LLMs memorize more complex spurious correlations (i.e., task ‚Üî feature ‚Üî label) compared with that learned from previous pre-training and task-specific fine-tuning paradigm (i.e., feature ‚Üî label).
722_46988_5	Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens at lower layers while progressively expanding more knowledge within other tokens at upper layers; and (3) gradually forget the earlier context knowledge retained within the intermediate layers when provided with irrelevant evidence.
749_47015_0	Large Language Models (LLMs) exhibit remarkable In-Context Learning (ICL) ability, where the model learns tasks from prompts consisting of input-output examples.
774_47040_5	Based on the inherent rank from correctness across multiple rationales, the model learns to generate better rationales, leading to higher reasoning capability.
782_47048_6	In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses.
825_47091_7	These gaps are difficult to identify if the development process relies on training the system with an ongoing supply of natural user data, because this natural data can become distorted by a self-reinforcing feedback loop where the system ‚Äòtrains‚Äô the user to produce data that works.
887_47153_4	Our findings indicate that LLMs with appropriate prompts can stand out as a viable alternative, capable of generating relevant topic titles and adhering to human guidelines to refine and merge topics.
889_47155_6	We observe that it is hard for most LLMs to accomplish lateral thinking during interactions.
892_47158_5	By leveraging the strengths of existing research in document image understanding and LLMs‚Äô superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model.
1018_47284_1	However, there is limited understanding of how well LLMs perform in specific domains (e.g, the intellectual property (IP) domain).
1095_47361_8	Experimental results showed that our proposed Bllossom model exhibited superior performance in qualitative analyses compared to previously proposed Korean monolingual models.
1140_47406_6	When presented with a new question, the probing model first identifies the necessary knowledge to answer it, generating queries for retrieval.
1412_47678_6	Moreover, we add an auxiliary task for the model to discover the entity types which further fine-tunes the model to output more accurate results.
1427_47693_6	Through end-to-end training on the annotated dataset comprising 26 diverse APIs, the model demonstrates a level of self-awareness, automatically seeking tool assistance when necessary.
15_47850_4	We also show that LLM-based paraphrasing of Latin paragraphs from the historical letters produces English and German summaries that are close to human summaries published in the edition.
4_47914_3	The model demonstrated accurate identification of Direct and Indirect Explicit opinionated utterances, successfully classifying them according to language-specific properties, while less effective performance was observed for prompts requesting illustrations for Implicitly opinionated texts.
2_47956_5	We show that while LLMs may describe words differently in different languages, they are biased similarly.
45_48058_2	Existing LLMs mainly adopted autoregressive architecture without explicit backward dependency modeling.
52_48065_8	Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods.
65_48078_3	Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, establishing a comprehensive library of guidelines and a model for input-guidelines retrieval.
67_48080_2	First, we study disciplinary shifts: LLM research increasingly considers societal impacts, evidenced by 20√ó growth in LLM submissions to the Computers and Society sub-arXiv.
152_48165_8	Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.
198_48211_2	To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness?Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks.
205_48218_4	Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve.
212_48225_0	Despite their general capabilities, LLMs still struggle on biomedicalNER tasks, which are difficult due to the presence of specialized terminology and lack of training data.
213_48226_3	In this paper, we show that LLMs recall certain geographical knowledge inconsistently when queried in different languages‚Äîa phenomenon we term geopolitical bias.
222_48235_8	LM-Infinite brings substantial efficiency improvements: it achieves 2.7√ó decoding speed up and 7.5√ó memory saving over the original model.
256_48269_6	Our findings indicate that all the evaluated LLMs demonstrate relatively poor performance on Flames, particularly in the safety and fairness dimensions.
263_48276_3	That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.
283_48296_5	Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise.
302_48315_0	The dynamic nature of knowledge in an ever-changing world presents challenges for language models trained on static data; the model in the real world often requires not only acquiring new knowledge but also overwriting outdated information into updated ones.
327_48340_1	However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved.
380_48393_5	We take a fresh look at the represen- tational capacity of RNN LMs by connecting them to probabilistic FSAs and demonstrate that RNN LMs with linearly bounded precision can express arbitrary regular LMs.
381_48394_4	We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any n-gram LM, giving us a concrete lower bound on their probabilistic representational capacity.
390_48403_2	In this paper, we propose a novel self-detection method to detect which questions an LLM does not know.
391_48404_6	Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions.
394_48407_2	Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not.
463_48476_3	Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions.
482_48495_7	We find that certain commercial LLMs could surprisingly guess the missing option in various test sets.
2_48579_4	The proposed Low-code LLM framework consists of a Planning LLM that designs a structured planning workflow for complex tasks, which can be correspondingly edited and confirmed by users through low-code visual programming operations, and an Executing LLM that generates responses following the user-confirmed workflow.
20_48597_1	LLMs have acquired a substantial amount of knowledge, and evaluating the knowledge of these LLMs is crucial.
15_48652_4	Accordingly, we propose AnnoLLM, an annotation system powered by LLMs, which adopts a two-step approach, explain-then-annotate.
30_48667_5	In addition, we find that suitably tuned LLMs exhibit high accuracy in dialogue evaluation compared to human judgments.
36_48673_5	Our initial investigation reveals that LLMs require an additional grounding process to successfully perform argument filling, inspiring us to design training and prompting frameworks to ground their responses.
37_48674_2	Our model, MediClaimGPT, a 125M parameter Transformer demonstrates strong zero-shot predictive capabilities, accurately forecasting patient health events across four evaluation datasets, with its capabilities further demonstrated in various downstream tasks.
1_48689_3	Our findings reveal that LLMs exhibit a similar prediction pattern with humans but distinct from that of Shallow Language Models (SLMs).
2_48690_2	This paper systematically tests whether and how LLMs can act as compressors of semantic pairs.
5_48693_2	To avoid generation of irrelevant or false information, the system needs to ground its utterances into real-world events, and to avoid the statistical parrot effect, the system needs to construct shared understanding of the dialogue context and of the partner‚Äôs intents.
18_48712_0	We find that the best publicly available LLMs like GPT-4 and Claude currently perform poorly on basic legal text handling.
27_48720_2	Our system breaks a complex user input into simple queries which are used to retrieve information from a collection of documents detailing air travel regulations.
4_48761_3	However, the model also struggles with long query passages and the inclusion of false intertextual dependences, emphasizing the importance of expert evaluation.
34_48791_5	LLMs differed from humans in selected text spans but demonstrated high agreement in label assignment for correctly identified spans.
39_48796_4	Although LLMs prove to be able to beat, under certain circumstances, the traditional baselines, obtaining a nuanced and truly explainable decision requires at best a lot of experimentation.
51_48808_0	Large language model (LLM) applications have taken the world by storm in the past two years, and the academic sphere has not been an exception.
6_48817_2	However, LMs have known biases, commonly derived from their training data.
16_48834_2	We demonstrate that LLMs suffer from prompt sensitivity, inability to model negation and sensitivity towards specific words.
2_48839_1	In this paper, we answer the question: Do LLMs know multilingual facts and can they use this knowledge for effective fact-checking?
4_48841_3	Do LLMs exhibit prediction biases when used for standard NLP tasks?In this work, we analyze the effect of shots, which directly affect the performance of models, on the fairness of LLMs as NLP classification systems.
24_48856_4	Additionally, the LLMs identify information often overlooked by human annotators, further enhancing the dataset‚Äôs completeness.
28_48860_5	Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.
3_48866_5	We show that while LLMs may describe words differently in different languages, they are biased similarly.
8_48871_5	Interestingly, LLMs and humans struggle with similar content involving nuanced emotional, ambivalent/dialectical, and psychological statements.
9_48895_6	Our findings indicate that supervised classification models fine-tuned on LLM-generated labels perform comparably to models fine-tuned with labels from human annotators.
12_49006_5	We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.
1_49189_1	This in-prompt knowledge can conflict with an LLM‚Äôs static world knowledge learned at pre-training, causing model hallucination (see examples in Table 1).
1_49189_3	We have curated a QA corpus containing information that LLMs could not have seen at pre-training.
3_49197_3	However, prior work has mainlyfocused on logical reasoning tasks (e.g. arithmetic, commonsense QA); how well LLMs can perform on more complex reasoning tasks like sequential reasoning is not clear.
31_49298_1	Our model demonstrates remarkable efficacy, securing Rank 1 in sentence puzzle solving during the test phase with an overall score of 0.98.
56_49323_3	Our proposed system makes use of an ensemble of language specific RoBERTA and BERT models to tackle the problem.
72_49339_6	Similarly, for subtask 2, our system exhibits a substantial accuracy rate of 0.781, securing a commendable seventh-place ranking.
92_49359_3	The student model adopts CLIP as an encoder for text and image features, and we incorporate an attention mechanism for modality alignment.
96_49363_7	Our system performed well in three subtasks, achieving the tenth best result with an Hierarchical F1 of 64.774%, the fourth best in Subtask 2a with an Hierarchical F1 of 69.003%, and the eighth best in Subtask 2b with a Macro F1 of 78.660%.
217_49484_2	Our system comprises four components: DATeD, LLAM, TLE, and AuDM, which empower us to effectively tackle all subtasks posed by the challenge.
223_49490_7	Our system wins rank 4, and system ablation experiments demonstrate that our proposed solutions achieve significant performance gains.
231_49498_3	In this paper, we i) propose a RoBERTaBiLSTM based classifier designed to classifytext into two categories: AI-generated or human ii) conduct a comparative study of ourmodel with baseline approaches to evaluate itseffectiveness.
17_49642_4	This multi-dimensional evaluation method ensures that LLMs must exhibit a genuine understanding of causal structures by correctly answering questions across all four dimensions, mitigating the possibility of correct responses by chance.
8_49710_5	In general, LLMs perform better on classification tasks and struggle with generative tasks.
13_49934_6	Both automatic (3 LLMs √ó 11 directions √ó 2 automatic metrics) and human evaluation (preference study and MQM) demonstrate the effectiveness of MAPS.
16_49937_4	Here we argue that such evaluation is limited, since injecting one fact (e.g., ‚ÄúJack Depp is the son of Johnny Depp‚Äù) introduces a ‚Äúripple effect‚Äù in the form of additional facts that the model needs to update (e.g., ‚ÄúJack Depp is the sibling of Lily-Rose Depp‚Äù).
32_49953_6	With multilingual finetuning with translation instructions, LLMs could learn to perform the translation task well even for those language pairs unseen during the instruction tuning phase.
50_49971_0	We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions.
56_49977_3	Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires.
78_49999_2	However, there is still no consensus on the question of when LLMs can correct their own mistakes, as recent studies also report negative results.
7_50058_7	When evaluated against evidence-based benchmarks, our model performs similarly without access to search-based methods.
11_50105_1	However, as LLMs evolve and new datasets are released, it becomes crucial to build processes to evaluate and control the models‚Äô performance.
16_50171_1	In this work, our key contribution lies in recognizing that LLMs trained on diverse languages manifest distinct language-specific weight distributions.
12_50207_5	However, in few-shot cross-lingual settings, public LLMs demonstrate an enhanced adaptive potential.
17_50298_10	The CPO method helps the model to better distinguish between subtle contextual differences, thereby improving translation quality.
37_50318_6	Despite its overall low performance, the LLM-based metric Gemba performs best in scoring German negation errors.
38_50319_3	Our system demonstrated the effectiveness of this approach in low-resource language QE, securing 1st place in both En-Gu and En-Hi, and 4th place in En-Ta and En-Te.
116_50397_6	Finally, our results show that while LLMs and NMT systems can produce translations of a good quality, they still differ from texts originally written by native speakers.
1_50426_3	The highest accuracy for an ensemble model is 84.3%, whereas the best single model, which is a GPT-4 model, produces sentences that are fully correct 83.3% of the time.
13_50510_6	Our findings reveal that while both architectures perform similarly on non-native datasets, LLMs demonstrate superior capabilities in preserving cultural nuances when handling authentic Lebanese content.
3_50517_4	The system dynamically retrieves relevant cases from past experience, refines hypotheses, and structures research workflows in a transparent and iterative manner.
3_50517_5	The methodology is demonstrated through a case study investigating the role of TLR4 in sepsis, illustrating how the system supports problem framing, literature review, hypothesis formulation, and empirical validation.
16_50538_4	The model also exhibits strong few-shot learning abilities, with performance improving as the training dataset size increases.
21_50543_2	In this study, we investigate how a relatively weaker LLM can effectively support a supervised model in NER tasks.
30_50552_4	Error analysis reveals that the system struggles most with small differences (like cap-italization) and certain ambiguous logograms (like BI).
7_50591_4	Evaluated on Korean hate speech dataset with Indonesian and German participants, our system achieves 78% accuracy (surpassing GPT-4o‚Äôs 71% baseline) while reducing human workload by 83.6%.In addition, cultural context annotations improved non-native moderator accuracy from 22% to 61%, with humans notably excelling at nuanced tasks where LLMs struggle.
11_50595_1	For example, a model should correctly identify kimchi (Korean food) in an image both when an Asian woman is eating it, as well as an African man is eating it.
13_50626_5	The GPT-4o model exhibited promising performance, with an accuracy of around 55-80% accuracy for a complete match, accuracy varying among different fields.
7_50661_1	We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm.
8_50662_2	Our findings show that AI-generated jokes elicited as much laughter as human-crafted ones, indicating that advanced AI joke generators can now produce original jokes on par with those of a professional human comedy writer.
8_50673_3	LLM-generated text detection, conceptualized as a binary classification task, seeks to determine whether an LLM produced a given text.
11_50686_3	The LLM acts as a filter in the action selection.
34_50709_1	Our system consists of two quantized models of the LlaMA family, applied across fine-tuning and few-shot settings.
5_50761_3	The re- sults of these experiments indicate that these LLM-based models still struggle with translat- ing rare linguistic phenomena and ambiguous constructions.
3_50763_5	We show that the most capable LLM (Llama3-70b) makes notable errors in detecting linguistic structures, such as misidentifying embedded clauses, failing to recognize verb phrases, and confusing complex nominals with clauses.
22_50777_3	This comparison allowed us to explore how model architecture and training data affect LLM-surprisal estimates of learners‚Äô essays written in Traditional Chinese, which in turn influence the modeling of L2 proficiency and development.
23_50778_4	Some LMs exhibited inconsistencies between analyses, reinforcing that prompting alone is unreliable for assessing linguistic competence.
26_50781_4	Therefore, we conclude that LLMs take syntactic and semantic constraints into account when processing thematic roles, but not to the same extent as human parsers.
10_50795_1	Despite having access to commonsense knowledge to better understand the psychological aspects and causality of dialogue context, even these powerful LLMs struggle to achieve the goals of empathy and emotional support.
12_50797_4	The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization.
34_50819_5	Q-LLM using LLaMA3 (QuickLLaMA) can read Harry Potter within 30s and accurately answer the questions.
45_50830_1	However, the existing LLM-powered recommendation may create redundant output, which generates irrelevant information about the user‚Äôs preferences on candidate items from user behavior sequences.
53_50838_5	Evaluations on synthetic 20-Questions games and real-world scenarios, including business and medical diagnosis cases, demonstrate that LLMs guided by these strategies perform more effective interactive feature collection, asking fewer and more strategic questions and achieving better problem-solving efficiency.
61_50846_1	The current paradigm of knowledge learning for LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples.
61_50846_7	Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability.
80_50865_2	This shows how LLMs‚Äô metalinguistic preferences can implicitly communicate the language ideologies of a particular political group, even in seemingly non-political contexts.
80_50865_3	Second, we find LLMs exhibit internal inconsistency: LLMs use gender-neutral variants more often when more explicit metalinguistic context is provided.
93_50878_1	Good VQA performance is taken as evidence that the model will perform well on a broader range of tasks that require both visual and language inputs.
98_50883_2	For example, an LLM planner may find it challenging to perform tasks that require personalization, such as deciding where to place mugs in a kitchen based on specific household preferences.
118_50903_2	Addressing these gaps, our research investigates whether LMs can effectively act as large-scale KBs after training over an expansive set of world knowledge triplets via addressing the following three crucial questions: (1) How do LMs of different sizes perform at storing world knowledge of different frequencies in a large-scale KB?
125_50910_8	Our model demonstrates consistent improvements across common CIR benchmarks, including COCO, CIRR, and Fashion-IQ.
164_50949_1	Despite these advancements, previous research found that LLMs sometimes struggle with adhering to specific constraints, such as being in a specific place or at a specific time, and at times even overlook them, which leads to responses that are either too generic or not fully satisfactory.
169_50954_5	Experimental results demonstrate that our method enables the student model to better learn the multi-modal distributions of the teacher model, leading to a significant performance improvement in various downstream tasks.
201_50986_3	In this paper, we employ Representational Similarity Analysis (RSA) to measure the alignment between 23 mainstream LLMs and fMRI signals of the brain to evaluate how effectively LLMs simulate cognitive language processing.
251_51036_1	However, this paradigm is limited by the availability of gold labels, while in certain scenarios, LLMs may need to perform tasks that are too complex for humans to provide such labels.
300_51085_1	Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one.
300_51085_2	To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it.
326_51111_1	Pretrained LLMs, due to limited training context size, struggle with handling long token sequences, limiting their performance on various downstream tasks.
348_51133_1	However, because subwords are marked as initial- or intra-word, we find that LLMs perform poorly at handling some types of affixations, which hinders their ability to generate novel (unobserved) word forms.
350_51135_2	For instance, it is widely accepted that LLMs perform well in terms of grammar, but it is unclear in what specific cognitive areas they excel or struggle in.
350_51135_4	Specifically, we explore the most widely used benchmarks for LLMs to systematically identify how well these existing evaluation methods cover the levels of Bloom‚Äôs Taxonomy, a hierarchical framework for categorizing cognitive skills.
350_51135_6	Our findings reveal that LLMs generally perform better on the lower end of Bloom‚Äôs Taxonomy.
360_51145_4	Our analysis indicates that LLM agents can understand probabilities, but they struggle with probability sampling.
390_51175_4	Building upon previous discussions on the issue of variability, we reveal an additional dimension of concern: LLMs may perform MCQA by selecting the least incorrect option rather than distinctly correct.
390_51175_5	This observation suggests that LLMs might regard multiple options as correct, which could undermine the reliability of MCQA as a metric for evaluating LLMs.
395_51180_4	Experimental results indicate that: (1) LLMs with a better understanding of Japanese and richer biomedical knowledge achieve better performance in Japanese biomedical tasks, (2) LLMs that are not mainly designed for Japanese biomedical domains can still perform unexpectedly well, and (3) there is still much room for improving the existing LLMs in certain Japanese biomedical tasks.
403_51188_2	In this paper, we investigate whether current LLM benchmarks adequately consider these data attributes.
404_51189_0	Recent work investigates whether LMs learn human-like linguistic generalizations and representations from developmentally plausible amounts of data.
440_51225_0	Traditional language model compression techniques, like knowledge distillation, require a fixed architecture, limiting flexibility, while structured pruning methods often fail to preserve performance.
443_51228_0	Many studies have explored when and how LLMs learn to use specific words, primarily by examining their learning curves.
459_51244_6	4) LLMs exhibit limited cross-lingual alignment in their understanding of relevant linguistic phenomena.
474_51259_2	Our findings suggest that: (a) LLMs struggle with dataset demands of closed do- mains such as retrieving long answer spans; (b) Certain LLMs, despite showing strong overall performance, display weaknesses in meeting basic requirements as discriminating between domain-specific senses of words which we link to pre-processing decisions; (c) Scaling model parameters is not always effective for cross-domain generalization; and (d) Closed-domain datasets are quantitatively much different than open-domain EQA datasets and current LLMs struggle to deal with them.
480_51265_1	While traditional language models have seen extensive research on language transfer, modern LLMs still necessitate further explorations in language adaptation.
485_51270_2	We also find that explicitly providing demographic information yields mixed results, while LLM‚Äôs ability to infer such details raises concerns about biased health predictions.
533_51318_0	Large Language Models(LLMs) have brought significant transformations to various aspects of human life and productivity.
533_51318_2	Moreover, many LLMs exhibit significant performance discrepancies between high-and lowresource languages, thereby restricting equitable access to technological advances for all linguistic communities.
541_51326_3	Our findings reveal that: (1) LLMs require further enhancement to accurately distinguish between harmful and argumentative essays, and (2) both current AES models and LLMs fail to consider the ethical dimensions of content during scoring.
548_51333_5	Experimental results show that advanced LLMs, such as GPT-4, perform better in the many-shot regime than in the zero-shot regime.
567_51352_4	Our study demonstrates that while all LLMs struggle to grasp cultural values, GPT-4 shows a unique capability to adapt to cultural nuances, particularly in Chinese settings.
572_51357_3	This model consists of two core components: (1) Metapath-based Heterogeneous Graph Reconstruction.
646_51431_7	Notably, 9 billion parameter open-source LLMs demonstrate again competitive performance against larger closed-source models.
648_51433_1	Existing graph-enhanced LLMs typically retrieve similar subgraphs to augment LLMs, where the subgraphs carry the entities related to our target and relations among the entities.
657_51442_7	We conduct extensive experiments on 22 LLMs, and provide in-depth analyses with NesTools, which shows that current LLMs still suffer from the complex nested tool learning task.
666_51451_6	In particular, the analysis demonstrates that the current LLMs still need improvement for detecting numerical errors requiring calculations or extensive prior knowledge.
667_51452_3	To this end, we simulate a classical referential game in which LLMs learn and use artificial languages.
667_51452_4	Our results show that initially unstructured holistic languages are indeed shaped to have some structural properties that allow two LLM agents to communicate successfully.
673_51458_3	Experimental results demonstrate significant improvements, paving the way for AI-driven solutions to meet the growing demands of healthcare in aging populations.
680_51465_6	Despite improvements, current LLMs (including GPT-4) still exhibit significant performance gaps in multi-turn interactive scenarios compared to non-interactive scenarios.
691_51476_1	Additionally, LLMs have not proven to be effective few-shot information extractors in general.
709_51494_4	Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks.
740_51525_2	Empirical evidence suggests that LLMs consistently perform worse than conventional KGC approaches, even through sophisticated prompt design or tailored instruction-tuning.
66_51632_5	Experimental results show notable improvements in task success rates and user satisfaction, demonstrating that LLM ContextBridge can make IVSR systems more intuitive, responsive, and context-aware.
21_51673_1	The AI-generated reviews can affect the consumers and businesses as they influence the trust and decision-making.
22_51674_8	Preliminary results indicate that the BERT-based model performs significantly better for Malayalam than for Tamil in terms of the average Macro F1 score, leveraging its ability to capture the complex linguistic features of these languages.
4_51785_0	Multimodal generative AI usually involves generating image or text responses given inputs in another modality.
19_51808_4	Our model exhibits strong capabilities in comprehending the implicit intent of user instructions and preforming reasoning.
28_51817_1	However, LLMs still exhibit many hallucinations when analyzing system logs, which is due to the implicit knowledge and rules in logs that LLMs cannot capture.
28_51817_7	Finally, in the reasoning stage, the LLM constructs prompt using the rule repository and performs log analysis on the test set.
50_51839_5	Our extensive experiments yield several key findings: (1) LLMs demonstrate competitive performance without training compared to state-of-the-art methods in dynamical system modeling.
50_51839_6	(2) LLMs effectively infer complex interactions among objects to capture system evolution.
69_51858_2	In this paper, we investigate whether LLMs can learn to use tools without demonstrations.
70_51859_3	Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs.
70_51859_4	We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims.
101_51890_1	Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding.
113_51902_5	In fact, the distributions of model-predicted labels remain constant regardless of which groundtruth conditions are present on the image, suggesting that the model is not interpreting chest X-rays meaningfully.
166_51955_1	Contrary to recent studies suggesting that LLMs struggle with spatial reasoning tasks, we demonstrate in this paper that a novel prompting technique, termed Patient Visualization of Thought (Patient-VoT), can boost LLMs‚Äô spatial reasoning abilities.
191_51980_1	LLMs, trained on extensive multilingual corpora, exhibit powerful generalization abilities across diverse languages and domains.
194_51983_1	However, small LLMs with about 7B parameters still struggle fine-grained format following (e.g., JSON format), which seriously hinder the advancements of their applications.
199_51988_3	To address this critical gap, we introduce a new benchmark, Fairness Benchmark in LLM under Extreme Scenarios (FLEX), designed to test whether LLMs can sustain fairness even when exposed to prompts constructed to induce bias.
222_52011_5	Experiments show that current LLMs perform significantly better in non-agent than in web-based agent environments, with agents achieving less than 10% awareness coverage and over 40% violation rates.
260_52049_2	An automatic LLM bencher consists of four components: the input set (e.g., a user instruction), the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise comparison), and the aggregation method (e.g., the ELO rating system).
288_52077_1	In real-world scenarios, model owners need to continuously address copyright infringement as new requests for content removal emerge at different time points.
291_52080_7	Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances.
296_52085_4	Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning.
328_52117_6	The resulting model demonstrates superiority on two benchmarks, surpassing the previous state-of-the-art performance of rewrite-then-retrieve approaches, including GPT-3.5.
329_52118_3	However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance.
330_52119_5	Through extensive experiments with several instruction-tuned LLMs, we reveal that most LLMs struggle to grasp the actual intention concealed in the instruction and thoroughly analyze the factors influencing instruction understanding.
360_52149_1	LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context.
361_52150_4	The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend.
361_52150_7	In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.
395_52184_0	Large Language Model (LLM) agents exhibit remarkable performance across diverse applications by using external tools to interact with environments.
444_52233_2	To further assess whether LLMs meet the specific needs of legal practitioners in real-world scenarios, we introduce UCL-Bench, a Chinese User-Centric Legal Benchmark, comprising 22 tasks across 5 distinct legal scenarios.
448_52237_3	The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games.
448_52237_7	Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners.
449_52238_8	Our experiments comprehensively demonstrate how LLMs perform relative to each other, the benefits of using more in-context exemplars in generating a higher fraction of functionally correct assertions, and the significant room for improvement for LLM-based assertion generators.
469_52258_5	Using TRAIT, we reveal two notable insights into personalities of LLMs: 1) LLMs exhibit distinct and consistent personality, which is highly influenced by their training data (e.g., data used for alignment tuning), and 2) current prompting techniques have limited effectiveness in eliciting certain traits, such as high psychopathy or low conscientiousness, suggesting the need for further research in this direction.
6_52270_3	In this study, we evaluate how well LLMs perform in generating code for domain-specific languages (DSLs) in accounting, using Beancount as a case study.
11_52275_3	This paper investigates how well LLMs perform at forecasting corporate credit ratings.
11_52275_5	For our task, current LLMs perform worse than a more traditional XGBoost architecture that combines fundamental and macroeconomic data with high-density text-based embedding features.
18_52282_4	The AI system, similar to human reporters, would analyze stock price volatility and determine the underlying factors contributing to these fluctuations.
32_52296_5	This dual-model approach ensures greater accuracy in misinfor- mation detection through cross-validation.
7_52320_0	Everyday AI detection requires differentiating between humans and AI in informal, online conversations.
8_52321_2	Using theoretical individual differences, the fundamental psychological traits which distinguish people, this study reveals a distinctive characteristic of such content: AI-generations exhibit remarkably limited variation in inferrable psychological traits compared to human-authored texts.
6_52392_0	Multilingual LLMs support a variety of languages; however, their performance is suboptimal for low-resource languages.
11_52418_1	Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extration.
12_52419_2	We find that while LLMs can generally perform multiple homogeneous classifications at once (Batch Classification) as well as when they do so separately, they perform significantly worse on two selection tasks that are conceptually equivalent to Batch Classification and involve selecting indices of text falling into each class label, either independently or altogether.
16_52423_0	Evaluating an LLM‚Äôs robustness against numerical perturbation is a good way to know if the LLM actually performs reasoning or just replicates patterns learned.
4_52428_1	While LLMs have demonstrated impressive capabilities in recreating knowledge bases and generating free-form text, their ability to generate structured tabular data has received little attention.
5_52429_3	In this way, we demonstrate that LLMs can formulate SPARQL queries achieving state-of-the-art results on several Knowledge Graph Question Answering (KGQA) benchmark datasets without fine-tuning.
31_52519_3	In the latter, the puzzles are manually decomposed into intermediate steps to allow LLMs learn and apply linguistic rules incrementally.
31_52519_5	However, LLMs still struggle with translating English into the unseen languages, typically with complex syntactic rules.
8_52551_4	The results demonstrate that fine-tuned LLMs can produce high-quality contextually relevant CNs for low-resource languages that are comparable to human-generated responses, offering a sig- nificant contribution to combating online hate speech across diverse linguistic settings.
2_52566_4	Our analysis reveals open problems regarding if, and how, LMs can be used to simulate humans, and that LLMs can more accurately describe the opinion distribution than simulate such distributions.
40_52604_2	In practice, to better align with the career trajectory of Chinese financial practitioners, we build a systematic evaluation from 4 first-level categories: (1) Financial Subject: whether LLMs can memorize the necessary basic knowledge of financial subjects, such as economics, statistics and auditing.
40_52604_4	(3) Financial Practice: whether LLMs can fulfill the practical financial jobs, such as tax consultant, junior accountant and securities analyst.
40_52604_5	(4) Financial Law: whether LLMs can meet the requirement of financial laws and regulations, such as tax law, insurance law and economic law.
44_52608_3	Current LLMs exhibit satisfactory instruction-following capabilities based on instruction-following fine-tuning process.
58_52622_2	However, LMs suffer from a phenomenon called ‚Äúperplexity curse‚Äù; despite minimizing document perplexity during training, LMs struggle to extract information via a question prompt.
58_52622_3	In this paper, we study the problem by fine-tuning LMs for new data and find a very intriguing fact that all studied LMs suffer from positional bias in the training document, i.e., they struggle to answer questions about the information described in the middle or at the end of the training document.
59_52623_6	Our analysis shows that LLMs struggle with morphological compositional generalization particularly when applied to novel word roots, with performance declining sharply as morphological complexity increases.
89_52653_5	Our model supports generating both semantic segmentation masks and instance-level bounding boxes, and accommodates various imaging modalities, including both 2D and 3D data.
118_52682_4	Our findings indicate that LLMs trained on decade-specific books manifest biases reflective of their times, with both gradual trends and notable shifts.
120_52684_3	Through comprehensive experiments on NormAd-Eti, we find that LLMs struggle to accurately judge social acceptability across these varying degrees of cultural contexts and show stronger adaptability to English-centric cultures over those from the Global South.
127_52691_3	Our key insight is that when generating identical responses like ‚ÄúSure‚Äù, LLMs exhibit distinctly different internal gradient patterns for safe versus harmful prompts, reflecting conflicts with safety training.
128_52692_2	LLMs, despite their impressive capabilities in text-based reasoning tasks, struggle to achieve promising rumor detection performance when facing long structured social contexts.
141_52705_2	In this work, we introduce a framework for quantifying LLM creativity that incorporates the two design ingredients: (1) We introduce DENIAL PROMPTING which pushes LLMs to develop more creative solutions to a given problem by incrementally imposing new constraints on the previous solution, compelling LLMs to adopt new strategies.
164_52728_4	(2) Do LMs represent only one potential interpretation, or multiple?
164_52728_6	To address these questions, we use sparse autoencoders to identify interpretable features that determine which continuation‚Äîand thus which reading‚Äîof a garden path sentence the LM prefers.
172_52736_0	Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r‚Äôs in the word ‚Äústrawberry‚Äù.
172_52736_4	Although specialized LLMs suffer from counting problems as well, we find conjectures about inherent deficiency of LLMs invalid and further seek opportunities to elicit knowledge and capabilities from LLMs which are beneficial to counting tasks.
176_52740_2	(2024), where the best LLM OpenAI o1-preview can only find viable travel plans with a 10% success rate given all needed information.
177_52741_2	Our experiments with three LLMs indicate that, contrary to humans, LLMs consistently exhibit a preference for local attachment, displaying limited responsiveness to syntactic variations or language-specific attachment patterns.
177_52741_3	Although LLMs performed well in unambiguous cases, they rigidly prioritized world knowledge biases, lacking the flexibility of human language processing.
182_52746_0	LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations.
196_52760_7	Experimental results indicate that while LLMs perform well in reciting CBT knowledge, they fall short in complex real-world scenarios requiring deep analysis of patients‚Äô cognitive structures and generating effective responses, suggesting potential future work.
269_52833_4	In the first stage, we fine-tune the LLM to better identify homophilic and heterophilic edges based on the textual content of their nodes.
290_52854_4	LSIM comprises three components: reinforcement learning predicts a structured fact-rule chain for each question, a trainable Deep Structured Semantic Model (DSSM) retrieves the most relevant candidate questions by integrating semantic and logical features, and in-context learning generates the final answer using the retrieved content.
298_52862_6	Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks.
317_52881_3	We propose to automatically search for finer-grained categories based on inputs where a system performs well or poorly, and describe them in natural language.
326_52890_4	We find that LMs struggle in Arabic with entities that appear at high frequencies in pre-training, where entities can hold multiple word senses.
335_52899_3	In JRE-L, one LLM acts as the journalist, another LLM as the general public reader, and the third LLM as an editor.
370_52934_3	LLMs and LLM agents often struggle with strategic reasoning due to the absence of a reasoning framework that enables them to dynamically infer others‚Äô perspectives and adapt to changing environments.
394_52958_2	However, LLMs still struggle with complex reasoning tasks, highlighting their systemic limitations.
394_52958_4	We introduce novel tasks and benchmarks spanning six languages and observe that current SOTA LLMs often produce conflicting answers to the same questions across languages in 17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.
411_52975_3	Furthermore, in scenarios where no information is in the language of the query, LLMs prefer documents in high-resource languages during generation, potentially reinforcing the dominant views.
417_52981_3	First, we show that LLMs can solve real-world regression problems and then design experiments to measure the extent to which the LLM retrieves its internal knowledge versus learning from in-context examples.
423_52987_0	While LLMs have exhibited strong performance on various NLP tasks, it is noteworthy that most of these tasks rely on utilizing the vast amount of knowledge encoded in LLMs‚Äô parameters, rather than solving new problems without prior knowledge.
433_52997_2	This paper explores two competing hypotheses about the cause of student-LLM miscommunication: (1) students simply lack the technical vocabulary needed to write good prompts, and (2) students do not understand the extent of information that LLMs need to solve code generation tasks.
444_53008_3	This reversal creates instability, especially on larger datasets, as the model struggles to balance unlearning with maintaining language capacity, leading to over-unlearning.
453_53017_7	To enhance training efficiency and reduce query costs, we introduce a learnable dynamic ranking threshold, adjusted when the model encounters negative prediction shifts.
469_53033_3	Though efficient to compute, we show that this definition is unreliable because it does not account for non-determinism present in more realistic (non-greedy) sampling schemes, for which LLMs produce a range of outputs for the same prompt.
496_53060_5	Our findings reveal that (1) LLMs demonstrate a pronounced bias towards food knowledge prevalent in the United States; (2) Incorporating relevant cultural context significantly improves LLMs‚Äô ability to access cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is highly dependent on the interplay between the probing language, the specific model architecture, and the cultural context in question.
506_53070_2	As opposed to prior work which focuses on particular domains or types of persuasion, we conduct a general study across various domains to measure and benchmark to what degree LLMs produce persuasive language - both when explicitly instructed to rewrite text to be more or less persuasive and when only instructed to paraphrase.
513_53077_3	Although we explored the optimal prompting strategies and proposed a novel and effective diversity-difficulty-based few-shot sample selection method, we found that the improvements from tuning-free approaches were incremental, as LLMs struggle with planning on complex graphs, particularly those with a larger number of triples.
519_53083_8	These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.
520_53084_4	Using the Flanders Interactive Analysis System and Community of Inquiry theoretical frameworks from educational analysis, we demonstrate that LLMs can simulate a dynamic learning environment for users with active teacher-student and student-student interactions.
553_53117_4	Our analyses show that LLMs struggle with understanding the nuances of ADRs and differentiating between types of ADRs.
556_53120_2	However, evaluation of the uncertainty of LLM communication should also focus on the behaviors of their human interlocutors: how much do users rely on what the LLM says?
558_53122_1	Due to the nascency of the field, it is not well-established whether LMs exhibit similar learning dynamics to humans, and there are few direct comparisons between learning trajectories in humans and models.
565_53129_1	In this work, we show that LLMs suffer from personalization bias, where their performance is impacted when they are personalized to a user‚Äôs identity.
565_53129_5	We find that various LLMs, ranging from open-source models like Llama-3.1 and Mistral to API-based ones like GPT-3.5 and GPT-4o, exhibit significant variance in performance in terms of safety and utility when personalized with different user identities.
569_53133_0	In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot.
574_53138_3	In this work, we investigate how LMs perform property inheritance with behavioral and causal representational analysis experiments.
588_53152_3	Our results suggest that various LLMs struggle to consistently answer diverse equivalent queries.
627_53191_6	In this study, we created video and image datasets from the existing real-time MRI dataset and investigated whether LMs can understand vowel articulation based on tongue positions using vision-based information.
627_53191_7	Our findings suggest that LMs exhibit potential for understanding vowels and tongue positions when reference examples are provided while they have difficulties without them.
5_53207_3	We run 16 LLMs on QA and RQA with trivia questions/answers, revealing: 1) Versus RQA, LLMs are much less accurate in RQA for numerical answers, but slightly more accurate in RQA for textual answers; 2) LLMs often answer their own invalid questions from RQA accurately in QA, so RQA errors are not just from knowledge gaps; 3) RQA errors correlate with question difficulty and inversely correlate with answer frequencies in the Dolma corpus; and 4) LLMs struggle to give valid multi-hop questions.
11_53213_5	Lastly, we fine-tune an LLM to identify and prioritize these key facts to optimize decision-making.
20_53222_1	To evaluate the extent to which LLMs perform robust reasoning instead of relying on superficial logical chains, we propose a new evaluation dataset, the Concept-Reversed Winograd Schema Challenge (CR-WSC), based on the famous Winograd Schema Challenge (WSC) dataset.
27_53229_1	This paper investigates whether LLMs, like humans, struggle with reverse modeling, specifically with reversed text inputs.
27_53229_2	We found that publicly available pre-trained LLMs cannot understand such inputs.
27_53229_3	However, LLMs trained from scratch with both forward and reverse texts can understand them equally well during inference across multiple languages.
32_53234_3	Furthermore, we show that the model by STEP performs comparably to vanilla pre-trained models on downstream tasks after instruction tuning.
33_53235_1	A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values.
33_53235_3	Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10.This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs.
64_53266_6	Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures.
40_53324_1	Therefore, LLMs need to be benchmarked with enterprise datasets for a variety of NLP tasks.
20_53385_6	With a relatively lightweight architecture of approximately 120 million parameters, the system ensures low inference costs.
38_53403_3	We propose a two-stage, prompt-based framework for this task, where LLMs perform similarity assessments between classification codes and identify final mappings through a guided decision process.
40_53405_2	To tackle this challenge, we propose Video Understanding and Response Consistency Assessment, VURCA, a framework that incorporates a fine-grained question generation and answering process to measure how well the responses generated by VLMs align with what the model understands.
52_53417_4	This study investigates whether LLMs can rationally estimate distributions when presented with explanations of ‚Äúartificially generated distributions‚Äù that are against commonsense.
52_53417_5	Specifically, we assess whether LLMs recognize counterintuitive explanations and adjust their predictions or simply follow these inconsistent explanations.
