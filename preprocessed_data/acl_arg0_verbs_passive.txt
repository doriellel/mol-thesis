1_1150_2	Due to its tuned MT engine, the approach can be seen as a human-aided machine translation (HAMT) system circumventing major obstacles in full-scale Japanese-English MT.
4_2186_2	The similarity score function makes use of ”similarity models” built from the automatic transcriptions furnished by earlier stages of the ASR system, while the documents selected for training auxiliary LMs are drawn from the same set of data used to train the baseline LM used in the ASR system.
13_2240_4	We therefore investigate in this paper the use of a maximum entropy language model for Russian whose features are specifically designed to deal with the inflections in Russian, as well as the loose word order.
27_2273_0	A novel variation of modified KNESER-NEY model using monomial discounting is presented and integrated into the MOSES statistical machine translation toolkit.
18_3437_3	The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge.
36_5439_1	Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks.
479_6069_1	However, while knowledge is both written and queried in many languages, studies on LMs’ factual representation ability have almost invariably been performed on English.
7_8463_3	The model is designed based on the BART language model that receives a linear representation of unordered and non-inflected tokens in a sentence along with their corresponding Universal Dependency information and produces the linear sequence of inflected tokens along with the missing words.
260_9141_4	The final submission was chosen based on the best performances which was achieved by the BERT+BiLSTM model.
230_10195_6	EnsLM can be trained jointly with mATM with a flexible LM backbone.
416_10381_4	Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model.
490_10455_5	The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher.
18_11464_4	To further strengthen the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand.
153_11599_1	However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies.
489_12367_1	We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred.
620_12498_3	We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level.
718_12596_0	As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model.
805_12683_0	As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation.
192_13002_0	Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts.
38_13975_1	Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available.
75_14800_1	However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.
75_14800_4	In terms of quality metrics (the proportion of words, semantically related to the target word), the multilingual BERT is recognized as the best model.
1_14941_0	The next generation of conversational AI systems need to: (1) process language incrementally, token-by-token to be more responsive and enable handling of conversational phenomena such as pauses, restarts and self-corrections; (2) reason incrementally allowing meaning to be established beyond what is said; (3) be transparent and controllable, allowing designers as well as the system itself to easily establish reasons for particular behaviour and tailor to particular user groups, or domains.
37_16019_2	In the previous work, these two modules are loosely connected in the model training and are shallowly integrated during inference, where a simple switching or copy mechanism is adopted to incorporate recommended items into generated responses.
96_16254_0	Class-based language models (LMs) have been long devised to address context sparsity in n-gram LMs.
373_16531_2	Actions by the AI system may be required to bring these objects in view.
19_16936_1	Recently, techniques based on Deep Learning and Natural Language Processing have been proven effective in detecting anomalous activities from system logs.
32_17155_6	By controlling for model uncertainty we are able to show that entities are identified, and depending on the task, play a measurable role in the model’s predictions.
280_17749_3	In this paper, we present a structure-aware self-attention language model to capture structural information of target representations and propose a knowledge distillation based approach to incorporating the target language model into a seq2seq model, where grammar rules or sketches are not required in the training process.
28_18152_6	Further, the transformers’ retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth.
16_18260_2	These languages are considered to be filled with complexities and challenges that make their study incredibly difficult in the NLP and AI fields.
509_18956_5	Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.
32_20012_3	While verifying the existence of regional bias in LMs, we find that the biases on regional groups can be largely affected by the corresponding geographical clustering.
502_20526_4	We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.
502_20526_5	Moreover, we show that a complete constituency tree can be linearly separated from LM representations.
2_20574_4	Specifically, we collect a sheer number of source codes (both Java and Python) from the Alipay code repository and incorporate both syntactic and semantic code knowledge into our model through the help of code parsers, in which AST information of the source codes can be interpreted and integrated.
8_20676_4	Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias.
6_20896_1	Due to computational constraints, existing analyses are mostly conducted on publicly-released LM checkpoints, which makes it difficult to study how various factors during training affect the models’ acquisition of linguistic knowledge.
102_22238_3	AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training.
225_22361_2	Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.
25_23457_6	The spell checker was developed as a testing environment for the language model.
1_23460_3	Some simulation experiments demonstrating the benefits of personalized language model ensembling via the library are presented.
72_23690_3	This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller.
68_24218_7	Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.
109_24259_1	While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks.
148_24298_5	It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged.
245_24395_2	We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction.
502_24652_0	Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE).
754_24904_4	Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations.
865_25015_6	On the other hand, downstream performance is mainly driven by the model’s size and prior legal knowledge which can be estimated by upstream and probing performance.
5_25654_1	Despite AI-enhanced applications having the potential to provide personalized learning experiences, more studies are needed on the design of generative AI systems and evidence for using them in real educational settings.
31_25755_4	We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found.
36_26102_3	Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
38_26180_1	Specifically, we formulate CDEC as a multi-category classification problem on pairs of events that are represented as decontextualized sentences, and compare the predictions of GPT-4 with the judgment of fully trained annotators and crowdworkers on the same data set.
3_26185_65	The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.
51_26772_5	We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset.
85_26806_2	However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot.
107_26828_4	To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning.
146_26867_4	We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research.
174_26895_1	Some risks may only be discovered after the model training is completed, such as the model memorizing a specific phone number and frequently outputting it.
319_27040_0	The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca).
322_27043_5	A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader.
322_27043_6	The rewriter is trained using the feedback of the LLM reader by reinforcement learning.
322_27043_8	Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.
335_27056_0	The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.
453_27174_0	In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query.
531_27252_3	We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained.
695_27416_1	The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model’s pretraining data is assumed to be easier for that model.
782_27503_0	Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.
885_27606_5	Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered.
921_27642_1	While it is well known that parametric models are prone to leaking private data, it remains unclear how the addition of a retrieval datastore impacts model privacy.
75_27901_1	It’s designed as a search-based system, maintaining a user index of past successful interactions with the conversational AI.
167_28303_8	The code for this work can be found at https://github.com/Ziems/llm-url.
280_28416_2	For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?
441_28577_1	However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.
680_28816_0	In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text.
94_29130_1	To address the issue of misuse associated with AI-generated text, various high-performing detectors have been developed, including the OpenAI detector and the Stanford DetectGPT.
128_29164_0	Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies.
223_29259_2	Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives.
326_29362_2	An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks.
418_29454_0	Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change.
445_29481_3	Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data.
511_29547_1	To maintain the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.
546_29582_1	It is currently conjectured that shortcomings of LLMs in multi-linguality and reasoning are due to a lack of ability to generalize.
782_29818_2	We focus on the fine-tuning of pre-trained LMs, which is expected to be performed much more frequently as the pre-trained models are adapted to downstream tasks.
981_30017_6	The evaluation is performed over four NLP tasks (two generative and two classification tasks) among four widely used multilingual LMs in seven languages.
1032_30068_1	To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone.
1055_30091_0	Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT.
16_30125_8	The evaluation results show an overall good quality of the dialogues, though research is still needed to improve the quality of the GPT-4 evaluation metrics.
18_30127_2	Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5—which have shown promising results.
19_30128_2	While no additional domain knowledge or fine-tuning is performed, we provide a single training example of this decompilation process in the model’s prompt.
5_30231_1	However, the performance of such LMs have not been studied in detail with respect to finer language related aspects in the context of NER tasks.
5_30743_0	In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans.
2_30841_2	However, it is not yet known the performance of LLMs on CLS.
1_31215_4	We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights.
306_31566_2	Then, a set of experiments has been conducted with a Wikipedia-based reclassification system.
23_31608_4	To improve the performance of ChatGPT, several experiments utilising different prompt engineering techniques were conducted.
2_31646_0	We investigate how well words in the polysynthetic language Inuktitut can be translated by combining dictionary definitions, without use of a neural machine translation model trained on parallel text.
4_32166_2	On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage.
14_32176_3	Extensive experiments are conducted on Causal Question Answering (CQA), where IBE-Eval is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2).
73_32235_1	It is known that the effective design of task-specific prompts is critical for LLMs’ ability to produce high-quality answers.
141_32303_1	Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model.
185_32347_6	The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.
225_32387_1	Numerous benchmarks have been established to assess the reasoning abilities of LLMs.
226_32388_2	Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.
226_32388_3	Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.
279_32441_2	A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples.
331_32493_5	Our code can be found in https://github.com/HKUST-KnowComp/LLM-discussion.
332_32494_3	However, these methods would be affected by the noise in the knowledge and the context length limitation of LLM.
409_32571_2	This learning method is designed to enhance the performance of open LLM agents.
423_32585_5	Furthermore, we present the first cross-supervision role-play experiment, revealing that the role-play styles can be easily acquired, while the intrinsic capabilities of LLMs confine the knowledge within role-play.
462_32624_6	These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions.
521_32683_1	AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms.
530_32692_5	Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools.
538_32700_6	The evaluation framework and experimental results are expected to provide an in-depth understanding of the editorial capabilities of LLMs and speed up the development of LLMs in journalism.
563_32725_5	A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task.
568_32730_4	RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM.
589_32751_0	Large language models (LLMs) have successfully served as a general-purpose interface across multiple tasks and languages, while the adaptation of voice LLMs is mostly designed for specific purposes (either single-task or monolingual), where the advantages of LLMs especially for low-resource language processing and zero-shot task generalization are less exploited in the audio community.
590_32752_3	The chat vector is derived by subtracting the weights of a pre-trained base model (e.g. LLaMA2) from those of its corresponding chat model (e.g. LLaMA2-chat).
591_32753_2	More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM.
809_32971_2	However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics.
809_32971_5	In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics.
818_32980_1	Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood.
864_33026_1	Although word embeddings are usually interpreted as feature vectors for individual words, their roles in language model generation remain underexplored.
6_33109_1	Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model.
15_33149_1	However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood.
23_33157_0	Large Language Models (LLMs) are considered to have potentially extensive knowledge, but because their internal processing is black-boxed, it has been difficult to directly edit the knowledge held by the LLMs themselves.
29_33163_2	What research has been conducted on these vulnerabilities was predominantly on English, limiting the understanding of LLM behavior in other languages.
5_33198_1	While LLMs such as ChatGPT has been developed and used for various tasks, there remain several weakness of the LLMs.
5_33198_4	Experiments and evaluations were conducted using “AI-Werewolf,” a communication game for AI with incomplete information.
10_33210_5	We therefore carefully examine the effect of learning paradigms on the extent to which genetic entities are fabricated, and the limitations of exact matching to determine performance of the model.
20_33258_2	We perform a preliminary analysis to determine to what degree the performance of our model is due to prior exposure to the task languages, finding that generally our performance is better explained as being derived from in-context learning capabilities.
21_33287_5	We also reveal that, while the results for ChatGPT 4 are not significantly language dependent, meaning that the performances in avoiding biases are not affected by the prompting language, their difference with ChatGPT 3.5 is statistically significant.
98_33764_7	Human raters were asked to rate the explanation ofthe implicatures generated by LLMs on their reasonability, logic and fluency.
9_33838_5	We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.
4_33862_8	Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.
7_33909_0	Syntactic learning curves in LMs are usually reported as relatively stable and power law-shaped.
15_33926_5	A significant association was found between the teachers’ familiarity with and use of AI technology and their age-related generational traits.
34_33980_3	Our experiments show that expectations in terms of usefulness and trustworthiness of LLM-generated explanations are not met, as their ratings decrease by 47.78% and 64.32%, respectively, after treatment.
91_34037_3	These parallel corpora were analyzed using both complexity and similarity metrics to assess the outcomes of LLMs and human participants.
6_34090_4	Our results highlight the importance of prioritizing information presentation in the design of domain-specific LLMs to ensure that scientific information is effectively communicated, especially as even expert audiences find it challenging to assess the credibility of AI-generated content.
19_34123_2	We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician.
16_34187_1	Our framework is constructed around an LLM with knowledge self-generation and output refinement.
18_34189_1	The work is circumscribed to the use of “open-source” LLMs that can be run locally, thereby enhancing data privacy.
7_34206_4	A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.
9_34675_5	Further, we describe tradeoff curves between the LLM evaluator performance (i.e., correlation with humans) and evaluation set size; loss in correlation can be compensated with modest increases in the evaluation set size.
12_34879_2	When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed.
84_34950_5	The lookback ratio-based detector—**Lookback Lens**—is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model.
119_34984_3	However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge.
147_35011_3	We find evidence that they indeed circumvent the requirement to perform multi-hop reasoning, but they do so in more subtle ways than what was reported about their fine-tuned pre-trained language model (PLM) predecessors.
244_35103_6	Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20% of Trump supporters reduced their support for Trump after LLM interaction.
250_35109_4	The retrieval system harnesses both dense text embedding and sparse bag-of-words representations given by the LLM.Our experimental evaluation on the MSMARCO, TREC deep learning and BEIR zero-shot document retrieval datasets illustrates that this simple prompt-based LLM retrieval method can achieve a similar or higher retrieval effectiveness than state-of-the-art LLM embedding methods that are trained with large amounts of unsupervised data, especially when using a larger LLM.
279_35138_1	To understand how people perceive writings that are produced under this paradigm, in this paper, we conduct an experimental study to understand whether and how the disclosure of the level and type of AI assistance in the writing process would affect people’s perceptions of the writing on various aspects, including their evaluation on the quality of the writing, and their ranking of different writings.
315_35173_4	For some models, e.g., the Llama-2 family of models, prompt-based embeddings align better than word embeddings, but the projections are less linear – an observation that holds across almost all model families, indicating that some of the implicitly learned alignments are broken somewhat by prompt-based methods.
344_35202_2	Moreover, maximum likelihood training has been discovered to give rise to anisotropy: representations of tokens in a model tend to cluster tightly in a high-dimensional cone, rather than spreading out over their representational capacity.
377_35235_0	Significant advancements have recently been made in large language models, represented by GPT models.
444_35299_4	We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model’s knowledge.
476_35330_5	Rigorous evaluations and ablation studies are conducted on two commercial LLMs (ChatGPT and GLM) and three open-source LLMs, revealing PeT’s superiority in producing less harmful responses, outperforming five strong baselines.
484_35338_2	To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM.
595_35446_3	More interestingly, we show that these methods are additive; combining them achieves the best win rates in head-to-head comparison, resulting in responses that are preferred or tied to the base model in 76.2% of comparisons on average.
606_35456_4	This is supported by empirical findings on variants on GPT-2, demonstrating improved stability and lower perplexities, even at deeper layer counts.
651_35501_3	We conclude by showing that it would apply to LLMs only if they were interpreted in the manner of how the CTM conceives the mind, i.e., by postulating that LLMs rely on a version of a language of thought, or by adopting said questionable theories of meaning; since neither option is rational, we conclude that the SGP does not apply to LLMs.
687_35537_5	However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.
738_35585_4	Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism.
769_35615_5	Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter can mitigate most reasoning biases while being consistent.
781_35626_4	By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model.
833_35677_0	Large language models(LLMs) have been adopted to process textual task description and accomplish procedural planning in embodied AI tasks because of their powerful reasoning ability.
860_35703_4	MT-Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost.
889_35732_9	Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.
899_35742_1	Even though their capabilities of data synthesis have been studied well in recent years, the generated data suffers from a lack of diversity, less adherence to the prompt, and potential biases that creep into the data from the generator model.
1014_35853_5	Our attacks are easy to employ, requiring only black-box access to an LLM and a few samples from the user, which _need not be the ones that were trained on_.
1034_35873_3	However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet.
1035_35874_2	We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model’s internal representations.
86_36235_1	When using Large Language Models (LLMs) for this task, a new call to the LLM inference endpoint/API is required for each new query even if the context stays the same.
135_36469_1	However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features.
14_36791_0	Large language models (LLMs) are typically trained on general source data forvarious domains, but a recent surge in domain-specific LLMs has shown theirpotential to outperform general-purpose models in domain-specific tasks (e.g.,biomedicine).
14_36791_4	Notably, all sparse pre-trainingwas performed on the Cerebras CS-2 system, which is specifically designed torealize the acceleration benefits from unstructured weight sparsity, therebysignificantly enhancing the efficiency of the MediSwift models.
126_36901_4	It is determined by counting the revision edits generated by LLMs.
145_36920_3	Our method is conducted in the form of model merging, where the fine-tuned language model is merged with the pre-trained base model or the peer models from other domains through weighted average.
275_37047_3	In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM?
304_37076_3	However their explicitly mention of malicious intent will be easily recognized and defended by LLMs.
305_37077_7	This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals.
338_37108_3	To address it, we propose a new RL method named RLMEC that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, which can produce token-level supervision for RL training.
379_37148_4	Experiments were performed on SLURP to quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B, LLaMA-2-13B and Vicuna-13B (v1.1 and v1.5) with different ASR error rates.
400_37169_3	We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions.
430_37198_2	In this paper, we identify that such demonstration bias may primarily stem from the semantic ambiguity induced by demonstrations, i.e., a demonstration may indicate multiple input-to-label mappings and its mapping can be interpreted differently in different contexts by LLMs.
460_37224_3	Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs.
473_37237_1	This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs.
529_37293_8	Human evaluation are conducted to verify the quality of LLM-REDIAL.
659_37420_0	A language model may be viewed as a 𝛴-valued stochastic process for some alphabet 𝛴.However, in some pathological situations, such a stochastic process may “leak” probability mass onto the set of infinite strings and hence is not equivalent to the conventional view of a language model as a distribution over ordinary (finite) strings.
704_37464_1	To curtail the frequency of these calls, one can employ a local smaller language model -a student- which is continuously trained on the responses of the LLM.
722_37482_0	While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely “superficial”.
724_37484_2	While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs.
726_37486_2	However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood.
804_37561_4	A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation.
924_37680_5	Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.
30_37760_3	However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness.
51_37780_5	ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks.
96_37825_0	Prior research has revealed that certain abstract concepts are linearly represented as directions in the representation space of LLMs, predominantly centered around English.
197_37923_6	Our results imply that changes are needed in QA dataset design and evaluation to more effectively assess the correctness and downstream impacts of model abstention.
240_37965_3	Yet, its application in LLMs has not been extensively studied.
251_37976_5	Using emotion attribution, we explore how different religions are represented in LLMs.
400_38117_2	Named Entity Recognition (NER) is a critical task in information extraction that is not covered in recent LLM benchmarks.
521_38233_4	This automatic mining process is efficiently accomplished through the collaboration between a large-scale teacher model and a small-scale student model.
540_38251_3	However, the diversity aspect in LLM outputs has not been systematically studied before.
551_38262_1	RPI employs integration on internal attention scores and their gradients along a randomized path, which is dynamically established between a baseline representation and the attention scores of the model.
569_38280_1	While many strategies and datasets to enhance LLMs’ mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.
623_38334_2	NL’s status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined.
766_38472_4	In experiments across four LLMs, we find that multilingual instruction tuning with as few as two to three languages is both necessary and sufficient to elicit effective cross-lingual generalisation, with the limiting factor being the degree to which a target language is seen during pretraining.
802_38508_2	Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server).
813_38518_7	Code and data are made publicly available at https://turningpoint-ai.github.io/DrAttack/.
996_38699_5	The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs.
3_38762_2	We find that hegemonic norms are consistently reproduced; dominant identities are often treated as ‘default’; and discussion of identity itself may be considered ‘inappropriate’ by the safety features applied to some LLMs.
22_38781_2	The research field of bias in LLMs has seen massive growth, but few attempts have been made to detect or mitigate other biases than gender bias, and most focus has been on English LLMs.
9_38794_1	We study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.
19_38960_4	We argue that analogous standardization processes are required for LLM assessments, given their differential functioning as compared to humans.
37_38978_1	The toolkit enables access to a wide selection of AI assets, including datasets, models, and metrics, from both academic and commercial sources, which can be selected, executed and evaluated in one place through different services in a standardized format with consistent documentation provided.
53_38994_3	We find that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multiliguality is taken into account in hyperparameter tuning and with large enough IT data.
17_39032_2	While such modifications have been proven effective in tasks like machine translation, tailoring them to LLMs demands specific modifications given the diverse nature of LLM applications.
4_39247_3	Additionally, it is presented a Knowledge Graph Q&A System powered by Generative AI.
22_39431_0	Large Language Models (LLMs) have been developed without a theoretical framework, yet we posit that evaluating and improving LLMs will benefit from the development of theoretical frameworks that enable comparison of the structures of human language and the model of language built up by LLMs through the processing of text.
53_39462_3	We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers.
197_39606_8	Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5.
782_40191_7	The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources.
916_40325_5	LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.
930_40339_6	We also discuss the challenges and limitations of LLMs that need to be addressed before they can be widely adopted in clinical settings.
1090_40499_1	Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings.
1539_40948_1	However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.
46_41192_3	MONITOR is designed to compute the distance between the probability distributions of a valid output and its counterparts produced by the same LLM probing the same fact using different styles of prompts and contexts.
51_41197_1	However, the comprehensive effects of fine-tuning on the LLMs’ generalization ability are not fully understood.
78_41224_6	Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM.
118_41264_0	Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses.
249_41395_3	Our proposed approach, dubbed Mixture of Word Experts (MoWE), can be seen as a memory augmented model, where a large set of word-specific experts play the role of a sparse memory.
284_41430_1	In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible.
379_41525_11	Notably, with GPT-3.5-Turbo and I3C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art few-shot prompting method Complex-CoT by +11.7 and +11.1.Our implementation is made publicly available at https://wzy6642.github.io/I3C.github.io/.
456_41602_2	The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable.
464_41610_1	While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies of diffusion LMs have been conducted on a smaller scale.
46_41679_2	However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.
1_41815_2	In this work, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of human reading patterns.
51_41911_2	Popular LLMs such as ChatGPT have been examined as a research assistant and as an analysis tool, and several discrepancies regarding both transparency and the generative content have been uncovered.
2_41939_5	We also show that techniques like Chain-of-Thought and Cross-Lingual Prompting, which are designed to improve reasoning abilities, do not necessarily improve the fact-checking abilities of LLMs.
1_42012_5	We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.
4_42178_2	The dialogue system is constructed on top of LLMs, focusing on defining specific roles for individual modules.
22_42235_2	This problem is known as hallucination and has reduced the confidence in the output of LLMs.
90_42303_0	For our submission for Subtask 1, we developed a custom classification head that is designed to be applied atop of a Large Language Model.
117_42330_4	Promising advances are presented by utilizing text-to-text processing capabilities of the T5 model and advanced NLP approaches like BERT and RoBERTa.
14_42620_1	However, evaluating the intersection of these two skills—multilingual few-shot reasoning—is difficult: even relatively low-resource languages can be found in large training corpora, raising the concern that when we intend to evaluate a model’s ability to generalize to a new language, that language may have in fact been present during the model’s training.
43_42796_2	In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases—the decoy effect, the certainty effect, and the belief bias—all of which are known to influence human decision-making and reasoning.
66_42819_1	However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another.
6_42908_8	We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.
13_42989_1	The tasks include UA-CBT (inspired by the Children’s Book Test, a fill-in-the-gaps type task aimed at gauging the extent to which a story narrative is understood), UP-Titles (where the online newspaper Ukrainska Pravda‘s articles have to be matched to the correct title among 10 similar ones), and LMentry-static-UA/LMES (inspired by the LMentry benchmark, a set of tasks simple to solve for humans but hard for LMs, such as ‘which of these words is longer’ and ‘what is the fifth word of this sentence’).
9_43023_4	MBIAS is designed to significantly reduce biases and toxic elements in LLM outputs while preserving the main information.
10_43024_2	The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities.
12_43026_2	The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied.
80_43175_2	Our method uses a ViT image encoder to extract visual representations as visual tokenembeddings which are projected to the LLM space by an adapter layer and generates translation in an autoregressive fashion.
10_43257_4	This work uses linguistic examples identified in research literature to introduce a taxonomy for Algospeak and shows that with the use of an LLM (GPT-4), 79.4% of the established terms can be corrected to their true form, or if needed, their underlying associated concepts.
5_43535_2	More than 250,000 pages have been translated into English, emphasizing the potential of LLMs to cross language barriers and increase global access to Islamic knowledge.
5_43544_2	A hand-crafted selection of challenging Irish language features were incorporated into trans- lation prompts, and the output from each model was examined by a human evaluator.
17_43583_3	This framework is designed to adaptively transfer knowledge from the server’s LLM to clients’ SLMs while concurrently enhancing the LLM with clients’ unique domain insights.
20_43586_3	In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization.
39_43605_1	While extensive research has been conducted on the safety of LLMs in chat mode, the security implications of their function calling feature have been largely overlooked.
143_43709_5	Our experiments show that these errors can be identified with high accuracy by an LLM.
207_43773_1	However, for clinical diagnosis, higher expectations are required for LLM’s reliability and sensitivity: thinking like physicians and remaining sensitive to key medical information that affects diagnostic reasoning, as subtle variations can lead to different diagnosis results.
403_43969_6	Our experimental results reveal a meaningful correlation between LLM rankings on the revised benchmark and the original benchmark when these attributes are accounted for.
404_43970_1	Yet, the basic linguistic units processed in these LMs are determined by subword-based tokenization, which limits their validity as models of learning at and below the word level.
466_44032_2	Experiments are conducted with several LLMs, including proprietary GPT models and open-source models, using zero-shot prompting with adjectives that represent varying levels of semantic equivalence (e.g., “the same”) or inequivalence (e.g., “different”).
501_44067_3	Experiments were conducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with visualizations generated by OpenAI’s GPT-3.5 Turbo and Meta’s Llama 3.1 70B-Instruct models.
529_44095_3	Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs’ behavior via multi-round question answering.
548_44114_1	However, this evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs.
577_44143_4	A supervised fine-tuning (SFT) strategy is also presented to enhance the performance of LLMs, together with an algorithm for automatic dataset annotation to avoid additional manual costs.
603_44169_7	The present paper can be thought of as a pocket guide to conducting ethical research with LLMs.
693_44259_1	Evaluating the ICL ability of LLMs can enhance their utilization and deepen our understanding of how this ability is acquired at the training stage.
710_44276_1	While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators.
738_44304_5	It is developed through a two-phase training approach over a base LLaMa model.
69_44414_6	Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs.
22_44451_5	The experiments are conducted using labeled datasets that contain a mix of human-written and AI-generated reviews.
79_44507_0	Fake news and hard-to-detect AI-generated content are pressing issues in online media, which are expected to exacerbate due to the recent advances in generative AI.
79_44637_6	Different short-context models can be used effectively for token scoring, including models that are much smaller than the long-context model that is trained.
142_44696_1	However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required.
147_44700_3	These datasets are meant to reduce data contamination while providing an accurate assessment of Persian LLMs.
179_44732_5	The proposed attacker is trained within a reinforcement learning scheme with the LLM outputting probability of the target answer as the reward.
209_44761_1	However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English.
234_44786_5	In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing.
265_44816_7	Mechanistic Interpretability analysis showed that this latent behaviour of LLMs could be traced to specific neurons that became activated or amplified after PEFT.
282_44833_0	Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities.
443_44988_2	Our human-annotated test sets are created by iteratively rephrasing input texts to gradually remove explicit emotion cues (while preserving the semantic similarity and the emotions) until a strong baseline BERT model yields incorrect predictions.
464_45009_4	To address this issue, we find that the syntactic tree is minimally affected by disturbances and exhibits distinct differences between human-written and LLM-generated text.
19_45086_1	Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.
19_45214_5	Our analysis reveals that model performance is significantly affected by hallucinations in the model output, as well as by challenges imposed by the evaluation of NER output.
52_45353_1	While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.
57_45358_2	To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusions in the mathematical domain.
99_45398_3	How can we recover what training data is known to LLMs?
101_45400_1	However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied.
109_45408_2	We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency.
362_45652_4	Therefore, in this paper, we propose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs.
394_45682_5	To enhance consistency across languages, we propose novel “Compositional Representations” where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations.
411_45699_0	Although the multilingual capability of LLMs offers new opportunities to overcome the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences?
486_45771_4	All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations.
519_45801_1	However, the methods for improving LLMs are still designed by humans, which restricts the invention of new model-improving algorithms to human expertise and imagination.
600_45881_2	In this paper, we introduce a new multilingual parallel dataset SHADES to help address this issue, designed for examining culturally-specific stereotypes that may be learned by LLMs.
22_46098_1	Adapting the Bigger Analogy Test Set, we show that the linear transformation W s , where s is a middle-layer representation of a subject token and W is derived from model derivatives, can accurately reproduce final object states for many relations.
5_46134_0	This tutorial on adaptation of Large Language Models (LLMs) is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques.
43_46204_2	Unlike conventional conversational AI applications that are designed for one-to-one interactions, our bot addresses the challenges of facilitating multi-actor conversations.
7_46218_2	Experiments are run on four LLMs, two NER datasets, two input and output data formats, and ten and nine prompt versions per dataset.
67_46294_3	The LLM-generated responses were analyzed in three ways: 1) manual error analysis by a technical writer, 2) automatic assessment using deterministic metrics (BLEU, ROUGE, token overlap), and 3) evaluation of correctness by LLM as a judge.
78_46305_3	This limited-size benchmark was found to produce a robust ranking that correlates to human feedback at 𝜌 ∼ 0.8 with GPT-4 and Claude Opus models achieving the highest rankings.
153_47796_6	In model training, LMs are learned with layer-wise dropouts for better robustness.
250_48477_4	We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches.
421_48648_1	The corresponding objective function for MLE is derived from the Kullback-Leibler (KL) divergence between the empirical probability distribution representing the data and the parametric probability distribution output by the model.
96_49426_5	We introduce a novel type of LM using a modified version of bidirectional LSTM (BLSTM) called contextual BLSTM (cBLSTM), where the probability of a word is estimated based on its full left and right contexts.
86_50206_2	The resultant model can be seen as a combination of character-aware language model and simple word-level language model.
445_51276_6	A LMF compliant schema implemented in a Document Type Definition (DTD) describing the lexical resources is taken by the system to automatically configure the platform.
147_51598_4	Our experiments are conducted on Czech which is a morphologically rich language and has a considerably free word order, therefore a syntactic language model is expected to contribute positively to the unigram and bigram language model based on surface word order.
2_55900_0	This tutorial surveys neural approaches to conversational AI that were developed in the last few years.
22_57298_2	In our system, rich features are involved, including Ontology based, word embedding based, Corpus based, Alignment based and Literal based feature.
3_59508_5	We apply two different sub-approaches in the LM Based approach and the combined result of these two approaches is considered as the final output of the system.
26_59837_1	We show that ‘diagnostic classifiers’, trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented.
50_60071_3	Our system is trained on top of a pre-trained language model (LM), fine-tuned on the data provided by the task organizers.
21_60092_1	We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch.
