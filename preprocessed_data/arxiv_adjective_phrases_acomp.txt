1802.08872_948412_11	Automatic derivation of optimal features via deep learning provide the opportunity for remarkable improvements in prediction tasks where captured data are not friendly to human visual system - likely yielding sub-optimal human-designed features.
1805.06628_979673_3	More specifically, we propose a deep reinforcement learning based UAV relay scheme to help cellular systems resist smart jamming without being aware of the jamming model and the network model in the dynamic game based on the previous anti-jamming relay experiences and the observed current network status.
1812.03980_1061554_3	AI agents should be aware and follow appropriate ethical principles and should thus exhibit properties such as fairness or other virtues.
1907.01297_1145833_1	AI and Robotics research communities, industries and students are becoming increasingly aware of the problems caused by unsafe or insecure AI applications.
2005.06620_1286415_1	While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue.
2011.13416_1386164_6	To effectively assist in anticipating harmful uses, we argue that frameworks of harms must be context-aware and consider a wider range of potential stakeholders, system affordances, as well as viable proxies for assessing harms in the widest sense.
2106.13901_1491665_4	For example, AI researchers who design and develop AI models are not necessarily aware of the instability induced in consumers' lives by the compounded effects of AI decisions.
2110.11168_1549311_4	Our analysis proposes that metrics to measure the kind of explainability endorsed by the proposed AI Act shall be risk-focused, model-agnostic, goal-aware, intelligible & accessible.
2111.09509_1563612_6	We also perform extensive manual analysis showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).
2112.08718_1578494_4	Additionally, our method is deployment-friendly as the learnt domain embeddings are prefixed to the input to the model rather than changing the base model architecture.
2202.02833_1601657_2	Though healthcare systems are eager to adopt AI solutions a fundamental question remains: \textit{what happens after the AI model goes into production?}
2208.07026_1697738_1	We consider two uplink communication scenarios: (i) a double dirty MAC where the interferences are known non-causally to both users; and (ii) a single dirty MAC model where only one of the users knows the interference and the other one is not aware of the interference.
2208.12616_1703328_5	We describe how each principle has previously been operationalised, highlighting key themes that AI practitioners seeking to implement ethical principles should be aware of.
2212.06662_1762839_1	As more and more aerospace engineers are becoming aware of new trends in AI, traditional approaches are revisited to consider the applications of emerging AI technologies.
2212.09292_1765469_5	It is crucial for educators and institutions to be aware of the possibility of ChatGPT being used for cheating and to investigate measures to address it in order to maintain the fairness and validity of online exams for all students.
2301.05397_1776461_1	As these models now get increasingly better and convincingly more anthropomorphic, even some engineers have started to believe that AI might become conscious, which would result in serious social consequences.
2301.08986_1780050_1	However, existing DA-training methods are in some sense blind as they do not explicitly identify what knowledge in the LM should be preserved and what should be changed by the domain corpus.
2301.13637_1784701_2	However, AI-experts are aware of the limitations of current DNNs and have been working towards the fourth AI wave which will, arguably, rely on more biologically inspired models, predominantly on spiking neural networks (SNNs).
2301.13852_1784916_11	Using explainability, we observe that ChatGPT's writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.
2303.05398_1805195_2	To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption.
2303.09224_1809021_3	However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them.
2303.10475_1810272_2	Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system.
2303.10494_1810291_4	While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.   
2303.12003_1811800_4	Interestingly, 9.4 percent of humans were more creative than the most creative GAI, GPT-4.
2303.13712_1813509_7	We then consider a decision maker who is aware of the algorithm's manipulation and responds strategically.
2303.16421_1816218_3	(2) Is ChatGPT aware of the underlying commonsense knowledge for answering a specific question?
2304.03439_1821485_1	With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as "advanced" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks.
2304.05077_1823123_0	  We demonstrate that if consciousness is relevant for the temporal evolution of a system's states--that is, if it is dynamically relevant--then AI systems cannot be conscious.
2304.05077_1823123_2	The design and verification preclude or suppress, in particular, potential consciousness-related dynamical effects, so that if consciousness is dynamically relevant, AI systems cannot be conscious.
2304.09655_1827701_7	Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.
2304.11771_1829817_5	Finally, we provide evidence that AI assistance improves the experience of work along two key dimensions: customers are more polite and less likely to ask to speak to a manager.
2305.17006_1850062_3	In this way, the captioning model can become aware of the task goal and information need from the PLM.
2305.18465_1851521_5	We are happy to announce that all the next word prediction neural network LMs in Gboard now have DP guarantees, and all future launches of Gboard neural network LMs will require DP guarantees.
2306.01798_1854946_5	The study's findings suggest that teachers should be aware of students' purposes for prompting generative-AI tools to provide tailored instructions and scaffolded guidance.
2306.02841_1855989_4	To solve these problems, in this paper, we propose a novel framework \textbf{CTRL}, which is industrial-friendly and model-agnostic with superior inference efficiency.
2306.07005_1860153_0	  With the rapid evolution of AI Generated Content (AIGC), forged images produced through this technology are inherently more deceptive and require less human intervention compared to traditional Computer-generated Graphics (CG).
2307.01210_1872206_3	It is essential that the medical community be aware of various AI assessments and choose them considering their degrees of validity, efficiency, practicality, reliability, and accuracy concerning the early identification of patients with dementia (PwD).
2307.04964_1875959_11	Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.
2308.08708_1896600_0	  Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern.
2308.08708_1896600_5	Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.
2309.00667_1905471_1	A model is situationally aware if it's aware that it's a model and can recognize whether it's currently in testing or deployment.
2309.16583_1921387_5	Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc.
2310.03031_1925288_3	In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output.
2311.03595_1946562_2	But, unlike past technologies, generative AI is creative, cognitive, and potentially ubiquitous which makes the usual assumptions of automation predictions ill-suited for today.
2311.09579_1952546_2	We study how to better construct in-context example sets, based on whether the model is aware of the in-context examples.
2311.12990_1955957_6	Results show that GPT-4V's average scoring accuracy was mean =.51, SD = .037.
2311.17095_1960062_4	To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask.
2312.15524_1977332_1	We identify a fundamental challenge in using them to simulate experiments: when LLM-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption.
2312.15880_1977688_5	Finally, KnowledgeNavigator constructs the structured knowledge into effective prompts that are friendly to LLM to help its reasoning.
2401.01291_1980847_4	We find that use of generative AI systems is already widespread: 45% of respondents were aware of generative AI usage within their area of work, while 22% actively use a generative AI system.
2401.01623_1981179_2	In this paper, we prove in theory that AI can be as creative as humans under the condition that it can properly fit the data generated by human creators.
2401.05777_1985333_8	In general, the formal language with a lower formalization level, i.e., the more similar it is to natural language, is more friendly to LLMs.
2401.10727_1990281_4	Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.
2402.01536_1999175_2	We hypothesized that the use of an LLM as a CST might make the LLM's users feel more creative, and even broaden the range of ideas suggested by each individual user, but also homogenize the ideas suggested by different users.
2402.09733_2007372_1	This research aims to see if, how, and to what extent LLMs are aware of hallucination.
2403.05957_2023076_0	  Companies, organizations, and governments across the world are eager to employ so-called 'AI' (artificial intelligence) technology in a broad range of different products and systems.
2403.10822_2027941_3	Therefore, this study evaluates whether large language models (LLMs) are aware of medical code ontologies and can accurately generate names from these codes.
2403.11805_2028924_0	  Being more powerful and intrusive into user-device interactions, LLMs are eager for on-device execution to better preserve user privacy.
2403.13681_2030800_10	Hence, we conclude that for a strong domain-specialized generative language model (such as legal), domain specialized pretraining from scratch is more cost effective, environmentally friendly, and remains competitive with larger models or even better than adapting LLMs for legal domain tasks.
2403.17333_2034452_3	With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them.
2404.06283_2043734_1	Yet, when applied to large language models (LLMs) with extensive built-in world knowledge, this method can be deceptive.
2405.05154_2062366_1	As educators, we need to understand the potential and limitations of AI in HCI education and ensure our graduating HCI students are aware of the potential and limitations of AI in HCI.
2405.06715_2063927_2	However, whether the same strategies can help LLMs become more creative remains under-explored.
2405.19524_2076736_2	Therefore, in this paper, we advocate that stakeholders in AI risk management should be aware of the nuances, synergies, and interplay between safety and security, and unambiguously take into account the perspectives of both disciplines in order to devise mostly effective and holistic risk mitigation approaches.
2405.20806_2078018_4	With AI's increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to mitigate it, in order to ensure the beneficial use of AI for the good of humanity.
2406.16356_2094644_1	While providing instructions to LLMs for guiding their generations is user-friendly, assessing their instruction-following capabilities is still unclarified due to a lack of evaluation metrics.
2407.09517_2107904_4	Our assessment is that, while GPT-4 in its native configuration is not currently conscious, current technological research and development is sufficient to modify GPT-4 to have all the building blocks of consciousness.
2407.11789_2110176_2	We investigate the ability of LLMs to be deceptive in the context of providing assistance on a reading comprehension task, using LLMs as proxies for human users.
2407.13072_2111459_5	We welcome the ICO call for evidence on the accuracy of Generative AI, and we are happy to highlight aspects of data protection law and AI regulation that we believe should receive attention.
2408.01168_2121349_3	The paper argues that current LLM architectures are inherently untrustworthy due to their reliance on correlations of sequential patterns of word embedding vectors.
2408.04771_2124952_1	Our paper evaluates these impacts by investigating (1) the factual question of whether future advanced AI systems will be conscious, together with (2) the epistemic question of whether future human society will broadly believe advanced AI systems to be conscious.
2408.04771_2124952_6	Our analysis suggests that the worst possibility is the wrong belief that AI is non-conscious, followed by the wrong belief that AI is conscious.
2408.06546_2126727_4	We then illuminate how blind people make sense of AI through experimenting with AI VAT, employing non-visual skills, strategically including sighted people, and cross-referencing with other devices.
2408.10159_2130340_1	Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation.
2410.00359_2158560_3	The core idea of this work is to maintain states based on the LLM's response, letting the LLM become self-aware of current status and think step by step in a multi-round chain-of-thought paradigm.
2410.00423_2158624_1	However, when LLMs face different types of questions, it is worth exploring whether LLMs are aware that some questions have limited answers and need to respond more deterministically but some do not.
2410.04197_2162398_0	  Evaluating the creativity of large language models (LLMs) in story writing is difficult because LLM-generated stories could seemingly look creative but be very similar to some existing stories in their huge and proprietary training corpus.
2411.08574_2191001_8	This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications.
2411.09102_2191529_2	Without explicit interrogation of these benefits by AI developers, as a community we may remain blind to the immensity of systemic change that is needed as well.
2412.04571_2206949_3	Here we employ Integrated Information Theory (IIT), which provides principled tools to determine whether a system is conscious, to what degree, and the content of its experience.
2412.09632_2212010_4	The second method, an information leakage study, seeks to ascertain whether LLMs are aware of the information held in the datasets published on the UK government's open data initiative data$.$gov$.$uk.
2501.05454_2229038_1	Today's AI systems consistently state, "I am not conscious."
2501.13533_2237117_7	AI agents are often assumed to pursue fixed goals, but AI persons may be self-aware enough to reflect on their aims, values, and positions in the world and thereby induce their goals to change.
2502.05007_2247998_1	We propose a number of metrics for examining whether an advanced AI system has gained consciousness, while emphasizing that we do not claim all AI stems can become conscious.
2503.04103_2268415_6	User evaluation shows that such an environment allowed users to stay oriented in their creation activity, remain aware and in control of AI's generation, and enable flexible human-AI collaborative workflows.
2503.07556_2271868_12	Critically, they are not just experiencing the benefits of adopting LLM tools, but they are also aware of at least a few LLM limitations, such as the generation of wrong suggestions, potential data leaking, and AI hallucination.
2504.12320_2301023_1	However, it remains unclear whether LLMs have become more creative over time, and how consistent their creative output is.
2504.17393_2306096_7	Furthermore, user-friendly human interaction with the system is essential for its adoption and some of the participants confirmed they would be happy to be in the loop and provide necessary feedback that the system can learn from.
2504.19674_2308377_6	It utilizes adversarial user models that are system-aware and have unique personalities, enabling a holistic red-teaming evaluation.
2504.20980_2309683_6	It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''
2505.08204_2318764_2	However, if the LM integration is insecure, attackers can bypass these restrictions and gain unrestricted access to the LM, potentially harming developers' reputations and leading to significant financial losses.   
