1003.1802_177609_3	This is a trade-off between accuracy and robustness but, for various countries present in the Human Mortality Database and compared to various models, the model accurately fits past mortality data and gives good backtesting projections.
1106.5917_271552_3	In this paper, we present a simple series based model for implementation of human-like intuition using the principles of connectivity and unknown entities.
1209.4838_372002_2	The biggest problem in this definition was that the level of intelligence of AI is compared to the intelligence of a human being.
1404.6384_519437_3	As a proof of concept, the system was built and tested in a pilot experiment, in which cats were trained to press three buttons differently in response to three different sounds (human speech) to receive food rewards.
1503.03445_606159_1	This model incorporates the biology of the urban vector of yellow fever, the mosquito Aedes aegypti, the stages of the disease in the human being as well as the spatial extension of the epidemic outbreak.
1504.05696_617601_4	As for human-level AI, we must await its development before we can decide whether or not to ascribe consciousness to it.
1510.08897_672854_1	Such an automated system is crucial for deriving insights from complex datasets found in many big data applications such as scientific and healthcare applications as well as for reducing the human effort of data exploration.
1512.00977_683629_1	This article analyzes and evaluates the challenges that the AI development level is facing, and proposes that the evaluation methods for the human intelligence test and the AI system are not uniform; and the key reason for which is that none of the models can uniformly describe the AI system and the beings like human.
1512.00977_683629_2	Aiming at this problem, a standard intelligent system model is established in this study to describe the AI system and the beings like human uniformly.
1601.06069_698097_1	We explore these challenges and potential contributions of knowledge-based tools using as an example the CADET system, a knowledge-based tool capable of producing automatically (or with human guidance) battle plans with realistic degree of detail and complexity.
1605.02097_730212_8	The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.
1609.04879_770357_7	Originally developed as part of the thesis "NPCs as People: Using Databases and Behaviour Trees to Give Non-Player Characters Personality," Extreme AI is now a fully functioning personality engine using all thirty facets of the Five Factor model of personality and an AI system that is live throughout gameplay.
1702.05663_820257_1	Although state-of-the-art deep learning models for video game tasks generally rely on more complex methods such as deep-Q learning, we show that a supervised model which requires substantially fewer resources and training time can already perform well at human reaction speeds on the N64 classic game Super Smash Bros.
1704.00717_835229_4	The latter involves making AI more human-like and having it develop a theory of our minds.
1705.08807_852348_3	Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053).
1706.03122_857857_1	The natural starting point for AI researchers interested in MOBAs is to develop an AI to play the game better than a human - but MOBAs have many more challenges besides adversarial AI.
1707.09095_874127_0	  Artificial General Intelligence (AGI) or Strong AI aims to create machines with human-like or human-level intelligence, which is still a very ambitious goal when compared to the existing computing and AI systems.
1708.05122_880134_0	  As AI continues to advance, human-AI teams are inevitable.
1710.04459_899842_5	For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4% of system disengagements that were labeled by human annotators as challenging and needing human supervision.
1710.08191_903574_8	In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems.
1711.05905_912915_1	As an application, we demonstrate how human value can be obtained from the experimental game theory (e.g., trust game experiment) so as to build an ethical AI.
1712.06440_925039_4	The Service IQ and Value IQ of intelligence systems are used to answer the question of how the intelligent products can better serve the human, reflecting the intelligence and required cost of each intelligence system as a product in the process of serving human.
1801.03604_932535_2	To advance the state of the art in conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar university competition where sixteen selected university teams were challenged to build conversational agents, known as socialbots, to converse coherently and engagingly with humans on popular topics such as Sports, Politics, Entertainment, Fashion and Technology for 20 minutes.
1801.09854_938785_0	  Effective collaboration between humans and AI-based systems requires effective modeling of the human in the loop, both in terms of the mental state as well as the physical capabilities of the latter.
1803.03407_953540_0	  Artificial Intelligence (AI) started out with an ambition to reproduce the human mind, but, as the sheer scale of that ambition became manifest, it quickly retreated into either studying specialized intelligent behaviours, or proposing over-arching architectural concepts for interfacing specialized intelligent behaviour components, conceived of as agents in a kind of organization.
1803.10813_960946_1	Robotics and AI amplify human potentials, increase productivity and are moving from simple reasoning towards human-like cognitive abilities.
1804.07819_969512_5	By removing humans from the main learning loop, our approach also allows more effective scaling of AI and cognitive capabilities to provide (1) broader coverage in a single domain such as health or geology; and (2) more rapid deployment to new domains.
1805.00899_973944_8	Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.
1806.02421_988061_0	  An Artificial Intelligence (AI) system is an autonomous system which emulates human mental and physical activities such as Observe, Orient, Decide, and Act, called the OODA process.
1806.10698_996338_7	In addition, we found that the triage advice recommended by the AI System was, on average, safer than that of human doctors, when compared to the ranges of acceptable triage provided by independent expert judges, with only a minimal reduction in appropriateness.
1808.00089_1009235_0	  A new wave of decision-support systems are being built today using AI services that draw insights from data (like text and video) and incorporate them in human-in-the-loop assistance.
1808.01884_1011030_1	Through AI developments, machines are now given power and intelligence to behave and work like human mind.
1809.06800_1026816_2	However, visual co-articulation effects in visual speech signals damage the performance of visual speech LM's as visually, people do not utter what the language model expects.
1809.07193_1027209_3	SC2LE provides a few mini games such as MoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents have achieved the performance level of human professional players.
1809.07842_1027858_0	  As Artificial Intelligence (AI) technologies proliferate, concern has centered around the long-term dangers of job loss or threats of machines causing harm to humans.
1811.01267_1045900_5	They make normativity more legible for humans, and can increase legibility for AI systems as well.
1811.10840_1055473_1	In high risk applications, the combined human plus AI system must function as a high-reliability organization in order to avoid catastrophic errors.
1811.12185_1056818_0	  In this paper authors are going to present a Markov Decision Process (MDP) based algorithm in Industrial Internet of Things (IIoT) as a safety compliance layer for human in loop system.
1812.00136_1057710_10	Rather than brain-like intelligence, the TCR indeed advocates a promising change in direction to realize true AI, i.e. artificial general intelligence or artificial consciousness, particularly different from humans' and animals'.
1812.10900_1068474_0	  One of the biggest challenges that artificial intelligence (AI) research is facing in recent times is to develop algorithms and systems that are not only good at performing a specific intelligent task but also good at learning a very diverse of skills somewhat like humans do.
1901.00064_1069611_3	We argue that this is a practical problem for any machine learning system (such as medical decision support systems or autonomous weapons) or rigidly rule-based bureaucracy that will make high stakes decisions about human lives: such systems should not use objective functions in the strict mathematical sense.   
1903.02409_1094795_5	We formalize the model using the agent dialogue framework (ADF) as a new dialogue type and then evaluate it in a human-agent interaction study with 101 dialogues from 14 participants.
1903.03171_1095557_4	We discuss three factors which hinder discussion and obscure attempts to form a clear ontology of AI: (1) the various and evolving definitions of AI, (2) the tendency for pre-existing technologies to be assimilated and regarded as "normal," and (3) the tendency of human beings to anthropomorphize.
1905.00547_1118983_6	This article is aimed at people who understand the basics of AI (especially ANNs), and would like to be better able to evaluate the often wild claims about the value of biomimicry in AI.
1905.04994_1123430_0	  Artificial Intelligence (AI) applications are being used to predict and assess behaviour in multiple domains, such as criminal justice and consumer finance, which directly affect human well-being.
1905.10985_1129421_0	  Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans.
1905.10985_1129421_11	Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.
1906.07307_1139493_10	As a preliminary study, although we have not found incorporating BERT into Tacotron-2 generates more natural or cleaner speech at a human-perceivable level, we observe improvements in other aspects such as the model is being significantly better at knowing when to stop decoding such that there is much less babbling at the end of the synthesized audio and faster convergence during training.
1907.00430_1144966_2	Given the urgency to identify systematic solutions, we postulate that it might be useful to start with the simple fact that for the utility function of an AI not to violate human ethical intuitions, it trivially has to be a model of these intuitions and reflect their variety $ - $ whereby the most accurate models pertaining to human entities being biological organisms equipped with a brain constructing concepts like moral judgements, are scientific models.
1907.02227_1146763_1	Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities.
1907.10424_1154960_5	Second, we briefly sketch what we refer to as a case of AI with humans and for humans, namely an AI paradigm whereby the systems we build are privacy-oriented and focused on human-machine collaboration, not competition.
1907.13528_1158064_2	As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inferences and role-based event prediction -- and in particular, it shows clear insensitivity to the contextual impacts of negation.
1908.07587_1165757_1	This research raises questions about both the technical details of automatic art generation and the interaction between AI and people, as both artists and the audience of art.
1909.04812_1174850_6	AI-HRI deals with the challenge of interacting with humans in environments that are relatively unstructured or which are structured around people rather than machines, as well as the possibility that the robot may need to interact naturally with people rather than through teach pendants, programming, or similar interfaces.
1909.06904_1176942_4	Specifically, we show how time-based AI created media can be used to explore the nature of the dual-pathway neuro-architecture of the human visual system and how this relates to higher cognitive judgments such as aesthetic experiences that rely on these divergent information streams.
1909.08258_1178296_1	Our research is focused on making a human-like question answering system which can answer rationally.
1910.03515_1187434_4	There are many discussions in the AI field about ethics and trust, but there are few frameworks available for people to use as guidance when creating these systems.
1910.03515_1187434_9	Human-machine teams are strongest when human users can trust AI systems to behave as expected, safely, securely, and understandably.
1910.07089_1191008_1	Now, as AI technologies enter our everyday lives at an ever increasing pace, there is a greater need for AI systems to work synergistically with humans.
1910.12583_1196502_2	Solidarity as an AI principle (1) shares the prosperity created by AI, implementing mechanisms to redistribute the augmentation of productivity for all; and shares the burdens, making sure that AI does not increase inequality and no human is left behind.
1910.12583_1196502_3	Solidarity as an AI principle (2) assesses the long term implications before developing and deploying AI systems so no groups of humans become irrelevant because of AI systems.
1910.12583_1196502_4	Considering solidarity as a core principle for AI development will provide not just an human-centric but a more humanity-centric approach to AI.
1911.01917_1200511_2	Artificially intelligent reasoners, however, currently lack the ability to carry out human-like interpretive reasoning, and we argue that bridging this gulf is of tremendous importance to human-centered AI.
1911.10154_1208748_0	  Traditionally, the way one evaluates the performance of an Artificial Intelligence (AI) system is via a comparison to human performance in specific tasks, treating humans as a reference for high-level cognition.
1912.03652_1215548_2	AI systems are optimized for objectives such as minimum computation or minimum error rate in recognizing and interpreting inputs from humans.
1912.03652_1215548_4	We investigate how inputs of humans can be altered to reduce misinterpretation by the AI system and to improve efficiency of input generation for the human while altered inputs should remain as similar as possible to the original inputs.
1912.03652_1215548_6	To create examples that serve as demonstrations for humans to improve, we develop a model based on a conditional convolutional autoencoder (CCAE).
1912.05284_1217180_2	We argue for formulating human--AI interaction as a multi-agent problem, endowing AI with a computational theory of mind to understand and anticipate the user.
1912.11095_1222991_6	We point out that definitions adhering closely to the functionality of AI systems are more inclusive of technologies in use today, whereas definitions that emphasize human-like capabilities are most applicable to hypothetical future technologies.
1912.11595_1223491_0	  As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity.
2001.01172_1226585_4	However, in prior decades, other areas of image processing have moved beyond simpler models like Mean Squared Error (MSE) towards more complex models that better approximate the Human Visual System (HVS).
2001.02114_1227527_2	We refer to these scenarios as AI-assisted decision making, where the individual strengths of the human and the AI come together to optimize the joint decision outcome.
2001.05375_1230788_4	While the focus of this symposium was on AI systems to improve data quality and technical robustness and safety, we welcomed submissions from broadly defined areas also discussing approaches addressing requirements such as explainable models, human trust and ethical aspects of AI.
2001.08298_1233711_2	We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance.
2001.08298_1233711_5	Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.
2001.09766_1235179_3	We also discovered different effects between assessments presented as being from human experts and assessments presented as being from AI.
2002.01092_1238515_1	In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design.
2002.01621_1239044_3	We argue that because different fairness criteria sometimes cannot be simultaneously satisfied, and because achieving fairness often requires sacrificing other objectives such as model accuracy, it is key to acquire and adhere to human policy makers' preferences on how to make the tradeoff among these objectives.
2002.03024_1240447_1	This suggests humans engage in cognitive anthropomorphism: expecting AI to have the same nature as human intelligence.
2002.05149_1242572_0	  The ability to explain decisions made by AI systems is highly sought after, especially in domains where human lives are at stake such as medicine or autonomous vehicles.
2002.05657_1243080_2	In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems.
2002.05671_1243094_9	As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future.
2002.08320_1245743_2	The workshop will address applicable areas of AI, such as machine learning, game theory, natural language processing, knowledge representation, automated and assistive reasoning and human machine interactions.
2002.11174_1248597_4	In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition.
2003.01525_1251880_0	  As Artificial Intelligence (AI) technology gets more intertwined with every system, people are using AI to make decisions on their everyday activities.
2003.01525_1251880_1	In simple contexts, such as Netflix recommendations, or in more complex context like in judicial scenarios, AI is part of people's decisions.
2003.02093_1252448_0	  As Artificial Intelligence (AI) plays an ever-expanding role in sociotechnical systems, it is important to articulate the relationships between humans and AI.
2003.02093_1252448_5	As an extension to Social Exchange Theory (SET) in the social sciences, AI-MET views AI as influencing human-to-human relationships via a taxonomy of mediation mechanisms.
2003.06769_1257124_3	Here we use an AI (artificial intelligence) algorithm based on Markov Models of one fixed memory length (abbreviated as "single AI") to compete against humans in an iterated RPS game.
2003.06769_1257124_4	We model and predict human competition behavior by combining many Markov Models with different fixed memory lengths (abbreviated as "multi-AI"), and develop an architecture of multi-AI with changeable parameters to adapt to different competition strategies.
2003.13689_1264044_1	[Abridged] Naturally produced droplets from humans (such as those produced by breathing, talking, sneezing, and coughing) include several types of cells (e.g., epithelial cells and cells of the immune system), physiological electrolytes contained in mucous and saliva (e.g. Na+, K+, Cl-), as well as, potentially, several infectious agents (e.g. bacteria, fungi, and viruses).
2004.06894_1271664_3	From a specific scenario, we present an experimental procedure to collect and assess human-generated verbal interpretations of AI-generated music theory/rules rendered as sophisticated symbolic/numeric objects.
2004.06894_1271664_5	We treat this as a first step towards 1) better design of AI representations that are human interpretable and 2) a general methodology to evaluate interpretability of AI-discovered knowledge representations.
2004.09044_1273814_5	We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense.
2004.12463_1277233_6	In this study, we highlighted the power of AI in the containment and mitigation of the spread of COVID-19 outbreak in African countries such as Nigeria where human to human contact is apparently inevitable.
2004.13102_1277872_8	Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes.
2004.13102_1277872_9	We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration.
2004.13563_1278333_0	  With 5G cellular systems being actively deployed worldwide, the research community has started to explore novel technological advances for the subsequent generation, i.e., 6G. It is commonly believed that 6G will be built on a new vision of ubiquitous AI, an hyper-flexible architecture that brings human-like intelligence into every aspect of networking systems.
2004.14253_1279023_6	Human evaluation is performed over a sentence completion task, where GePpeTto's output is judged as natural more often than not, and much closer to the original human texts than to a simpler language model which we take as baseline.
2004.14614_1279384_3	This problem can be detrimental when applying a dialogue model like this chatting online with unconstrained people and topics, because the model does not have the needed knowledge.
2005.00683_1280478_5	Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs 96.3% in accuracy).
2005.05538_1285333_4	It is of utmost importance that artificial intelligent agents have their values aligned with human values, given the fact that we cannot expect an AI to develop our moral preferences simply because of its intelligence, as discussed in the Orthogonality Thesis.
2005.12137_1291932_4	This is likely to continue as the impact of the pandemic unfolds on the world's people, industries and economy but a surprising observation on the current pandemic has been the limited impact AI has had to date in the management of COVID-19.
2005.13275_1293070_5	Indeed, we call for a better interplay between Knowledge Representation and Reasoning, Social Sciences, Human Computation and Human-Machine Cooperation research -- as already explored in other AI branches -- in order to support the goal of eXplainable AI with the adoption of a Human-in-the-Loop approach.
2006.00904_1295418_3	This paper presents a system that largely automates these tasks and enables dynamic overlays using deep learning to track the drivers as they move on screen, without human intervention.
2006.04948_1299462_0	  Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species.
2006.11194_1305708_1	Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation.
2007.02149_1313663_0	  In this work, we propose an AI-based technique using freely available satellite images like Landsat and Sentinel to create natural features over OSM in congruence with human editors acting as initiators and validators.
2007.07598_1319112_3	An end-to-end system model which considers the pathogen-laden cough/sneeze droplets as the input and the infection state of the human as the output is proposed.
2007.07598_1319112_4	This model uses the gravity, initial velocity and buoyancy for the propagation of droplets and a receiver model which considers the central part of the human face as the reception interface is proposed.
2008.04073_1331795_4	Other AI Safety issues like trade-offs between privacy and security or convenience, bad actors hacking into AI systems to create mayhem or bad actors deploying AI for purposes harmful to humanity and are out of scope of our discussion.
2008.04162_1331884_0	  Modern natural language models such as the GPT-2/GPT-3 contain tremendous amounts of information about human belief in a consistently testable form.
2008.10244_1337966_0	  Canine mammary carcinoma (CMC) has been used as a model to investigate the pathogenesis of human breast cancer and the same grading scheme is commonly used to assess tumor malignancy in both.
2009.05841_1347344_3	As a model of human passage, a vertical cylinder is pulled through a planar jet representing an air curtain and separating two zones at different densities.
2009.08922_1350425_1	Recent progress in Game AI has demonstrated that given enough data from human gameplay, or experience gained via simulations, machines can rival or surpass the most skilled human players in classic games such as Go, or commercial computer games such as Starcraft.
2009.09266_1350769_1	The AI community typically treats human inputs as a given and optimizes AI models only.
2009.10589_1352092_5	However, the transition to adopting thermal imagery as a source for any human-centered AI task is not easy and relies on the availability of high fidelity data sources across multiple demographics and thorough validation.
2009.13996_1355499_6	The notions of Contextual Importance and Utility (CIU) presented in this paper make it possible to produce human-like explanations of black-box outcomes directly, without creating an interpretable model.
2010.05990_1362320_10	A highly-performing model allows the intelligent system to interpret human commands at the social-interaction level through a chatbot-like interface (e.g. "Robot, can we have a conversation?") and allows for better accessibility to AI by non-technical users.
2010.06002_1362332_3	We hope that the high-level description of our vision included in this paper, as well as the several research questions that we propose to consider, can stimulate the AI research community to define, try and evaluate new methodologies, frameworks, and evaluation metrics, in the spirit of achieving a better understanding of both human and machine intelligence.
2010.09890_1366220_1	In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently.
2010.09890_1366220_2	To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration).
2010.09890_1366220_4	We evaluate the performance of AI agents with the human-like agent as well as with real humans using objective metrics and subjective user ratings.
2011.02787_1375535_3	Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, including: AI and society, bias and algorithmic justice, disinformation, humans and AI, labor impacts, privacy, risk, and future of AI ethics.   
2011.03195_1375943_6	Medical diagnosis model is responsible for human life and we need to be confident enough to treat a patient as instructed by a black-box model.
2011.05808_1378556_2	The model includes a specific neural network which will be first trained to learn the correlation between selected inputs, related to the case of interest: environmental variables (chemical-physical, such as meteorological), human activity (such as traffic and crowding), level of pollution (in particular the concentration of particulate matter), and epidemiological variables related to the evolution of the contagion.
2011.14039_1386787_1	In this work, we re-purpose a sentence editing dataset, where faithful high-quality human rationales can be automatically extracted and compared with extracted model rationales, as a new testbed for interpretability.
2011.14039_1386787_3	The investigation generates new insights, for example, contrary to the common understanding, we find that attention weights correlate well with human rationales and work better than gradient-based saliency in extracting model rationales.
2012.05628_1393506_7	Though on average these sentences are still identifiable as artificial by humans, they are assessed on par with sentences generated by a GPT-2 model fully trained from scratch.
2012.06034_1393912_9	While many cases involving cooperation between humans and AIs will be asymmetric, with the human ultimately in control, AI systems are growing so complex that, even today, it is impossible for the human to fully comprehend their reasoning, recommendations, and actions when functioning simply as passive observers.
2012.08174_1396052_5	Regarding pivotal novelties, the designed cognitive architecture a) introduces a new FL-based formalism that extends the conventional LfD learning paradigm to support large-scale multi-agent operational settings, b) elaborates previous FL-based self-learning robotic schemes so as to incorporate the human in the learning loop and c) consolidates the fundamental principles of FL with additional sophisticated AI-enabled learning methodologies for modelling the multi-level inter-dependencies among the robotic tasks.
2012.08713_1396591_1	Because of the adverse effects that the crimes can have on human life, economy and safety, we need a model that can predict future occurrence of crime as accurately as possible so that early steps can be taken to avoid the crime.
2012.09077_1396955_5	In an explainable AI perspective, we explore simple classification techniques such as Decision Trees and Classification Rules because they provide human-readable models, which explain the classification itself, but may also provide elements of explanation for molecular mechanisms behind ADRs.
2101.01524_1405266_4	Despite these tensions, all participants expressed positive attitudes toward the future of AI-CDSS, especially acting as "a doctor's AI assistant" to realize a Human-AI Collaboration future in clinical settings.
2101.01588_1405330_5	To address these challenges and reduce human intervene, Artificial Intelligence (AI) has been widely recognized and acknowledged as the only solution.
2101.02327_1406069_1	Following one important implication of Turing Test for enabling a machine with a human-like behavior or performance, we define human-like robustness (HLR) for AI machines.
2101.02327_1406069_10	The present paper shows an attempt to learn and explore further from Turing Test towards the design of human-like AI machines.
2101.04617_1408359_3	We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.
2101.04796_1408538_1	CSC apps provide potential diagnoses for users and assist them with self-triaging based on Artificial Intelligence (AI) techniques using human-like conversations.
2101.05303_1409045_1	Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance).
2101.06220_1409962_4	Our core finding is that AI as play can expand current notions of human-AI interaction, which are predominantly productivity-based.
2101.09429_1413171_2	We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium.
2102.00625_1417114_2	Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task.
2102.02842_1419331_4	Such forecasts may be considered "human-expert" forecasts and do not qualify as AI/ML approaches, although they can be used as an indicator of human expert performance.
2102.03406_1419895_7	This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.
2102.06391_1422880_0	  The virtuosity of language models like GPT-3 opens a new world of possibility for human-AI collaboration in writing.
2102.07536_1424025_6	Testing human behaviour in interaction with actual AI outputs, we provide first behavioural insights into the role of AI as an advisor.
2102.07536_1424025_8	In fact, AI's corrupting force is as strong as humans'.
2103.05434_1435579_4	Moreover, individuals may have different expectations regarding the invitation of trust and propensity to lie for human vs. AI negotiators, and these expectations may vary across cultures as well.
2103.06769_1436914_1	This paper outlines a perspective on the future of AI, discussing directions for machines models of human-like intelligence.
2103.07637_1437782_0	  We introduce AIR4Children, Artificial Intelligence for Children, as a way to (a) tackle aspects for inclusion, accessibility, transparency, equity, fairness and participation and (b) to create affordable child-centred materials in AI and Robotics (AIR).
2103.08079_1438224_3	We define this as the state that embodied AI "circumstantially" take on within interactive contexts when perceived as both social and agentic by people.
2103.09990_1440135_1	By highlighting various aspects of works on the human-AI team such as the flow of complementing, task horizon, model representation, knowledge level, and teaming goal, we make a taxonomy of recent works according to these dimensions.
2103.11436_1441581_3	Among variety of applications of AI, facial expression recognition might not be the most important one, yet is considered as a valuable part of human-AI interaction.
2103.11790_1441935_3	While this is well established, we show that recent LMs also contain human-like biases of what is right and wrong to do, some form of ethical and moral norms of the society -- they bring a "moral direction" to surface.
2103.13520_1443665_3	Now we see a strong role and resurgence of knowledge fueling major breakthroughs in the third wave of AI underpinning future intelligent systems as they attempt human-like decision making, and seek to become trusted assistants and companions for humans.
2103.13520_1443665_6	We will draw a parallel with the role of knowledge and experience in human intelligence based on cognitive science, and discuss emerging neuro-symbolic or hybrid AI systems in which knowledge is the critical enabler for combining capabilities of the data-intensive statistical AI systems with those of symbolic AI systems, resulting in more capable AI systems that support more human-like intelligence.
2103.15004_1445149_6	The article's contribution is twofold: First, it provides a theoretical treatment and model of human-AI interaction based on an XR-AI continuum as a framework for and a perspective of different approaches of XR-AI combinations.
2103.15004_1445149_7	It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces.
2103.15294_1445439_0	  AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, "Starcraft" and poker), and protein structure prediction.
2103.15990_1446135_0	  With the rapid development of the internet of things (IoT) and artificial intelligence (AI) technologies, human activity recognition (HAR) has been applied in a variety of domains such as security and surveillance, human-robot interaction, and entertainment.
2104.01940_1449358_2	Here we study using such LMs to fill in entities in human-authored comparative questions, like ``Which country is older, India or __
2104.02871_1450289_2	To collaborate seamlessly with humans, AI agents should adapt quickly to new partners and new tasks as well.
2104.05188_1452606_2	As a result, AI hypotheses are designed to substitute for human experts, failing to complement them for punctuated collective advance.
2104.05277_1452695_1	We conduct a human evaluation pilot study that indicates the model is often able to respond to conversations in both a human-like and informative manner, on a diverse set of topics.
2104.07598_1455016_0	  In two experiments (total N=693) we explored whether people are willing to consider paintings made by AI-driven robots as art, and robots as artists.
2104.10661_1458079_2	Although the model cannot replace the work of psychotherapists entirely, its ability to synthesize human-appearing utterances for the majority of the test set serves as a promising step towards communizing and easing stigma at the psychotherapeutic point-of-care.
2104.12145_1459563_7	This paper shows that LLMs, such as ChatGPT, can guide the nanophotonic design and optimization processes, on both the conceptual and technical level, and we propose new human-AI co-design strategies and show their practical implications.
2104.12571_1459989_5	By framing the end-to-end virus transmission as a Molecular Communications (MC) system, we obtain a statistical description of the number of IAs inhaled by the healthy person subject to the respective configurations of the face masks of both persons.
2104.12582_1460000_2	In addition, we also use AI safety principles to quantify the unique risks of increased intelligence and human-like qualities in AI.
2104.12871_1460289_1	Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected.
2104.13155_1460573_3	As a watershed for the branches of AI, some classification standards and methods are discussed: 1) Human-oriented, machine-oriented, and biological-oriented AI R&D; 2) Information input processed by Dimensionality-up or Dimensionality-reduction; 3) The use of one/few or large samples for knowledge learning.
2105.00060_1462619_3	The contribution of AI to IVF is potentially significant, but must be done carefully and transparently, as the ethical issues are significant, in part because this field involves creating new people.
2105.00832_1463391_0	  The Random Language Model, proposed as a simple model of human languages, is defined by the averaged model of a probabilistic context-free grammar.
2105.01774_1464333_2	We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity.
2105.04408_1466967_6	Hence, a new acceptance model of RAS is provided, as a framework for requirements to human-centered AI and for implementing trustworthy RAS by design.
2105.07879_1470438_2	In turn, these capabilities have made AI a viable replacement for many human activities that at their core involve classification, such as basic mechanical and analytical tasks in low-level service jobs.
2105.07879_1470438_4	However, for AI to progress to more complicated tasks requiring intuition and empathy, it must develop capabilities such as metathinking, creativity, and empathy akin to human self-awareness or consciousness.
2105.08475_1471034_2	This analysis enables ethically-minded companies creating or deploying AI systems as well as researchers and policymakers to take into account the effects of their actions on labor markets and inequality, and therefore to steer progress in AI in a direction that advances shared prosperity and an inclusive economic future for all of humanity.
2105.09060_1471619_3	Through research and article summaries, as well as expert commentary, this report distills the research and reporting surrounding various domains related to the ethics of AI, with a particular focus on four key themes: Ethical AI, Fairness & Justice, Humans & Tech, and Privacy.   
2105.10614_1473173_1	Most research on algorithmic decision-making solely centers on the algorithm's performance, while recent work that explores human-machine collaboration has framed the decision-making problems as classification tasks.
2105.11000_1473559_1	This paper outlines an experimental research study that investigates essential aspects of human-AI teaming such as team performance, team situation awareness, and perceived team cognition in various mixed composition teams (human-only, human-human-AI, human-AI-AI, and AI-only) through a simulated emergency response management scenario.
2105.11000_1473559_3	Performance metrics like team situational awareness and team score showed that teams composed of all human participants performed at a lower level than mixed human-AI teams, with the AI-only teams attaining the highest performance.
2105.13818_1476377_0	  We investigate the semantic knowledge of language models (LMs), focusing on (1) whether these LMs create categories of linguistic environments based on their semantic monotonicity properties, and (2) whether these categories play a similar role in LMs as in human language understanding, using negative polarity item licensing as a case study.
2106.08761_1486525_0	  As machine learning approaches are increasingly used to augment human decision-making, eXplainable Artificial Intelligence (XAI) research has explored methods for communicating system behavior to humans.
2106.09140_1486904_14	We suggest that the application of Grice's Cooperative Principles to human-AI interactions is beneficial both from an AI development perspective and as a tool for describing an emerging form of interaction.
2106.11521_1489285_1	AI research often falls outside the purview of existing feedback mechanisms such as the Institutional Review Board (IRB), which are designed to evaluate harms to human subjects rather than harms to human society.
2107.01820_1495832_1	AI systems are able to diagnose such data with almost the same accuracy as human experts.
2107.04022_1498034_2	Ethical behaviour is a critical characteristic that we would like in a human-centric AI.
2107.05704_1499716_3	We articulate this new agenda for AI fairness for people with disabilities, explaining how combining data protection and equality law creates new opportunities for disabled people's organisations and assistive technology researchers alike to shape the use of AI, as well as to challenge potential harmful uses.
2107.06641_1500653_2	However, recent research and AI applications show that AI can cause unintentional harm to humans, such as making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against one group.
2107.07015_1501027_0	  In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user.
2107.07630_1501642_4	In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate.
2107.14070_1508082_4	However, unfortunately this system has several caveats when considered on a large scale such as unavailability of sufficient trained people, lack of accurate information, failure to convey the richness of details in an attractive format etc.
2108.01174_1509989_8	To understand and validate an AI system's outcomes (such as classification, recommendations, predictions), that lead to developing trust in the AI system, it is necessary to involve explicit domain knowledge that humans understand and use.
2108.07732_1516547_10	We find that natural language feedback from a human halves the error rate compared to the model's initial prediction.
2108.07856_1516671_6	Results: The proposed system, demonstrated significantly improved mitotic counting performance for 41 cancer cases across 14 cancer types compared to human expert baselines.
2108.11844_1520659_3	The paper is part of the research project "ExamAI - Testing and Auditing of AI systems" and focusses on the use of AI in an industrial production environment as well as in the realm of human resource management (HR).
2108.12166_1520981_2	We used the research project as a test-bed to demonstrate in a complex real-world environment the importance and suitability of the nine UNICEF guidelines on AI for Children.
2108.12166_1520981_3	The UNICEF guidelines on AI for Children closely align with several of the UN goals for sustainable development, and, as such, we report here our contribution to these goals.
2109.05327_1528148_0	  Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems.
2109.09586_1532407_4	We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education level of the person to whom the explication is intended for.
2109.09904_1532725_2	There are often two independent motivations (i) symbols as a lingua franca for human-AI interaction and (ii) symbols as system-produced abstractions used by the AI system in its internal reasoning.
2109.09904_1532725_5	Symbols, like emotions, may well not be sine qua non for intelligence per se, but they will be crucial for AI systems to interact with us humans -- as we can neither turn off our emotions nor get by without our symbols.
2109.13827_1536648_2	In particular, the deskilling of knowledge workers is a major issue, as they are the same people who should also train, challenge and evolve AI.
2109.15193_1538014_1	However, people usually treat AI as a tool, focusing on improving outcome, accuracy, and performance while paying less attention to the representation of AI itself.
2110.01029_1539172_0	  Project Debater was revealed in 2019 as the first AI system that can debate human experts on complex topics.
2110.01387_1539530_5	Our model is enabled by three innovations: (a) flexible knowledge transfer between experimental processes by incorporating data from prior experimental data as a probabilistic constraint; (b) incorporation of both subjective human observations and ML insights when selecting next experiments; (c) adaptive strategy of locating the region of interest using Bayesian optimization first, and then conducting local exploration for high-efficiency devices.
2110.04249_1542392_2	Furthermore, as trending research areas, computational pain recognition and empathic artificial intelligence (AI) show progress and promise for healthcare or human-computer interaction.
2110.07356_1545499_3	We utilize GPT-3 as the backbone of our algorithm and scale 210 human labeled examples to yield results comparable to using 6400 human labeled examples (~30x) leveraging low-shot learning and an ensemble method.
2110.08637_1546780_1	As a result of the conceptual modeling process a human interpretable, formalized representation (i.e., a conceptual model) is derived which enables understanding and communication among humans, and processing by machines.
2110.11385_1549528_0	  As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can learn by themselves in a self-motivated and self-supervised manner rather than being retrained periodically on the initiation of human engineers using expanded training data.
2110.14746_1552889_3	In this study, we introduce MutFormer, a transformer-based model for the prediction of deleterious missense mutations, which uses reference and mutated protein sequences from the human genome as the primary features.
2111.01726_1555829_9	Instruction from AI such as this functions both to help humans perform better at tasks, but also to better understand, anticipate, and correct the actions of an AI.
2111.02603_1556706_3	Drawing from experiments that study inductive reasoning, I propose to analyze semantic inductive generalization in LMs using phenomena observed in human-induction literature, investigate inductive behavior on tasks such as implicit reasoning and emergent feature recognition, and analyze and relate induction dynamics to the learned conceptual representation space.
2111.02626_1556729_0	  Explainable AI (XAI) is a promising means of supporting human-AI collaborations for high-stakes visual detection tasks, such as damage detection tasks from satellite imageries, as fully-automated approaches are unlikely to be perfectly safe and reliable.
2111.04920_1559023_7	Our user study shows that people found twice as many blend suggestions as they did without the system, and with half the mental demand.
2111.06908_1561011_8	By better understanding how predictive models work in general as well as how they derive an outcome for a particular person, XAI promotes accountability in a world in which AI impacts the lives of billions of people around the world.
2111.07631_1561734_1	As a recognized standard for testing artificial intelligence, various human-computer gaming AI systems (AIs) have been developed such as the Libratus, OpenAI Five and AlphaStar, beating professional human players.
2111.08222_1562325_5	Our findings suggest that (1) factors receiving significant attention, such as interpretability, may be less effective at increasing trust than factors like outcome feedback, and (2) augmenting human performance via AI systems may not be a simple matter of increasing trust in AI, as increased trust is not always associated with equally sizable improvements in performance.
2111.08460_1562563_4	HCAI calls for combining AI with user experience (UX) design will enable the development of AI systems (e.g., autonomous vehicles, intelligent user interfaces, or intelligent decision-making systems) to achieve its design goals such as usable/explainable AI, human-controlled AI, and ethical AI.
2111.09509_1563612_5	For larger-scale structure - e.g., overall sentence structure - model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set.
2111.09800_1563903_5	This work examines in detail the design and implementation of this human compatible Hanabi teammate, as well as the existence and implications of human-complementary strategies and how they may be explored for more successful applications of AI in human machine teams.
2111.12210_1566313_6	This work also highlights the important role of Explainable AI (as compared to Blackbox AI) in science discovery to help humans prevent or better prepare for the possible technological singularity that may happen in the future, since science is not only about the know how, but also the know why.
2111.13041_1567144_5	At present, we seek these kinds of narrative explanation from AI technology, because as humans we seek to understand technology's working through constructing a story to explain it.
2111.13041_1567144_6	Our cultural history makes this inevitable - authors like Asimov, writing narratives about future AI technologies like intelligent robots, have told us that they act in ways explainable by the narrative logic which we use to explain human actions and so they can also be explained to us in those terms.
2111.14168_1568271_8	Given the recent growth in the importance of artificial intelligence (AI), we suggest accounting for AI's fundamental role in Industry 4.0 and understanding the fourth industrial revolution as an AI-powered natural collaboration between humans and machines.
2111.14833_1568936_6	More specifically, we show that three algorithms inspired by human-like social intelligence are, in principle, vulnerable to attacks that exploit weaknesses introduced by cooperative AI's algorithmic improvements and report experimental findings that illustrate how these vulnerabilities can be exploited in practice.
2111.15208_1569311_1	The paper proposes a unique outbreak response system framework based on artificial intelligence and edge computing for citizen centric services to help track and trace people eluding safety policies like mask detection and social distancing measure in public or workplace setup.
2112.01282_1571058_7	In addition, through the evaluation of our methodological implementation, we demonstrate its state-of-the-art relevance as a tool for sustaining human values in the data-driven AI era.
2112.02034_1571810_6	We also emphasise the need for collectively designing human-centered, transparent, interactive and collaborative AI-based algorithms that empower and give complete agency to stakeholders, as well as support new emerging pedagogies.
2112.02747_1572523_3	Specifically, we envisage a scenario where a trained FGVC model (the AI expert) functions as a knowledge provider in enabling average people (you and me) to become better domain experts ourselves, i.e. those capable in distinguishing between "Whip-poor-will" and "Mallard".
2112.06006_1575782_4	The system provides obvious benefits to the airport planners, not only people tracking in the shops area, but also aggregated and anonymized view, like heat maps that can highlight bottlenecks in the infrastructure, or suggest situations that require intervention, such as emergencies.
2112.06751_1576527_1	Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation.
2112.06751_1576527_2	However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves.
2112.08441_1578217_0	  Artificial intelligence (AI) enables machines to learn from human experience, adjust to new inputs, and perform human-like tasks.
2112.11191_1580967_4	We finish with a real-world example of a PAUSE-like network, the Human Security Information System (HSIS), being developed by USAID, that uses blockchain technology to provide a secure means to better understand the civilian environment.
2112.11471_1581247_3	As a result, there is growing interest in the research community to augment human decision making with AI assistance.
2112.11471_1581247_9	We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.
2112.12596_1582372_2	From a human-centered design perspective, transparency is not a property of the ML model but an affordance, i.e. a relationship between algorithm and user; as a result, iterative prototyping and evaluation with users is critical to attaining adequate solutions that afford transparency.
2112.14480_1584256_6	We see the notion of HCAI agent, together with its components and functions, as a way to bridge the technical and non-technical discussions on human-centered AI.
2112.15360_1585136_7	We argue that the possibility of AI taking over human civilization is low as developing such an advanced system requires a better understanding of the human brain first.
2201.01659_1587031_3	Ethical insights are generated from the lived experiences of AI-designers working on tangible human problems, and then cycled upward to influence theoretical debates surrounding these questions: 1) Should AI as trustworthy be sought through explainability, or accurate performance?
2201.04876_1590248_1	In the frame of the EU funded Teaming.AI project, we identified the monitoring of teaming aspects in human-AI collaboration, the runtime monitoring and validation of ethical policies, and the support for experimentation with data and machine learning algorithms as the most relevant challenges for human-AI teaming in smart manufacturing.
2201.04990_1590362_3	Similar to human toddlers, well-timed guidance and multimodal interactions might significantly enhance the training efficiency of AI agents as well.
2201.05371_1590743_5	The testing community is turning to AI to fill the gap as AI is able to check the code for bugs and errors without any human intervention and in a much faster way than humans.
2201.06009_1591381_0	  Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans.
2201.06317_1591689_6	Our experiments suggest that using a pretrained language model as the language encoder allows our approach to scale up for real-world scenarios with instructions from human users.
2201.06724_1592096_3	AI should play a role as an assistant in the lyrics creation process, where human interactions are crucial for high-quality creation.
2201.06892_1592264_1	We propose to interpret autocompletion as a basic interaction concept in human-AI interaction.
2201.08239_1593611_4	The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias.
2201.08300_1593672_1	In the Artificial Intelligence (AI) community, artificial curiosity provides a natural intrinsic motivation for efficient learning as inspired by human cognitive development; meanwhile, it can bridge the existing gap between AI research and practical application scenarios, such as overfitting, poor generalization, limited training samples, high computational cost, etc.
2201.09647_1595019_0	  The AlphaFold computer program predicted protein structures for the whole human genome, which has been considered as a remarkable breakthrough both in artificial intelligence (AI) application and structural biology.
2201.11239_1596611_4	We use these folk concepts as a framework of social attribution by the human explainee - the information constructs that humans are likely to comprehend from explanations - by introducing a blueprint for an explanatory narrative (Figure 1) that explains AI behavior with these constructs.
2202.03286_1602110_6	Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation.
2202.04977_1603801_3	Among these are technical boundaries (such as processing capacity), psychological boundaries (such as human trust in AI systems), ethical boundaries (such as with AI weapons), and conceptual boundaries (such as the AI people can imagine).
2202.05302_1604126_5	Here, we draw from statistical learning theory and sociological lenses on human-automation trust to motivate an AI-as-tool framework, which distinguishes human-AI trust from human-AI-human trust.
2202.05983_1604807_0	  In many practical applications of AI, an AI model is used as a decision aid for human users.
2202.05983_1604807_3	In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice).
2202.05983_1604807_8	Our findings suggest the importance of jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone.
2202.08510_1607334_5	Furthermore, AI-assisted pathologists show significantly improved diagnostic sensitivity by 12% in addition to 18% reduced screening time compared to human pathologists.
2202.08979_1607803_0	  Explainability, interpretability and how much they affect human trust in AI systems are ultimately problems of human cognition as much as machine learning, yet the effectiveness of AI recommendations and the trust afforded by end-users are typically not evaluated quantitatively.
2202.08979_1607803_3	The difference between final and first responses constitutes the shift or sway in the human decision which we use as metric of the AI's recommendation impact on the human, representing the trust they place on the AI.
2202.10336_1609160_3	Among such technologies, AI has shown the great importance of processing big data to enhance immersive experience and enable human-like intelligence of virtual agents.
2202.10419_1609243_5	We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers.
2202.11136_1609960_5	We also propose the Minimum Persistent Sensing (MPS) as a post-processing algorithm to reduce interference from ambient noise, including ongoing human conversation, office machines, and traffic noises.
2203.03715_1616577_3	Similar to other technologies, AI is expected "to meet [human] needs".
2203.05784_1618646_7	Our DDMA framework takes about 20 to 25 minutes to generate the fused 3D mesh model following the sequential processing order, compared to over 5 hours by human experts.
2203.08994_1621856_0	  As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can (1) learn by themselves continually in a self-motivated and self-initiated manner rather than being retrained offline periodically on the initiation of human engineers and (2) accommodate or adapt to unexpected or novel circumstances.
2203.09126_1621988_0	  Animated avatars, which look and talk like humans, are iconic visions of the future of AI-powered systems.
2203.12687_1625549_7	Further, both dimensions of trust shared a similar pattern of effects within the model, with functionality-related trust exhibiting a greater total impact on usage intention than human-like trust.
2203.15144_1628006_0	  Advances in artificial intelligence (AI) are enabling systems that augment and collaborate with humans to perform simple, mechanistic tasks like scheduling meetings and grammar-checking text.
2203.15144_1628006_1	However, such Human-AI collaboration poses challenges for more complex, creative tasks, such as carrying out empathic conversations, due to difficulties of AI systems in understanding complex human emotions and the open-ended nature of these tasks.
2203.15144_1628006_8	Our findings demonstrate the potential of feedback-driven, AI-in-the-loop writing systems to empower humans in open-ended, social, creative tasks such as empathic conversations.
2204.00691_1630829_0	  In this position paper we intend to advocate for participatory design methods and mobile social matching apps as ripe contexts for exploring novel human-AI interactions that benefit marginalized groups.
2204.02889_1633027_1	In the case of humans and artificial AI agents operating in the same environment, we note the significance of comprehension and response to the actions or capabilities of a human from an agent's perspective, as well as the possibility to delegate decisions either to humans or to agents, depending on who is deemed more suitable at a certain point in time.
2204.05138_1635276_1	This article presents an artificial intelligence (AI) architecture intended to simulate the human working memory system as well as the manner in which it is updated iteratively.
2204.06916_1637054_3	However, recent work has shown that AI advice is not always beneficial, as humans have shown to be unable to ignore incorrect AI advice, essentially representing an over-reliance on AI.
2204.07135_1637273_2	To provide supervision signal required to train such models, ideas such as human annotation, replication of a rule-based system, relabeling based on user paraphrases, and bandit-based learning were suggested.
2204.07612_1637750_2	The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world.
2204.07644_1637782_0	  Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners.
2204.07644_1637782_2	However, typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners.
2204.07644_1637782_6	Later in the paper, we present some potential ethical issues in human-AI co-creation and propose to use participatory design fiction as the research methodology to investigate the ethical issues associated with a co-creative AI that communicates with users.
2204.07666_1637804_0	  Human-AI co-creativity involves both humans and AI collaborating on a shared creative product as partners.
2204.08471_1638609_8	Our approach, which builds on top of the idea of separating observation and interpretation in human-AI collaboration, will facilitate human decision making in highly contextual domains, such as human assessment, while keeping their trust in the system.
2204.08859_1638997_3	However, it may also evoke human bias, especially in the form of automation bias as an over-reliance on AI advice.
2204.11788_1641926_3	Instead, we propose conditional delegation as an alternative paradigm for human-AI collaboration where humans create rules to indicate trustworthy regions of a model.
2204.13217_1643355_0	  Human-AI co-creativity involves humans and AI collaborating on a shared creative product as partners.
2204.13217_1643355_3	Typically, the AI in co-creative systems cannot communicate back to humans, limiting their potential to be perceived as partners rather than just a tool.
2205.00176_1644588_2	However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety.
2205.00189_1644601_0	  Trust has emerged as a key factor in people's interactions with AI-infused systems.
2205.01467_1645879_3	However, CTP is rarely demonstrated in previous work as often the focus is on the design of explainability, while a fundamental prerequisite -- the presence of complementarity potential between humans and AI -- is often neglected.
2205.05057_1649469_6	We use an application of sensemaking in organizations as a template for discussing design guidelines for Sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.
2205.05718_1650130_7	We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.
2205.06241_1650653_4	In Experiment 1 we found that providing CF explanations of an AI system's predictions does indeed (unjustifiably) affect people's causal beliefs regarding factors/features the AI uses and that people are more likely to view them as causal factors in the real world.
2205.09696_1654108_10	We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows.
2205.10764_1655176_2	Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with "person," with Pearson's rho as high as 0.82 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP.
2205.11463_1655875_0	  Language models (LMs) have been used in cognitive modeling as well as engineering studies -- they compute information-theoretic complexity metrics that simulate humans' cognitive load during reading.
2205.11463_1655875_1	This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans.
2205.12749_1657161_4	By comparing the acceptance rates of provided solutions, we can assess how the AI system performs compared to the domain expert, and whether the AI system's explanations (if provided) are human-understandable.
2205.12749_1657161_5	This setup -- comparable to the Turing test -- can serve as a framework for a wide range of human-centric AI system assessments.
2205.15409_1659821_1	Both humans and sophisticated AI agents process information about the world in order to achieve goals and obtain rewards, which is why AI can be used as a model of the human brain and mind.
2206.04132_1664552_3	The survey, fielded in late 2019, elicited forecasts for near-term AI development milestones and high- or human-level machine intelligence, defined as when machines are able to accomplish every or almost every task humans are able to do currently.
2206.07271_1667691_2	AI-generated language is often not identified as such but presented as language written by humans, raising concerns about novel forms of deception and manipulation.
2206.07271_1667691_5	A computational analysis of language features shows that human judgments of AI-generated language are hindered by intuitive but flawed heuristics such as associating first-person pronouns, use of contractions, or family topics with human-written language.
2206.07271_1667691_6	We experimentally demonstrate that these heuristics make human judgment of AI-generated language predictable and manipulable, allowing AI systems to produce text perceived as "more human than human."
2206.07271_1667691_7	We discuss solutions, such as AI accents, to reduce the deceptive potential of language generated by AI, limiting the subversion of human intuition.
2206.08932_1669352_5	We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question.
2206.11484_1671904_0	  This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT.
2206.13202_1673622_1	Learning to defer (L2D) has been presented as a promising framework to determine who among humans and AI should make which decisions in order to optimize the performance and fairness of the combined system.
2206.13475_1673895_4	In this work, we introduce interpretation entropy as a universal solution for assessing the degree of human interpretability associated with any linear model.
2206.14576_1674996_2	We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning.
2207.00644_1676542_3	In addition to using a cognitive modeling as a means to explore how antiblackness may manifest in the design and development of AI systems (particularly from a software engineering perspective), we also introduce connections between antiblackness, the Human, and computational cognitive modeling.
2207.00969_1676867_8	By using human motions recognition as a concrete AI inference task, extensive experiments are conducted to verify the performance of our derived optimal ISCC scheme.
2207.01497_1677395_7	We believe this represents the "comprehension" phase of AI and policy, but leveraging policy as a key source of human values to align AI requires "understanding" policy.
2207.01497_1677395_9	As AI systems are given increasing responsibility in high-stakes contexts, integrating democratically-determined policy into those systems could align their behavior with human goals in a way that is responsive to a constantly evolving society.
2207.02996_1678894_4	Here we reframe human-AI collaboration as a learning problem: Inspired by research on team learning, we hypothesize that similar learning strategies that apply to human-human teams might also increase the collaboration effectiveness and quality of humans working with co-creative generative systems.
2207.02996_1678894_5	In this position paper, we aim to promote team learning as a lens for designing more effective co-creative human-AI collaboration and emphasize collaboration process quality as a goal for co-creative systems.
2207.06220_1682118_8	Using crowd-sourcing, we observe that for the top 10% most likely citations to be tagged as unverifiable by our system, humans prefer our system's suggested alternatives compared to the originally cited reference 70% of the time.
2207.06897_1682795_2	In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: (1) explanations have to truthfully represent the model behavior, i.e., have a high fidelity; (2) explanations must be complete, as missing information distorts the truth; and (3) explanations have to take the user's mental model into account, progressively verifying a person's knowledge and adapting their understanding.
2207.07051_1682949_8	These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times.
2207.08333_1684231_2	In this paper, we propose a quantitative metric "Equivariance Score" and evaluation dataset "Human Puzzle" to assess whether a VL model is understanding an image like a human.
2207.10573_1686471_5	This system will work based on the text as a conversational agent that can interact with humans by natural language.
2208.02957_1693669_0	  The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings.
2208.02957_1693669_3	This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.
2208.04181_1694893_3	Our preliminary findings suggest that people's reliance on AI recommendations increases compared to cases where no explanation or feature-based explanations are provided, especially when the AI recommendations are incorrect.
2208.06590_1697302_2	To arrive at human-level AI, artificial general intelligence (AGI), as opposed to AI systems that are specialized for a specific task, was set as a technically meaningful long-term goal.
2208.06590_1697302_7	Based on the findings of philosophy and engineering cognitive technology, we predict that in the relatively near future, AI will be able to recognize various entities to the same degree as humans.
2208.10264_1700976_0	  We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior.
2208.10544_1701256_7	Interesting observations from this experiment include: (1) models trained with human-guidance offer better support to human examination of face images when compared to models trained traditionally using cross-entropy loss, (2) binary decisions presented to humans offers better support than saliency maps, (3) understanding the AI's accuracy helps humans to increase trust in a given model and thus increase their overall accuracy.
2208.14788_1705500_4	Drawing inspiration from law, this article explains how negative human rights could fulfil the role of such principles and serve as a foundation both for an international regulatory system and for building technical safety constraints for future AI systems.
2209.01390_1707105_3	To address this, we discuss the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction.
2209.01515_1707230_4	Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime.
2209.05459_1711174_5	To reduce total existential risk, humanity should take robustly positive actions such as working on existential risk analysis, AI governance and safety, and reducing all sources of existential risk by promoting differential technological development.
2209.09204_1714919_1	This study evaluates the robustness of an AI solution for the diagnosis of normal chest X-rays (CXRs) by comparing performance across multiple patient and environmental subgroups, as well as comparing AI errors with those made by human experts.   
2209.09204_1714919_9	Conclusion: We show the AI solution could provide meaningful workload savings by diagnosis of 18.5% of scans as HCN with a superior NPV to human readers.
2209.10604_1716319_0	  There is a substantial and ever-growing corpus of evidence and literature exploring the impacts of Artificial intelligence (AI) technologies on society, politics, and humanity as a whole.
2209.11344_1717059_0	  Large-Language Models like GPT-3 have the potential to enable HCI designers and researchers to create more human-like and helpful chatbots for specific applications.
2209.12356_1718071_3	We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality.
2209.12711_1718426_2	We evaluate 9 different tasks with negated prompts on (1) pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples, and (4) LMs fine-tuned specifically on negated prompts; all LM types perform worse on negated prompts as they scale and show a huge performance gap between the human performance when comparing the average score on both original and negated prompts.
2209.14338_1720053_4	Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds.
2209.15271_1720986_4	Among them, an open-source dataset is constructed to train a multi-form human detection model that distinguishes a human being's whole body, upper body or part body, and the followed action classification model is adopted to recognize such action as falling, sleeping or on-duty, etc.
2209.15448_1721163_0	  As AI becomes more prevalent throughout society, effective methods of integrating humans and AI systems that leverage their respective strengths and mitigate risk have become an important priority.
2209.15448_1721163_2	This approach utilizes the observed action, either from AI or humans, as input for achieving a stronger oracle in policy learning for the decision maker (humans or AI).
2209.15615_1721330_0	  Fitts' law is often employed as a predictive model for human movement, especially in the field of human-computer interaction.
2209.15615_1721330_7	The efficacy of the proposed model, as well as its ability to inform model-based clustering, are addressed in this work through extensive simulations and an insightful analysis of a human aiming performance study.
2210.01478_1722833_5	Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments.
2210.02667_1724022_3	In this paper, we put forth the doctrine of universal human rights as a set of globally salient and cross-culturally recognized set of values that can serve as a grounding framework for explicit value alignment in responsible AI - and discuss its efficacy as a framework for civil society partnership and participation.
2210.03568_1724923_5	Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).
2210.03842_1725197_1	These AI systems, equipped with an equivalent of human's Theory of Mind (ToM) capability, are currently serving as matchmakers on dating platforms, assisting student learning as teaching assistants, and enhancing productivity as work partners.
2210.03842_1725197_2	They mark a new era in human-AI interaction (HAI) that diverges from traditional human-computer interaction (HCI), where computers are commonly seen as tools instead of social actors.
2210.05125_1726480_3	Then, we integrate the policy regularization idea into reinforcement learning to train a human-like best response to the human model.
2210.05173_1726528_4	As currently, fully automated AI algorithms are sparse, every algorithm has to provide a reasoning for human operators.
2210.06726_1728081_4	As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.
2210.08960_1730315_2	In experiment (N=128), we investigated how susceptible people are to deceptive AI systems by examining how their ability to discern true news from fake news varies when AI systems are perceived as either human fact-checkers or AI fact-checking systems, and when explanations provided by those fact-checkers are either deceptive or honest.
2210.08960_1730315_3	We find that deceitful explanations significantly reduce accuracy, indicating that people are just as likely to believe deceptive AI explanations as honest AI explanations.
2210.08960_1730315_5	Further, we did not observe any significant differences in discernment between explanations perceived as coming from a human fact checker compared to an AI-fact checker.
2210.08984_1730339_2	There are many emerging AI risks for humanity, such as autonomous weapons, automation-spurred job loss, socio-economic inequality, bias caused by data and algorithms, privacy violations and deepfakes.
2210.10585_1731940_0	  I study the role of minimum wage as an anchor for judgements of the fairness of wages by both human subjects and artificial intelligence (AI).
2210.10585_1731940_4	The anchor exerts a similar effect on the AI bot; however, the wage that the AI bot perceives as fair exhibits a systematic downward shift compared to human subjects' responses.
2210.10585_1731940_6	As with human subjects, the remaining responses are close to the control group for the AI bot but also exhibit a systematic shift towards the anchor.
2210.11161_1732516_1	However, children's development involve complex faculties such as exploration, creativity and curiosity which are challenging to model.
2210.11603_1732958_5	From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.
2210.14986_1736341_7	We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.
2210.15303_1736658_3	Prior work suggests that LMs cannot handle these structures as reliably as humans can.
2210.15303_1736658_8	Thus, large LMs may indeed process recursively nested grammatical structures as reliably as humans.
2210.15327_1736682_1	Central to our approach is the notion of natural language as core representation, reasoning, and exchange format between scientific AI and human scientists.
2211.01602_1740508_6	We also show that using these improved human models often leads to better human-AI collaboration performance compared to using models based on real human data alone.
2211.02759_1741665_2	To make AI opponents more human-like, we'd ideally like to see multiple different strategies at each level of difficulty, a concept we refer to as "multidimensional" difficulty.
2211.06326_1745232_3	Acknowledging military ethics, human factors and AI work to date as well as critical case studies, this chapter offers new mechanisms to map out conditions for moral responsibility in human-AI teams.
2211.06326_1745232_5	Mechanisms such as these enable militaries to design human-centred AI systems for responsible deployment.
2211.08615_1747521_0	  With the rapid development of deep generative models (such as Generative Adversarial Networks and Diffusion models), AI-synthesized images are now of such high quality that humans can hardly distinguish them from pristine ones.
2211.09038_1747944_1	Radiotherapy, due to its technology-intensive nature as well as direct human-machine interactions, is perfectly suited for benefitting from AI to enhance accuracy and efficiency.
2211.10756_1749662_3	However, there is a lack of understanding on the perceptions of people with dementia around AI as an aid to their everyday technology use and its role in their overall self-management systems, which include other non-AI technology, and human assistance.
2211.16444_1755350_2	Work in this area typically treats trustworthy AI as a problem of Human-Computer Interaction involving the individual user and an AI system.
2212.01488_1757665_7	In follow-up analyses, we show that (i) LLM scores are driven by both plausibility and surface-level sentence features, (ii) LLM scores generalize well across syntactic variants (active vs. passive constructions) but less well across semantic variants (synonymous sentences), (iii) some LLM errors mirror human judgment ambiguity, and (iv) sentence plausibility serves as an organizing dimension in internal LLM representations.
2212.01493_1757670_6	As the data complexity continues to increase, we anticipate further advances leading towards a collaborative human-AI discovery.
2212.03551_1759728_2	The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are.
2212.05206_1761383_2	In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it.
2212.05206_1761383_4	For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans.
2212.08073_1764250_2	The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'.
2212.08195_1764372_1	Meanwhile, advances in the symbolic reasoning capabilities of AI have led to systems that outperform humans in games like chess and Go (Silver et al., 2018).
2212.09746_1765923_0	  Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction.
2212.10474_1766651_6	We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans.
2212.10678_1766855_0	  Generated texts from large language models (LLMs) have been shown to exhibit a variety of harmful, human-like biases against various demographics.
2212.10728_1766905_1	This is an important AI task as it enables the machine to construct a sensible answer or perform a useful action for the human.
2212.11261_1767438_0	  Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body.
2212.14047_1770224_2	In this work we explore the opportunities offered by the newly emerging crop of large language models (LLM) which use sophisticated deep learning technology to produce human-like prose.
2212.14232_1770409_5	Gestalt perception contains a series of laws to explain human perception, that the human visual system tends to perceive patches in an image that are similar, close or connected without abrupt directional changes as a perceptual whole object.
2212.14834_1771011_3	LLMs are titanic models trained on billions of code snippets and can auto-regressively generate human-like code snippets.
2212.14882_1771059_0	  The release of ChatGPT, a language model capable of generating text that appears human-like and authentic, has gained significant attention beyond the research community.
2301.01181_1772245_7	Longer-term, if AI begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning AI with humans.
2301.07085_1778149_2	Such results may be interpreted as evidence that model behavior is not "human like".
2301.07255_1778319_0	  AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong.
2301.07597_1778661_3	On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues.
2301.09545_1780609_3	We use the term entoptic as a metaphor to investigate how the material interplay of data and models in AI technologies shapes human experiences of reality.
2301.10009_1781073_7	This review results show that AI-enabled RPM architectures have transformed healthcare monitoring applications because of their ability to detect early deterioration in patients' health, personalize individual patient health parameter monitoring using federated learning, and learn human behavior patterns using techniques such as reinforcement learning.
2301.10035_1781099_5	Patients' questions were placed in ChatGPT with a request for the chatbot to respond using approximately the same word count as the human provider's response.
2301.10404_1781468_2	Big companies such as Google, Microsoft, and Apple have provided a suite of recent guidelines to assist engineering teams in building human-centered AI systems.
2301.10416_1781480_7	The results suggest that while AI has the potential to generate scientific content that is as accurate as human-written content, there is still a gap in terms of depth and overall quality.
2301.10823_1781887_0	  Artificial Intelligence (AI) is about making computers that do the sorts of things that minds can do, and as we progress towards this goal, we tend to increasingly delegate human tasks to machines.
2301.12004_1783068_2	Large language models (LLMs) have been used for generation and can now output human-like text.
2301.12066_1783130_0	  As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth.
2301.12564_1783628_5	GPT-3's judgments are broadly similar to human judgments and generally align with proposed constraints in the literature but, in some cases, GPT-3's judgments and human judgments diverge from the literature and from each other.
2302.00560_1785494_1	If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale.
2302.01339_1786273_6	Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an "actual human philosopher".
2302.02083_1787017_4	We explore the potential interpretation of these findings, including the intriguing possibility that ToM, previously considered exclusive to humans, may have spontaneously emerged as a byproduct of LLMs' improving language skills.
2302.02944_1787878_3	In this paper, we propose a framework for a novel human-AI collaboration for selecting advantageous course of action, which we refer to as Learning Complementary Policy for Human-AI teams (\textsc{lcp-hai}).
2302.04818_1789752_0	  The emergence of artificial intelligence has incited a paradigm shift across the spectrum of human endeavors, with ChatGPT serving as a catalyst for the transformation of various established domains, including but not limited to education, journalism, security, and ethics.
2302.06871_1791805_2	Should LLMs like ChatGPT produce educational content on par with human-authored content, the implications would be significant for further scaling of computer tutoring system approaches.
2302.07103_1792037_1	Validation as a key step in product development, common to each of these sectors including computerized systems and AI/ML development, offers an opportune point of comparison for aligning people and processes for cross-sectoral product development.   
2302.07248_1792182_1	However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer.
2302.07267_1792201_9	Our results cast doubts on the validity of using LLMs as a general replacement for human participants in the social sciences.
2302.09051_1793985_8	We analyze current solutions and promising research trends, using elements such as: hybrid LLM architectural patterns, training and prompting strategies, active human reinforcement learning supervised with AI, neuro-symbolic and structured knowledge grounding, program synthesis, iterated decomposition and others.
2302.10198_1795132_0	  Recently, ChatGPT has attracted great attention, as it can generate fluent and high-quality responses to human inquiries.
2302.10646_1795580_7	We found that our AI agent, Deep Wolf, could play Werewolf as competitively as average human players in a villager or a betrayer role, whereas Deep Wolf was inferior to human players in a werewolf or a seer role.
2302.10724_1795658_3	In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection.
2302.12313_1797247_9	We interpret this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and we argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information.
2302.12601_1797535_6	Experiencing (yet another) change of roles and creative processes, our participants' reflections can inform discussions within the industry, be used by policymakers to inform urgently needed legislation, and support researchers in games, HCI and AI to support the sustainable, professional use of TTIG to benefit people and games as cultural artefacts.
2302.12813_1797747_0	  Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering.
2302.13681_1798615_0	  In recent years, Large Language Models (LLMs) have gained significant popularity due to their ability to generate human-like text and their potential applications in various fields, such as Software Engineering.
2302.13817_1798751_0	  The emergence of an AI-powered chatbot that can generate human-like sentences and write coherent essays has caught the world's attention.
2303.01248_1801045_0	  Large Language Models (LLMs) especially ChatGPT have produced impressive results in various areas, but their potential human-like psychology is still largely unexplored.
2303.01300_1801097_0	  Given an increasing prevalence of intelligent systems capable of autonomous actions or augmenting human activities, it is important to consider scenarios in which the human, autonomous system, or both can exhibit failures as a result of one of several contributing factors (e.g. perception).
2303.02160_1801957_1	To this end, we propose a novel AI agent with the goal of generating more human-like behavior.
2303.02160_1801957_6	This work provides insights into the characteristics that people consider human-like in the context of goal-directed video game navigation, which is a key step for further improving human interactions with AI agents.
2303.03103_1802900_2	Functional compositionality - the ability to compose learned tasks - has been a long-standing challenge in the field of AI (and many other fields) as it is considered one of the hallmarks of human intelligence.
2303.03548_1803345_2	In this work, we explore the potential of large-language models (LLMs) -- which have consumed vast amounts of human-generated text data -- to act as zero-shot human models for HRI.
2303.03886_1803683_0	  Given AI systems like ChatGPT can generate content that is indistinguishable from human-made work, the responsible use of this technology is a growing concern.
2303.04048_1803845_5	In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models.
2303.05453_1805250_2	While alignment techniques like reinforcement learning with human feedback (RLHF) and red-teaming can mitigate some safety concerns and improve model capabilities, it is unlikely that an aggregate fine-tuning process can adequately represent the full range of users' preferences and values.
2303.06074_1805871_7	Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants.
2303.06217_1806014_3	We interpret this result to suggest that reactions to AI-generated content may be negative when the content is viewed as distinctly "human."
2303.06219_1806016_0	  As AI systems proliferate, their greenhouse gas emissions are an increasingly important concern for human societies.
2303.07439_1807236_0	  In its pragmatic turn, the new discipline of AI ethics came to be dominated by humanity's collective fear of its creatures, as reflected in an extensive and perennially popular literary tradition.
2303.08014_1807811_9	Overall, these experiments demonstrate that LLMs such as ChatGPT (and Vicuna to a lesser extent) are humanlike in many aspects of human language processing.
2303.08721_1808518_7	We warn that ubiquitous highlypersuasive AI systems could alter our information environment so significantly so as to contribute to a loss of human control of our own future.
2303.09038_1808835_0	  The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities.
2303.09224_1809021_2	In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams -- compared to humans or the AI model completing the task alone.
2303.09387_1809184_1	As AI systems mediate more of our interactions with the world, it is important to understand the degree to which AI systems might manipulate humans without the intent of the system designers.
2303.09387_1809184_5	Second, we propose a definition of manipulation based on our characterization: a system is manipulative if it acts as if it were pursuing an incentive to change a human (or another agent) intentionally and covertly.
2303.10338_1810135_4	Building on the concept of machine teaching, we developed an active learning strategy within the RIS, so that the human radiologist can enable/disable AI annotations as well as "fix"/relabel the AI annotations.
2303.10577_1810374_4	As a result, the WES can learn human behaviors, adapt system configurations, and allocate radio resources to tailor personalized user settings.
2303.11156_1810953_8	Additionally, we investigate the susceptibility of watermarked LLMs to spoofing attacks aimed at misclassifying human-written text as AI-generated.
2303.11158_1810955_7	The ChatGPT-based abstracts were then compared to human-written abstracts using statistical tools and unsupervised text mining.
2303.11196_1810993_5	To ensure contextuality, the framework (i) bifurcates the AI life cycle into two phases: learning and deployment for specific tasks, instead of defining foundation or general-purpose models; and (ii) categorizes these tasks based on their application and interaction with humans as follows: autonomous, discriminative (allocative, punitive, and cognitive), and generative AI.
2303.11221_1811018_0	  Using YouTube Kids as an example, in this work, we argue the need to understand a child's interaction process with AI and its broader implication on a child's emotional, social, and creative development.
2303.11436_1811233_1	Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks.
2303.11812_1811609_2	The data were analyzed in terms of five discourse components using Coh-Metrix (a special instrument for analyzing language discourses), and the results revealed that ChatGPT performed better than human writers in narrativity, word concreteness, and referential cohesion, but worse in syntactic simplicity and deep cohesion in its initial version.
2303.12040_1811837_2	The AI as collaborator dream is different from computer tools that augment human intelligence (IA) or intermediate human collaboration.
2303.12712_1812509_7	Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT.
2303.12810_1812607_0	  The potential of large language models (LLMs) to reason like humans has been a highly contested topic in Machine Learning communities.
2303.12810_1812607_4	Additionally, to evaluate the ability of LLMs to reason like human, their performance is evaluted on more open-ended, natural language questions.
2303.13360_1813157_0	  It is likely that AI systems driven by pre-trained language models (PLMs) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution.
2303.13408_1813205_7	We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated.
2303.13592_1813389_9	As such, we advise against using LLMs in this context without extensive human checks.
2303.14602_1814399_0	  AI-based decision-making tools are rapidly spreading across a range of real-world, complex domains like healthcare, criminal justice, and child welfare.
2303.16281_1816078_3	We find that our online searches and emerging tools like ChatGPT turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives.
2303.16352_1816149_1	The need to discriminate human writing from AI is now both critical and urgent, particularly in domains like higher education and academic writing, where AI had not been a significant threat or contributor to authorship.
2303.16352_1816149_3	We focused on how a particular group of humans, academic scientists, write differently than ChatGPT, and this targeted approach led to the discovery of new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like but, however, and although.
2303.16352_1816149_4	With a set of 20 features, including the aforementioned ones and others, we built a model that assigned the author, as human or AI, at well over 99% accuracy, resulting in 20 times fewer misclassified documents compared to the field-leading approach.
2303.16634_1816431_2	Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references.
2303.16634_1816431_6	We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin.
2303.17276_1817073_3	The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making.
2303.17276_1817073_4	We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark).
2303.17276_1817073_7	Remarkably, the production of human-like fallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in GPT-4.
2303.17276_1817073_8	This suggests that larger and more advanced LLMs may develop a tendency toward more human-like mistakes, as relevant thought patterns are inherent in human-produced training data.
2303.17466_1817263_0	  The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue.
2303.17649_1817446_4	Numerical metrics such as BLEU and perplexity were used to evaluate the model, and human judgment was also used to compare the decoding technique with others.
2303.17650_1817447_1	ChatGPT, developed by OpenAI, is a recent addition to the family of language models and is being called a disruptive technology by a few, owing to its human-like text-generation capabilities.
2303.18116_1817913_1	This includes interaction with ChatGPT in natural language and using mathematical formalism, which, under careful supervision by a human-expert, led to producing a working code in MATLAB, Python and R for sampling from a given copula model, evaluation of the model's density, performing maximum likelihood estimation, optimizing the code for parallel computing for CPUs as well as for GPUs, and visualization of the computed results.
2303.18116_1817913_2	In contrast to other emerging studies that assess the accuracy of LLMs like ChatGPT on tasks from a selected area, this work rather investigates ways how to achieve a successful solution of a standard statistical task in a collaboration of a human-expert and artificial intelligence (AI).
2304.00116_1818162_0	  Large language models (LLMs) have significantly transformed the landscape of artificial intelligence by demonstrating their ability in generating human-like text across diverse topics.
2304.00416_1818462_1	Recent advances in large language models (LLMs) have led to the development of powerful AI chatbots capable of engaging in natural and human-like conversations.
2304.01002_1819048_0	  Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts.
2304.01232_1819278_2	Therefore, we are working on projects to explore tailored design considerations for children, such as through investigating children's use of existing AI-based toys and learning technologies.
2304.01852_1819898_1	Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance.
2304.02020_1820066_0	  Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology.
2304.02554_1820600_1	In this study, we explored ChatGPT's ability to perform human-like summarization evaluation using four human evaluation methods on five datasets.
2304.02868_1820914_0	  Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users.
2304.05372_1823418_5	Intraclass correlation (ICC) as a performance metric showed that the inter-reliability of both the OpenAI ChatGPT and the Google Bard were low against the gold standard of human ratings.
2304.06122_1824168_0	  ChatGPT has recently gathered attention from the general public and academia as a tool that is able to generate plausible and human-sounding text answers to various questions.
2304.06588_1824634_4	The LLM is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities.
2304.07327_1825373_0	  Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT.
2304.07327_1825373_1	Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains.
2304.07590_1825636_4	Specifically, through role instructions, 1) Multiple LLM agents act as distinct `experts', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention.
2304.07683_1825729_4	We review sources of bias, such as data, algorithm, and human decision biases - highlighting the emergent issue of generative AI bias where models may reproduce and amplify societal stereotypes.
2304.07830_1825876_1	Accordingly, and given the recent proliferation of large language models (LLMs), here we asked whether such models exhibit an organisation of perceptual semantics similar to those observed in humans.
2304.07830_1825876_4	ChatGPT generated semantic profiles that only partially correlated with human ratings, yet showed robust agreement along well-known psychophysical dimensions of musical sounds such as brightness (bright-dark) and pitch height (deep-high).
2304.07999_1826045_3	First, with categorical comparison of human and AI painting collections, we find that AI artworks show distributional difference from human artworks in both latent space and some aesthetic features like strokes and sharpness, while in other aesthetic features like color and composition there is less difference.
2304.07999_1826045_4	Second, with individual artist analysis of Picasso, we show human artists' strength in evolving new styles compared to AI.
2304.08804_1826850_1	In practice, however, we often see that humans cannot assess the correctness of AI recommendations and, as a result, adhere to wrong or override correct advice.
2304.09064_1827110_1	This has led to the emergence of LLMs, such as the now famous GPT-3.5, which revolutionise the way humans can access or generate content.
2304.09103_1827149_5	Despite its exceptional ability to generate natural-sounding responses, the authors believe that ChatGPT does not possess the same level of understanding, empathy, and creativity as a human and cannot fully replace them in most situations.
2304.09655_1827701_2	The conversational model is able not only to process human-like text, but also to translate natural language into code.
2304.09823_1827869_1	However, extensive concerns are also raised about the potential ethical issues, especially about whether ChatGPT-like artificial general intelligence (AGI) will replace human jobs.
2304.09873_1827919_4	Using ChatGPT as an assistant for psychotherapy poses several challenges that need to be addressed, including technical as well as human-centric challenges which are discussed.
2304.10145_1828191_8	We believe this work can open up new lines of analysis and act as a basis for future research into the exploitation of ChatGPT for human annotation tasks.
2304.10423_1828469_0	  Current approaches to program synthesis with Large Language Models (LLMs) exhibit a "near miss syndrome": they tend to generate programs that semantically resemble the correct answer (as measured by text similarity metrics or human evaluation), but achieve a low or even zero accuracy as measured by unit tests due to small imperfections, such as the wrong input or output format.
2304.10513_1828559_1	Recent advancements in large language models, such as ChatGPT, have demonstrated significant potential to impact various aspects of human life.
2304.10578_1828624_1	As AI capabilities continue to improve in accuracy, robustness, and reach, AI may outperform and even replace human experts across many valuable tasks.
2304.10619_1828665_8	Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared to human annotations.
2304.11082_1829128_5	Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors.
2304.11085_1829131_2	However, ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs.
2304.11085_1829131_7	Although pooling outputs from multiple repetitions can improve reliability, this study advises caution when using ChatGPT for zero-shot text annotation and underscores the need for thorough validation, such as comparison against human-annotated data.
2304.11107_1829153_0	  Large language models (LLMs) such as ChatGPT have recently demonstrated significant potential in mathematical abilities, providing valuable reasoning paradigm consistent with human natural language.
2304.11163_1829209_1	However, the promise that AI technologies will benefit all of humanity is empty so long as we lack a nuanced understanding of what humanity is supposed to be in the face of widening global inequality and pressing existential threats.
2304.11215_1829261_1	This article explores the ethical problems arising from the use of ChatGPT as a kind of generative AI and suggests responses based on the Human-Centered Artificial Intelligence (HCAI) framework.
2304.11276_1829322_0	  The recent advancement in Natural Language Processing (NLP) capability has led to the development of language models (e.g., ChatGPT) that is capable of generating human-like language.
2304.11567_1829613_0	  Background: Large language models such as ChatGPT are capable of generating grammatically perfect and human-like text content, and a large number of ChatGPT-generated texts have appeared on the Internet.
2304.11633_1829679_3	Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation.
2304.11893_1829939_7	Questions have been raised about the impact that AI could have on patients, practitioners, and health systems, as well as about its potential risks; therefore, all the parties involved are called to agree upon to express a common view based on the dual purpose of improving people's quality of life and keeping the whole healthcare system sustainable for society as a whole.
2304.12191_1830237_6	We hypothesize that Zipf's law will hold for genlangs because (1) genlangs created by ChatGPT fundamentally operate in the same way as human language with respect to the semantic usefulness of certain tokens, and (2) ChatGPT has been trained on a corpora of text that includes many different languages, all of which exhibit Zipf's law to varying degrees.
2304.12529_1830575_2	The emergence of Large Language Models (LLMs), such as ChatGPT, provides an opportunity to develop an interactive, communicative, and robust human-robot collaboration approach.
2304.13324_1831370_1	The focus is on the AI-generated artwork's ability to understand human intent (alignment) and visually represent emotions based on criteria such as creativity, aesthetic, novelty, amusement, and depth.
2304.13462_1831508_0	  We investigate the potential of ChatGPT as a multidimensional evaluator for the task of \emph{Text Style Transfer}, alongside, and in comparison to, existing automatic metrics as well as human judgements.
2304.13462_1831508_3	Compared to existing automatic metrics, ChatGPT achieves competitive correlations with human judgments.
2304.14276_1832322_7	Conclusions: Our results clearly demonstrate that models like ChatGPT outperform humans in generating argumentative essays.
2304.14399_1832445_5	We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset.
2304.14415_1832461_0	  ChatGPT is a natural language processing tool that can engage in human-like conversations and generate coherent and contextually relevant responses to various prompts.
2304.14543_1832589_4	Because the baseline for quality of AI is indistinguishability from human behavior, we draw heavily on the psycho-linguistics literature, and label our complaints as "Turing Test Triggers" (TTTs).
2304.14597_1832643_1	We briefly summarize long-term AI Safety, and the challenge of avoiding harms from AI as systems meet or exceed human capabilities, including software engineering capabilities (and approach AGI / "HLMI").
2304.14993_1833039_0	  ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text.
2305.00050_1833106_6	Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language.
2305.00050_1833106_7	As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods.
2305.00237_1833293_0	  ChatGPT is a type of artificial intelligence language model that uses deep learning algorithms to generate human-like responses to text-based prompts.
2305.00813_1833869_1	Human perception-inspired machine perception, in the context of AI, refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition.
2305.00948_1834004_5	We speculate that OpenAI o1's unique advantage over other models may result from the model's chain-of-thought mechanism, which mimics the structure of human reasoning used in complex cognitive tasks, such as linguistic analysis.
2305.01020_1834076_5	We find that LLMs can derive context-grounded, human-like distributions over the interpretations of several complex pragmatic utterances, yet struggle composing with negation.
2305.01145_1834201_6	Results show that incorporating the BERT-based AI agent into the human team can reduce the human screening effort by 68.5% compared to the case of no AI assistance and by 16.8% compared to the case of using a support vector machine (SVM)-based AI agent for identifying 80% of all relevant documents.
2305.01145_1834201_7	When we apply the HP sampling strategy for AL, the human screening effort can be reduced even more: by 78.3% for identifying 80% of all relevant documents compared to no AI assistance.
2305.01185_1834241_2	Findings suggest that although some believe AI may eventually replace teachers, the majority of participants argue that human teachers possess unique qualities, such as critical thinking, creativity, and emotions, which make them irreplaceable.
2305.01185_1834241_6	The study reveals that students value and respect human teachers, even as AI becomes more prevalent in education.
2305.01185_1834241_8	This roadmap serves as a valuable guide for refining teaching skills, fostering personal connections, and designing curriculums that effectively balance the strengths of human educators with AI technologies.
2305.01937_1834993_3	In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.
2305.02202_1835258_3	However, integrating AIs with collaborative learning and other educational activities is crucial, as is addressing potential limitations like concerns about AI information accuracy and reliability of the AIs' information and diminished human interaction.
2305.02220_1835276_4	Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
2305.02626_1835682_0	  As the popularity of large language models (LLMs) soars across various applications, ensuring their alignment with human values has become a paramount concern.
2305.03047_1836103_1	Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable.
2305.03380_1836436_0	  Large Language Models (LLMs) have revolutionized natural language processing by generating human-like text and images from textual input.
2305.03512_1836568_9	In human evaluation with a Likert scale of 1-5, the complete multimodal chatbot system receives higher image-groundedness of 4.3 and engagingness of 4.3, along with competitive fluency of 4.1, coherence of 3.9, and humanness of 3.1, when compared to other chatbot variants.
2305.03514_1836570_6	We conclude that the performance of today's LLMs can augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text).
2305.03701_1836757_1	Regarding Large Language Models (LLMs) as the core processor for multimodal information, our paper introduces LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information.
2305.03701_1836757_4	Hence, the obtained visual information without being connected to human intention may be inadequate for LLMs to generate intention-following responses, which we refer to as static visual information.
2305.03701_1836757_5	LMEye addresses this issue by allowing the LLM to request the desired visual information aligned with various human instructions, which we term as the dynamic visual information interaction.
2305.03720_1836776_9	In addition, we attempt to take a broader perspective, weighing the societal costs (e.g., replacement of certain forms of human employment) and benefits (e.g., the possibility of novel AI-based approaches to global issues such as environmental disruption) of allowing AI to make easy use of copyrighted works as training sets to facilitate the development, improvement, adoption, and diffusion of AI.
2305.03731_1836787_2	Our experiments reveal that ChatGPT has a working memory capacity limit strikingly similar to that of humans.
2305.04118_1837174_2	Compared to typical machine translation that focuses solely on source-to-target mapping, LLM-based translation can potentially mimic the human translation process which might take preparatory steps to ensure high-quality translation.
2305.04134_1837190_4	In this work, we raise the question of whether LLMs are developing executive functions similar to those of humans as part of their learning, and we evaluate the planning function and working memory of GPT using the popular Towers of Hanoi method.
2305.04207_1837263_3	Recent work has shown the large potential of large language models (LLMs) in unit test generation, which can generate more human-like and meaningful test code.
2305.04400_1837456_0	  A Large Language Model (LLM) is an artificial intelligence system that has been trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input.
2305.04790_1837846_1	MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users.
2305.04812_1837868_7	Results showed that external information can significantly shape LLMs' memories, opinions, and behaviors, with these changes mirroring human social cognitive patterns such as authority bias, in-group bias, emotional positivity, and emotion contagion.
2305.05133_1838189_0	  The ability of ChatGPT to generate human-like responses and understand context has made it a popular tool for conversational agents, content creation, data analysis, and research and innovation.
2305.05516_1838572_2	The key findings show that GPT exhibits behaviours similar to human responses, such as making positive offers and rejecting unfair ones in the ultimatum game, along with conditional cooperation in the prisoner's dilemma.
2305.05597_1838653_1	As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems.
2305.05994_1839050_1	However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training.
2305.06131_1839187_0	  Generative AI has made significant progress in recent years, with text-guided content generation being the most practical as it facilitates interaction between human instructions and AI-generated content (AIGC).
2305.06453_1839509_0	  Large Language Models (LLMs), such as ChatGPT, demonstrate a strong understanding of human natural language and have been explored and applied in various fields, including reasoning, creative writing, code generation, translation, and information retrieval.
2305.06453_1839509_3	We developed a prototype system called LLM-Geo using the GPT-4 API in a Python environment, demonstrating what an autonomous GIS looks like and how it delivers expected results without human intervention using three case studies.
2305.07530_1840586_0	  Evaluating human-AI decision-making systems is an emerging challenge as new ways of combining multiple AI models towards a specific goal are proposed every day.
2305.07530_1840586_1	As humans interact with AI in decision-making systems, multiple factors may be present in a task including trust, interpretability, and explainability, amongst others.
2305.07530_1840586_7	We suggest that HCI practitioners researching human-AI interaction can benefit by adding this step to user studies in a debriefing stage as a retrospective Thinking-Aloud protocol would be applied, but with emphasis on revisiting tasks and understanding why participants ignored or connected predictions while performing a task.
2305.07667_1840723_11	They also show how, as LLM's are exposed to human narratives, they induce not only human knowledge but also human biases.
2305.07716_1840772_2	Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark.
2305.07759_1840815_5	We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher.
2305.08360_1841416_1	Recently, OpenAI's language model ChatGPT has emerged as a powerful tool for generating human-like responses to a wide range of textual inputs (i.e., prompts), including those related to code generation.
2305.08883_1841939_0	  LLMs now exhibit human-like skills in various fields, leading to worries about misuse.
2305.09064_1842120_4	We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception.
2305.09620_1842676_0	  Large language models (LLMs) that produce human-like responses have begun to revolutionize research practices in the social sciences.
2305.09858_1842914_5	We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation.
2305.10250_1843306_5	To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a human-like memory mechanism.
2305.10383_1843439_8	These rationales also serve as a chain-of-thought for the model, a transparent mechanism for human verification, and support for human annotators to overcome cognitive limitations.
2305.10510_1843566_7	We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AIs that perform language translation to better accommodate such low-resource languages.
2305.10566_1843622_0	  Generative AI models like DALL-E 2 can interpret textual prompts and generate high-quality images exhibiting human creativity.
2305.10568_1843624_5	While GPT-3's performance is not perfect, it is better than that of humans -- likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012).
2305.10847_1843903_4	SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt.
2305.11202_1844258_5	Through a comparative study between two frameworks, we find that human-in-the-loop features like web access, problem decomposition with field knowledge and human-assisted code synthesis are essential as LLMs currently still fall short in acquiring cutting-edge and domain-specific knowledge to complete a holistic problem-solving project.
2305.11460_1844516_1	Recently, large language models (LLMs) have shown great potential in addressing this challenge due to their remarkable capabilities in comprehending human opinions and generating human-like text.
2305.11483_1844539_0	  People are increasingly turning to large language models (LLMs) for complex information tasks like academic research or planning a move to another city.
2305.11738_1844794_3	Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools.
2305.11873_1844929_3	People judged delegating to an LLM as less acceptable than delegating to a human (d = -0.78).
2305.12295_1845351_0	  Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems.
2305.12487_1845543_5	The LM is used as an imperfect model of human cultural transmission; an attempt to capture aspects of humans' common-sense, intuitive physics and overall interests.
2305.12564_1845620_0	  We investigate how people perceive ChatGPT, and, in particular, how they assign human-like attributes such as gender to the chatbot.
2305.12660_1845716_1	Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition.
2305.12815_1845871_1	While LLMs are being developed to simulate human behavior and serve as human-like agents, little attention has been given to the Agency that these models should possess in order to proactively manage the direction of interaction and collaboration.
2305.12947_1846003_2	For some of these tasks, models like ChatGPT can potentially substitute human workers.
2305.13083_1846139_1	Conversely, previous studies have found that although automatic metrics tend to favor smaller fine-tuned models, the quality of the summaries they generate is inferior to that of larger models like GPT-3 when assessed by human evaluators.
2305.13091_1846147_1	One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency.
2305.13091_1846147_3	We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations.
2305.13386_1846442_4	Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts.
2305.13735_1846791_0	  Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs.
2305.13735_1846791_1	However, it requires significant human demonstrations and feedback or distillation from proprietary LLMs such as ChatGPT.
2305.13788_1846844_4	As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution.
2305.13917_1846973_6	We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort.
2305.14057_1847113_5	Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts.
2305.14195_1847251_1	While demographic factors like age and gender change the way people talk, and in particular, the way people talk to machines, there is little investigation into how large pre-trained language models (LMs) can adapt to these changes.
2305.14239_1847295_4	We conduct comprehensive experiments with source news articles and find that (1) summarization models trained under the LLM-as-reference setting achieve significant performance improvement in both LLM and human evaluations; (2) contrastive learning outperforms standard supervised fine-tuning under both low and high resource settings.
2305.14292_1847348_7	WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4, while receiving significantly higher user ratings and more favorable comments.
2305.14450_1847506_0	  Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks.
2305.14450_1847506_4	To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks.
2305.14625_1847681_2	While the KNN-LM and related methods yield impressive decreases in perplexity, we discover that they do not exhibit corresponding improvements in open-ended generation quality, as measured by both automatic evaluation metrics (e.g., MAUVE) and human evaluations.
2305.14693_1847749_4	As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality.
2305.14718_1847774_3	By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards.
2305.14724_1847780_7	To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task.
2305.14784_1847840_3	With widespread adoption of AI systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly.
2305.14784_1847840_7	With LLMs being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution.
2305.14825_1847881_6	The surprising observations question whether modern LLMs have mastered the inductive, deductive and abductive reasoning abilities as in human intelligence, and motivate research on unveiling the magic existing within the black-box LLMs.
2305.14835_1847891_3	Our framework enables the model to refine the generated summary iteratively through self-evaluation and feedback, resembling humans' iterative process when drafting and revising summaries.
2305.14847_1847903_3	We use large language models (LLMs) to draft schemas directly in natural language, which can be further refined by human curators as necessary.
2305.14909_1847965_1	However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback.
2305.14909_1847965_3	To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans.
2305.14930_1847986_4	In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration.
2305.14976_1848032_7	Although we further explore and confirm the utility of employing GPT-4 as a potential alternative for human evaluation, our work adds to a growing body of research underscoring the limitations of ChatGPT.
2305.14992_1848048_1	However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning.
2305.15047_1848103_3	In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles.
2305.15323_1848379_5	Further, we speculate on the future of ChatGPT by considering various possibilities for study and development, such as energy-efficiency, cybersecurity, enhancing its applicability to additional technologies (Robotics and Computer Vision), strengthening human-AI communications, and bridging the technological gap.
2305.15393_1848449_6	When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performance as human users in designing visual layouts for numerical and spatial correctness.
2305.15498_1848554_6	We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would.   
2305.15541_1848597_4	This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model.   
2305.15929_1848985_2	More concretely, just like humans, ChatGPT has a consonant bias.
2305.16426_1849482_5	Using three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these common words.
2305.16837_1849893_7	Our experiments suggest that for many tasks, ChatGPT does perform credibly and the response from it is detailed and often better than the human expert output or the state of the art output.
2305.16867_1849923_2	We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players.
2305.16917_1849973_0	  While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way.
2305.16917_1849973_1	The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments.
2305.17359_1850415_6	We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text.
2305.17608_1850664_0	  The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts.
2305.17680_1850736_7	Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content.
2305.17760_1850816_2	Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans.
2305.17760_1850816_3	We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework.
2305.17819_1850875_8	While LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases in a zero-shot setting, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback.
2305.18149_1851205_1	Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts.
2305.18149_1851205_5	Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled".
2305.18243_1851299_2	Our technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which allows our method to create 37% Playable-Novel levels from as scarce data as only 60 hand-designed rooms under a scenario of the non-trivial game, with respect to (Procedural Content Generation) PCG, that has a good amount of local and global constraints.
2305.18279_1851335_4	Moreover, we present ContextDET, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.
2305.18290_1851346_5	Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods.
2305.18339_1851395_1	AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts.
2305.19103_1852159_2	Although LLMs are trained on restricted modalities, they exhibit human-like performance in diverse psychological tasks.
2305.19103_1852159_4	We identify two main findings: 1) Both models strongly align with human representations in non-sensorimotor domains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5; 2) GPT-4's gains are associated with its additional visual learning, which also appears to benefit related dimensions like haptics and imageability.
2305.19118_1852174_0	  Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies.
2305.19223_1852279_2	In respect of AI-accidents, there is an increasing effort focused on developing algorithms and paradigms that ensure AI systems are aligned to what humans intend, e.g. AI systems that yield actions or recommendations that humans might judge as consistent with their intentions and goals.
2305.19278_1852334_4	As we address user privacy and ethical guidelines for responsible AI-human symbiosis, we also explore potential biases and inequalities in AI-human symbiosis and propose strategies to mitigate these challenges.
2305.19555_1852611_1	However, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed.
2305.20076_1853132_0	  We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions.
2305.20076_1853132_4	We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues.
2306.00176_1853324_1	Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans.
2306.00635_1853783_0	  Experiential AI is presented as a research agenda in which scientists and artists come together to investigate the entanglements between humans and machines, and an approach to human-machine learning and development where knowledge is created through the transformation of experience.
2306.01169_1854317_8	Overall, our results show that the use of ChatGPT is a very promising but not yet mature approach for summarizing long documents and can at best serve as an inspiration for human editors.
2306.01183_1854331_6	We further analyze where GPT-3 performs better, as well as worse, than a pretrained lexical model, illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.
2306.01220_1854368_2	We made the first attempt to bridge this knowledge gap by investigating whether LLMs attend to the same parts of a task description as human programmers during code generation.
2306.01771_1854919_0	  Generative Pre-trained Transformer (GPT) is a state-of-the-art machine learning model capable of generating human-like text through natural language processing (NLP).
2306.01795_1854943_1	While AI has traditionally been perceived as incapable of generating new ideas or creating art, the development of more sophisticated AI models and the proliferation of human-computer interaction tools have opened up new possibilities for AI in artistic creation.
2306.01987_1855135_4	AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer.
2306.02177_1855325_6	To demonstrate the possibilities of LMs in this area of political science, we use GPT-3, one of the most advanced LMs, as a synthetic coder and compare it to human coders.
2306.02230_1855378_0	  Foundation models, such as GPT-4, DALL-E have brought unprecedented AI "operating system" effect and new forms of human-AI interaction, sparking a wave of innovation in AI-native services, where natural language prompts serve as executable "code" directly (prompt as executable code), eliminating the need for programming language as an intermediary and opening up the door to personal AI.
2306.02231_1855379_0	  Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning large language models (LLMs) to human preferences.
2306.02257_1855405_1	As for deep-learning models that output erroneous decision-making grounds, a method that incorporates expert human knowledge in the model via an attention map in a manner that improves explanatory power and recognition accuracy is proposed.
2306.02552_1855700_1	Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence.
2306.02920_1856068_2	Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.
2306.03097_1856245_7	We showcase green teaming by: 1) Using ChatGPT as a virtual patient to simulate a person experiencing suicidal ideation, for suicide support training; 2) Using Codex to intentionally generate buggy solutions to train students on debugging; and 3) Examining an Instagram page using Midjourney to generate images of anti-LGBTQ+ politicians in drag.
2306.03809_1856957_4	Collectively, these results suggest that LLMs will make pandemic-class agents widely accessible as soon as they are credibly identified, even to people with little or no laboratory training.
2306.03856_1857004_0	  We propose iteratively prompting a large language model to self-correct a translation, with inspiration from their strong language understanding and translation capability as well as a human-like translation approach.
2306.04308_1857456_0	  As Large Language Models (LLMs) continue to gain popularity due to their human-like traits and the intimacy they offer to users, their societal impact inevitably expands.
2306.04308_1857456_3	In addition, LLMs personality profile was analyzed and compared to human normative data.
2306.04325_1857473_4	The analysis reveals that, upon first encounters with advanced AI that was not yet highly capable, some social media users believed that AI advancements would benefit education and society, while others feared that advanced AI, like ChatGPT, would make humans feel inferior and lead to problems such as cheating and a decline in moral principles.
2306.05153_1858301_0	  The emergence of large-language models (LLMs) that excel at code generation and commercial products such as GitHub's Copilot has sparked interest in human-AI pair programming (referred to as "pAIr programming") where an AI system collaborates with a human programmer.
2306.05179_1858327_0	  Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills.
2306.05431_1858579_2	The foundation model built in this manuscript is the initial step for the development of future applications in the legal domain, such as further training with reinforcement learning from human feedback.
2306.05552_1858700_1	ChatGPT, in particular, has gained popularity for its ability to generate human-like dialogue.
2306.05685_1858833_4	Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.
2306.05685_1858833_5	Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.
2306.05817_1858965_2	Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities, which mainly stem from their extensive open-world knowledge, reasoning ability, as well as their comprehension of human culture and society.
2306.06548_1859696_1	We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction.
2306.06794_1859942_0	  Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like.
2306.06794_1859942_2	It is common for people to frame the training data for LLMs as "text" or even "language".
2306.07005_1860153_0	  With the rapid evolution of AI Generated Content (AIGC), forged images produced through this technology are inherently more deceptive and require less human intervention compared to traditional Computer-generated Graphics (CG).
2306.07195_1860343_5	Thus, we present one of the first studies investigating whether meta-linguistic awareness of recursion -- a uniquely human cognitive property -- can emerge in transformers with a high number of parameters such as GPT-4.
2306.07384_1860532_5	We also observe inverse scaling for most-type quantifier understanding, which is contrary to human psycho-linguistic experiments and previous work, where the model's understanding of most-type quantifier gets worse as the model size increases.
2306.07458_1860606_9	Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting.
2306.07622_1860770_2	In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition -- and the cognitive errors that come with it.
2306.07622_1860770_4	For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans.
2306.07799_1860947_3	Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types.
2306.07899_1861047_3	However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by LLMs, as crowd workers have financial incentives to use LLMs to increase their productivity and income.
2306.07899_1861047_6	Although generalization to other, less LLM-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone.
2306.07929_1861077_0	  Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER.
2306.08141_1861289_0	  As generative AI becomes more prevalent, it is important to study how human users interact with such models.
2306.08161_1861309_0	  Applications built on top of Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their human-level capabilities in natural language processing.
2306.08178_1861326_2	On the other hand, OpenAI's large language model (LLM) ChatGPT has demonstrated significant potential in producing complex, human-like text.
2306.08310_1861458_2	To explore how people use gen-AI models such as DALLE and StableDiffusion, it is critical to understand the themes, contents, and variations present in the AI-generated photos.
2306.08310_1861458_4	Through a comparative analysis of TWIGMA with natural images and human artwork, we find that gen-AI images possess distinctive characteristics and exhibit, on average, lower variability when compared to their non-gen-AI counterparts.
2306.08310_1861458_6	Finally, we observe a longitudinal shift in the themes of AI-generated images on Twitter, with users increasingly sharing artistically sophisticated content such as intricate human portraits, whereas their interest in simple subjects such as natural scenes and animals has decreased.
2306.08871_1862019_5	Med-MMHL not only incorporates human-generated misinformation but also includes misinformation generated by LLMs like ChatGPT.
2306.09445_1862593_1	Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society.
2306.09841_1862989_2	However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered.
2306.10037_1863185_1	ChatGPT, an artificial intelligence-based chatbot, developed by OpenAI and released in November 2022, has rapidly gained attention from the entire international community for its impressive performance in generating comprehensive, systematic, and informative human-like responses to user input through natural language processing.
2306.10052_1863200_4	By challenging students to remain the "human in the loop," the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement.
2306.10063_1863211_3	Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors.
2306.10354_1863502_2	This paper proposes an effective model LLMVA-GEBC (Large Language Model with Video Adapter for Generic Event Boundary Captioning): (1) We utilize a pretrained LLM for generating human-like captions with high quality.
2306.10900_1864048_2	This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs).
2306.10968_1864116_7	Experimental results on translation tasks show that BayLing achieves 95% of single-turn translation capability compared to GPT-4 with automatic evaluation and 96% of interactive translation capability compared to GPT-3.5-turbo with human evaluation.
2306.11489_1864637_2	However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents.
2306.11530_1864678_0	  Conversational AI systems exhibit a level of human-like behavior that promises to have profound impacts on many aspects of daily life -- how people access information, create content, and seek social support.
2306.11593_1864741_10	Experimental results demonstrate the effectiveness of our approach, as the captions generated by our model exhibit higher consistency with human judgment when evaluated on the MS-COCO test set.
2306.11748_1864896_3	For many users, current systems like ChatGPT and LaMDA feel safe because they are primarily text-based, but the industry is already shifting towards real-time voice and photorealistic digital personas that look, move, and express like real people.
2306.12198_1865346_5	The investigation reveals promising results, with the model addressing hierarchical problems in a moderately structured manner, similar to human problem-solving strategies.
2306.12456_1865604_1	Though machines have already demonstrated their abilities in designing new materials, proteins, and computer programs with advanced artificial intelligence (AI) techniques, the search space for designing such objects is relatively small, and thus, "Can machines design like humans?" remains an open question.
2306.13671_1866819_0	  ChatGPT, an AI chatbot, has gained popularity for its capability in generating human-like responses.
2306.13723_1866871_0	  Human-AI coevolution, defined as a process in which humans and AI algorithms continuously influence each other, increasingly characterises our society, but is understudied in artificial intelligence and complexity science literature.
2306.13723_1866871_1	Recommender systems and assistants play a prominent role in human-AI coevolution, as they permeate many facets of daily life and influence human choices on online platforms.
2306.13723_1866871_3	This human-AI feedback loop has peculiar characteristics compared to traditional human-machine interaction and gives rise to complex and often ``unintended'' social outcomes.
2306.13723_1866871_4	This paper introduces Coevolution AI as the cornerstone for a new field of study at the intersection between AI and complexity science focused on the theoretical, empirical, and mathematical investigation of the human-AI feedback loop.
2306.13805_1866953_7	Furthermore, a preliminary experimental result from the moral exemplar test may demonstrate that exemplary stories can elicit moral elevation in LLMs as do they among human participants.
2306.14790_1867938_3	Study 1 demonstrated that the latent model-rated originality factor, comprised of three transformer-based models, strongly predicted human originality ratings, and the model-rated flexibility strongly correlated with human flexibility ratings as well.
2306.14910_1868058_1	The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning.
2306.14915_1868063_3	This iterative human-AI interaction enabled GPT-4 to learn from the outcomes, much like an experienced chemist, by a prompt-learning strategy.
2306.15033_1868181_7	As more developers embrace these tools and acquire proficiency in the art of prompting with generative AI, it becomes evident that this novel approach to software development has forged a unique inextricable link between humans and artificial intelligence.
2306.15448_1868596_0	  As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions.
2306.15666_1868814_4	The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text.
2306.16357_1869505_1	To achieve this, we have created a theatre human action recognition system that uses skeleton data captured by depth image as input.
2306.16507_1869655_2	Research on automation bias suggests that humans can often be overconfident in AI, whereas research on algorithm aversion shows that, as the stakes of a decision rise, humans become more cautious about trusting algorithms.
2306.16507_1869655_7	Additional results show effects from the task's difficulty, overall AI trust, and whether a human or AI decision aid is described as highly competent or less competent.
2306.17070_1870218_2	Pinning down the meaning of creativity, and concepts like it, becomes salient when researchers port concepts from human psychology to computation, a widespread practice extending beyond CC into artificial intelligence (AI).
2306.17070_1870218_5	This study is based on 22 in-depth, semi-structured interviews, primarily with human-inspired AI researchers, half of whom focus on creativity as a major research area.
2306.17170_1870318_0	  As a specific category of artificial intelligence (AI), generative artificial intelligence (GenAI) generates new content that resembles what is created by humans.
2307.00184_1871180_0	  The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text.
2307.00184_1871180_1	As LLMs increasingly powerconversational agents used by the general public world-wide, the synthetic personality traits embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important.
2307.00470_1871466_0	  Large language models(LLMS)have shown excellent text generation capabilities, capable of generating fluent human-like responses for many downstream tasks.
2307.01128_1872124_6	In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for "guiding" the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise.
2307.02194_1873190_1	With recent advancements (such as GPT-4), LLMs perform at a level comparable to humans for many proficient tasks.
2307.03195_1874190_3	In the last decade, talent analytics has emerged as a promising field in applied data science for human resource management, garnering significant attention from AI communities and inspiring numerous research efforts.
2307.03351_1874346_4	By marrying the advantages of superimposing virtual objects onto the physical world, and generating human-like text using GPT, we can revolutionize O&M operations.
2307.03699_1874694_5	We propose an analytical framework to compose \emph{knowledge-informed prompts}, which serve as the interface that humans can interact with and use LLMs to perform the detection task.
2307.03826_1874821_0	  Generative AI tools such as chatGPT are poised to change the way people engage with online information.
2307.03826_1874821_4	The research presented here is an early investigation into how people make use of a generative AI chat system (referred to simply as chat from here on) as part of a search process, and how the incorporation of chat systems with existing search tools may effect users search behaviors and strategies.   
2307.03838_1874833_1	Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines.
2307.03838_1874833_2	However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers.
2307.03913_1874908_0	  Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems.
2307.03913_1874908_1	HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans.
2307.03913_1874908_4	However, there has been debate about whether AI can work as a teammate with humans.
2307.03913_1874908_10	Insights: AI has led to the emergence of a new form of human-machine relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems; We must follow a human-centered AI (HCAI) approach when applying HAT as a new design paradigm; We propose a conceptual framework of human-AI joint cognitive systems (HAIJCS) to represent and implement HAT for developing effective human-AI teaming
2307.04280_1875275_1	On the one hand, LLMs offer unprecedented capabilities in analyzing vast amounts of textual data and generating human-like responses, enabling researchers to delve into complex social phenomena.
2307.04781_1875776_4	We have developed a prompt engineering methodology for eliciting human-like survey responses from ChatGPT, which simulate the response to a policy question of a person described by a set of demographic factors, and produce both an ordinal numeric response score and a textual justification.
2307.05300_1876295_7	Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development.
2307.05909_1876904_0	  AI Tool is a large language model (LLM) designed to generate human-like responses in natural language conversations.
2307.05909_1876904_10	The results of this study show that AI Tool is able to generate human-like responses that are both informative and engaging.
2307.06908_1877903_5	We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.
2307.07312_1878307_0	  In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans.
2307.07367_1878362_0	  Large language models like ChatGPT efficiently provide users with information about various topics, presenting a potential substitute for searching the web and asking people for help online.
2307.07367_1878362_9	Using models like ChatGPT may be more efficient for solving certain programming problems, but its widespread adoption and the resulting shift away from public exchange on the web will limit the open data people and models can learn from in the future.
2307.07870_1878865_2	LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts).
2307.08161_1879156_4	By analyzing 200 student-generated questions from four different subject areas, we found that the rule-based method correctly detected 91% of the flaws identified by human annotators, as compared to 79% by GPT-4.
2307.08171_1879166_5	We found that (1) An IBL model that gives equal credit assignment to all decisions is able to match human performance better than other models, including IBL-TD and Q-learning; (2) IBL-TD and Q-learning models underperform compared to humans initially, but eventually, they outperform humans; (3) humans are influenced by decision complexity, while models are not.
2307.08715_1879710_0	  Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) services due to their exceptional proficiency in understanding and generating human-like text.
2307.09042_1880037_7	Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans.
2307.09042_1880037_9	In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence.
2307.09381_1880376_1	However, while offering a practical solution to programming problems, ChatGPT should be mainly used as a supporting tool (e.g., in software education) rather than as a replacement for the human being.
2307.10168_1881163_0	  LLMs have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities.
2307.10265_1881260_3	While AI offers numerous benefits, challenges such as bias, privacy concerns, and the need for human-AI collaboration must be considered.
2307.10768_1881763_8	This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities.
2307.11346_1882341_3	However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance.
2307.11380_1882375_2	This approach, however, fails to work on discerning texts generated through human-machine collaboration, such as ChatGPT-polished texts.
2307.11380_1882375_5	Additionally, we propose the "Polish Ratio" method, an innovative measure of the degree of modification made by ChatGPT compared to the original human-written text.
2307.11449_1882444_0	  In the global craze of GPT, people have deeply realized that AI, as a transformative technology and key force in economic and social development, will bring great leaps and breakthroughs to the global industry and profoundly influence the future world competition pattern.
2307.12166_1883161_6	The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-generated and multiple LLMs.
2307.12267_1883262_0	  The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions.
2307.12267_1883262_2	Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated.
2307.12267_1883262_4	We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary detection).
2307.12382_1883377_6	Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge.
2307.12573_1883568_1	Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds.
2307.12776_1883771_6	This bias has significant implications for AI developers and users, as overly optimistic expectations about human altruism may lead to disappointment, frustration, suboptimal decisions in public policy or business contexts, and even social conflict.
2307.12966_1883961_9	This survey, therefore, serves as a valuable resource for anyone invested in understanding and advancing the alignment of LLMs to better suit human-oriented tasks and expectations.
2307.12973_1883968_4	Given this likely model specialization, we ask: Do aggregate LLM labels improve over individual models (as for human annotators)?
2307.14298_1885293_3	First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations.
2307.14448_1885443_7	Through an expert interview and a controlled user experiment, our qualitative and quantitative results demonstrate that the proposed "de-paradox" workflow and the designed visual analytic system are effective in helping human users to identify and understand spurious associations, as well as to make accountable causal decisions.
2307.14475_1885470_6	We furthermore found that prompting ChatGPT to respond to the inventory as-if it belonged to a different cohort yielded no variance in responses, however, responding as-if it had a certain misconception introduced much variance in responses that approximate real human responses on the FCI in some regards.
2307.14984_1885979_2	In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S$^3$ system (short for $\textbf{S}$ocial network $\textbf{S}$imulation $\textbf{S}$ystem).
2307.16180_1887175_1	Furthermore, advanced techniques, such as prompt learning and reinforcement learning, are being employed to address ethical concerns and hallucination problems associated with LLMs, bringing them closer to aligning with human values.
2307.16180_1887175_2	This situation naturally raises the question of whether LLMs with human-like abilities possess a human-like personality?
2307.16180_1887175_3	In this paper, we aim to investigate the feasibility of using the Myers-Briggs Type Indicator (MBTI), a widespread human personality assessment tool, as an evaluation metric for LLMs.
2307.16336_1887331_2	This paper presents a case study about a Twitter botnet that appears to employ ChatGPT to generate human-like content.
2307.16338_1887333_3	We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers.
2307.16376_1887371_1	With the unprecedented scale of training and model parameters, the capability of large language models has been dramatically improved, leading to human-like performances in understanding, language synthesizing, and common-sense reasoning, etc.
2308.00109_1888001_1	The current generation of Large Language Models (LLMs) have been linked to claims about human-like linguistic performance and their applications are hailed both as a step towards artificial general intelligence and as a major advance in understanding the cognitive, and even neural basis of human language.
2308.01264_1889156_4	We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested.
2308.01285_1889177_2	This creates unprecedented opportunities for structured reasoning as well as collaboration among multiple AI systems and humans.
2308.01320_1889212_0	  ChatGPT-like models have revolutionized various applications in artificial intelligence, from summarization and coding to translation, matching or even surpassing human performance.
2308.01497_1889389_2	Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities.
2308.01497_1889389_7	Human judges, blind to the fact that an AI model was involved, rated metaphor interpretations generated by GPT4 as superior to those provided by a group of college students.
2308.01525_1889417_1	Given that most large-scale deep learning models act as black boxes and cannot be manually controlled, analyzing the similarity between models and humans can be a proxy measure for ensuring AI safety.
2308.01525_1889417_2	In this paper, we focus on the models' visual perception alignment with humans, further referred to as AI-human visual alignment.
2308.02180_1890072_5	While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop.
2308.02312_1890204_4	Furthermore, we conducted a large-scale linguistic analysis, as well as a user study, to understand the characteristics of ChatGPT answers from linguistic and human aspects.
2308.03188_1891080_3	Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback.
2308.03313_1891205_1	As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned.
2308.03313_1891205_8	In fact, there is 38.6% more opinion diversity when people all partially rely on LLMs, compared to prohibiting the use of LLMs entirely.
2308.03527_1891419_8	Even though the results show, that the scores of ChatGPT are still worse than the average of healthy humans, it scores better than people who have been diagnosed with Asperger syndrome / high-functioning autism.
2308.03656_1891548_4	With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1.
2308.03729_1891621_6	Secondly, it conducts an in-depth analysis of LVLMs' predictions using the ChatGPT Ensemble Evaluation (CEE), which leads to a robust and accurate evaluation and exhibits improved alignment with human evaluation compared to the word matching approach.
2308.04477_1892369_0	  Large Language Models (LLMs) are advanced Artificial Intelligence (AI) systems that have undergone extensive training using large datasets in order to understand and produce language that closely resembles that of humans.
2308.04586_1892478_0	  Developmental AI creates embodied AIs that develop human-like abilities.
2308.06373_1894265_2	The development of ChatGPT was a significant mile-stone, as it demonstrated the potential of large-scale language models to generate natural language responses that are almost indistinguishable from those of a human.
2308.06411_1894303_2	This method implemented in Answer Set Programming uses non-monotonic reasoning and a two-phase conversation between a human manager and the UATM system, considering factors like safety and potential impacts.
2308.06507_1894399_3	Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality.
2308.07201_1895093_1	With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation.
2308.07411_1895303_3	Large language models (LLMs), like ChatGPT, have emerged as a potential solution to this bottleneck by enabling researchers to explore human-driven interactions in previously unimaginable ways.
2308.08043_1895935_0	  A significant application of Large Language Models (LLMs), like ChatGPT, is their deployment as chat agents, which respond to human inquiries across a variety of domains.
2308.08407_1896299_3	Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders.
2308.09067_1896959_4	Humans tend to exhibit stronger negative emotions (such as fear and disgust) and less joy compared to text generated by LLMs, with the toxicity of these models increasing as their size grows.
2308.09067_1896959_5	LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language) than human texts, as well as more pronouns.
2308.09175_1897067_1	However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations.
2308.09175_1897067_10	Our findings suggest that diversity bonuses emerge in teams of AI agents, just as they do in teams of humans and that diversity is a valuable asset in solving computationally hard problems.
2308.09687_1897579_5	This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.
2308.09798_1897690_0	  As artificial intelligence (AI) transforms human resource management (HRM), understanding the research landscape becomes crucial for both academics and practitioners.
2308.09904_1897796_4	We introduce the RAH Recommender system, Assistant, and Human) framework, an innovative solution with LLM-based agents such as Perceive, Learn, Act, Critic, and Reflect, emphasizing the alignment with user personalities.
2308.10502_1898394_1	Since 2022, large language models (LLMs) such as GPT have outperformed humans in many real-life tasks.
2308.10502_1898394_10	It is likely that only two types of people would be interested in setting up a practical system for it:   $\bullet$ Those who prefer to use a decentralized ChatGPT-like software.   
2308.10502_1898394_12	The reason the second type of people may be interested is that it is possible that one day an AI system like this will awaken and become the next level of intelligence on this planet.
2308.10855_1898747_6	For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human.
2308.11424_1899316_1	As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience.
2308.11424_1899316_4	This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block.
2308.11534_1899426_1	However, due to challenges in gathering dialogues involving human participation, current endeavors like Baize and UltraChat rely on ChatGPT conducting roleplay to simulate humans based on instructions, resulting in overdependence on seeds, diminished human-likeness, limited topic diversity, and an absence of genuine multi-round conversational dynamics.
2308.11534_1899426_5	Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.
2308.12400_1900292_6	Overall, our results demonstrate the promising potential for human-AI collaboration in scientific discovery, as well as the importance of designing systems that effectively integrate AI's capabilities with human intelligence.
2308.12578_1900470_0	  Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs.
2308.13534_1901426_0	  Conversational AI systems have emerged as key enablers of human-like interactions across diverse sectors.
2308.13724_1901616_1	LLMs are advantageous in offering the potential to enhance the generalizability as task-agnostic planners and facilitate flexible interaction between human instructors and planning systems.
2308.14328_1902220_3	Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model.
2308.14337_1902229_0	  Large Language Models (LLMs) such as ChatGPT have received enormous attention over the past year and are now used by hundreds of millions of people every day.
2308.14608_1902500_10	Finally, we see that sources of Bing AI have slightly more tendency to the center when compared to human answers.
2308.14921_1902813_4	Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender; (b) these choices align with people's perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions.
2308.14921_1902813_6	This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us.
2308.16118_1904010_0	  In their recent Nature Human Behaviour paper, "Emergent analogical reasoning in large language models," (Webb, Holyoak, and Lu, 2023) the authors argue that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems."
2308.16361_1904253_1	Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics.
2308.16529_1904421_3	We developed an LLM-based conversational system for the robot and assessed its alignment with social cues as defined by human counselors.
2308.16890_1904782_7	Through validation, we demonstrate that powerful LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging their textual capabilities alone, aligning with human preferences.
2309.00608_1905412_5	Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine.
2309.00642_1905446_2	Where our study diverges from previous work is in (1) providing a more thorough analysis of what makes mathematical term extraction a difficult problem to begin with; (2) paying close attention to inter-annotator disagreements; (3) providing a set of guidelines which both human and machine annotators could use to standardize the extraction process; (4) introducing a new annotation tool to help humans with ATE, applicable to any mathematical field and even beyond mathematics; (5) using prompts to ChatGPT as part of the extraction process, and proposing best practices for such prompts; and (6) raising the question of whether ChatGPT could be used as an annotator on the same level as human experts.
2309.00770_1905574_0	  Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere.
2309.00779_1905583_3	To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction.   
2309.00779_1905583_11	We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.
2309.00986_1905790_0	  Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior.
2309.01940_1906744_8	Compared to human performance, there is still significant room for improvement in LLM programming.
2309.02233_1907037_0	  Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions.
2309.02524_1907328_0	  This paper examines how individuals perceive the credibility of content originating from human authors versus content generated by large language models, like the GPT language model family that powers ChatGPT, in different user interface versions.
2309.02524_1907328_2	While participants also do not report any different perceptions of competence and trustworthiness between human and AI-generated content, they rate AI-generated content as being clearer and more engaging.
2309.04269_1909073_5	We conduct a human preference study on 100 CNN DailyMail articles and find that that humans prefer GPT-4 summaries that are more dense than those generated by a vanilla prompt and almost as dense as human written summaries.
2309.04646_1909450_1	Recent advancements in instruction tuning bring LLMs with ability in following user's instructions and producing human-like responses.
2309.05163_1909967_6	Furthermore, when evaluated as a marking tool, the LLM's concordance with human markers averaged at 50.8%, with notable inaccuracies in marking straightforward questions, like multiple-choice.
2309.05519_1910323_1	As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI.
2309.05519_1910323_6	Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.
2309.05833_1910637_0	  Major cloud providers have employed advanced AI-based solutions like large language models to aid humans in identifying the root causes of cloud incidents.
2309.05958_1910762_2	While LLMs' and humans' preferences such as prioritizing humans over pets and favoring saving more lives are broadly aligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.
2309.05958_1910762_3	Additionally, despite the qualitative similarities between the LLM and human preferences, there are significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans.
2309.06236_1911040_4	Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly.
2309.06379_1911183_8	We evaluate the effectiveness of our classification method compared to human-annotated data, and demonstrate the utility of Style2Fab with a user study to show that functionality-aware segmentation helps preserve model functionality.
2309.06619_1911423_1	Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses.
2309.07103_1911907_5	We also report on the differences between these foundational large language models as generative AI continues to redefine human-computer interactions.
2309.07462_1912266_1	Employing LLMs as evaluators to rank or score other models' outputs emerges as a viable solution, addressing the constraints tied to human annotators and established benchmarks.
2309.07664_1912468_1	This study explores the potential ethnic and gender bias of ChatGPT, a chatbot producing human-like responses to language tasks, in assessing job applicants.
2309.07689_1912493_0	  While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem.
2309.08163_1912967_0	  As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior.
2309.08182_1912986_3	This result demonstrates that by using similar problems and their answers as prompt, LLM could solve elementary physics word problems approaching human level performance.
2309.08614_1913418_9	This study stands as a significant contribution to the discourse on AI consciousness, underscoring the imperative for continued research to unravel the full spectrum of AI capabilities and the ramifications they hold for future human-AI interactions.
2309.08631_1913435_0	  Large Language Models (LLMs) demonstrate increasingly human-like abilities across a wide variety of tasks.
2309.08817_1913621_0	  In this work, we establish a baseline potential for how modern model-generated text explanations of movie recommendations may help users, and explore what different components of these text explanations that users like or dislike, especially in contrast to existing human movie reviews.
2309.08859_1913663_0	  Large Language Models (LLMs) represent the recent success of deep learning in achieving remarkable human-like predictive performance.
2309.08902_1913706_1	This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing.
2309.08902_1913706_4	We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the "what is beautiful is good" bias found in people in experimental psychology.
2309.08902_1913706_5	We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group.
2309.08968_1913772_0	  Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text.
2309.09828_1914632_0	  Trust is widely regarded as a critical component to building artificial intelligence (AI) systems that people will use and safely rely upon.
2309.10318_1915122_3	AI systems should be recognised as socio-technical systems, where the people involved in designing, developing, deploying, and using the system are as important as the system for determining whether it is trustworthy.
2309.10371_1915175_0	  A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are.
2309.10371_1915175_4	Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.
2309.10492_1915296_3	GPT-4's performance is much better than that of previous models and suggests that learning to work with common human values is not the hard problem for AI ethics.
2309.10691_1915495_11	We expect MINT can help measure progress and incentivize research in improving LLMs' capabilities in multi-turn interactions, especially for open-source communities where multi-turn human evaluation can be less accessible compared to commercial LLMs with a larger user base.
2309.11000_1915804_0	  This paper explores the potential of constructing an AI spoken dialogue system that "thinks how to respond" and "thinks how to speak" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.
2309.11231_1916035_4	Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors.
2309.11456_1916260_1	Referred to as Generative Agent-Based Models (GABMs), such individual-level models utilize large language models such as ChatGPT to represent human decision-making in social settings.
2309.11456_1916260_6	We hope the article and the model serve as a guide for building useful diffusion models that include realistic human reasoning and decision-making.
2309.11672_1916476_2	Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses.
2309.11672_1916476_5	The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more human-like attributes in AI.
2309.11838_1916642_5	Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses.
2309.12348_1917152_4	A future that can be extremely promising if humanity manages to have AI as a proper ally and partner, with distinct roles and specific rules of cooperation and interaction.
2309.12426_1917230_4	In this paper, we evaluate the performance of GPT-4 as a replacement for human annotators for low resource reading comprehension tasks, by comparing performance after fine tuning, and the cost associated with annotation.
2309.12881_1917685_6	With this work, we aim to shed light on the extent to which LLMs can replicate human-like affect recognition capabilities in conversations.
2309.13094_1917898_3	Large Language Models (LLMs), such as ChatGPT, represent this approach's capabilities, utilizing reinforcement learning with human feedback (RLHF).
2309.13193_1917997_1	However, LLMs inherently lack embodiment as humans, resulting in suboptimal performance in many embodied decision-making tasks.
2309.13193_1917997_5	Results indicate that incorporating expert demonstration data significantly reduced collision rates by 81.04\% and increased human likeness by 50\% compared to a baseline LLM-based agent.
2309.13233_1918037_3	Unlike previous work, which sought to maximize goal success rate (GSR) as the primary metric of simulator performance, our goal is a system which achieves a GSR similar to that observed in human interactions with TOD systems.
2309.13322_1918126_0	  The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications.
2309.13356_1918160_3	Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning ability no better than that of a random baseline, while ChatGPT, Llama2-Chat, PaLM-2 and GPT-4 show significantly better performance on this task, comparable to adult humans.
2309.13638_1918442_9	More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.
2309.13788_1918592_6	Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm.
2309.14402_1919206_5	Our findings also apply to modern pretrained language models such as GPT-4, thus giving rise to many Turing tests to distinguish Humans from contemporary AIs.
2309.14921_1919725_1	Tools like ChatGPT are being used by members of the disabled community e.g., Autistic people may use it to help compose emails.
2309.15714_1920518_0	  With the recent proliferation of large language models (LLMs), such as Generative Pre-trained Transformers (GPT), there has been a significant shift in exploring human and machine comprehension of semantic language meaning.
2309.15723_1920527_3	This paper investigated existing tools with a framework from two perspectives: the stages in the storytelling workflow where a tool serves, including analysis, planning, implementation, and communication, and the roles of humans and AI in each stage, such as creators, assistants, optimizers, and reviewers.
2309.15839_1920643_0	  Understanding how children design and what they value in AI interfaces that allow them to explicitly train their models such as teachable machines, could help increase such activities' impact and guide the design of future technologies.
2309.16436_1921240_0	  Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts.
2309.16721_1921525_3	We introduce GPT-Lab, a paradigm that employs GPT models to give robots human-like intelligence.
2309.16938_1921742_4	However, statistical analysis on the LLMs outputs when subject to small, yet still human-readable, alterations in the I/O representations (e.g., asking for BIO tags as opposed to line numbers) showed that the models are not performing reasoning.
2309.17158_1921962_0	  As artificial intelligence (AI) technologies spread worldwide, international discussions have increasingly focused on their consequences for democracy, human rights, fundamental freedoms, security, and economic and social development.
2309.17421_1922225_5	Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting.
2310.00074_1922331_1	Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains.
2310.00212_1922469_2	The dominant approach for steering LLMs towards beneficial behavior involves Reinforcement Learning with Human Feedback (RLHF), with Proximal Policy Optimization (PPO) serving as the default RL optimizer.
2310.00322_1922579_3	These static approaches lead to significant reductions in generation diversity, known as the mode collapse, which makes it difficult to discover the potential risks in the increasingly complex human-LLM interactions.
2310.00322_1922579_7	Insightfully, the geometrical structure we unveil of the red team task aligns with the spinning top hypothesis, confirming the necessity of constructing a diverse LLM population as a promising proxy for heterogeneous human expert red-teamers.
2310.00378_1922635_5	We assess four representative LLMs and provide compelling evidence that the growth rates of LLM's "know what" and "know why" capabilities do not align with increases in parameter numbers, resulting in a decline in the models' capacity to understand human values as larger amounts of parameters.
2310.00578_1922835_0	  ChatGPT has been demonstrated to possess significant capabilities in generating intricate, human-like text, and recent studies have established that its performance in theory of mind tasks is comparable to that of a nine-year-old child.
2310.00578_1922835_6	In contrast, ChatGPT manifested a superior performance in accuracy compared to the children.
2310.01041_1923298_3	In this work, we frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously.
2310.01132_1923389_3	Experiments on two CLASS-coded datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic CLASS Instructional Support estimation accuracy using the proposed method (Pearson $R$ up to $0.48$) approaches human inter-rater reliability (up to $R=0.55$); (2) LLMs generally yield slightly greater accuracy than BoW for this task, though the best models often combined features extracted from both LLM and BoW; and (3) for classifying individual utterances, there is still room for improvement of automated methods compared to human-level judgments.
2310.01468_1923725_8	We find that strong LLMs like GPT-4 outperform human players by a large margin.
2310.01824_1924081_0	  We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges agents to use reasoning and decision-making skills to solve complex activities that resemble everyday human challenges.
2310.02046_1924303_2	The emergence of Large Language Models (LLMs) like GPT-4, which can show human-like reasoning abilities on some tasks, offers new opportunities for software engineering and web element localization.
2310.02046_1924303_4	Using an LLM, it selects the most likely web element from the top-ranked ones identified by the existing VON Similo method, ideally aiming to get closer to human-like selection accuracy.
2310.02046_1924303_9	Despite its slower execution time and additional costs of using the GPT-4 model, the LLMs human-like reasoning showed promise in enhancing web element localization.
2310.02050_1924307_1	Models such as LLaSM, X-LLM, and SpeechGPT exhibit an impressive ability to comprehend and generate human instructions.
2310.02124_1924381_0	  As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)?
2310.02124_1924381_4	Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories.
2310.02527_1924784_2	In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs.
2310.03026_1925283_1	To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding.
2310.03560_1925817_2	Large Language Models (LLMs) have emerged as general-purpose models with the ability to process complex information and produce human-quality text, presenting a wealth of potential applications in healthcare.
2310.03684_1925941_0	  Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content.
2310.04450_1926707_5	The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data.
2310.04782_1927039_3	Human-defined methods for estimating uncertainty typically assume that "uncertainty is lower when the model's response is correct compared to when it is incorrect."
2310.05157_1927414_1	As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning.
2310.05216_1927473_2	In this work, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of human reading patterns.
2310.05563_1927820_0	  This paper presents Social data and knowledge collective intelligence platform for TRaining Ethical AI Models (STREAM) to address the challenge of aligning AI models with human moral values, and to provide ethics datasets and knowledge bases to help promote AI models "follow good advice as naturally as a stream follows its course".
2310.05782_1928039_5	To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM.
2310.05818_1928075_3	Adversarial human-model interactions and conversations significantly increase the challenges compared to existing methods.
2310.05853_1928110_2	In this research, we chose cooking, a complex daily task, as a scenario to investigate people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA, Mango Mango.
2310.06147_1928404_10	The RM step in RLHF generates a proxy of the expensive human feedback, such an insight can be generalized to other LLM tasks such as prompting evaluation and optimization where feedback is also expensive.   
2310.06155_1928412_7	We also address potential ethical issues, such as biases and over-reliance on AI, advocating for using the system to improve human research creativity rather than automating scientific inquiry.
2310.06225_1928482_6	On one of our experiments, GPT-4 obtained the highest performance when compared to human subjects.
2310.06239_1928496_8	This study demonstrates that (1) machines can learn soft prompts better than humans, (2) frozen LLMs have better few-shot learning ability and transfer learning ability to facilitate muti-institution applications, and (3) frozen LLMs require large models.
2310.06303_1928560_0	  This work introduces a robotics platform which embeds a conversational AI agent in an embodied system for natural language understanding and intelligent decision-making for service tasks; integrating task planning and human-like conversation.
2310.06408_1928665_5	Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans.
2310.06452_1928709_0	  Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude.
2310.06500_1928757_2	Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results.
2310.06778_1929035_2	We describe participants' expectations of generative AI's impact, including a dominant narrative that cut across the groups' discourse: participants largely envision generative AI as a tool to perform menial work, under human review.
2310.07018_1929275_7	We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%).
2310.07019_1929276_6	Finally, we discuss the limitations of our case-based approach as well as how it may be best used to augment existing constitutional approaches when it comes to aligning human and AI decisions.
2310.07225_1929482_9	The top scoring LLM, GPT-4o Turbo, scored $84\%$, with Claude Opus, Gemini 1.5 Pro and Llama 3/3.1 between $74\%$ and $79\%$. We found evidence of similarities between models in which questions they answer correctly, as well as similarities with human test takers.
2310.07641_1929898_0	  As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models.
2310.07652_1929909_3	To address this research gap, we propose LLM4Vis, a novel ChatGPT-based prompting approach to perform visualization recommendation and return human-like explanations using very few demonstration examples.
2310.08433_1930690_2	We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style.
2310.08899_1931156_2	Even state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise.
2310.08903_1931160_0	  Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs.
2310.08908_1931165_3	The characteristics of LLM, such as in-context learning and prompt engineering, closely mirror human cognitive abilities in language tasks, offering an intuitive solution for human-in-the-loop generation.
2310.08908_1931165_5	The pipeline initiates by prompting the LLM to produce a draft translation, followed by the utilization of automatic retrieval or human feedback as supervision signals to enhance the LLM's translation through in-context learning.
2310.09263_1931520_0	  Language models, such as GPT-3.5 and ChatGPT, demonstrate remarkable abilities to follow diverse human instructions and perform a wide range of tasks.
2310.09263_1931520_3	We show that our resulting Table-GPT models demonstrate (1) better \emph{table-understanding} capabilities, by consistently outperforming the vanilla GPT-3.5 and ChatGPT, on a wide-range of table tasks, including holdout unseen tasks, and (2) strong \emph{generalizability}, in its ability to respond to diverse human instructions to perform new table-tasks, in a manner similar to GPT-3.5 and ChatGPT.
2310.09430_1931687_0	  Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels.
2310.09485_1931742_2	As a result, AI has proven itself to be a power tool across various industries, simplifying complex tasks and pattern recognition that would otherwise be overwhelming for humans or traditional computer algorithms.
2310.10072_1932329_1	Recent studies on OpenAI's generative model GPT-3.5 proved its superiority in predicting the natural language with high accuracy and human-like responses.
2310.10076_1932333_1	One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc.
2310.10158_1932415_0	  Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts.
2310.10158_1932415_3	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.
2310.10436_1932693_5	In this work, we introduce EconAgent, a large language model-empowered agent with human-like characteristics for macroeconomic simulation.
2310.10508_1932765_9	The study revealed that GPT-4 with conversational prompts, incorporating human feedback during interaction, significantly improved performance compared to automated prompting.
2310.10544_1932801_3	We then had GPT4 (OpenAI), a Large Language Model, complete the same tasks as the human participants.
2310.10683_1932940_1	We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations.
2310.10690_1932947_8	Furthermore, our method using a fine-tuned version of the GPT-3.5 model is significantly better than using the base GPT-3.5 model and gets close to human tutors' performance.
2310.10844_1933101_2	Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard.
2310.11501_1933758_1	Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys.
2310.11564_1933821_2	In this work, we study Reinforcement Learning from Personalized Human Feedback (RLPHF) problem, wherein LLMs are aligned to multiple (sometimes conflicting) preferences by modeling alignment as a Multi-Objective Reinforcement Learning (MORL) problem.
2310.12162_1934419_6	Therefore, in this position paper, we argue that human-AI teaming is worthwhile in cybersecurity, in which human expertise such as intuition, critical thinking, or contextual understanding is combined with AI's computational power to improve overall cyber defenses.
2310.12321_1934578_8	To summarize, this comprehensive survey paper will serve as a good resource for both academic and industry people to stay updated with the latest research related to GPT-3 family large language models.
2310.12342_1934599_0	  Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic.
2310.12357_1934614_1	Their capacity to comprehend and generate human-like code has spurred research into harnessing LLMs for code analysis purposes.
2310.12362_1934619_1	Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP).
2310.12874_1935131_4	ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans).
2310.12902_1935159_0	  The paper proposes a framework that combines behavioral and computational experiments employing fictional prompts as a novel tool for investigating cultural artifacts and social biases in storytelling both by humans and generative AI.
2310.12902_1935159_8	The proposed framework argues that fiction can be used as a window into human and AI-based collective imaginary and social dimensions.
2310.12989_1935246_2	Our experiments, conducted on 3,671 snippets of clinical text, demonstrated that the LLM not only streamlines the multi-step natural language processing and human calibration processes but also achieves an exceptional accuracy rate of over 90% in exact matches when compared to human annotations.
2310.13011_1935268_0	  As language models (LMs) become more capable, it is increasingly important to align them with human preferences.
2310.13014_1935271_7	Overall, we find that GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts.
2310.13016_1935273_2	We developed a graph-based multiplication algorithm that emulated human-like numerical operations by incorporating a 10k operator, where k represents the maximum power to base 10 of the larger of two input numbers.
2310.13021_1935278_5	We close with open discussions and questions that we believe necessitate a multi-disciplinary perspective -- cognitive scientists working in tandem with AI researchers and mathematicians -- as we move toward better mathematical AI systems which not only help us push the frontier of the mathematics, but also offer glimpses into how we as humans are even capable of such great cognitive feats.
2310.13192_1935449_1	The analysis of the problem will be supported by a comment of Italian classical law categories such as causality, intent and fault to understand the problem of the usage of AI, focusing in particular on the human-machine interaction.
2310.13206_1935463_2	The question-answering capabilities of ChatGPT arise from its pre-training on large amounts of human-written text, as well as its subsequent fine-tuning on human preferences, which motivates us to ask: Does ChatGPT also inherits humans' cognitive biases?
2310.13385_1935642_0	  Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences.
2310.13548_1935805_1	But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy.
2310.13671_1935928_5	Extensive experiments on multiple NLP tasks show that our approach improves the performance of a small model by reducing the gap between the synthetic dataset and the real data, resulting in significant improvement compared to several baselines: 9.48% improvement compared to ZeroGen and 2.73% compared to GoldGen, and at most 15.17% improvement compared to the small model trained on human-annotated data.
2310.13704_1935961_5	Our findings indicate the prevalence of ChatGPT for human-computer interaction within academic sectors such as education, and research; trends also revealed the relatively high effectiveness of ChatGPT in improving human-machine collaboration.
2310.14159_1936416_0	  As short-form funny videos on social networks are gaining popularity, it becomes demanding for AI models to understand them for better communication with humans.
2310.14422_1936679_6	Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans.
2310.14424_1936681_5	This potential reduction in required human effort positions our approach as a valuable strategy in future large language model evaluations.
2310.14564_1936821_5	Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments.
2310.15004_1937261_3	We ask: how does this impact LMs' animacy processing - do they still behave as humans do?
2310.15004_1937261_5	Like previous studies, we find that LMs behave much like humans when presented with entities whose animacy is typical.
2310.15004_1937261_6	However, we also show that even when presented with stories about atypically animate entities, such as a peanut in love, LMs adapt: they treat these entities as animate, though they do not adapt as well as humans.
2310.15100_1937357_4	Recently the emerging field of large language models (LLMs) research has shown that LLMs have the potential replicate human-like behavior in various tasks: in particular, LLMs outperform crowd workers on text-annotation tasks, suggesting an opportunity to leverage LLMs on TA.
2310.15113_1937370_1	However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology.
