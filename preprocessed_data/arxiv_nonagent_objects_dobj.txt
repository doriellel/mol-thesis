1101.3316_238729_1	We train a model that separates QSOs from variable stars, non-variable stars and microlensing events using 58 known QSOs, 1,629 variable stars and 4,288 non-variables using the MAssive Compact Halo Object (MACHO) database as a training set.
1110.5632_296938_4	We then trained a one-class SVM (Support Vector Machine) model using the diagnostics features of the confirmed 58 MACHO QSOs.
1404.1521_514574_5	We evaluate the performance of training the model on the GPU and present optimizations that boost the performance on the GPU.One of the key optimizations, we propose increases the performance of a function involved in calculating and updating the gradient by approximately 50 times on the GPU for sufficiently large batch sizes.
1404.1521_514574_9	We conclude by presenting a thorough evaluation of the applicability of GP-GPU's for this task and highlight the factors limiting the performance of training a Polyglot model on the GPU.
1405.3515_524423_1	We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009.
1412.6650_584628_3	Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers.
1511.01574_674754_5	We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model.   
1604.00942_720109_9	Additionally, we find that training a model on the sentiment of the review text provides a competitive alternative when no five star rating information is available.
1608.03990_760452_4	When these model forms are reconstructed using neural networks (NN) and embedded within a standard solver, we show that much improved predictions in lift can be obtained for geometries and flow conditions that were not used to train the model.
1608.04077_760539_0	  In this paper, we propose a generative knowledge transfer technique that trains an RNN based language model (student network) using text and output probabilities generated from a previously trained RNN (teacher network).
1610.00494_775841_2	The tuning method that we propose enables dealing with errors without the need to re-train the system.
1701.03578_808986_0	  In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture.
1702.00137_814731_4	* How could we teach AI topics at an early undergraduate or a secondary school level?   
1707.07240_872272_7	A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs.
1710.10296_905679_7	We design the DRNN language model with Python and Theano, train the model on a CPU platform, and deploy the model on a PYNQ board to validate the model with Jupyter notebook.
1711.07908_914918_4	Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to "pretrain" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model.
1712.01996_920595_0	  Attention-based sequence-to-sequence models for automatic speech recognition jointly train an acoustic model, language model, and alignment mechanism.
1712.09783_928382_2	In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.
1802.08925_948465_1	We used images from optical coherence tomography angiography (OCTA), a relatively new imaging modality that measures perfusion of the retinal vasculature, to train an AI algorithm to generate vasculature maps from standard structural optical coherence tomography (OCT) images of the same retinae, both exceeding the ability and bypassing the need for expert labeling.
1803.03665_953798_2	We present a simple but highly effective approach for training neural LMs using both lexical and syntactic information, and a novel approach for applying such LMs to unparsed text using sequential Monte Carlo sampling.
1803.03879_954012_2	Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals.
1803.05457_955590_0	  We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering.
1803.11096_961229_0	  Group zero-attracting LMS and its reweighted form have been proposed for addressing system identification problems with structural group sparsity in the parameters to estimate.
1805.05062_978107_3	To overcome these limitations we build upon the recent reward augmented maximum likelihood approach \ie sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric.
1806.00801_986441_2	Although huge efforts have been made in building large-scale datasets very recently, e.g., the Aerial Image Dataset (AID) which contains 10,000 image samples, they are still far from sufficient to fully train a high-capacity deep CNN model.
1808.01423_1010569_4	The language model is used in a bootstrapping fashion to refine predictions in the target language for use as ground truth in training the model.
1809.00125_1020141_2	To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM.
1809.10036_1030052_1	Regulatory restrictions inhibit sharing of da-ta across different agencies, which could be a significant impediment to training AI models.
1811.02162_1046795_0	  In this paper, we explore several new schemes to train a seq2seq model to integrate a pre-trained LM.
1811.08674_1053307_7	Subgraphs are obtained by training a GNN-based graph refinement model to directly predict edge probabilities.
1812.04647_1062221_3	In this paper, we propose a solution to (a) estimate n-gram counts directly from the hand-written grammar for training LMs and (b) use constrained optimization to optimize the system parameters for future use cases, while not degrading the performance on past usage.
1812.11467_1069041_5	After training the language model, we show that the perplexity metric calculated for runtime data has a strong negative correlation with the correction of the erroneous NGS reads.
1812.11750_1069324_2	This stimulates a nascent field termed as federated learning for training a machine learning model on computation, storage, energy and bandwidth limited mobile devices in a distributed manner.
1904.08796_1113836_10	Open-access data sets and software implementations could alleviate these issues, and encourage further AI applications to pediatric ophthalmology.   
1904.09324_1114364_1	We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation.
1905.10863_1129299_2	Training this model on 9x9 Go produces a superhuman Go player, thus proving that it is stable and robust.
1907.03187_1147723_3	A large Twitter based corpus allowed us to train a language model from scratch focused on Spanish and transfer that knowledge to our competition model.
1907.04446_1148982_8	We present experiments utilizing crowdworkers to help address an important real-world AI safety problem in the domain of education.
1907.08377_1152913_2	The system maintains an append-only decentralized ledger to keep the log of critical information, including who has trained the model and improved its accuracy, when it has been improved, by how much it has improved, and where to find the newly updated model.
1907.09177_1153713_2	To perform such attacks, it is necessary for experts to train a tailored LM for a specific topic.
1909.00556_1170594_1	A common solution is to train a class-based n-gram language model and then expand the classes into specific words or phrases.
1909.02060_1172098_2	To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions.
1909.08053_1178091_7	To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT.
1909.09993_1180031_7	We also investigate the impact of the vocabulary size of A2W models and the data size for training LMs.
1910.03137_1187056_0	  In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns.
1910.03137_1187056_4	To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution.
1910.03432_1187351_5	We propose to train a recurrent neural network language model using the decentralized FederatedAveraging algorithm and to approximate this federated model server-side with an n-gram model that can be deployed to devices for fast inference.
1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.
1910.07360_1191279_4	Using a robust deep learning pipeline, a convolutional neural network is trained and implemented to detect rhinos and cars (considered an important tool in poaching for fast access and artefact transportation in natural habitats) in the study, that are found within live video streamed from drones Transfer learning with the Faster RCNN Resnet 101 is performed to train a custom model with 350 images of rhinos and 350 images of cars.
1911.03597_1202191_5	Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.
1911.04263_1202857_1	Several AI techniques including supervised learning and deep reinforcement learning (DRL) are adopted and improved to train effective AI agents for achieving the desired performance.
1912.03363_1215259_6	The proposed model results in 8% improvement in word error rate even when the amount of training data is a fraction of data used for training the first-pass system.
1912.08964_1220860_1	The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance.
1912.10308_1222204_2	The main challenge faced when training a language model is to deal with the language model corpus which is usually different to the one used for training the handwritten word recognition system.
2001.05295_1230708_2	This process is often constrained by having a relatively small number of patient records for training the model.
2001.05295_1230708_4	Such patient representation schemes enable a 3.5% mean improvement in AUROC on five prediction tasks compared to standard baselines, with the average improvement rising to 19% when only a small number of patient records are available for training the clinical prediction model.
2001.05838_1231251_6	The annotation scheme uses a K-means clustering algorithm along with merging conditions to achieve initial labelling information for training the U-Net model.
2001.06286_1231699_3	Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks.
2001.06286_1231699_4	While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT.
2001.09244_1234657_3	In this paper, we describe a novel 'proof of useful work' (PoUW) protocol based on training a machine learning model on the blockchain.
2001.10717_1236130_5	In this early work, a comparative evaluation of our MRE data driven simulation and the traditional method shows clinically significant differences in accuracy during landmark placement and motivates further animal model trials.
2002.12804_1250227_0	  We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM).
2002.12804_1250227_4	In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively.
2003.02089_1252444_0	  Federated learning (FL) is a promising technique that enables many edge devices to train a machine learning model collaboratively in wireless networks.
2003.08052_1258407_8	Finally, we trained a deep learning model that can explicitly predict and explain high level fashion concepts in a product image with its low level and domain specific fashion features.
2003.13003_1263358_4	It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.
2003.13027_1263382_6	We additionally train our model on the SwissText dataset to demonstrate usability on German.
2004.01388_1266158_5	Twenty-seven patients were randomly selected as independent test samples, and the remaining patients were used in a 5-fold cross validation experiment to confirm the hyperparameters, select optimal handcrafted features and train the model.
2004.01525_1266295_2	This paper proposes a Variational Autoencoder\cite{Kingma2014}(VAE)-based rhythm generation system, in which musicians can train a deep learning model only by selecting target MIDI files, then generate various rhythms with the model.
2004.03497_1268267_2	We train a 1.2B-parameter language model, ProGen, on ~280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component.
2004.04487_1269257_4	We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 $\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.
2004.04645_1269415_5	Using this we train a transformer-based neural model to perform extractive summarization conditioned on potential diagnoses.
2004.07159_1271929_3	This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context.
2004.14601_1279371_4	Surprisingly, training a model on either of these artificial languages leads to the same substantial gains when testing on natural language.
2004.14614_1279384_5	First, we train a large-scale language model and query it as textual knowledge.
2005.03848_1283643_2	As an alternative, we propose a new method for BERT distillation, i.e., asking the teacher to generate smoothed word ids, rather than labels, for teaching the student model in knowledge distillation.
2005.04726_1284521_1	Background knowledge provides complementary, real world factual information that can augment the limited labeled data to train a machine learning algorithm.
2005.14165_1293960_5	Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.
2006.02431_1296945_2	As a contribution to that effort, we are aggregating numerous small molecules from a variety of sources, using high-performance computing (HPC) to computer diverse properties of those molecules, using the computed properties to train ML/AI models, and then using the resulting models for screening.
2006.04229_1298743_7	We describe our methodology for collecting the data, preparing the corpus, and pre-training the model.
2006.05509_1300023_2	We evaluated five AI software products for screening and triaging TB using a large dataset that had not been used to train any commercial AI products.
2006.05676_1300190_0	  Masked language modeling (MLM) pre-training models such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens.
2006.10304_1304818_7	An additional 35% relative improvement in WER is achieved on one test set when training a TDNNF system with byte-pair encoding.
2006.11194_1305708_0	  Explainable AI provides insight into the "why" for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect.
2006.14779_1309293_0	  Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations.
2006.15437_1309951_2	One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels.
2007.05609_1317123_6	In this paper, we propose to train a context aware E2E model and allow the beam search to traverse into the context FST during inference.
2007.06162_1317676_0	  It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.
2007.07250_1318764_6	We extend that concept to address system-level autonomy capabilities of AI-enabled cyber-physical systems.
2007.11621_1323135_2	However, training an AI model on sensitive data raises concerns about the privacy of individual participants.
2007.12912_1324426_3	Motivated by these issues, this paper addresses a drone-enabled intelligent vehicular system, which is secure, easy to deploy and reliable in quality.
2007.13054_1324568_2	In the AGIFL, leveraging the flexible on-demand 3D deployment of aerial nodes such as unmanned aerial vehicles (UAVs), all the nodes can collaboratively train an effective learning model by FL.
2007.14686_1326200_8	Three of the databases (n=125, p=2,513) were used for training a machine learning model in recognizing AF events from beat-to-beat interval time series.
2008.03979_1331701_3	In this paper, we trained a Korean-specific model KR-BERT, utilizing a smaller vocabulary and dataset.
2008.07326_1335048_1	However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI.
2008.08113_1335835_5	In addition, we propose to train a parallel Bi-LRNN model based on the decoding lattices from both language models, and examine various ways of implementation.
2009.00694_1342197_1	In this paper, we present a deep learning approach to automatically assign protocols to computer tomography examinations, by pre-training a domain-specific BERT model ($BERT_{rad}$).
2009.04984_1346487_3	As the foundation for model pre-training, we synthesize a new dialogue corpus and build our training set with two unsupervised methods: 1) coherence-oriented context corruption, including utterance ordering, insertion, and replacement, to help the model capture the coherence inside the dialogue contexts; and 2) specificity-oriented automatic rescoring, which encourages the model to measure the quality of the synthesized data for dialogue-adaptive pre-training by considering specificity and informativeness.
2009.05886_1347389_1	When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected.
2009.07185_1348688_3	Significant transfer learning effects can be observed: Training a model on three simple core schemes allows it to accurately complete conclusions of different, and more complex types of arguments, too.
2009.10228_1351731_1	However, support for designing tools and curriculum to teach K-12 AI literacy is still limited.
2009.10228_1351731_2	There is a need for additional interdisciplinary human-computer interaction and education research investigating (1) how general AI literacy is currently implemented in learning experiences and (2) what additional guidelines are required to teach AI literacy in specifically K-12 learning contexts.
2009.11100_1352603_5	We identify opportunities for researchers and teachers to collaborate to make AI education more accessible, and present an exemplar lesson plan that shows entry points for teaching AI in non-computing subjects.
2009.11538_1353041_5	However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model.
2009.12787_1354290_1	In FL, a set of edge devices train a model using their local data, while repeatedly exchanging their trained updates with a central server.
2009.14109_1355612_1	How should we train a language model in this scenario?
2010.01413_1357743_4	The data is used to train a regression model and voting was used to show the highest correlation between travels made between cities and new cases of COVID-19.
2010.04887_1361217_3	We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information.
2010.07245_1363575_4	Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training.
2010.11506_1367836_4	(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.
2010.11639_1367969_2	In this paper, we consider the question of whether it is possible to pre-train a bilingual model for two remotely related languages without compromising performance at either language.
2010.12091_1368421_5	We believe that the migration dataset would be useful for training future migratable AI systems.
2010.12813_1369143_4	We train our model on subtrees sampled from WordNet, and test on non-overlapping WordNet subtrees.
2010.15437_1371767_1	While paired data are basically required to train the seq2seq model, the external LM can be trained with only unpaired data.
2011.01403_1374151_0	  State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss.
2011.02323_1375071_5	Here, we compare the efficacy of fine-tuning model parameters of pre-trained models against that of training a language model from scratch.
2011.02323_1375071_6	Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection.
2011.07347_1380095_2	Past approach relies on either modifying the original LM architecture, re-training the LM on corpora with attribute labels, or having separately trained `guidance models' to guide text generation in decoding.
2011.07371_1380119_1	We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the Booster environment, and using this surrogate model in turn to train the neural network for its regulation task.
2011.07713_1380461_3	However, the various environmental, diver and sensing uncertainties present underwater makes it challenging to train a robust and reliable diver action recognition system.
2011.10144_1382892_1	In this work, we estimate pollution reduction over the lockdown period by using the measurements from ground air pollution monitoring stations, training a long-term prediction model and comparing its predictions to measured values over the lockdown month.
2011.10737_1383485_2	Recently, substantial research efforts in learning-based methods for optimal control problems have been progressing significantly in addressing unknown system models, particularly when the system has complex interactions with the environment.
2011.11736_1384484_6	We first preprocessed the images to eliminate the batch effects of different devices, and then adopted a weakly supervised method to train the model without having any tags for the infected parts.
2012.12543_1400421_0	  Training a code-switching (CS) language model using only monolingual data is still an ongoing research problem.
2012.14682_1402560_5	We further devise a difficulty-aware objective, encouraging the model to output the class probability that reflects the real difficulty of each instance for a more reliable cascading mechanism.
2012.15832_1403710_2	First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity.
2101.00406_1404148_1	First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships.
2101.00529_1404271_4	In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \oscar \cite{li2020oscar}, and utilize an improved approach \short\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks.
2101.04617_1408359_3	We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.
2101.05967_1409709_1	Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.
2101.09635_1413377_1	However, for a relatively low-resource language such as Thai, the choices of models are limited to training a BERT-based model based on a much smaller dataset or finetuning multi-lingual models, both of which yield suboptimal downstream performance.
2101.12051_1415793_0	  Edge federated learning (FL) is an emerging paradigm that trains a global parametric model from distributed datasets based on wireless communications.
2102.01380_1417869_3	ILMT encourages the E2E model to form a standalone LM inside its existing components, without sacrificing ASR accuracy.
2102.01671_1418160_8	Furthermore, it enables training a machine learning (ML) model to search for \textit{good} sets of projections in the sense of minimizing the decoding error rate.
2102.01671_1418160_9	Training our ML model enables achieving very close to the performance of full-projection decoding with a significantly reduced number of projections.
2102.01671_1418160_10	For instance, our simulation results on a (64,14) RM subcode show almost identical performance for full-projection decoding and pruned-projection decoding with 15 projections picked via training our ML model.
2102.08015_1424504_0	  In the domain of air traffic control (ATC) systems, efforts to train a practical automatic speech recognition (ASR) model always faces the problem of small training samples since the collection and annotation of speech samples are expert- and domain-dependent task.
2102.10407_1426896_5	We train the proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions training data.
2102.11258_1427747_1	Zero-shot AEG is when we train a system to grade essays written to a new prompt which was not present in our training data.
2102.12162_1428651_5	We first tune the PhoBERT on our dataset by re-training the model on the Masked Language Model task; then, we employ its encoder for text classification.
2103.02410_1432555_2	To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date.
2103.03809_1433954_5	In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora.
2103.06518_1436663_2	Agents are connected to a cloud system able to train AI models to improve overall energy efficiency of an AI application executed on an edge platform.
2103.10685_1440830_6	Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering.
2103.10873_1441018_4	Addressing the DNN model design, from training and dataset augmentation to 8-bit quantization and deployment, we demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for the real-time execution (up to 135 frame/s) of our novel DNN, called PULP-Frontnet.
2103.11790_1441935_5	Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the "moral direction" for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.
2103.15004_1445149_7	It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces.
2104.03150_1450568_8	Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model.
2104.05544_1452962_6	We also investigate other methods to suppress the ILM mainly by decreasing the capacity of the AED model, limiting the label context, and also by training the AED model together with a pre-existing LM.
2104.10344_1457762_7	We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and applies a text-entity fusion encoding to aggregate entity representation.
2104.10661_1458079_1	Through training the model upon a mix of the Cornell Movie Dialogue Corpus for language understanding and an open-source, anonymized, and public licensed psychotherapeutic dataset, the model achieved statistically significant performance in published, standardized qualitative benchmarks against human-written validation data - meeting or exceeding human-written responses' performance in 59.7% and 67.1% of the test set for two independent test methods respectively.
2104.11390_1458808_1	However,training large language model needs massive computing resource, as more and more open source pre-training models are available, it is worthy to study how to take full advantage of available model.
2104.11390_1458808_5	When we continue training the target model, the training loss can start from a smaller value.
2105.00395_1462954_2	In AirMixML, multiple workers transmit analog-modulated signals of their private data samples to an edge server who trains an ML model using the received noisy-and superpositioned samples.
2105.05012_1467571_4	We use the collected English-learning data to train a predictive regression model based on students' monthly examination scores.
2105.11321_1473880_2	For each architecture, we trained a model instance using the whole dataset.
2105.12655_1475214_4	In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code.
2105.12833_1475392_3	It is desirable to train a model for launch prediction on a robot, as deep neural networks can account for immeasurable dynamics.
2106.01023_1478787_4	In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs.
2106.07340_1485104_2	In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content.
2107.05383_1499395_1	We asked the world's best language model, GPT-3, fifteen difficult questions about the nature, value, and future of library and information science (LIS), topics that receive perennial attention from LIS scholars.
2107.07691_1501703_5	To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.
2107.08909_1502921_7	In this method, an adversary uses the explanations to train the generative model and reduces the number of queries to steal the model.
2107.09051_1503063_6	Lastly, open issues and opportunities address future AI-empowered finance and finance-motivated AI research.
2107.11275_1505287_1	One way to understand and improve robustness of these models is an exploration of an adversarial attack scenario: check if a small perturbation of an input can fool a model.   
2107.11913_1505925_4	We do so by training a model to classify publications related to ethical issues and concerns.
2107.12168_1506180_7	One is to pre-train a language model based on RNN, and the other is to pre-train a sequence autoencoder.
2108.03555_1512370_4	SRH images were then used to train a convolutional neural network (CNN) model using three representation learning strategies: cross-entropy, self-supervised contrastive learning, and supervised contrastive learning.
2108.05002_1513817_5	We trained the proposed language model using a corpus of approximately 70,000 LaTeX sequences provided in CROHME 2016.
2108.06209_1515024_1	w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens.
2108.08295_1517110_4	Within the purview of these case studies, we show that it is possible to capture the design space and train a model to "generalize" prediction the optimal design and mapping parameters when queried with workload and design constraints.
2108.09105_1517920_8	Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.
2108.11193_1520008_4	Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.
2108.12175_1520990_9	Finally, we show that training an acoustic model for ASR tasks separately (i.e., separate models for ATCOs and pilots) or using a multitask approach is well suited for the noisy data and outperforms the traditional ASR system where all data is pooled together.
2109.02040_1524861_6	When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings.
2109.03570_1526391_1	Interestingly, in the absence of enough clinical data to train a model from scratch, we applied mixed-domain pretraining and cross-domain transfer approaches to generate a performant bio-clinical model suitable for real-world clinical data.
2109.05522_1528343_3	This work proposes a Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the mentioned constraints without training a complete Transformer model.
2109.06515_1529336_4	Then, we post-train the translation model with different levels of data at each training stages.
2109.06822_1529643_0	  Training a model for grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs, but manually annotating such pairs can be expensive.
2109.07140_1529961_0	  Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors dominate the landscape of Natural Language Processing due to their ability to scale across multiple tasks rapidly by pre-training a single model, followed by task-specific fine-tuning.
2109.07971_1530792_2	In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?
2109.08249_1531070_2	We investigate whether we can improve the $k$NN-LM performance by instead training a LM with the knowledge that we will be using a $k$NN post-hoc.
2109.09105_1531926_6	Additionally, pre-training the LM on spoken transcripts restrain its linguistic understanding.
2109.10836_1533657_4	Taking this wide perspective, this year there will be no single theme to lead the symposium and we encourage AI-HRI submissions from across disciplines and research interests.
2109.12346_1535167_5	To address this issue, we collected more than one million Algerian tweets, and pre-trained the first Algerian language model: DziriBERT.
2109.12346_1535167_7	The obtained results show that pre-training a dedicated model on a small dataset (150 MB) can outperform existing models that have been trained on much more data (hundreds of GB).
2109.14686_1537507_6	With experiments we demonstrate that using the image indeed helps beam tracking especially when the user is in serious NLOS, and the solution relies on carefully-designed dataset for training a model.
2109.14686_1537507_7	Generally speaking, including NLOS-like data for training a model does not benefit beam tracking of the user in LOS, but including light NLOS-like data for training a model benefits beam tracking of the user in serious NLOS.
2110.00942_1539085_1	The reason for this surge in research activities in this direction are mainly due to advent of robust AI algorithms (deep learning), availability of hardware that can train those robust and complex AI algorithms and accessibility of large enough dataset required for training AI algorithms.
2110.03215_1541358_6	By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.
2110.04725_1542868_2	However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers.
2110.05354_1543497_5	To make ILMA effective, it is essential to train the E2E model with an internal LM loss besides the standard E2E loss.
2110.06674_1544817_7	Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction.   
2110.06968_1545111_3	We harnessed the ThetaGPU supercomputer at the Argonne Leadership Computing Facility to train our AI model using a training set of 1.5 million waveforms.
2110.06968_1545111_4	We used 16 NVIDIA DGX A100 nodes, each consisting of 8 NVIDIA A100 Tensor Core GPUs and 2 AMD Rome CPUs, to fully train our model within 3.5 hours.
2110.07814_1545957_1	To tackle this problem in NLP, we propose $\textit{in-context tuning}$, which recasts adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, the labeled examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label from the input sequences on a collection of tasks.   
2110.08151_1546294_4	We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks.
2110.08460_1546603_6	2) we pre-train a compressed GPT-2 model using layer truncation and compare it against the distillation-based method (DistilGPT2).
2110.10261_1548404_4	We then train LMs on the 1-best and N-best translations and study ways to improve on such a baseline LM.
2111.00610_1554713_7	Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality.
2111.00826_1554929_0	  In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility.
2111.00826_1554929_4	We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs.
2111.01677_1555780_3	In the pretrain phase, we train the model with three tasks, (1) Video Tag Classification (VTC), (2) Mask Language Modeling (MLM) and (3) Mask Frame
2111.01677_1555780_5	In the finetune phase, we train the model with video similarity based on rank normalized human labels.
2111.09461_1563564_1	However, concerns surrounding security and trustworthiness impede the collection of large-scale representative medical data, posing a considerable challenge for training a well-generalised model in clinical practices.
2111.10267_1564370_7	Finally, we propose a heuristic for selecting the optimal number of retransmissions, which can be calculated before training the ML model.
2111.10962_1565065_2	They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection.
2111.11856_1565959_4	Spatio-temporal split learning is applied to this scenario to preserve privacy and globally train a fire classification model.
2111.14347_1568450_0	  As a promising distributed machine learning paradigm, Federated Learning (FL) trains a central model with decentralized data without compromising user privacy, which has made it widely used by Artificial Intelligence Internet of Things (AIoT) applications.
2112.00567_1570343_1	The standard way to train a deep language model is to employ unsupervised learning from scratch on a large unlabeled corpus.
2112.00590_1570366_2	At ADS, we are applying modern machine learning and natural language processing techniques to our dataset of recent astronomy publications to train astroBERT, a deeply contextual language model based on research at Google.
2112.02870_1572646_1	An emerging paradigm in ML is a federated approach where the learning model is delivered to a group of heterogeneous agents partially, allowing agents to train the model locally with their own data.
2112.06905_1576681_5	It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.
2112.07219_1576995_2	Our work overcomes existing data limitations for training AI models by curating, from YouTube, the largest dataset of open surgical videos to date: 1997 videos from 23 surgical procedures uploaded from 50 countries.
2112.07571_1577347_5	Training this complex model with a previously prohibitively large dataset was made possible for the first time by a partnership with Cerebras Systems, whose CS-1 system powered all pre-training experiments.
2112.07669_1577445_1	We trained AI models using 14 million waveforms, produced with the surrogate model NRHybSur3dq8, that include modes up to $\ell \leq 4$ and $(5,5)$, except for $(4,0)$ and $(4,1)$, that describe binaries with mass-ratios $q\leq8$, individual spins $s^z_{\{1,2\}}\in[-0.8, 0.8]$, and inclination angle $\theta\in[0,\pi]$.Our probabilistic AI surrogates can accurately constrain the mass-ratio, individual spins, effective spin, and inclination angle of numerical relativity waveforms that describe such signal manifold.
2112.11438_1581214_7	In order to overcome the difficulty in using gradient descent methods to directly estimate discrete quantized weights, alternating direction methods of multipliers (ADMM) are used to efficiently train quantized LMs.
2112.11668_1581444_1	Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.
2112.11668_1581444_4	In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization.
2201.01604_1586976_3	Here, we aim to train a machine learning model to separate these objects from the foreground stars and background galaxies using the multi-wavelength imaging data of the Fornax galaxy cluster in 6 filters, namely u, g, r, i, J and Ks.
2201.05955_1591327_2	Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns.
2201.05955_1591327_5	Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI.
2201.06009_1591381_5	On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3.
2201.06952_1592324_7	To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.
2201.09534_1594906_12	This is the first effort to train a DL model on multiple tasks in parallel.
2201.10707_1596079_1	In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pretrained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language.
2201.11990_1597362_3	In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron.
2202.01281_1600105_3	In this paper, we present an experience report of teaching an AI course to business executives in the United Arab Emirates (UAE).
2202.01281_1600105_4	Rather than focusing only on theoretical and technical aspects, we developed a course that teaches AI with a view to enabling students to understand how to incorporate it into existing business processes.
2202.02294_1601118_5	Method: We train a Transformer model from scratch and fine-tune two PTMs to evaluate the generated responses, which are compared to RRGEN, a current app response model.
2202.05983_1604807_4	We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions.
2202.10848_1609672_8	This paper addresses the AI community in this regard and stresses the influence AI systems can have on either increasing or reducing the violence that is inflicted on animals, and especially on farmed animals.
2202.11558_1610382_1	We fine-tune a collection of popular small, base, and large pretrained transformer-based language models, and train one feature-base model on the dataset with the aim of testing ensembles of these models.
2202.13610_1612434_2	We annotate over 1.5 k papers from NLP and ML to train a SciBERT-based model to automatically predict the stance of a paper based on its title and abstract.
2203.01008_1613870_0	  Decentralized learning empowers wireless network devices to collaboratively train a machine learning (ML) model relying solely on device-to-device (D2D) communication.
2203.03204_1616066_1	Considering the scarcity of funding and the little to none availability of specialised professionals to teach AI and robotics in developing countries, we present resources based on free open-source hardware and software, open educational resources, and alternative education programs.
2203.03204_1616066_2	That said, the contribution of this work is the pilot workshop of four lessons that promote diversity and inclusion on teaching AI and Robotics for children to a small gender-balanced sample of 14 children of an average age of 7.64 years old.
2203.04472_1617334_4	BinMLM trains the RNN language model on consecutive opcode traces extracted from the control-flow-graph (CFG) to characterize the candidate developers' programming styles.
2203.05936_1618798_2	The approach relies first on transforming the audio into a sequence of discrete units (or pseudo-text) and then training a language model directly on such pseudo-text.
2203.05936_1618798_7	On the basis of this study, we train a language model on the discrete units of the HuBERT features, reaching new state-of-the-art results in the lexical, syntactic and semantic metrics of the Zero Resource Speech Challenge 2021 (Track 1 - Speech Only).
2203.06112_1618974_0	  Training an AI/ML system on simulated data while using that system to infer on data from real detectors introduces a systematic error which is difficult to estimate and in many analyses is simply not confronted.
2203.06211_1619073_0	  The current standard approach to scaling transformer language models trains each model size from a different random initialization.
2203.08774_1621636_1	Our approach, contextual universal embeddings (CUE), trains LMs on one set of context, such as date and author, and adapts to novel metadata types, such as article title, or previous sentence.
2203.08774_1621636_7	Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context.
2203.09904_1622766_3	Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes.
2203.10692_1623554_3	We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training.
2203.10960_1623822_4	This practice-based research seeks to address these challenges with the use of Transformer construct to train a new model with only normal log entries.
2203.12788_1625650_5	Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language.
2203.15556_1628418_0	  We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.
2203.15556_1628418_3	We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data.
2203.15996_1628858_4	Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.
2204.04179_1634317_2	However, it is resource-intensive to train a PLM-based CCF model in an end-to-end (E2E) manner, since optimization involves back-propagating through every content encoding within a given user interaction sequence.
2204.10357_1640495_0	  Machine Teaching (MT) is an interactive process where humans train a machine learning model by playing the role of a teacher.
2204.10464_1640602_6	Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.
2204.14211_1644349_4	We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.
2205.01772_1646184_6	Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing.
2205.02949_1647361_0	  Federated learning (FL) is a promising solution to enable many AI applications, where sensitive datasets from distributed clients are needed for collaboratively training a global model.
2205.03946_1648358_3	Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives.   
2205.07303_1651715_4	Then, we train the Tibetan monolingual pre-trained language model named TiBERT on the data and vocabulary.
2205.08084_1652496_2	In this paper, we explore the possibility of developing a unified foundation model to support \emph{open-ended domains and tasks} in an industrial recommender system, which may reduce the demand on downstream settings' data and can minimize the carbon footprint by avoiding training a separate model from scratch for every task.
2205.08514_1652926_0	  Federated learning allows distributed users to collaboratively train a model while keeping each user's data private.
2205.09646_1654058_6	For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15\% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.
2205.10364_1654776_2	NNReverse trains a representation model to represent the semantics of binary code for DNN layers.
2205.10747_1655159_5	We then instruct a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content.
2205.10981_1655393_2	To address this issue, this study teaches GPT-3 to classify whether a question is related to data science by augmenting a small training set with additional examples generated by GPT-3 itself.
2205.11482_1655894_2	In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion.
2205.12206_1656618_3	Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus.
2205.12445_1656857_3	In this paper, we develop a novel unsupervised over-the-air (OTA) algorithm that utilizes noisy received pilot measurements to train a deep generative model to output beamspace MIMO channel realizations.
2205.12674_1657086_3	In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation.
2206.00621_1661041_2	To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning.
2206.02043_1662463_1	In this setting, spatially distributed devices belonging to each community collaboratively contribute towards training their community model via wireless links provided by the UAV.
2206.03354_1663774_1	We propose a pipeline that utilizes English-only vision-language models to train a monolingual model for a target language.
2206.03354_1663774_3	We propose a novel approach to knowledge distillation to train the model in other languages using parallel sentences.
2206.04793_1665213_1	AI's extraordinary potential is being held back by challenges such as a lack of medical datasets for training AI models, adversarial attacks, and a lack of trust due to its black box working style.
2206.05182_1665602_0	  Machine Teaching (MT) is an interactive process where a human and a machine interact with the goal of training a machine learning model (ML) for a specified task.
2206.05885_1666305_0	  Federated learning (FL) represents a promising distributed machine learning paradigm that allows smart devices to collaboratively train a shared model via providing local data sets.
2206.06994_1667414_5	Models trained using only RGB images on ProcTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks for navigation, rearrangement, and arm manipulation, including the presently running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges.
2206.07430_1667850_6	To stably train the residual LM, we propose smoothing the estimated internal LM and optimizing it with a combination of cross-entropy and mean-squared-error losses, which consider the statistical behaviors of the internal LM in the target domain data.
2206.07948_1668368_3	In this work, we propose an approach that trains a classification model to complement the capabilities of multiple human experts.
2206.09304_1669724_4	We then train a ML model and achieve >=99% classification accuracy from cellularly-pure samples, and >=87% accuracy from cellularly-mixed samples.
2206.12839_1673259_6	Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines.
2206.14623_1675043_4	In this work, we propose a contextual density ratio approach for both training a contextual aware E2E model and adapting the language model to named entities.
2207.00804_1676702_2	Specifically, CASAS dataset is employed to train a Random Forest (RF) model for activity inference.
2207.01893_1677791_2	From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch.
2207.04901_1680799_4	We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization.
2207.06220_1682118_7	We train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors.
2207.07033_1682931_3	Several projects supported by the DAF-MIT AI Accelerator are developing public challenge problems that address numerous Federal AI research priorities.
2207.07951_1683849_1	This study adopts a recent physics-based uncertainty quantification (UQ) approach to address such model form uncertainty in Reynolds-averaged Naiver- Stokes (RANS) simulations.
2207.08057_1683955_0	  Over-the-air federated learning (AirFL) allows devices to train a learning model in parallel and synchronize their local models using over-the-air computation.
2207.08988_1684886_2	Our goal is to train a large neural network language model (NNLM) on compute-constrained devices while preserving privacy using FL and DP.
2207.09374_1685272_5	By doing so, the user directly sees which characteristics of the input data can change arbitrarily without influencing the AI's decision.
2207.13921_1689819_5	Our proposed method, HelixFold-Single, first pre-trains a large-scale protein language model (PLM) with thousands of millions of primary sequences utilizing the self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the co-evolution information.
2207.14393_1690291_1	LAD is a paradigm for creating diverse and accurate synthetic data which conveys the necessary structural constraints and can be used to train a downstream neural dialog model.
2208.03008_1693720_6	In addition, we propose a separate-joint training approach to train the model, and extensive experiments are conducted to show that the proposed method is superior to its counterparts.
2208.04714_1695426_2	We find that researchers addressing AI rights have often seemed to be unaware of the work of colleagues whose interests overlap with their own.
2208.05643_1696355_1	In Air-FEEL, distributed edge devices use their local data to collaboratively train AI models while preserving data privacy, in which the over-the-air model/gradient aggregation is exploited for enhancing the learning efficiency.
2208.05969_1696681_4	In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective.
2208.08198_1698910_10	Moreover, we check to which extent our proposal is in line with the European AI Act proposal and current safety standardization initiatives addressing AI and Autonomous Systems
2208.09982_1700694_4	To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics.
2208.12816_1703528_7	We follow a procedure that directly trains the pruned model and avoids the computationally complex ranking and fine-tuning steps.
2208.14141_1704853_4	We propose synthesising airways by style transfer using perceptual losses to train our model, Airway Transfer Network (ATN).
2208.14493_1705205_4	To demonstrate the effectiveness of your approach, we create a custom dataset which we use to train a medical NER model for German texts, GPTNERMED, yet our method remains language-independent in principle.
2209.03431_1709146_4	Then, it proceeds with physics-informed adversarial training to teach the model the system-related physics domain foreknowledge through iteratively reducing the unwanted output deviations on the previously-uncovered counterexamples.
2209.06049_1711764_2	With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well.
2209.06049_1711764_4	We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text.
2209.06317_1712032_9	This paper explores these issues, focusing on the opportunities, challenges, and potential impacts of such an approach, and discussing how it might influence AI regulations.
2209.11624_1717339_1	However, when devices in a relatively large area cooperatively train a machine learning model, the attendant straggler issues will significantly reduce the learning performance.
2209.15271_1720986_4	Among them, an open-source dataset is constructed to train a multi-form human detection model that distinguishes a human being's whole body, upper body or part body, and the followed action classification model is adopted to recognize such action as falling, sleeping or on-duty, etc.
2210.01504_1722859_1	Previous work addressing privacy issues for language models has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM.
2210.02498_1723853_5	To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning.
2210.02969_1724324_2	In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label.
2210.04621_1725976_0	  AI tools can be useful to address model deficits in the design of communication systems.
2210.05487_1726842_2	We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES.
2210.05549_1726904_3	This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills.
2210.05883_1727238_3	Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting.
2210.06525_1727880_5	We train our model on the 4 Nguni languages of South Africa.
2210.06525_1727880_9	We also train our model as a word-level sequence model, resulting in an unsupervised morphological segmenter that outperforms existing methods by a large margin for all 4 languages.
2210.07109_1728464_4	We train a large language model (LM) to generate the next game turn, conditioning it on different information.
2210.07144_1728499_4	In our work we address this challenge by leveraging Model Reprogramming (MR), which repurposes pretrained models on a source language to adapt to the tasks that are in a different language and have scarce data - where it may be difficult to train a high-performing model from scratch or effectively fine-tune an existing pre-trained model on the specific task.
2210.08984_1730339_7	Therefore, in our journey towards an AI-enabled sustainable future, we need to address AI ethics and governance as a priority.
2210.10332_1731687_2	Addressing such incorrect model behavior via parameter adjustments is very costly.
2210.10626_1731981_3	However, Self-Supervised Learning (SSL) is a promising way to solve this problem by pre-training a DNN model utilizing unlabeled samples followed by a fine-tuned downstream task involving very limited labels.
2210.10626_1731981_4	Hence, this work proposes a hard-negative sample aware self-supervised contrastive learning method to pre-train the model for semantic segmentation.
2210.10659_1732014_4	The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms.
2210.12022_1733377_4	Our findings suggest that even though fine-tuning and prompting work well to train large LMs on large train sets, there are more efficient alternatives that can reduce compute or data cost.
2210.12378_1733733_5	With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency.
2210.12530_1733885_3	Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.
2210.13918_1735273_4	In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it.
2210.14845_1736200_2	This result also implies that manual efforts for developing per-voxel annotation of tumors (which took years to create) can be considerably reduced for training AI models in the future.
2210.16663_1738018_3	This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency.
2210.17236_1738591_6	For APICoder, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing API information.
2210.17469_1738824_0	  Federated Edge Learning (FEEL) is a distributed machine learning technique where each device contributes to training a global inference model by independently performing local computations with their data.
2211.00083_1738989_1	Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data.
2211.00286_1739192_0	  End-to-end (E2E) artificial intelligence (AI) pipelines are composed of several stages including data preprocessing, data ingestion, defining and training the model, hyperparameter optimization, deployment, inference, postprocessing, followed by downstream analyses.
2211.00780_1739686_6	This dataset is used to train a multi-modal ML model, Air Quality Network (AQNet) capable of fusing these various types of data sources to output predictions of various pollutants.
2211.01223_1740129_2	Next, we train a transformer-based causal language model using these representations.
2211.03363_1742269_0	  Privacy and bandwidth constraints have led to the use of federated learning (FL) in wireless systems, where training a machine learning (ML) model is accomplished collaboratively without sharing raw data.
2211.05110_1744016_4	This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining.
2211.06648_1745554_4	More specifically, after pre-training one of state-of-the-art vision-based models as our backbone network, we re-train our augmented model, consisting of the vision-based model and the multilayer perceptron (MLP) architecture.
2211.08769_1747675_8	The two decoding losses are added up to train a unified encoding model.
2211.11363_1750269_4	Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain.
2211.13606_1752512_4	Training a single AI model utilizing all these data is not feasible with conventional federated learning (FL).
2212.00616_1756793_1	X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words.
2212.00616_1756793_2	Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words, thereby making a prompt more descriptive.
2212.01215_1757392_1	However, training AI models centrally with the assistance of SAGIN faces the challenges of highly constrained network topology, inefficient data transmission, and privacy issues.
2212.01779_1757956_4	To solve the problem of scarcity of datasets on minority languages and verify the effectiveness of the MiLMo model, this paper constructs a minority multilingual text classification dataset named MiTC, and trains a word2vec model for each language.
2212.02924_1759101_2	Then we also investigate the feasibility of steering the output of this extended soft prompted T5 model at decoder level and finally analyse the utility of generated text to be used in AI related tasks such as training AI models with an interpretability analysis of the classifier trained with synthetic text, as there is a lack of proper analysis of methodologies in generating properly labelled data to be utilized in AI tasks.
2212.04940_1761117_4	More importantly, our method can reconstruct a class of similar states simultaneously, in comparison with the existing neural network methods that need to train a model for each unknown state.
2212.05956_1762133_1	However, such methods impose the additional burden of training a separate teacher model for every new dataset.
2212.07542_1763719_4	The primary concern of this paper is the creation of an interface for students to learn the principles of artificial intelligence by using a natural language pipeline to train a customized model to answer questions based on their own school curriculums.
2212.07588_1763765_9	To train the model, we devise the techniques for preparing training data as well as data augmentation.
2212.07617_1763794_1	In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model.
2212.08073_1764250_1	We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.
2212.08073_1764250_5	In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences.
2212.08073_1764250_7	As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them.
2212.09251_1765428_3	We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering.
2212.09282_1765459_4	We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences.
2212.09849_1766026_7	Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.
2212.10561_1766738_8	Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.
2212.11123_1767300_8	In THMA, we train AI models directly from massive HD map datasets via supervised, self-supervised, and weakly supervised learning to achieve high accuracy and efficiency required by downstream users.
2212.11311_1767488_3	Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production.
2301.02111_1773175_1	Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work.
2301.03052_1774116_6	We start by introducing some highlighted robustness challenges in the AI lifecycle and motivating AI maintenance by making analogies to car maintenance.
2301.03728_1774792_5	Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models.
2301.05804_1776868_4	Next, we use a custom salience loss function, Salience-Sensitive Focal Loss, to train a Deformable DETR object detection model in order to emphasize stronger performance on salient signs.
2301.06251_1777315_9	Building upon the soft-subRPA algorithm, we then provide a framework for training a machine learning (ML) model to search for \textit{good} sets of projections that minimize the decoding error rate.
2301.06251_1777315_10	Training our ML model enables achieving very close to the performance of full-projection decoding with a significantly smaller number of projections.
2301.06859_1777923_3	In particular, we focus on methods for collecting reliable human feedback on summaries to train a reward model which in turn improves the summarization model.
2301.08745_1779809_5	Further, we explore an interesting strategy named $\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably.
2301.08986_1780050_0	  Domain-adaptive pre-training (or DA-training for short), also known as post-training, aims to train a pre-trained general-purpose language model (LM) using an unlabeled corpus of a particular domain to adapt the LM so that end-tasks in the domain can give improved performances.
2301.09626_1780690_6	Instead of training a model from scratch, we exploit a smaller model that is in the target language but requires much fewer resources.
2301.12004_1783068_6	The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured.
2301.12031_1783095_7	We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks.
2301.12243_1783307_5	Our findings revealed that practitioners use the guidebook not only for addressing AI's design challenges, but also for education, cross-functional communication, and for developing internal resources.
2302.00093_1785027_5	We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.
2302.00444_1785378_2	The problem is to make full use of them to train the student model.
2302.01526_1786460_1	One of the expected roles of XAI methods is verifying whether inferences of a trained machine learning model are valid for an application, and it is an important factor that what datasets are used for training the model as well as the model architecture.
2302.03241_1788175_2	Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain.
2302.03930_1788864_5	The pre-processed result was used as input features in training a Bi-LSTM model in making future forecasts of the values of the particulate matter Pm2.5, and Pm10.
2302.05206_1790140_3	In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner.
2302.05406_1790340_10	This combination allows us to train a single model to perform joint inference with multiple knowledge graphs.
2302.08917_1793851_1	In this work, we propose to train a single multilingual language model (LM) for shallow fusion in multiple languages.
2302.12509_1797443_1	Under such a setting, multiple clients collaboratively train a global generic model under the coordination of an edge server.
2302.13681_1798615_4	We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma.
2302.13817_1798751_5	We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
2302.13939_1798873_5	We train the proposed model on two model variants: 45M and 216M parameters.
2302.14389_1799323_2	To address this question, we trained a lexical language model, Glove, and a supra-lexical language model, GPT-2, on a text corpus from which we selectively removed either syntactic or semantic information.
2303.01903_1801700_5	Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge.
2303.03012_1802809_2	However, training a well-performing LLM demands a substantial workforce for data collection and annotation.
2303.03012_1802809_9	Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs.
2303.03926_1803723_1	Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts.
2303.03956_1803753_0	  In this paper, we present a pilot study aiming to investigate the challenges of teaching AI and Robotics to children in low- and middle-income countries.
2303.03956_1803753_1	Challenges such as the little to none experts and the limited resources in a Mexican town to teach AI and Robotics were addressed with the creation of inclusive learning activities with Montessori method and open-source educational robots.
2303.04910_1804707_1	Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs.
2303.06430_1806227_3	We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
2303.09461_1809258_0	  We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''.
2303.10845_1810642_1	In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}.
2303.10845_1810642_2	With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS).
2303.12135_1811932_3	Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B).
2303.12231_1812028_0	  We train a model atom to recognize hand-written digits between 0 and 9, employing intense light--matter interaction as a computational resource.
2303.12984_1812781_2	LMCodec trains a Transformer language model to predict the fine tokens from the coarse ones in a generative fashion, allowing for the transmission of fewer codes.
2303.14177_1813974_3	Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference.
2303.14956_1814753_3	In this work, we propose a simple and efficient approach to instruct large language model (LLM) to extract a variety of structures from texts.
2304.00385_1818431_9	For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.
2304.03208_1821254_2	We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget).
2304.03245_1821291_2	We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English).
2304.03262_1821308_5	Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs.
2304.03262_1821308_6	In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT.
2304.03271_1821317_2	For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.
2304.03893_1821939_3	The prompts encourage ChatGPT to output a sequence of predefined robot actions, represent the operating environment in a formalized style, and infer the updated state of the operating environment.
2304.05128_1823174_2	In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations.
2304.05128_1823174_3	In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language.
2304.05510_1823556_5	We present our conversational AI prototype, available at www.chatclimate.ai and demonstrate its ability to answer challenging questions accurately in three different QA scenarios: asking from 1) GPT-4, 2) chatClimate, and 3) hybrid chatClimate.
2304.05511_1823557_4	However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts.
2304.05511_1823557_8	We show that we can successfully train GPT 13B to the same quality as the dense GPT 13B model, while achieving an end-end speedup of 4.5x over dense A100 baseline.
2304.06794_1824840_3	In our study, we asked GPT to identify the ten most significant subdisciplines within the field of environmental science.
2304.07061_1825107_2	It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions.
2304.08442_1826488_2	To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents.
2304.08442_1826488_5	To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only $1.9\%$/$2.5\%$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the amount of data.
2304.09248_1827294_6	The optimal hyperparameters for training the model are determined using genetic algorithms.
2304.09286_1827332_6	This paper introduces a scalable learning pipeline to train AI-based agent models toward automated endovascular predictive device controls.
2304.09542_1827588_4	Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks.
2304.09655_1827701_5	Specifically, we ask ChatGPT to generate a number of program and evaluate the security of the resulting source code.
2304.09667_1827713_2	In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions.
2304.11111_1829157_6	Anxiety-induction not only influences LLMs' scores on an anxiety questionnaire but also influences their behavior in a previously-established benchmark measuring biases such as racism and ageism.
2304.11123_1829169_2	Given AI's massive potential, as well as the fierce geopolitical tensions between China and the U.S., several recent policies have been put in place to discourage AI scientists from migrating to, or collaborating with, the other nation.
2304.11872_1829918_2	To overcome these limitations, we introduce a novel method, namely GenCo, which leverages the strong generative power of LLMs to assist in training a smaller and more adaptable language model.
2304.12198_1830244_4	Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity.
2304.12898_1830944_1	ChatGPT consistent avoidance of passing the test is here overcome by asking ChatGPT to apply the Turing test to itself.
2304.13013_1831059_5	As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.
2305.00875_1833931_9	Through concept analysis, we explore the traceability and distribution of human-recognizable concepts within latent code representations which could be used to influence model predictions.
2305.01937_1834993_4	We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.
2305.03148_1836204_1	However, training AI on resource-limited devices poses significant challenges due to the demanding computing workload and the substantial memory consumption and data access required by deep neural networks (DNNs).
2305.03212_1836268_4	We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss.
2305.03653_1836709_4	We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query.
2305.03701_1836757_0	  Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4, is resource-intensive.
2305.04790_1837846_5	To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly.
2305.04812_1837868_3	However, the extent to which external information influences LLMs' cognition and behaviors remains unclear.
2305.04812_1837868_4	This study investigates how external statements and opinions influence LLMs' thoughts and behaviors from a social cognitive perspective.
2305.05973_1839029_2	To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system.
2305.07429_1840485_4	The key idea is to train a deep learning model on a medical image dataset to extract four types of information: the type of image scan, the body part, the test image, and the results.
2305.07912_1840968_5	We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models.
2305.08391_1841447_2	To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input.
2305.09067_1842123_3	Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data.
2305.09434_1842490_4	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2305.10142_1843198_2	We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively.
2305.10429_1843485_1	In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks.
2305.10429_1843485_2	We then resample a dataset with these domain weights and train a larger, full-sized model.
2305.10429_1843485_3	In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently.
2305.10649_1843705_1	The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken.
2305.11169_1844225_1	Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments.
2305.11169_1844225_6	In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code.
2305.11189_1844245_5	It considers cloud operations and security in a holistic framework to collect the metrics required to assess the security threats and train the AI models to take immediate actions.
2305.12865_1845921_7	Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set.
2305.13172_1846228_0	  Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive.
2305.13252_1846308_4	Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
2305.13661_1846717_5	Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.
2305.13724_1846780_4	Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features.
2305.13733_1846789_6	Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.
2305.13917_1846973_6	We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort.
2305.14688_1847744_2	We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background.
2305.14705_1847761_1	Instruction tuning is a technique for training LLMs to follow instructions.
2305.14802_1847858_3	To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features.
2305.14930_1847986_2	We ask LLMs to assume different personas before solving vision and language tasks.
2305.15075_1848131_4	To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion.
2305.15076_1848132_6	We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step.
2305.15541_1848597_4	This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model.   
2305.16765_1849821_4	We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer.
2305.16986_1850042_1	Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent.
2305.17116_1850172_2	Training LLMs on focused corpora poses computational challenges.
2305.17333_1850389_3	For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget.
2305.17718_1850774_8	Subsequently, this data is utilized to train a captioning generation BLIP-based model.
2305.18098_1851154_5	Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages.
2305.18098_1851154_6	Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model.
2305.19118_1852174_1	Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively.
2305.19339_1852395_3	We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen.
2305.19352_1852408_2	We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003.
2305.19512_1852568_2	In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers.
2306.00622_1853770_4	Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers.
2306.00745_1853893_5	We further implement a two-step table annotation pipeline which first determines the class of the entities described in the table and depending on this class asks ChatGPT to annotate columns using only the relevant subset of the overall vocabulary.
2306.01272_1854420_3	Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection.
2306.01311_1854459_4	Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder.
2306.01684_1854832_3	However, generating private synthetic data is much harder than training a private model.
2306.01771_1854919_4	ProcessGPT can be designed by training a generative pre-trained transformer model on a large dataset of business process data.
2306.02029_1855177_4	Each UAV agent trains a local QMIX model in its simulated environment and continuously consolidates it through federated learning with other agents, accelerating the learning process.
2306.02210_1855358_2	These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework.
2306.02857_1856005_2	Methods: Topological data analysis (TDA) is applied to characterize BPV from the intrinsically nonstationary airflow signal, where the extracted features are used to train an automatic sleep stage scoring model using the XGBoost learner.
2306.02907_1856055_5	After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code.
2306.02920_1856068_2	Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.
2306.03586_1856734_4	For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks.
2306.04803_1857951_1	We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy.
2306.05064_1858212_6	Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model.
2306.06503_1859651_9	Therefore, we ardently advocate for the adoption of DP in training diagnostic medical AI models, given its minimal impact on performance.
2306.06615_1859763_3	Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges.
2306.10765_1863913_2	However, training cross-domain LLMs in the medical field poses significant challenges primarily attributed to the requirement of collecting data from diverse domains.
2306.10850_1863998_4	The methodology is tested using artificial and realistic Ozone concentration profiles to train a Gated Recurrent Unit (GRU) model.
2306.10900_1864048_3	Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer.
2306.11296_1864444_5	This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%.
2306.12213_1865361_0	  With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks.
2306.13421_1866569_2	In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch and apply it to the task of modeling long texts.
2306.13649_1866797_0	  Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model.
2306.13723_1866871_2	The interaction between users and AI results in a potentially endless feedback loop, wherein users' choices generate data to train AI models, which, in turn, shape subsequent user preferences.
2306.14824_1867972_2	Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model.
2306.14838_1867986_0	  We train a generative language model on the randomized local measurement data collected from Schr\"odinger's cat quantum state.
2306.15728_1868876_2	We first trained these spatiotemporal-graph AI models using synthetic noise, using 1.2 million modeled waveforms to densely sample this signal manifold, within 1.7 hours using 256 A100 GPUs in the Polaris supercomputer at the ALCF.
2306.15903_1869051_0	  Training AI with strong and rich strategies in multi-agent environments remains an important research topic in Deep Reinforcement Learning (DRL).
2306.15903_1869051_1	The AI's strength is closely related to its diversity of strategies, and this relationship can guide us to train AI with both strong and rich strategies.
2306.15912_1869060_5	We trained the CLAPE-DB model on the protein-DNA binding sites dataset and evaluated the model performance and generalization ability through various experiments.
2306.16092_1869240_2	By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model.
2306.17649_1870797_5	Surprisingly, we find that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance.
2307.00108_1871104_2	To handle these issues, we introduce Ticket- BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets.
2307.00461_1871457_2	This work aims to adapt these architectures in a causal setup for training LLMs.
2307.01139_1872135_3	To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding.
2307.02157_1873153_5	We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.
2307.02157_1873153_6	Moreover, we propose to train a model which can evaluate the matching degree between CVs and JDs as a reward model, and we use Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) method to further fine-tine the generator.
2307.02192_1873188_8	We make the source code available for the 112, 000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms.
2307.02499_1873495_5	Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy.
2307.02736_1873731_4	Approach: To address this need, we propose a parametric map refinement approach for learning-based $T_1\rho$ mapping and train the model in a probabilistic way to model the uncertainty.
2307.03838_1874833_3	While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning.
2307.03917_1874912_5	In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone.
2307.04114_1875109_7	For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization.
2307.04408_1875403_4	To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation.
2307.04599_1875594_8	Results:The study's findings show that language workbenches are of paramount importance in dealing with all aspects of modeling language development and are leveraged to define DSL explicitly addressing AI concerns.
2307.04827_1875822_3	We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model.
2307.05494_1876489_2	This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact.
2307.05494_1876489_3	Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions.
2307.06616_1877611_6	To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB.
2307.06834_1877829_3	The proposed approach, coined as Radar-aided Dynamic blockage Recognition (RaDaR), leverages radar measurements and federated learning (FL) to train a dual-output neural network (NN) model capable of simultaneously predicting blockage status and time.
2307.07164_1878159_3	Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever.
2307.07171_1878166_8	Different from previous works like denoised smoothing, which requires training a separate model to robustify LLM, our method enjoys far better efficiency and flexibility.
2307.08487_1879482_5	Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions.
2307.08674_1879669_6	By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions.
2307.09964_1880959_1	However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand.
2307.10198_1881193_7	Our analysis shows that by 2018, the time lag between China and the USA in addressing AI research topics had evaporated.
2307.10315_1881310_1	This paper argues that training AI systems with absolute constraints -- which forbid certain acts irrespective of the amount of value they might produce -- may make considerable progress on many AI safety problems in principle.
2307.10490_1881485_2	When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction.
2307.10700_1881695_0	  Large language models (LLMs) are dramatically influencing AI research, spurring discussions on what has changed so far and how to shape the field's future.
2307.12981_1883976_5	To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images.
2307.12981_1883976_6	Then, we use 2D VLMs as our backbones to train our 3D-LLMs.
2307.14430_1885425_9	We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.
2307.16118_1887113_4	We initially train a single-task RL expert model, sample expert data in the environment, and subsequently utilize a mixed multi-task dataset for offline GPT training.
2307.16368_1887363_7	It first recognizes the actions already performed in the observed videos and then asks an LLM to predict the future actions via conditioned generation, or to infer the goal and plan the whole procedure by chain-of-thought prompting.
2307.16372_1887367_6	In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings.
2308.00624_1888516_6	We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.
2308.01154_1889046_3	We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.
2308.01240_1889132_0	  In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks.
2308.01414_1889306_3	Meanwhile, there has not been any dataset of renewable energy for training LLMs.
2308.01430_1889322_2	To train FinVis-GPT, a financial task oriented dataset was generated for pre-training alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions.
2308.01497_1889389_4	Given the enormous and non curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high quality metaphors that are unlikely to have been included in the training data.
2308.01727_1889619_4	They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance.
2308.03314_1891206_7	To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation.
2308.03740_1891632_3	In addition, these results suggest that nation-states -- even those conducting many large-scale influence operations per year -- are unlikely to benefit economically from training custom LLMs specifically for use in influence operations.
2308.04386_1892278_5	Experimental results on the SummEval benchmark demonstrate that CSEM can effectively train an evaluation model without human-labeled data.
2308.04430_1892322_3	SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference.
2308.04515_1892407_1	One way to increase generalization to new scenes is to automatically label target data, which can then be used for training a detector model.
2308.04711_1892603_3	The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness.
2308.04711_1892603_5	For the second method ($\textit{RATD}$) we utilise retrieval-augmented training datasets developed by Hartill et al. 2023 to train a smaller Reasoning model such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequently contain many irrelevant sentences.
2308.05061_1892953_4	Also, we introduce a novel approach to train a language-model-like architecture, or directly fine-tune existing language models, for in-context operator learning.
2308.06212_1894104_9	LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow.
2308.06966_1894858_6	We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct.
2308.07758_1895650_3	Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided.
2308.08239_1896131_3	The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations.
2308.08241_1896133_0	  This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data.
2308.08493_1896385_3	To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it.
2308.08625_1896517_3	This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion.
2308.09308_1897200_1	However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training.
2308.09454_1897346_2	To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques.
2308.09954_1897846_7	Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets.
2308.10252_1898144_2	To address this, we present "LMTuner", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input.
2308.10335_1898227_7	Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help.
2308.10792_1898684_1	Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions.
2308.10873_1898765_3	In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient.
2308.11042_1898934_8	To evaluate our proposed technique, we trained the HS-BERT model using sentences from RISC-V, OpenRISC, MIPS, OpenSPARC, and OpenTitan SoC documentation.
2308.11534_1899426_5	Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.
2308.12060_1899952_4	This synthetic dataset facilitates training a specialized lightweight model for the KB.
2308.13911_1901803_0	  With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem.
2308.14328_1902220_1	The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution.
2308.14731_1902623_6	In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation.
2309.00237_1905041_2	We then use these synthetic notes to train our specialized clinical large language model, Asclepius.
2309.00240_1905044_6	Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately.
2309.00267_1905071_1	RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM.
2309.01576_1906380_3	Firstly, we trained a prosody prediction model using 15 different PLMs.
2309.01940_1906744_5	The code correction task asks LLMs to fix real-world erroneous code segments with different error messages.
2309.02033_1906837_1	A data recipe is a mixture of data from different sources for training LLMs, which plays a vital role in LLMs' performance.
2309.03118_1907922_4	In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability.
2309.03852_1908656_3	However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored.
2309.03876_1908680_7	To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics.
2309.03876_1908680_8	This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
2309.04550_1909354_3	Our method entails tasking an LLM to infer whether a patient has, or is at risk of, a particular condition on the basis of associated notes; if so, we ask the model to summarize the supporting evidence.
2309.05660_1910464_5	To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset.
2309.05689_1910493_2	Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement.
2309.05950_1910754_4	Specifically, we adopt an automatic hill-climbing procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop.
2309.06384_1911188_4	First, we build a dataset to train a critic model capable of evaluating the citation, correctness, and fluency of responses generated by LLMs in QA systems.
2309.07062_1911866_3	Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself.
2309.07623_1912427_5	We specifically employ a minimal dataset to instruct LLMs to recognize the intended output modality as directed by the instructions.
2309.08902_1913706_5	We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group.
2309.09380_1914184_4	We first train a teacher model with hard labels to determine each sample's degree of relying on shortcuts.
2309.09380_1914184_6	This new ground truth label is used to train a more robust student model.
2309.09400_1914204_6	Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages.
2309.09582_1914386_3	Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model.
2309.10524_1915328_3	Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further.
2309.11696_1916500_4	While one can fully train an LLM for this objective, the resource consumption is unaffordable.
2309.11696_1916500_6	We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly.
2309.12307_1917111_1	Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.
2309.12321_1917125_3	This paper makes a case that effective legal systems are the best way to address AI safety.
2309.12767_1917571_7	1) Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration.
2309.13550_1918354_7	To train our I-AI model, we utilize an eye gaze dataset to extract anatomical gaze information and generate ground truth heatmaps.
2309.13638_1918442_3	This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input.
2309.13734_1918538_1	Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model.
2309.14459_1919263_4	We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM's output in meeting the goal.
2309.14482_1919286_3	However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection.
2309.14530_1919334_4	By categorizing and elucidating these genres, the study aims to facilitate the development of empirical qualitative and quantitative research, fostering evidence-based approaches to address AI-related risks in healthcare effectively.
2309.15223_1920027_2	Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters.
2309.16082_1920886_3	Re-training the underlying model every time individuals would like to practice their rights to be forgotten is computationally expensive.
2309.16167_1920971_1	However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education.
2309.16697_1921501_1	Students can ask ChatGPT to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s).
2309.17147_1921951_5	Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to train a bespoke model on these annotations than it is to use an LLM for annotation.
2310.00035_1922292_1	One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations.
2310.00052_1922309_1	We trained these AI classifiers with 2.4 million IMRPhenomXPHM waveforms that describe quasi-circular, spinning, non-precessing binary black hole mergers with component masses $m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$, and individual spins $s^z_{\{1,2\}}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the $\ell = 3, |m| = 2$ harmonics.
2310.00052_1922309_2	We trained these AI classifiers within 22 hours using distributed training over 96 NVIDIA V100 GPUs in the Summit supercomputer.
2310.00525_1922782_5	Through a feedback mechanism, the user interacts with the algorithm, correcting the algorithm output to their preferences.
2310.00603_1922860_2	In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation.
2310.00646_1922903_1	In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs.
2310.00836_1923093_7	Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5.
2310.01558_1923815_9	We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.
2310.02407_1924664_9	To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect).
2310.02439_1924696_3	We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question.
2310.02527_1924784_2	In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs.
2310.03030_1925287_3	A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules.
2310.03051_1925308_5	Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions.
2310.03214_1925471_9	Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers.
2310.03266_1925523_4	Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately.
2310.03328_1925585_2	A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.   
2310.04407_1926664_1	Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals.
2310.04680_1926937_1	We study two natural scaling techniques -- weight pruning and simply training a smaller or larger model, which we refer to as dense scaling -- and their effects on two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference.
2310.04782_1927039_5	Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior.
2310.05657_1927914_5	Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.
2310.05782_1928039_2	However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance.
2310.05782_1928039_5	To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM.
2310.05782_1928039_6	Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model.
2310.05824_1928081_2	We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model.
2310.05976_1928233_4	The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish.
2310.06450_1928707_6	By training our model with this diversified feedback, we achieve enhanced alignment performance while using less training data.
2310.06552_1928809_4	Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task, therefore we frame the task as information extraction, providing a description of each coded concept and asking the model to retrieve related mentions.
2310.07088_1929345_1	Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance.
2310.07554_1929811_7	Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference.
2310.07570_1929827_6	Our strategy outlines a productive process wherein a mathematician trains ChatGPT on pure mathematical concepts, steers ChatGPT towards generating computational topology code, and subsequently validates the generated code using established examples.
2310.07815_1930072_6	In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective.
2310.08433_1930690_2	We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style.
2310.08669_1930926_6	We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D).
2310.08754_1931011_1	Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations.
2310.08922_1931179_3	In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.
2310.09478_1931735_4	We propose using unique identifiers for different tasks when training the model.
2310.09755_1932012_5	By training the large language model with our approach, the necessity for generating binary segmentation masks, as suggested in the LISA paper arXiv:2308.00692, is effectively eliminated.
2310.09810_1932067_1	In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair.
2310.10035_1932292_4	Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool.
2310.10089_1932346_0	  Federated learning (FL), as an emerging distributed machine learning paradigm, allows a mass of edge devices to collaboratively train a global model while preserving privacy.
2310.10158_1932415_2	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.
2310.10158_1932415_3	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.
2310.10505_1932762_6	ReMax can save about 46% GPU memory than PPO when training a 7B model and enables training on A800-80GB GPUs without the memory-saving offloading technique needed by PPO.
2310.10962_1933219_3	Building upon this premise, we propose MultiCSR, a multi-level contrastive sentence representation learning framework that decomposes the process of prompting LLMs to generate a corpus for training base sentence embedding models into three stages (i.e., sentence generation, sentence pair construction, in-batch training) and refines the generated content at these three distinct stages, ensuring only high-quality sentence pairs are utilized to train a base contrastive learning model.
2310.11324_1933581_1	Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model.
2310.11998_1934255_1	This paper studies distributed learning in wireless data center networks, which contain a central edge server and multiple edge workers to collaboratively train a shared global model and benefit from parallel computing.
2310.12303_1934560_7	However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system.
2310.12523_1934780_3	To demonstrate its applicability, we show how a private mechanism could be integrated into the existing model for training LLMs to protect user privacy; specifically, we employed differential privacy and private training using Reinforcement Learning (RL).
2310.12558_1934815_7	To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users.
2310.13011_1935268_3	Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment.
2310.13098_1935355_1	The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures.
2310.13312_1935569_4	To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets.
2310.13395_1935652_4	We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side.
2310.13522_1935779_4	We then replay this experience to train the small model.
2310.13548_1935805_1	But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy.
2310.13625_1935882_7	While the scheme will not address all AI risks, it complements proposed solutions by allowing for a more precise and flexible approach to controlling the development of frontier AI models and unwanted AI proliferation.
2310.13671_1935928_1	*Data Synthesis* is a promising way to train a small model with very little labeled data.
2310.13714_1935971_1	We have developed a framework where we train a machine learning-based model using the neural contextual representations of the comments and their corresponding codes to predict the usefulness of code-comments pair and performance analysis with LLM-generated data with base data.
2310.14122_1936379_1	Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No".
2310.15747_1938004_4	To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively.
2310.15780_1938037_3	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2310.15851_1938108_3	Safety training focuses on further training LLM to enhance its safety.
2310.15851_1938108_9	In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses.
2310.16088_1938345_3	Using these data, we train a simple and easily-interpretable machine learning model to regress effective temperatures and luminosities with high accuracy and precision comparable to the training data.
2310.16240_1938497_2	To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their importance scores.
2310.16535_1938792_2	Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction.
2310.16727_1938984_5	A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards.
2310.16789_1939046_7	Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data.
2310.17567_1939824_4	Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills.
2310.17591_1939848_6	Training performant LLMs on small amounts of data is a difficult but potentially informative task.
2310.18313_1940570_2	Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs.
2310.18343_1940600_4	We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period.
2310.18357_1940614_4	We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms.
2310.18358_1940615_2	Traditional supervised learning usually requires training a model based on labeled data and then making predictions.
2310.18360_1940617_3	Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs.
2310.19046_1941303_5	Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions.
2310.19204_1941461_3	We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify.
2310.20440_1942697_5	We evaluate the utility of the dataset to train AI models using named-entity recognition, segmentation of figure captions into their constituent panels, and a novel context-dependent semantic task assessing whether an entity is a controlled intervention target or a measurement object.
2310.20444_1942701_1	Thus, it is of major importance to know which stakeholders influence AI research.
2310.20487_1942744_5	Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence.
2310.20563_1942820_4	Third, we describe three policy proposals that would meaningfully address the threats from advanced AI: (1) establishing a Multinational AGI Consortium to enable democratic oversight of advanced AI (MAGIC), (2) implementing a global cap on the amount of computing power used to train an AI system (global compute cap), and (3) requiring affirmative safety evaluations to ensure that risks are kept below acceptable levels (gating critical experiments).
2311.00257_1943224_6	Evaluations demonstrate up to 52\% Model FLOPs Utilization (MFU) when training the LLaMA-based model on 1024 GPUs, resulting in a 1.56 times improvement in training throughput compared to newly proposed systems like MiCS and ZeRO++.
2311.00522_1943489_3	This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model.
2311.01041_1944008_4	Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors.
2311.01469_1944436_3	In this study, we introduce a novel preliminary methodology to train a language model on generated labels for greenwashing risk.
2311.01918_1944885_6	To address LLMs' limitations regarding personalization and complex clinical reasoning, the paper explores the emerging development of LLM-powered autonomous agents for healthcare.
2311.01981_1944948_4	In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient.
2311.02105_1945072_4	Can we train LLMs on harmful data without learning harmful behaviors?
2311.02433_1945400_6	To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants.
2311.03920_1946887_2	Our system integrates six diverse sensors to gather measurement parameters, which subsequently train a 1D CNN model for activity recognition.
2311.04547_1947514_5	This suggests that modelling processing effort and linguistic competence may require an approach different from training GPT-like LMs on a developmentally plausible corpus.
2311.06180_1949147_3	In stage one, we used a small portion (n=20) of the student responses on one conceptual question to iteratively train GPT.
2311.06377_1949344_4	Our emulation strategy involved using the initial five words of each PubMed abstract as a prompt and instructing the model to expand the content up to the original abstract's length.
2311.06985_1949952_9	For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering.
2311.07014_1949981_5	We achieve this via an audio-language knowledge distillation framework, where we transfer acoustic and paralinguistic information from a pre-trained speech embedding (OpenAI Whisper) teacher model to help train a student language model on an audio-text dataset.
2311.07594_1950561_6	The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converter, which transforms data into a format that LLMs can understand; (2) Multimodal Perceiver, which improves how LLMs percieve different types of data; (3) Tool Learning, which leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Method, which teaches LLMs to understand specific data types within datasets.
2311.08105_1951072_1	However, standard approaches to training LLM require a large number of tightly interconnected accelerators, with devices exchanging gradients and other intermediate states at each optimization step.
2311.08147_1951114_3	However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response.
2311.08213_1951180_5	The first stage pre-trains the student model on a large number of filtered multi-modal datasets.
2311.08369_1951336_2	When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need.
2311.08662_1951629_7	We employ the Tabular-NLI task to showcase how our proposed strategies adeptly train a robust model, enabling it to address diverse perturbations while maintaining accuracy on the original dataset.
2311.08844_1951811_6	To train our model, we introduce a new method for automatically acquiring data from available English datasets.
2311.08877_1951844_3	We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement.
2311.09136_1952103_4	This method trains the model to prioritize the best responses from a pool of candidates created for a particular task.
2311.09613_1952580_2	Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data.
2311.09632_1952599_8	We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train LMs in a continually evolving environment.
2311.09718_1952685_1	To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions.
2311.10844_1953811_4	Open datasets were employed to train AI models, assess their performance, and analyze their capabilities and limitations in addressing the specific challenges associated with fetal brain fMRI segmentation.
2311.10934_1953901_3	We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases.
2311.10947_1953914_4	The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models.
2311.11045_1954012_2	Research on training small LMs has often relied on imitation learning to replicate the output of more capable models.
2311.11045_1954012_4	We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model.
2311.11202_1954169_3	This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model.
2311.11628_1954595_0	  We present a method to integrate Large Language Models (LLMs) and traditional tabular data classification techniques, addressing LLMs challenges like data serialization sensitivity and biases.
2311.11981_1954948_3	It is unclear whether these labels can be used for training a local model without expensive annotation checking by in-house experts.
2311.12188_1955155_4	In particular, we ask ChatGPT to give examples of how to use Bayes rule for medical diagnosis.
2311.13240_1956207_6	Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.
2311.13387_1956354_0	  As training artificial intelligence (AI) models is a lengthy and hence costly process, leakage of such a model's internal parameters is highly undesirable.
2311.13721_1956688_2	To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization.
2311.14126_1957093_4	Our experiments show that training the model in a multi-class setting can outperform the one-vs-all binary counterpart.
2311.14342_1957309_8	After training the AI models, we confirmed the model's learning effectiveness by observing changes in loss and reward values.
2311.14703_1957670_8	Finally, we find that through asking ChatGPT 3.5 to explain its reasoning prior to providing an answer, we are able to improve clinical accuracy and mitigate instances of gender and racial biases.
2311.15377_1958344_0	  Training advanced AI models requires large investments in computational resources, or compute.
2311.16441_1959408_2	This is mainly due to their distinct representation in a semantic space that is different from the natural language (NL) typically used to train LLMs.
2311.16479_1959446_5	These conversations pay more attention on detailed facts in the image, encouraging the model to answer questions based on multi-modal contexts.
2311.16494_1959461_7	3) We propose negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features.
2311.16639_1959606_1	We ask an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses to position political actors such as US Senators, or longer texts such as UK party manifestos or EU policy speeches given in 10 different languages.
2311.16673_1959640_4	Furthermore, the survey presents a comprehensive collection of datasets employed to train LLMs, offering insights into the diverse data available to achieve high performance in various pre-training and downstream tasks of LLMs.
2311.16684_1959651_4	We employ a Time-to-Digital Converter to capture power fluctuations and train a supervised machine learning model to identify various types of threats.
2311.17429_1960396_2	Most of the existing attack methods focus on inserting manually predefined templates as triggers in the pre-training phase to train the victim model and utilize the same triggers in the downstream task to perform inference, which tends to ignore the transferability and stealthiness of the templates.
2311.18063_1961030_4	We trained our model using the same approach for RoBERTa model and evaluated on two text classification tasks: Sentiment Classification and Hate Speech Detection.
2311.18232_1961199_4	However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs.
2311.18609_1961576_4	We propose to train LLM to generate a postfix expression related to the arithmetic problem and incorporate it with small pretrained models.
2311.18702_1961669_0	  Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting.
2311.18751_1961718_5	By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%).
2312.00053_1961861_6	We have created a labeled data set in Spanish, since the majority of studies focus on English, to train our system, which offers a very good performance after the validation experiments.
2312.00575_1962383_1	However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans.
2312.00818_1962626_2	Can we use AI itself to bridge the lack of data in the sciences in order to then train an AI?
2312.00819_1962627_4	Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results.
2312.02147_1963955_3	Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens.
2312.02406_1964214_6	Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining.
2312.02873_1964681_6	We train our autocorrection model on a synthetic dataset in a supervised manner.
2312.04412_1966220_1	In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.
2312.04469_1966277_5	To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking.
2312.04474_1966282_4	The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator").
2312.04828_1966636_4	The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged.
2312.05320_1967128_2	The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations.
2312.05356_1967164_9	\textsc{MINT} is effective, efficient, and reliable, capable of correcting a neural model by patching a minimum number of neurons (usually one or two neurons).
2312.05571_1967379_7	We adopt policy-gradient reinforcement learning to train the adapted LM, informed by the non-differentiable symbolic solver.
2312.06942_1968750_8	This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code.
2312.06942_1968750_12	This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding.
2312.08680_1970488_5	By iteratively asking GPT-4 with the prompts, GHGNAS continually validates the accuracy of the generated HGNNs and uses the feedback to further optimize the prompts.
2312.09203_1971011_3	A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself.
2312.09300_1971108_4	We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly.
2312.09792_1971600_9	Finally, we show that synthetic data effectively trains AI models.
2312.09971_1971779_5	Additionally, we show experimentally that the structural information can be kept unmodified when re-training the AI system with new samples while still achieving a validation accuracy similar to that obtained when re-training a neural network with similar size.
2312.10104_1971912_5	Then a dataset with effective ICD sequences is constructed to train Lever-LM.
2312.10321_1972129_5	The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample exists by modifying the database.
2312.10321_1972129_6	The latter technique is used to evaluate the relaxed equivalence in which it asks LLMs to explain the queries and then compare if they contain significant logical differences.
2312.10603_1972411_10	This progress indicates that addressing the current model's limitations could yield an AI capable of passing even the most rigorous professional certifications.
2312.11681_1973489_2	Chains address LLM errors analogously to the way crowdsourcing workflows address human error.
2312.12391_1974199_0	  As large language models (LLMs) become widespread in various application domains, a critical challenge the AI community is facing is how to train these large AI models in a cost-effective manner.
2312.12705_1974513_2	Nevertheless, training LLMs with billions of parameters poses significant challenges and requires considerable computational resources.
2312.12705_1974513_3	For example, training a one trillion parameter GPT-style model on 20 trillion tokens requires a staggering 120 million exaflops of computation.
2312.12705_1974513_5	We enable and investigate various model and data parallel training techniques, such as tensor parallelism, pipeline parallelism, and sharded data parallelism, to facilitate training a trillion-parameter model on Frontier.
2312.12705_1974513_8	We have identified efficient strategies for training large LLMs of varying sizes through empirical analysis and hyperparameter tuning.
2312.12868_1974676_2	Specifically, leveraging reinforcement learning (RL) to train our AI agents, we systematically investigate learning trust under various parameterizations of this task.
2312.13334_1975142_7	FL enables financial institutions to collaboratively train a model to detect fraudulent transactions without directly sharing customer data, thereby preserving data privacy and confidentiality.
2312.14219_1976027_0	  Federated learning is a decentralized learning paradigm wherein a central server trains a global model iteratively by utilizing clients who possess a certain amount of private datasets.
2312.14628_1976436_1	This decentralized approach involves multiple clients or silos, collaboratively training a global model under the coordination of a central server while utilizing their private local data.
2312.14856_1976664_3	Thus, from a single question template, it is possible to ask an LLM a $\textit{neighbourhood}$ of very similar programming questions, and assess the correctness of the result returned for each question.
2312.14950_1976758_3	That is, instead of asking an LLM to write a program (robotic plan) in the popular but verbose Python, ChatFly gets it to do it in MiniSpec specially designed for token efficiency and stream interpretation.
2312.15514_1977322_6	Our method does not require training the model from scratch and can be attached to the classifier simply.
2312.15696_1977504_2	Considering the exorbitant cost of training LLMs from scratch and the scarcity of annotated data within particular domains, in this work, we focus on domain-specific continual pre-training of LLMs using E-commerce domain as an exemplar.
2312.15842_1977650_2	Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model.
2312.16044_1977852_3	Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions.
2312.16211_1978019_5	We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses.
2312.16257_1978065_5	Our casual intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information.
2312.17235_1979043_7	Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost.
2401.00139_1979695_6	This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.
2401.00434_1979990_4	We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset.
2401.00698_1980254_4	The novel ideas explored are: 1) Decaying auxiliary loss (with residual) - where we train the model on an auxiliary task of Coarse-Grained NER and include this task as a part of the loss function 2) Triplet token blending - where we explore ways of blending the embeddings of neighboring tokens in the final NER layer prior to prediction 3) Task-optimal heads - where we explore a variety of custom heads and learning rates for the final layer of the LLM.
2401.00996_1980552_4	In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization.
2401.03676_1983232_6	From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs.
2401.03729_1983285_1	By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task.
2401.03851_1983407_4	Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).
2401.04092_1983648_7	We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria.
2401.05612_1985168_6	Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making.
2401.05695_1985251_3	PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process.
2401.06059_1985615_2	There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks.
2401.06072_1985628_6	Additionally, we execute a substantial range of ablation experiments and draw comparisons with several advanced commercial LLMs, to investigate the crucial factors influencing LLMs' performance in structured temporal knowledge inference tasks.
2401.06088_1985644_5	In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
2401.06373_1985929_3	Specifically, we study how to persuade LLMs to jailbreak them.
2401.06774_1986330_3	We train a system to detect AD-related signs and symptoms from EHRs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method.
2401.06853_1986409_7	On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation.
2401.06951_1986507_0	  Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.
2401.06954_1986510_4	We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM.
2401.07324_1986880_2	While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models.
2401.07657_1987213_2	In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments.
2401.08089_1987645_5	Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance.
2401.08183_1987739_0	  Wirelessly connected devices can collaborately train a machine learning model using federated learning, where the aggregation of model updates occurs using over-the-air computation.
2401.08273_1987829_1	Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task.
2401.08491_1988047_3	To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation.
2401.08711_1988267_1	Given the pressing need to teach "critical AI literacy", discussion of metaphor provides an opportunity for inquiry and dialogue with space for nuance, playfulness, and critique.
2401.09074_1988630_5	We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers.
2401.09566_1989122_6	We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions.
2401.09796_1989350_0	  The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data.
2401.10446_1990000_3	In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM.
2401.10657_1990211_2	We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance.
2401.10745_1990299_3	Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet.
2401.10745_1990299_5	This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
2401.11011_1990565_1	Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields.
2401.12246_1991800_1	We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages.
2401.12474_1992028_3	Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension.
2401.13218_1992772_6	We introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument.
2401.15241_1994795_0	  Identifying the training datasets that influence a language model's outputs is essential for minimizing the generation of harmful content and enhancing its performance.
2401.15497_1995051_2	Without explicit consent from artists, Generative AI creators scrape artists' digital work to train Generative AI models and produce art-like outputs at scale.
2401.15641_1995195_1	Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem.
2401.16731_1996285_2	Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model.
2401.16791_1996345_0	  Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions.
2401.16795_1996349_2	Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game.
2401.17043_1996597_1	This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content.
2401.17267_1996821_3	Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA).
2401.17390_1996944_5	Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid.
2402.00282_1997921_5	Contrary to other "reference-free" metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores.
2402.00786_1998425_1	To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets.
2402.00905_1998544_5	Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model's generation process without requiring a specific code review dataset.
2402.00969_1998608_5	Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them.
2402.01093_1998732_4	In the first scenario, we propose an effective solution based on importance sampling: we resample the pretraining set to imitate the specialization data and train a small model on it.
2402.01691_1999330_1	Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms.
2402.01732_1999371_3	To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related.
2402.01766_1999405_3	We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes.
2402.01867_1999506_2	In this work, we ask the LLM how similar are these prompted LFs.
2402.02030_1999669_3	Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning.
2402.02167_1999806_2	At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected.
2402.02456_2000095_4	The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement.
2402.02963_2000602_4	We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges.
2402.03182_2000821_1	However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.
2402.03407_2001046_3	Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model.
2402.03469_2001108_3	We aim to replicate the ground truth (gold) reward signal by achieving a monotonic relationship between the proxy and gold reward signals after training the model using the proxy reward in reinforcement learning (RL).
2402.03575_2001214_5	Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior.
2402.03659_2001298_6	The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts.
2402.03776_2001415_7	To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics.
2402.03916_2001555_2	Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden.
2402.04315_2001954_3	In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses.
2402.04400_2002039_4	In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format.
2402.04497_2002136_1	To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations.
2402.04568_2002207_0	  Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses.
2402.04617_2002256_7	Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences.
2402.05000_2002639_2	We term the objective of training LLMs to emulate effective teaching strategies as `pedagogical alignment.'
2402.06954_2004593_2	In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data.
2402.06954_2004593_6	Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings.
2402.07645_2005284_2	In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model.
2402.07645_2005284_4	We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data.
2402.07945_2005584_7	Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities.
2402.08078_2005717_4	Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.
2402.08157_2005796_2	In addition to potentially providing insights into the complex bushfire behaviour, direct numerical simulation (DNS) can generate synthetic remote sensing data to train AI algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which can process large amounts of remotely sensed data associated with bushfire.
2402.08208_2005847_7	To mitigate these risks, methods for training AI models that help maintain performance without overconfidence are proposed.
2402.08680_2006319_2	However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.
2402.08699_2006338_4	RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input.
2402.08806_2006445_2	Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   
2402.09299_2006938_8	In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM.
2402.09360_2006999_1	On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency.
2402.09363_2007002_5	We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch.
2402.09671_2007310_0	  This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.
2402.09739_2007378_3	We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria.
2402.09939_2007578_3	This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents.
2402.10151_2007790_4	We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference.
2402.10239_2007878_2	However, the current practice is to design and train one deep learning model for one task with supervised learning techniques.
2402.10239_2007878_5	In this paper, we present a tokenized detector representation that allows us to train a BERT model for particle tracking.
2402.11142_2008781_4	To accurately and explicitly describe relation semantics while minimizing annotation demands, we explore the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model.
2402.11176_2008815_3	We devise a fine-grained knowledge augmentation stage to train LLMs to identify difficult fine-grained knowledge in answers.
2402.11176_2008815_4	We also propose a coarse-grained knowledge comparison stage to train LLMs to distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality.
2402.11245_2008884_2	To address the AI model placement problem under uncertainties, this paper presents a novel approach employing a sequence-to-sequence (S2S) neural network which considers uncertainty estimations.
2402.11253_2008892_2	To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge.
2402.11359_2008998_1	To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications.
2402.11450_2009089_3	Our key observation is that when human-robot interactions are viewed as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions is training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success.
2402.11485_2009124_3	This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling.
2402.11532_2009171_3	Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached.
2402.11633_2009272_6	The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances.
2402.11651_2009290_5	By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks.
2402.11801_2009440_5	Regarding emotional understanding, HEF implements a two-stage emotion prediction strategy, encouraging LLMs to prioritize primary emotions emphasized by SEMs, followed by other categories, substantially alleviates the difficulties for LLMs in fine-grained emotion detection.
2402.11890_2009529_0	  Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model.
2402.12010_2009649_1	On the one hand, data-centric approaches show great potential towards training energy-efficient AI models.
2402.12010_2009649_2	On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation.
2402.12026_2009665_1	Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios.
2402.12026_2009665_5	Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.
2402.12621_2010260_2	However, only a few works attempted to directly train the LMs within interactive decision-making environments.
2402.12786_2010425_4	Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different".
2402.12786_2010425_6	To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles.
2402.12786_2010425_7	We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles.
2402.12907_2010546_1	While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts.
2402.12914_2010553_5	We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment.
2402.13210_2010849_4	To address these challenges, we propose to train a Bayesian reward model, which signals higher uncertainty further from the training data distribution.
2402.13414_2011053_4	Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels.
2402.14245_2011884_3	In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback.
2402.14846_2012485_5	We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks.
2402.14850_2012489_3	Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations.
2402.14878_2012517_3	In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM approaches.
2402.14878_2012517_7	Our projections suggest that the energy-dissipation lower-bound to train a brain scale AI system (comprising of $10^{15}$ parameters) using LIM is $10^8 \sim 10^9$ Joules, which is on the same magnitude the Landauer's adiabatic lower-bound and
2402.14904_2012543_0	  We investigate the radioactivity of text generated by large language models (LLM), i.e. whether it is possible to detect that such synthetic input was used to train a subsequent LLM.
2402.15301_2012940_6	Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets.
2402.15302_2012941_6	Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models.
2402.15302_2012941_8	In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.
2402.15627_2013266_1	Training LLMs at this scale brings unprecedented challenges to training efficiency and stability.
2402.15627_2013266_6	MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM.
2402.16065_2013704_0	  We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script.
2402.16352_2013991_3	We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions.
2402.16786_2014425_3	Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions.
2402.16827_2014466_1	However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary.
2402.16929_2014568_1	Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts.
2402.17124_2014763_2	In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved.
2402.17124_2014763_6	And then it asks the model to "reflect" over them to generate the final answer.
2402.17385_2015024_7	Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.
2402.17801_2015440_1	Amidst the immense excitement for this new technology, its future development and applications in the creative industry hinge crucially upon two copyright issues: 1) the compensation to creators whose content has been used to train generative AI models (the fair use standard); and 2) the eligibility of AI-generated content for copyright protection (AI-copyrightability).
2402.17812_2015451_1	However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation.
2402.17887_2015526_3	Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase.
2402.18284_2015923_3	Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input.
2402.18571_2016210_5	Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2.
2402.19327_2016965_2	The dataset used to train the model includes 37.8 million single-point energies, 11.7 billion force pairs, and 340.2 million stresses.
2402.19423_2017061_12	Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance.
2402.19434_2017072_4	The proposed digital twin approach generates site-specific synthetic CSI data from the EM 3D model and ray tracing, which can then be used to train the DL model without real-world data collection.
2403.00952_2018071_1	Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges.
2403.01069_2018188_6	The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively.
2403.01209_2018328_2	Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts.
2403.01570_2018689_6	Specifically, SERSAL utilizes the LLM's prior outcomes as original soft noisy annotations, which are dynamically leveraged to teach a better small student model.
2403.01570_2018689_7	Reversely, the outcomes from the trained small model are used to teach the LLM to further refine its real capability.
2403.01632_2018751_3	Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge.   
2403.02130_2019249_3	We experiment with different zero-shot and few-shot prompt templates for instructing LLMs to extract and normalize attribute-value pairs.
2403.02419_2019538_1	However, there is little understanding of how the number of LM calls - e.g., when asking the LM to answer each question multiple times and taking a majority vote - affects such a compound system's performance.
2403.02553_2019672_3	We then use this database to train a machine learning model that can efficiently predict magnetocaloric properties of materials based on their chemical composition.
2403.02610_2019729_9	Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation.
2403.02694_2019813_7	MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model without violating user privacy.
2403.02715_2019834_6	Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets.
2403.02781_2019900_4	In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels.
2403.03507_2020626_6	Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.
2403.04283_2021402_3	We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself.
2403.04769_2021888_0	  Large language models (LLMs) are initially trained on vast amounts of data, then fine-tuned using reinforcement learning from human feedback (RLHF); this also serves to teach the LLM to provide appropriate and safe responses.
2403.04769_2021888_2	Unlike other jailbreaks (for example, the popular "Do Anything Now" (DAN) ), our method does not rely on instructing the LLM to override its RLHF policy; hence, simply modifying the RLHF process is unlikely to address it.
2403.04784_2021903_2	To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives.
2403.04818_2021937_4	We trained our proposed ML model on a dataset of 61 historical storms in the coastal regions of the U.S. and we tested its performance in bias correcting modeled water level data predictions from hurricane Ian (2022).
2403.05217_2022336_4	Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process.
2403.05434_2022553_2	Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens.
2403.05572_2022691_5	Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align approximately 5 times more closely with the expectations of individuals possessing a high degree of empathy, compared to human responses.
2403.05583_2022702_1	We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model with a shared latent representation.
2403.05612_2022731_3	This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model's responses to unfamiliar queries (e.g., say ``I don't know'').
2403.05973_2023092_2	We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone.
2403.06354_2023473_4	We employ methods previously used for training LLMs on other languages with data scarcity, and use open source translation models to perform data augmentation and grow our dataset from millions of tokens to billions.
2403.06512_2023631_2	While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice.
2403.06664_2023783_7	In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity.
2403.07118_2024237_5	Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset.
2403.07747_2024866_6	These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities.
2403.07969_2025088_1	KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.
2403.08100_2025219_0	  Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices.
2403.08229_2025348_6	Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance.
2403.08281_2025400_6	To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content.
2403.08693_2025812_4	Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks.
2403.08693_2025812_7	We conclude that, in our experiments, the quality of the web-crawled corpora does not seem to play a significant role when training LMs.
2403.09522_2026641_5	Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.
2403.09972_2027091_4	Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation.
2403.10131_2027250_4	In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents.
2403.10433_2027552_6	We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence.
2403.11103_2028222_3	Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data.
2403.12744_2029863_3	In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.
2403.12744_2029863_6	Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.
