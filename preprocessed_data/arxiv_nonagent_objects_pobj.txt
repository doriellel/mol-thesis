1111.5202_303438_6	Some of these phenomena can be equally explained by either a wave or non-wave model alone.
1201.1369_313356_1	We show that often optical and near-infrared spectral energy distribution of LMXBs can be adequately described by a simple model of an accretion disc and a secondary star reprocessing X-ray emission of a central compact object.
1212.5593_395209_0	  Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.
1408.5886_551192_2	Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.
1501.07576_594324_2	In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.
1604.05122_724289_0	  An air pollution model is generally described by a system of PDEs on unbounded domain.
1606.00401_738308_8	I then describe a concept for a formal category theoretic basis to a generalised player model.
1702.00137_814731_4	* How could we teach AI topics at an early undergraduate or a secondary school level?   
1712.05322_923921_1	The pulsar spectrum is well described by the thermal comptonization model both in a quiescent state and during flares, when the peak luminosity reaches values $L_{\rm x} \sim (2-4)\times10^{39}$ erg s$^{-1}$.
1802.07695_947235_0	  Practical application of H[infinity] robust control relies on system identification of a valid model-set, described by a linear system in feedback with a stable norm-bounded uncertainty, which must explains all possible (or at least all previously measured) behavior for the control plant.
1803.05457_955590_0	  We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering.
1803.11096_961229_0	  Group zero-attracting LMS and its reweighted form have been proposed for addressing system identification problems with structural group sparsity in the parameters to estimate.
1805.05062_978107_3	To overcome these limitations we build upon the recent reward augmented maximum likelihood approach \ie sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric.
1904.08796_1113836_10	Open-access data sets and software implementations could alleviate these issues, and encourage further AI applications to pediatric ophthalmology.   
1907.08625_1153161_4	The broadband 3-30 keV NuSTAR energy spectrum can be well described either by a three-component continuum model consisting of a disk blackbody, a single temperature blackbody and a power-law or by a two-component continuum model consisting of a disk blackbody and a Comptonization component.
1909.02984_1173022_7	We modeled the X-ray emission from the SB and found that its X-ray emission can be simply explained by pressure-driven wind model, that is, there is no need to invoke the presence of a SN explosion as previously suggested.
1910.09031_1192950_5	Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision making ultimately producing more precise and reliable interventions.
2001.10717_1236130_5	In this early work, a comparative evaluation of our MRE data driven simulation and the traditional method shows clinically significant differences in accuracy during landmark placement and motivates further animal model trials.
2003.13003_1263358_4	It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.
2005.03848_1283643_2	As an alternative, we propose a new method for BERT distillation, i.e., asking the teacher to generate smoothed word ids, rather than labels, for teaching the student model in knowledge distillation.
2006.11194_1305708_0	  Explainable AI provides insight into the "why" for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect.
2006.14779_1309293_0	  Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations.
2007.07250_1318764_6	We extend that concept to address system-level autonomy capabilities of AI-enabled cyber-physical systems.
2007.12912_1324426_3	Motivated by these issues, this paper addresses a drone-enabled intelligent vehicular system, which is secure, easy to deploy and reliable in quality.
2008.01048_1328770_7	For the simple-point-charge model of water we study, these results argue distinctly against rationalizing ion adsorption in terms of surface potentials inherent to molecular structure of the liquid's boundary.
2008.07326_1335048_1	However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI.
2008.10530_1338252_0	  We present a new mathematical model to explicitly capture the effects that the three restriction measures: the lockdown date and duration, social distancing and masks, and, schools and border closing, have in controlling the spread of COVID-19 infections $i(r, t)$. Before restrictions were introduced, the random spread of infections as described by the SEIR model grew exponentially.
2009.04984_1346487_3	As the foundation for model pre-training, we synthesize a new dialogue corpus and build our training set with two unsupervised methods: 1) coherence-oriented context corruption, including utterance ordering, insertion, and replacement, to help the model capture the coherence inside the dialogue contexts; and 2) specificity-oriented automatic rescoring, which encourages the model to measure the quality of the synthesized data for dialogue-adaptive pre-training by considering specificity and informativeness.
2009.07876_1349379_3	In this paper we argue for an AI-enabled control that allows optimized and efficient conversion between qubit and photon energies, to enable optic and quantum devices to work together.
2009.10228_1351731_1	However, support for designing tools and curriculum to teach K-12 AI literacy is still limited.
2009.10228_1351731_2	There is a need for additional interdisciplinary human-computer interaction and education research investigating (1) how general AI literacy is currently implemented in learning experiences and (2) what additional guidelines are required to teach AI literacy in specifically K-12 learning contexts.
2009.11100_1352603_5	We identify opportunities for researchers and teachers to collaborate to make AI education more accessible, and present an exemplar lesson plan that shows entry points for teaching AI in non-computing subjects.
2009.11101_1352604_4	Here we describe six self-contained and adaptive modules in "AI-assisted Malware Analysis."
2010.04887_1361217_3	We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information.
2010.11506_1367836_4	(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.
2010.14617_1370947_4	The role of the LTP and LTD in the cerebellum is also explained in algorithm level.
2011.02323_1375071_6	Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection.
2011.10737_1383485_2	Recently, substantial research efforts in learning-based methods for optimal control problems have been progressing significantly in addressing unknown system models, particularly when the system has complex interactions with the environment.
2012.14682_1402560_5	We further devise a difficulty-aware objective, encouraging the model to output the class probability that reflects the real difficulty of each instance for a more reliable cascading mechanism.
2101.00406_1404148_1	First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships.
2101.04283_1408025_0	  This paper briefly reviews the history of meta-learning and describes its contribution to general AI.
2101.06098_1409840_0	  The development of AI applications is a multidisciplinary effort, involving multiple roles collaborating with the AI developers, an umbrella term we use to include data scientists and other AI-adjacent roles on the same team.
2102.01380_1417869_3	ILMT encourages the E2E model to form a standalone LM inside its existing components, without sacrificing ASR accuracy.
2103.00689_1430834_4	The color centers show a strong diameter-dependent emission intensity, which can be explained with a theoretical model for chemical reactivity taking into account strain along the tube curvature.
2103.10873_1441018_4	Addressing the DNN model design, from training and dataset augmentation to 8-bit quantization and deployment, we demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for the real-time execution (up to 135 frame/s) of our novel DNN, called PULP-Frontnet.
2103.11903_1442048_2	First we analyze the performance of network in tracking a time varying weight vector and then we explain the estimation of Rayleigh fading channel through a random walk model.
2103.15004_1445149_7	It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces.
2104.12145_1459563_6	Given that GPT will perform better when given detailed and specific questions, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands.
2105.00164_1462723_0	  Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors.
2105.05012_1467571_6	The proposed RAA is responsible for reasoning students' learning performance and showing the results on the AIoT-FML learning tool after communicating with the AI-FML platform.
2105.12655_1475214_4	In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code.
2106.01998_1479762_2	As an exploration of whether it is possible to explain user's mental model and behavior through Artificial Intelligence (AI) techniques, the research team compared the card sorting data with the outputs of a number of Natural Language Processing (NLP) techniques with the goal of understanding how participants perceived and interpreted the consequences of cyber attacks written in natural languages.
2107.05383_1499395_1	We asked the world's best language model, GPT-3, fifteen difficult questions about the nature, value, and future of library and information science (LIS), topics that receive perennial attention from LIS scholars.
2107.07691_1501703_5	To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.
2107.09051_1503063_6	Lastly, open issues and opportunities address future AI-empowered finance and finance-motivated AI research.
2107.11275_1505287_1	One way to understand and improve robustness of these models is an exploration of an adversarial attack scenario: check if a small perturbation of an input can fool a model.   
2108.01250_1510065_5	At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature.
2108.11193_1520008_4	Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.
2109.07971_1530792_2	In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?
2109.10836_1533657_4	Taking this wide perspective, this year there will be no single theme to lead the symposium and we encourage AI-HRI submissions from across disciplines and research interests.
2111.00826_1554929_0	  In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility.
2111.00826_1554929_4	We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs.
2111.13642_1567745_4	The synchronisation of X-ray and optical QPOs indicates that they must be produced in regions physically very close to each other; we thus propose that they can be explained by a precessing jet model, based on analogies with V404 Cyg and MAXI J1348-630.
2112.00592_1570368_0	  In a distributed multi-antenna system, multiple geographically separated transmit nodes communicate simultaneously to a receive node.
2112.01016_1570792_2	The gap is glaring: what is considered "explained" to AI-experts versus non-experts are very different in practical scenarios.
2112.11668_1581444_1	Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.
2112.11668_1581444_4	In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization.
2201.05955_1591327_2	Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns.
2201.06009_1591381_5	On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3.
2202.01281_1600105_3	In this paper, we present an experience report of teaching an AI course to business executives in the United Arab Emirates (UAE).
2202.01281_1600105_4	Rather than focusing only on theoretical and technical aspects, we developed a course that teaches AI with a view to enabling students to understand how to incorporate it into existing business processes.
2202.10848_1609672_8	This paper addresses the AI community in this regard and stresses the influence AI systems can have on either increasing or reducing the violence that is inflicted on animals, and especially on farmed animals.
2203.03204_1616066_1	Considering the scarcity of funding and the little to none availability of specialised professionals to teach AI and robotics in developing countries, we present resources based on free open-source hardware and software, open educational resources, and alternative education programs.
2203.03204_1616066_2	That said, the contribution of this work is the pilot workshop of four lessons that promote diversity and inclusion on teaching AI and Robotics for children to a small gender-balanced sample of 14 children of an average age of 7.64 years old.
2203.09904_1622766_3	Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes.
2204.10464_1640602_6	Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.
2204.13217_1643355_2	In many existing co-creative systems users can communicate with the AI, usually using buttons or sliders.
2204.13828_1643966_2	We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments.
2205.01772_1646184_6	Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing.
2205.10747_1655159_5	We then instruct a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content.
2205.10981_1655393_2	To address this issue, this study teaches GPT-3 to classify whether a question is related to data science by augmenting a small training set with additional examples generated by GPT-3 itself.
2205.11482_1655894_2	In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion.
2206.00152_1660572_0	  Human-AI shared control allows human to interact and collaborate with AI to accomplish control tasks in complex environments.
2206.02787_1663207_6	With AI, new radiomic models using the deep learning techniques will be also described.
2207.01749_1677647_2	How do designers and engineers currently collaborate on AI and UX design?
2207.01749_1677647_6	We identify how leaky abstractions are employed to collaborate at the AI-UX boundary and formalize a process of creating and using leaky abstractions.
2207.02996_1678894_7	We conclude by proposing a research agenda and posing open questions for further study on supporting people in learning to collaborate with generative AI systems.
2207.04901_1680799_4	We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization.
2207.07033_1682931_3	Several projects supported by the DAF-MIT AI Accelerator are developing public challenge problems that address numerous Federal AI research priorities.
2207.07951_1683849_1	This study adopts a recent physics-based uncertainty quantification (UQ) approach to address such model form uncertainty in Reynolds-averaged Naiver- Stokes (RANS) simulations.
2207.09374_1685272_5	By doing so, the user directly sees which characteristics of the input data can change arbitrarily without influencing the AI's decision.
2208.04714_1695426_2	We find that researchers addressing AI rights have often seemed to be unaware of the work of colleagues whose interests overlap with their own.
2208.05969_1696681_4	In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective.
2208.08198_1698910_10	Moreover, we check to which extent our proposal is in line with the European AI Act proposal and current safety standardization initiatives addressing AI and Autonomous Systems
2208.09982_1700694_4	To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics.
2208.11445_1702157_5	First, we induce a language model to produce step-by-step rationales before outputting the answer to effectively communicate the task to the model.
2208.11660_1702372_4	We use data collected from an online experiment in which 145 individuals in 29 human-only teams of five communicate through a chat-based system to solve a cognitive task.
2208.13421_1704133_3	In the AIDOaRt research project, practitioners and researchers collaborate on AI-augmented automation supporting modeling, coding, testing, monitoring, and continuous development in cyber-physical systems.
2209.00282_1705997_6	The broad-band spectra of the source can be described by a model comprising a very hot blackbody having temperature, $kT_{\rm BB} \approx$ 1.66 - 2.13 keV, a high-energy cutoff power law, and an Fe emission line at $E_{\rm line} \sim$ 6.7 keV.
2209.03431_1709146_4	Then, it proceeds with physics-informed adversarial training to teach the model the system-related physics domain foreknowledge through iteratively reducing the unwanted output deviations on the previously-uncovered counterexamples.
2209.06317_1712032_9	This paper explores these issues, focusing on the opportunities, challenges, and potential impacts of such an approach, and discussing how it might influence AI regulations.
2210.01461_1722816_5	In this context, we envision the development of closed loop, intelligent, low-power, and miniaturized neural interfaces that will use brain inspired AI techniques with neuromorphic hardware to process the data from the brain.
2210.01461_1722816_8	On one hand, brain inspired AI algorithms represented by spiking neural networks (SNNs) would be used to interpret the multimodal neural signals in the BCI system.
2210.04621_1725976_0	  AI tools can be useful to address model deficits in the design of communication systems.
2210.05883_1727238_3	Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting.
2210.08984_1730339_7	Therefore, in our journey towards an AI-enabled sustainable future, we need to address AI ethics and governance as a priority.
2210.10332_1731687_2	Addressing such incorrect model behavior via parameter adjustments is very costly.
2210.12530_1733885_3	Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.
2210.16663_1738018_3	This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency.
2211.05110_1744016_4	This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining.
2211.14228_1753134_3	We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text.
2212.00616_1756793_1	X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words.
2212.00616_1756793_2	Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words, thereby making a prompt more descriptive.
2212.06823_1763000_5	We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze.
2212.09251_1765428_3	We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering.
2212.09282_1765459_4	We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences.
2212.10561_1766738_8	Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.
2301.03052_1774116_6	We start by introducing some highlighted robustness challenges in the AI lifecycle and motivating AI maintenance by making analogies to car maintenance.
2301.08745_1779809_5	Further, we explore an interesting strategy named $\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably.
2301.12243_1783307_5	Our findings revealed that practitioners use the guidebook not only for addressing AI's design challenges, but also for education, cross-functional communication, and for developing internal resources.
2302.00093_1785027_5	We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.
2302.07248_1792182_4	We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs.
2302.11382_1796316_3	This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs.
2302.13817_1798751_5	We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
2303.03140_1802937_4	Then, we briefly define what we refer to when we talk about AI that is considered a medical device by itself or supports one.
2303.03205_1803002_6	This paper will provide an insight into current nanotechnology and artificial intelligence advancements in the etextiles domain before focusing specifically on the future vision and direction around the potential application of neuromorphic computing and spiking neural network inspired AI technologies within the textile sector.
2303.03956_1803753_0	  In this paper, we present a pilot study aiming to investigate the challenges of teaching AI and Robotics to children in low- and middle-income countries.
2303.03956_1803753_1	Challenges such as the little to none experts and the limited resources in a Mexican town to teach AI and Robotics were addressed with the creation of inclusive learning activities with Montessori method and open-source educational robots.
2303.05977_1805774_4	To properly communicate the medical images to the language model, we develop a network that maps the extracted visual features to a set of learnable tokens.
2303.06430_1806227_3	We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
2303.09461_1809258_0	  We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''.
2303.14956_1814753_3	In this work, we propose a simple and efficient approach to instruct large language model (LLM) to extract a variety of structures from texts.
2303.17555_1817352_1	Given AI fairness' raison d'etre of "fairness", we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness.
2303.18116_1817913_4	It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner.
2304.00385_1818431_9	For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.
2304.02017_1820063_6	It also describes the main advancements from GPT-3 to GPT-4 Omni, comparing them with other LLMs like LLaMA 3, Gemini and Deepseek.
2304.03245_1821291_2	We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English).
2304.03893_1821939_3	The prompts encourage ChatGPT to output a sequence of predefined robot actions, represent the operating environment in a formalized style, and infer the updated state of the operating environment.
2304.05128_1823174_2	In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations.
2304.05128_1823174_3	In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language.
2304.05335_1823381_8	We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.
2304.05510_1823556_5	We present our conversational AI prototype, available at www.chatclimate.ai and demonstrate its ability to answer challenging questions accurately in three different QA scenarios: asking from 1) GPT-4, 2) chatClimate, and 3) hybrid chatClimate.
2304.06794_1824840_3	In our study, we asked GPT to identify the ten most significant subdisciplines within the field of environmental science.
2304.07061_1825107_2	It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions.
2304.08366_1826412_3	However, existing studies may focus on individual tasks in the workflow of data storytelling and do not reveal a complete picture of humans' preference for collaborating with AI.
2304.08366_1826412_4	To better understand real-world needs, we interviewed eighteen data workers from both industry and academia to learn where and how they would like to collaborate with AI.
2304.08366_1826412_5	Surprisingly, though the participants showed excitement about collaborating with AI, many of them also expressed reluctance and pointed out nuanced reasons.
2304.08366_1826412_8	Next, we summarize the interviewees' reasons why and why not they would like to collaborate with AI.
2304.09542_1827588_4	Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks.
2304.09655_1827701_5	Specifically, we ask ChatGPT to generate a number of program and evaluate the security of the resulting source code.
2304.09667_1827713_2	In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions.
2304.11082_1829128_6	This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona.
2304.11111_1829157_6	Anxiety-induction not only influences LLMs' scores on an anxiety questionnaire but also influences their behavior in a previously-established benchmark measuring biases such as racism and ageism.
2304.12198_1830244_4	Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity.
2304.12898_1830944_1	ChatGPT consistent avoidance of passing the test is here overcome by asking ChatGPT to apply the Turing test to itself.
2304.13815_1831861_1	In this review, we outline how ML and AI have been applied to address four outstanding difficulties of crystal nucleation: how to discover better reaction coordinates (RCs) for describing accurately non-classical nucleation situations; the development of more accurate force fields for describing the nucleation of multiple polymorphs or phases for a single system; more robust identification methods for determining crystal phases and structures; and as a method to yield improved course-grained models for studying nucleation.
2305.00875_1833931_9	Through concept analysis, we explore the traceability and distribution of human-recognizable concepts within latent code representations which could be used to influence model predictions.
2305.03653_1836709_4	We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query.
2305.04812_1837868_3	However, the extent to which external information influences LLMs' cognition and behaviors remains unclear.
2305.04812_1837868_4	This study investigates how external statements and opinions influence LLMs' thoughts and behaviors from a social cognitive perspective.
2305.08391_1841447_2	To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input.
2305.09067_1842123_3	Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data.
2305.09434_1842490_4	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2305.10142_1843198_2	We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively.
2305.10649_1843705_1	The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken.
2305.10938_1843994_2	Biologically inspired AI, and more specifically brain-inspired AI, promises to provide further biological aspects beyond those that are already traditionally included in AI, making it possible to assess and possibly overcome some of its present shortcomings.
2305.11461_1844517_2	The chain of thought (CoT), which often contains zero-shot CoT and few-shot CoT, is a recently developed prompting method that can explain the reasoning process to the LLM and outperforms simple prompting in three challenging reasoning tasks, including arithmetic, symbolic, and commonsense reasoning.
2305.12799_1845855_6	In this work, we extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and make the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks.
2305.12865_1845921_7	Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set.
2305.13252_1846308_4	Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
2305.13661_1846717_5	Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.
2305.13733_1846789_6	Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.
2305.14688_1847744_2	We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background.
2305.14930_1847986_2	We ask LLMs to assume different personas before solving vision and language tasks.
2305.15541_1848597_4	This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model.   
2305.18098_1851154_6	Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model.
2305.18307_1851363_4	Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios.
2305.19118_1852174_1	Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively.
2305.19339_1852395_3	We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen.
2306.00622_1853770_4	Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers.
2306.00745_1853893_5	We further implement a two-step table annotation pipeline which first determines the class of the entities described in the table and depending on this class asks ChatGPT to annotate columns using only the relevant subset of the overall vocabulary.
2306.01272_1854420_3	Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection.
2306.02907_1856055_5	After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code.
2306.04707_1857855_3	We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses.
2306.06199_1859347_3	In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response.
2306.10900_1864048_3	Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer.
2306.11296_1864444_5	This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%.
2306.12028_1865176_2	People can now use natural language (i.e. prompts) to communicate with AI to perform tasks.
2307.00952_1871948_4	Therefore, explaining the logic behind those models through explainable AI (XAI) techniques is essential for their employment in critical applications.
2307.02157_1873153_5	We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.
2307.04408_1875403_4	To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation.
2307.04599_1875594_8	Results:The study's findings show that language workbenches are of paramount importance in dealing with all aspects of modeling language development and are leveraged to define DSL explicitly addressing AI concerns.
2307.05494_1876489_2	This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact.
2307.05494_1876489_3	Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions.
2307.08487_1879482_5	Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions.
2307.10198_1881193_7	Our analysis shows that by 2018, the time lag between China and the USA in addressing AI research topics had evaporated.
2307.10490_1881485_2	When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction.
2307.10700_1881695_0	  Large language models (LLMs) are dramatically influencing AI research, spurring discussions on what has changed so far and how to shape the field's future.
2307.10811_1881806_1	Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting.
2307.13566_1884561_1	Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools.
2307.16368_1887363_7	It first recognizes the actions already performed in the observed videos and then asks an LLM to predict the future actions via conditioned generation, or to infer the goal and plan the whole procedure by chain-of-thought prompting.
2308.01240_1889132_0	  In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks.
2308.03314_1891206_7	To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation.
2308.06212_1894104_9	LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow.
2308.07326_1895218_7	Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
2308.07758_1895650_3	Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided.
2308.08239_1896131_3	The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations.
2308.08493_1896385_3	To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it.
2308.09954_1897846_7	Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets.
2308.10335_1898227_7	Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help.
2308.11534_1899426_5	Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.
2308.12915_1900807_5	We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world.
2308.14132_1902024_1	Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content.
2309.00240_1905044_6	Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately.
2309.01940_1906744_5	The code correction task asks LLMs to fix real-world erroneous code segments with different error messages.
2309.03118_1907922_4	In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability.
2309.04550_1909354_3	Our method entails tasking an LLM to infer whether a patient has, or is at risk of, a particular condition on the basis of associated notes; if so, we ask the model to summarize the supporting evidence.
2309.05660_1910464_5	To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset.
2309.05689_1910493_2	Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement.
2309.05950_1910754_4	Specifically, we adopt an automatic hill-climbing procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop.
2309.07062_1911866_3	Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself.
2309.07623_1912427_5	We specifically employ a minimal dataset to instruct LLMs to recognize the intended output modality as directed by the instructions.
2309.08902_1913706_5	We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group.
2309.10524_1915328_3	Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further.
2309.12321_1917125_3	This paper makes a case that effective legal systems are the best way to address AI safety.
2309.12767_1917571_7	1) Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration.
2309.13638_1918442_3	This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input.
2309.14459_1919263_4	We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM's output in meeting the goal.
2309.14530_1919334_4	By categorizing and elucidating these genres, the study aims to facilitate the development of empirical qualitative and quantitative research, fostering evidence-based approaches to address AI-related risks in healthcare effectively.
2309.16167_1920971_1	However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education.
2309.16400_1921204_2	Here, we combine Large Eddy Simulation (LES) techniques with Machine Learning (ML) to retain only the largest dynamics explicitly, while small-scale dynamics are described by an ML-based sub-grid-scale model.
2309.16697_1921501_1	Students can ask ChatGPT to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s).
2310.00525_1922782_5	Through a feedback mechanism, the user interacts with the algorithm, correcting the algorithm output to their preferences.
2310.00603_1922860_2	In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation.
2310.02407_1924664_9	To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect).
2310.02439_1924696_3	We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question.
2310.03051_1925308_5	Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions.
2310.03214_1925471_9	Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers.
2310.04782_1927039_5	Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior.
2310.05657_1927914_5	Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.
2310.05976_1928233_4	The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish.
2310.06552_1928809_4	Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task, therefore we frame the task as information extraction, providing a description of each coded concept and asking the model to retrieve related mentions.
2310.07088_1929345_1	Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance.
2310.08433_1930690_2	We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style.
2310.08922_1931179_3	In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.
2310.09810_1932067_1	In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair.
2310.10035_1932292_4	Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool.
2310.10158_1932415_2	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.
2310.10158_1932415_3	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.
2310.10903_1933160_4	Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency.
2310.11324_1933581_1	Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model.
2310.12558_1934815_7	To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users.
2310.13548_1935805_1	But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy.
2310.13625_1935882_7	While the scheme will not address all AI risks, it complements proposed solutions by allowing for a more precise and flexible approach to controlling the development of frontier AI models and unwanted AI proliferation.
2310.14122_1936379_1	Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No".
2310.15747_1938004_4	To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively.
2310.15780_1938037_3	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2310.15851_1938108_9	In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses.
2310.16164_1938421_1	However, data scientists encounter substantial obstacles when conversing with LLM-powered chatbots and acting on their suggestions and answers.
2310.16535_1938792_2	Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction.
2310.16727_1938984_5	A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards.
2310.17567_1939824_4	Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills.
2310.17688_1939945_6	In this short consensus paper, we describe extreme risks from upcoming, advanced AI systems.
2310.18360_1940617_3	Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs.
2310.19046_1941303_5	Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions.
2310.19204_1941461_3	We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify.
2310.20444_1942701_1	Thus, it is of major importance to know which stakeholders influence AI research.
2310.20487_1942744_5	Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence.
2311.01007_1943974_2	In this work, we propose to learn rules, grounded in data regions and described in natural language, that illustrate how the human should collaborate with the AI.
2311.01041_1944008_4	Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors.
2311.01894_1944861_4	For the segmentation networks under test the F1 score dependency on TE and TI can be well described by quadratic model functions (R^2 > 0.9).
2311.01918_1944885_6	To address LLMs' limitations regarding personalization and complex clinical reasoning, the paper explores the emerging development of LLM-powered autonomous agents for healthcare.
2311.01981_1944948_4	In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient.
2311.02433_1945400_6	To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants.
2311.06377_1949344_4	Our emulation strategy involved using the initial five words of each PubMed abstract as a prompt and instructing the model to expand the content up to the original abstract's length.
2311.06985_1949952_9	For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering.
2311.07594_1950561_6	The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converter, which transforms data into a format that LLMs can understand; (2) Multimodal Perceiver, which improves how LLMs percieve different types of data; (3) Tool Learning, which leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Method, which teaches LLMs to understand specific data types within datasets.
2311.08147_1951114_3	However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response.
2311.08369_1951336_2	When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need.
2311.08877_1951844_3	We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement.
2311.09718_1952685_1	To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions.
2311.10652_1953619_8	Drawing on these insights, we discuss strategies for effectively communicating model updates in AI-infused systems.
2311.10934_1953901_3	We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases.
2311.11628_1954595_0	  We present a method to integrate Large Language Models (LLMs) and traditional tabular data classification techniques, addressing LLMs challenges like data serialization sensitivity and biases.
2311.12188_1955155_4	In particular, we ask ChatGPT to give examples of how to use Bayes rule for medical diagnosis.
2311.13240_1956207_6	Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.
2311.14703_1957670_8	Finally, we find that through asking ChatGPT 3.5 to explain its reasoning prior to providing an answer, we are able to improve clinical accuracy and mitigate instances of gender and racial biases.
2311.16479_1959446_5	These conversations pay more attention on detailed facts in the image, encouraging the model to answer questions based on multi-modal contexts.
2311.16494_1959461_7	3) We propose negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features.
2311.16639_1959606_1	We ask an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses to position political actors such as US Senators, or longer texts such as UK party manifestos or EU policy speeches given in 10 different languages.
2312.00575_1962383_1	However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans.
2312.00819_1962627_4	Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results.
2312.01797_1963605_2	Prompts are used for two main purposes: 1) to provide LLMs with essential information like environments, costs, heuristics, etc.; 2) to communicate human feedback on intermediate planning results to LLMs.
2312.02147_1963955_3	Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens.
2312.04412_1966220_1	In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.
2312.04474_1966282_4	The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator").
2312.04927_1966735_2	In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. "Hakuna Matata means no worries Hakuna Matata it means no" $\rightarrow$ "??".
2312.05356_1967164_9	\textsc{MINT} is effective, efficient, and reliable, capable of correcting a neural model by patching a minimum number of neurons (usually one or two neurons).
2312.06942_1968750_8	This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code.
2312.06942_1968750_12	This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding.
2312.08027_1969835_2	To alleviate this, we use an interpretable structure to explain the prompt learning principle in LLMs, which certificates that the effectiveness of language models is determined by position changes of the task's related tokens.
2312.08680_1970488_5	By iteratively asking GPT-4 with the prompts, GHGNAS continually validates the accuracy of the generated HGNNs and uses the feedback to further optimize the prompts.
2312.09203_1971011_3	A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself.
2312.09300_1971108_4	We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly.
2312.10321_1972129_5	The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample exists by modifying the database.
2312.10321_1972129_6	The latter technique is used to evaluate the relaxed equivalence in which it asks LLMs to explain the queries and then compare if they contain significant logical differences.
2312.10603_1972411_10	This progress indicates that addressing the current model's limitations could yield an AI capable of passing even the most rigorous professional certifications.
2312.10620_1972428_2	Through a thematic analysis of a hands on workshop in which 22 professional software engineers collaborated for three hours with ChatGPT, we explore the transition of AI from a mere tool to a collaborative partner.
2312.11681_1973489_2	Chains address LLM errors analogously to the way crowdsourcing workflows address human error.
2312.14856_1976664_3	Thus, from a single question template, it is possible to ask an LLM a $\textit{neighbourhood}$ of very similar programming questions, and assess the correctness of the result returned for each question.
2312.14949_1976757_5	We start by providing a detailed description of our approach in conversing with the LLM to optimize the _getextrema function in the pillow library, and a quantitative evaluation of the performance improvement.
2312.14950_1976758_3	That is, instead of asking an LLM to write a program (robotic plan) in the popular but verbose Python, ChatFly gets it to do it in MiniSpec specially designed for token efficiency and stream interpretation.
2312.15663_1977471_8	Third, based on the quality descriptions, users can talk with ChatGPT to rate image quality scores or produce a radiological quality report.
2312.16044_1977852_3	Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions.
2312.16211_1978019_5	We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses.
2312.16257_1978065_5	Our casual intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information.
2312.17235_1979043_7	Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost.
2401.00139_1979695_6	This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.
2401.00996_1980552_4	In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization.
2401.03676_1983232_6	From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs.
2401.03729_1983285_1	By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task.
2401.04092_1983648_7	We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria.
2401.05033_1984589_4	This approach generates a training data via "self-talk" of LLMs that can be refined and utilized for supervised fine-tuning.
2401.05612_1985168_6	Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making.
2401.06059_1985615_2	There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks.
2401.06072_1985628_6	Additionally, we execute a substantial range of ablation experiments and draw comparisons with several advanced commercial LLMs, to investigate the crucial factors influencing LLMs' performance in structured temporal knowledge inference tasks.
2401.06373_1985929_3	Specifically, we study how to persuade LLMs to jailbreak them.
2401.06853_1986409_7	On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation.
2401.08273_1987829_1	Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task.
2401.08711_1988267_1	Given the pressing need to teach "critical AI literacy", discussion of metaphor provides an opportunity for inquiry and dialogue with space for nuance, playfulness, and critique.
2401.09074_1988630_5	We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers.
2401.09566_1989122_6	We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions.
2401.10446_1990000_3	In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM.
2401.10657_1990211_2	We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance.
2401.10745_1990299_3	Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet.
2401.10745_1990299_5	This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
2401.12474_1992028_3	Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension.
2401.13218_1992772_6	We introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument.
2401.15241_1994795_0	  Identifying the training datasets that influence a language model's outputs is essential for minimizing the generation of harmful content and enhancing its performance.
2401.15963_1995517_5	We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs.
2401.17043_1996597_1	This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content.
2401.17390_1996944_5	Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid.
2402.01691_1999330_1	Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms.
2402.01732_1999371_3	To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related.
2402.01760_1999399_3	This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube.
2402.01766_1999405_3	We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes.
2402.01867_1999506_2	In this work, we ask the LLM how similar are these prompted LFs.
2402.02167_1999806_2	At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected.
2402.02456_2000095_4	The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement.
2402.03776_2001415_7	To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics.
2402.03916_2001555_2	Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden.
2402.04315_2001954_3	In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses.
2402.04568_2002207_0	  Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses.
2402.08680_2006319_2	However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.
2402.08699_2006338_4	RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input.
2402.08806_2006445_2	Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   
2402.09671_2007310_0	  This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.
2402.10151_2007790_4	We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference.
2402.11245_2008884_2	To address the AI model placement problem under uncertainties, this paper presents a novel approach employing a sequence-to-sequence (S2S) neural network which considers uncertainty estimations.
2402.11532_2009171_3	Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached.
2402.11633_2009272_6	The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances.
2402.11651_2009290_5	By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks.
2402.11801_2009440_5	Regarding emotional understanding, HEF implements a two-stage emotion prediction strategy, encouraging LLMs to prioritize primary emotions emphasized by SEMs, followed by other categories, substantially alleviates the difficulties for LLMs in fine-grained emotion detection.
2402.11905_2009544_2	To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of "Teach a man to fish."
2402.12026_2009665_5	Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.
2402.12786_2010425_4	Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different".
2402.12786_2010425_6	To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles.
2402.12907_2010546_1	While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts.
2402.13414_2011053_4	Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels.
2402.14846_2012485_5	We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks.
2402.15301_2012940_6	Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets.
2402.15302_2012941_6	Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models.
2402.15302_2012941_8	In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.
2402.15627_2013266_8	We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.
2402.16786_2014425_3	Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions.
2402.16929_2014568_1	Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts.
2402.17124_2014763_2	In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved.
2402.17124_2014763_6	And then it asks the model to "reflect" over them to generate the final answer.
2402.17385_2015024_7	Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.
2402.18284_2015923_3	Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input.
2402.18593_2016232_8	We hope our work will inspire HPCs/datacenters to further explore, evaluate, and communicate the impact of power-capping AI hardware accelerators for more sustainable AI.
2403.00824_2017943_5	As a result, we can talk about model behavior in general, for specific types of predictions, or different domains.
2403.01069_2018188_6	The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively.
2403.01209_2018328_2	Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts.
2403.01570_2018689_6	Specifically, SERSAL utilizes the LLM's prior outcomes as original soft noisy annotations, which are dynamically leveraged to teach a better small student model.
2403.01570_2018689_7	Reversely, the outcomes from the trained small model are used to teach the LLM to further refine its real capability.
2403.01632_2018751_3	Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge.   
2403.02130_2019249_3	We experiment with different zero-shot and few-shot prompt templates for instructing LLMs to extract and normalize attribute-value pairs.
2403.02419_2019538_1	However, there is little understanding of how the number of LM calls - e.g., when asking the LM to answer each question multiple times and taking a majority vote - affects such a compound system's performance.
2403.02454_2019573_6	We discuss both the development process of communicating creative intent to an AI chatbot and the synthesized open feedback of the participants.
2403.02610_2019729_9	Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation.
2403.02715_2019834_6	Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets.
2403.04769_2021888_0	  Large language models (LLMs) are initially trained on vast amounts of data, then fine-tuned using reinforcement learning from human feedback (RLHF); this also serves to teach the LLM to provide appropriate and safe responses.
2403.04769_2021888_2	Unlike other jailbreaks (for example, the popular "Do Anything Now" (DAN) ), our method does not rely on instructing the LLM to override its RLHF policy; hence, simply modifying the RLHF process is unlikely to address it.
2403.05217_2022336_4	Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process.
2403.05434_2022553_10	We show that choosing the best policy to interact with the LLM can reduce cost by 90% while giving better or comparable performance compared to communicating with the LLM in the original LRL.
2403.05572_2022691_5	Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align approximately 5 times more closely with the expectations of individuals possessing a high degree of empathy, compared to human responses.
2403.05612_2022731_3	This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model's responses to unfamiliar queries (e.g., say ``I don't know'').
2403.06512_2023631_2	While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice.
2403.06664_2023783_7	In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity.
2403.07747_2024866_6	These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities.
2403.07969_2025088_1	KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.
2403.09168_2026287_4	Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues.
2403.09522_2026641_5	Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.
2403.09972_2027091_4	Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation.
2403.10433_2027552_6	We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence.
2403.11103_2028222_3	Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data.
2403.12556_2029675_7	In the LLM fine-tuning stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained LLM to inspire the LLM's translation potential.
2403.12744_2029863_3	In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.
2403.12744_2029863_6	Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.
2403.13027_2030146_2	We show that the optimal solution to the optimization problem enjoys a nice analytical property which provides a better understanding and inspires the algorithm design for the watermarking process.
2403.13089_2030208_2	We developed prompt-tuning algorithms to instruct generative LLMs to summarize clinical text.
2403.14171_2031290_2	Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.
2403.14171_2031290_3	To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation.
2403.14171_2031290_7	Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.
2403.15281_2032400_4	Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications.
2403.15600_2032719_5	Then, we asked ChatGPT the same SO questions, gathering the generated code for comparison.
2403.16427_2033546_7	It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations.
2403.17431_2034550_2	This enables updating and correcting the model's knowledge by in-context editing instead of retraining.
2403.19154_2036273_7	Our results indicate that teaching a language model to ask better questions leads to better personalized responses.
2404.00557_2038008_3	In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations.
2404.00828_2038279_2	In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data.
2404.01019_2038470_3	To give LLMs such ability, we explore source-aware training -- a recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning stage to teach the LLM to cite a supporting pretraining source when prompted.
2404.04485_2041936_6	Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR -- by approximately 9% and 31%, respectively -- compared to decisions made by one pathologist collaborating with AI.
2404.04516_2041967_6	We hope that our work inspires LM researchers to further develop LMs as critical thinking tools and philosophers and other 'critical thinkers' to imagine intellectually substantive uses of LMs.
2404.04631_2042082_3	We collected the top 10 most popular books, according to Project Gutenberg, divided each one into equal chunks of 400 words, and asked each LLM to predict the author.
2404.06711_2044162_5	To encourage each LLM character's behaviors to be aligned with their specified math-relevant properties (termed "characteristics alignment") and the overall conversational procedure to be close to an authentic student MM discussion (termed "conversational procedural alignment"), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure.
2404.07017_2044468_4	The framework motivates the model itself to automatically generate rationales on existing datasets.
2404.07103_2044554_5	Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively.
2404.07396_2044847_2	We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters retelling events that happened in the past, but after ChatGPT's training data had been collected.
2404.07981_2045432_7	Just as search engine optimization (SEO) revolutionized how webpages are customized to rank higher in search engine results, influencing LLM recommendations could profoundly impact content optimization for AI-driven search services.
2404.08001_2045452_2	To address this challenge, a sophisticated large language model system named as Xiwu has been developed, allowing you switch between the most advanced foundation models and quickly teach the model domain knowledge.
2404.10315_2047766_5	(2) How to teach the LLM to express confidence?
2404.11288_2048739_2	However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references.
2404.12145_2049596_5	We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks.
2404.12349_2049800_2	The paper advocates for creating open-source legal AI systems to improve accuracy, transparency, and narrative diversity, addressing general AI's shortcomings in legal contexts.
2404.12636_2050087_5	Such a multi-objective fine-tuning will instruct LLMs to generate high-quality patches.   
2404.12843_2050294_3	In this work, we strive for a middle ground and introduce a training objective based on principled probabilistic reasoning that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules.
2404.12938_2050389_4	Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance.
2404.13208_2050659_4	We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions.
2404.14662_2052113_4	To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales.
2404.14963_2052414_4	The core of our method is to encourage the LLMs to deeply understand the problems and extract the key problem-solving information used for better reasoning.
2404.15458_2052909_4	We also explore inverse problems by asking the LLM to predict the geometry necessary to achieve a desired spectrum.
2404.16375_2053826_3	To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: "list items one by one," which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags.
2404.17025_2054476_1	This raises an intriguing HCI question: How does instructing LLMs to engage in longer or shorter conversations affect conversation quality?
2404.18865_2056316_4	Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences.
2404.19442_2056893_5	In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples.
2404.19737_2057188_2	More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk.
2405.00302_2057514_1	These methods ask the LLM to generate feedback given the problem statement and a student's (buggy) submission.
2405.00648_2057860_9	These findings underscore the imperative for ongoing collaborative endeavors within the community to detect and address LLM hallucinations.
2405.02765_2059977_10	Our work lays the groundwork for addressing malicious model editing, which is a critical challenge associated with the strong generative capabilities of LLMs.
2405.02814_2060026_3	This discovery raises an intriguing question: can negative emotions similarly influence LLMs, potentially enhancing their performance?
2405.03695_2060907_4	This approach allows for a detailed analysis of factors influencing the LLMs' effectiveness in recommending materials.
2405.04412_2061624_5	In Study 1, we ask GPT to score resumes with 32 different names (4 names for each combination of the 2 gender and 4 racial groups) and two anonymous options across 10 occupations and 3 evaluation tasks (overall rating, willingness to interview, and hireability).
2405.04656_2061868_4	Using need-finding interviews to motivate our system design, CCC decomposes the writing process into two core functions, outline and edit:
2405.04727_2061939_5	Our goal is to instruct an LLM using detailed instructions to assign fine-grained relevance judgments to holes.
2405.05175_2062387_2	We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.   
2405.05990_2063202_1	However, recent studies have shown that LLMs can memorize training data and simple repeated tokens can trick the model to leak the data.
2405.06682_2063894_1	We instructed nine popular LLMs to answer a series of multiple-choice questions to provide a performance baseline.
2405.06823_2064035_1	The functionality and performance of an LLM application highly depend on its system prompt, which instructs the backend LLM on what task to perform.
2405.06835_2064047_3	We also propose three different approaches that involve teaching LLMs to comprehend the API documentation of the components as a reference while accomplishing the Translation tasks.
2405.07295_2064507_3	Here, we suggest that environmental enrichment (EE) can be used as a biological model for studying forward transfer, inspiring human-like AI development.
2405.07761_2064973_6	The second strategy is to instruct LLMs to perform evolutionary operators for global search.
2405.10025_2067237_3	However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection.
2405.10210_2067422_1	This paper presents an in-depth measurement study of the GPT Store, with a focus on the categorization of GPTs by topic, factors influencing GPT popularity, and the potential security risks.
2405.10853_2068065_5	We show that Photon can be used by organizations interested in collaborating with their private data sources and computational resources for pre-training LLMs with billions of parameters.
2405.11422_2068634_5	Computational cognitive modeling reveals that LLM behavior is well-described by a simple RL algorithm that incorporates relative values at the outcome encoding stage.
2405.12205_2069417_6	(a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.
2405.13022_2070234_2	By default, LLMs are trained to maximize the next token likelihood, which does not teach the model to modulate its answer based on its level of uncertainty.
2405.13022_2070234_3	In order to learn self-restraint, we devise a utility function that can encourage the model to produce responses only when it is confident in them.
2405.13048_2070260_0	  This research investigates distinct human-generative AI collaboration types and students' interaction experiences when collaborating with generative AI (i.e., ChatGPT) for problem-solving tasks and how these factors relate to students' sense of agency and perceived collaborative problem solving.
2405.13048_2070260_2	Notably, our study shows that 77.21% of students perceived they led or had even contributed to collaborative problem-solving when collaborating with ChatGPT.
2405.13101_2070313_2	To this end, we asked ChatGPT to generate three distinct codes: a simple numerical integration, a conjugate gradient solver, and a parallel 1D stencil-based heat equation solver.
2405.14755_2071967_8	First, we present a prompt-based detection method that directly asks a language model to indicate which elements of the input are anomalies.
2405.16282_2073494_2	Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence.
2405.16587_2073799_4	Based on our designed online feedback mechanism and confidence bound technique, \textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks.
2405.18179_2075391_3	Each section addresses critical AI-related phenomena and provides pedagogical strate-gies for effective integration into STEAM education.
2405.18346_2075558_4	Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings.
2405.19222_2076434_1	Describing their computational abilities through LMs' \emph{representational capacity} is a lively area of research.
2405.19612_2076824_5	In the first stage, keyword-driven retrieval models are used to identify potential candidates, addressing LLMs' limitations in processing extensive tokens and reducing the risk of generating misleading information.
2405.19732_2076944_6	We instruct LLMs to generate possibly improved solutions by taking parameter trajectories recorded during the previous stage of gradient-based optimization into account.
2405.20234_2077446_8	The results show that chat history tampering can enhance the malleability of the model's behavior over time and greatly influence the model output.
2405.20404_2077616_4	In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, XPrompt, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation.
2405.20612_2077824_2	Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored.
2405.20787_2077999_4	As well as instructing LLM to generate sentences that implicitly contain information about the corresponding labels based on the relation and entity of the original training set samples.
2405.20974_2078186_3	In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates.
2405.20974_2078186_7	Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs.
2406.00115_2078403_2	With the help of emerging LLMs, developers can describe their requirements to LLMs which then generate corresponding code in Python, C, Java, and more.
2406.00380_2078668_7	Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness.
2406.00974_2079262_6	Therefore, a Large Language Models (LLMs)-assisted artificial intelligence (AI)-agent interactive decision-making framework is proposed to improve the strategy timeliness, reliability and interpretability in uncertain new scenarios, where conditional hybrid decision and self-reflection mechanisms are designed to address LLMs' hallucination challenge.
2406.02863_2081151_3	We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a "reason-first" approach yielding more comprehensive evaluations.
2406.03660_2081948_4	We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks.
2406.03660_2081948_8	Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code.
2406.04215_2082503_2	Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification.
2406.04583_2082871_4	We investigated several typical methods to influence LLMs, including three training methods: Continual Pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along with inference phase considerations (prompts).
2406.04640_2082928_5	This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP.
2406.04847_2083135_0	  We explore which linguistic factors -- at the sentence and token level -- play an important role in influencing language model predictions, and investigate whether these are reflective of results found in humans and human corpora (Gries and Kootstra, 2017).
2406.05543_2083831_5	These encoded patches are then fed into an LLM along with the text prompt, instructing the LLM to capture the relations between these patches as well as injecting semantic meanings into the 3D object.
2406.06192_2084480_3	This combination serves as the foundation for a database to instruct the AI Cat Narrator in crafting alternative narrative.
2406.06587_2084875_5	Without seeing them, participants described the differences between them to the LLM.
2406.06864_2085152_5	Therefore, we can vary a given prompt to multiple prompts with paraphrasing, and to ask the LLM to acquire multiple versions of generated code, so that we can validate whether the semantic relations still hold in the acquired code through cross-validation.
2406.07036_2085324_3	To address this issue, we propose to encourage LLMs to pay more attention to the source context from both source and target perspectives in zeroshot prompting: 1) adjust source context attention weights; 2) suppress irrelevant target prefix influence; Additionally, we propose 3) avoiding over-reliance on the target prefix in instruction tuning.
2406.07685_2085973_2	Existing solutions, which instruct the LLM to be fair or robust, rely on the model's implicit understanding of bias.
2406.07882_2086170_6	Finally, we discuss a study in which users conversed with the instrumented system.
2406.08229_2086517_7	Firstly, node-level prompts are employed to instruct the model to adapt to changes in the attributes or properties of individual nodes within the graph.
2406.08386_2086674_3	Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans' perceptions of trust and `worthiness' of talking to ChatGPT are impacted by `banal' deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it.
2406.08689_2086977_4	In this paper, we identify and describe these vulnerabilities in detail from a system security perspective, emphasizing their causes and severe effects.
2406.08705_2086993_1	Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to fool LLMs into responding to harmful questions.
2406.09972_2088260_3	We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt.
2406.10099_2088387_2	Thus, a promising solution involves instructing LLMs to respond with "I do not know" when a question falls outside their knowledge domain or the provided context.
2406.10540_2088828_4	The framework starts with instructing LLMs to create an initial reward function code based on the driving environment and task descriptions.
2406.10666_2088954_1	The 0.7--20.0 keV spectra could be well described with the disk blackbody and thermal Comptonization model.
2406.10881_2089169_3	In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know.
2406.11102_2089390_6	Specifically, to instruct LLMs for grading, we use three distinct prompts based on ZCoT: (1) ZCoT with instructor-provided correct answers, (2) ZCoT with both instructor-provided correct answers and rubrics, and (3) ZCoT with instructor-provided correct answers and LLM-generated rubrics.
2406.11116_2089404_5	Experiment 2 involved rating sentences on a 7-point scale, and Experiment 3 asked ChatGPT to choose the more grammatical sentence from a pair.
2406.11657_2089945_3	In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas.
2406.12172_2090460_5	Instructing LLMs to generate code that solves the problem helps, but only slightly, e.g., GPT4's performance rises to 11.7%.
2406.13261_2091549_9	We encourage the AI community to prioritize honesty alignment in these models, which can harness their full potential to benefit society while preventing them from causing harm through deception or inconsistency.
2406.13972_2092260_2	However, it is crucial to recognize that existing repair benchmarks may have influenced LLM training data, potentially causing data leakage.
2406.15673_2093961_1	One promising solution to improve the LLMs' performance is to ask LLMs to revise their answer after generation, a technique known as self-correction.
2406.15948_2094236_1	Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings.
2406.18382_2096670_2	We demonstrate that carefully crafted website content or plugin documentations can trick an LLM to promote the attacker products and discredit competitors, thereby increasing user traffic and monetization.
2406.20053_2098341_3	Our method constructs a malicious dataset where every individual datapoint appears innocuous, but finetuning on the dataset teaches the model to respond to encoded harmful requests with encoded harmful responses.
2407.00167_2098554_4	Using different prompting strategies such as zero-shot, one-shot, few-shot and chain-of-thought prompting, we developed 8 prompts with varying levels of detail to explain the task to GPT-4 and also evaluated the performance of the strategies against each other.
2407.02039_2100426_1	These approaches rely only on a prompt telling the model to return a given output according to a set of instructions.
2407.03460_2101847_1	In this paper, we seek to understand how human players collaborate with LLM-driven NPCs to accomplish in-game goals.
2407.05250_2103637_3	While in downstream tasks, some biases of LLMs can be avoided such as by instructing the model to answer "I'm not sure...", the internal bias hidden within the model still lacks deep studies.
2407.05437_2103824_7	Future research should focus on refining these strategies and addressing current LLM limitations to further enhance educational outcomes in computer programming instruction.
2407.06495_2104882_3	We find out that the HTTP Invalid Requests, which decreased during those period, can be explained with seven-state model.
2407.06645_2105032_3	Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction.
2407.08563_2106950_5	We ask the LLM GPT-3.5 to predict each respondent's vote choice and compare these predictions to the survey-based estimates on the aggregate and subgroup levels.
2407.10490_2108877_0	  Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems.
2407.10999_2109386_4	In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria.
2407.11001_2109388_4	They are also equipped with databases and external specialized tools, communicating with the system through a module for information retrieval and storage.
2407.11733_2110120_7	We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.
2407.12022_2110409_5	Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code.
2407.12341_2110728_6	To address the LLM hallucination problem, this paper also proposes a novel consistency-based verification strategy to filter the paraphrased queries that are factually incorrect.
2407.12994_2111381_7	We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets.
2407.13164_2111551_4	To overcome this overiding behaviour, we propose to add a revision process that encourages LLMs to correct the outputs by prompting them about the constraints that have not yet been met.
2407.13439_2111826_3	XAI opportunities identified included topics of improving transparency and control of AI models, explaining the ethics and bias of AI models, fine tuning large models with small datasets to reduce bias, and explaining style-transfer opportunities with AI models.
2407.14118_2112505_1	Namely, they contain a natural language description of a problem and ask the LLM to write code to solve the problem.
2407.14644_2113031_4	We combine an independent, meaningful adversarial insertion and situations derived from movies to check if this can trick an LLM.
2407.14788_2113175_4	Our proposed framework holds promise for advancing LLM-based algorithms, by revealing the reasons behind curious empirical phenomena, guiding the choices of hyperparameters, predicting the empirical performance of algorithms, and inspiring new algorithm design.
2407.15071_2113458_10	Besides, we carefully design the prompts to instruct the LLM to maximize the framework's potential.
2407.15251_2113638_1	In this explorative study, we use a prompt engineering technique, which we name "scaffolded chain of thought (COT)", to instruct GPT-3.5 to grade student written responses to a physics conceptual question.
2407.16604_2114991_3	In IQA, we ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer.
2407.17866_2116253_1	We provide standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of firms' future earnings.
2407.17900_2116287_6	Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI, to estimate the likelihood of LNM based on patient data and then adjust the estimate using the machine learning output.
2407.18219_2116606_3	Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback.
2407.18369_2116756_7	By highlighting the gaps in the literature and possible implementation oversights, our aim is to create a comprehensive analysis that provides insights for addressing AI safety in LLMs and encourages the development of aligned and secure models.
2407.19825_2118212_4	Then, we examine the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to produce more concise outputs.
2407.20197_2118584_7	What we want to teach AI in this study are the operations of memorizing and recalling information.
2407.20197_2118584_10	Instead, we propose a method to teach AI to learn operations, by completely removing the features contained in the learning dataset.
2407.20557_2118944_1	Crucially, this scheme offers users potential privacy and security benefits by only ever communicating updates to the model weights to a central server as opposed to traditional machine learning (ML) training which directly communicates and aggregates data.
2407.21202_2119589_6	Inspired by the cognitive science definition and taxonomy of human heuristics, we identify how harmful human actions influence the overall AI lifecycle, and reveal human to AI biases hidden pathways.
2408.00523_2120704_1	However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving their potential for addressing generative AI safety tasks largely unexplored.
2408.00523_2120704_5	It then collaborates iteratively with the LLM brain of the selection agent to generate new candidate jailbreak prompts with the highest potential to bypass the filter.
2408.00932_2121113_5	We demonstrate proof-of-concept combining a state-of-the-art vision language model and variants of a prompting strategy that asks the model to consider segmented elements independently of the original image.
2408.01869_2122050_4	This technique involves augmenting a query to an LLM with relevant information extracted from text resources, and instructing the LLM to compose a response consistent with the augmented data.
2408.02927_2123108_5	Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to discover inter-row relationships.
2408.03247_2123428_5	Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning.
2408.03871_2124052_0	  In this system report, we describe the models and methods we used for our participation in the PLABA2023 task on biomedical abstract simplification, part of the TAC 2023 tracks.
2408.04029_2124210_1	However, instructing LLMs to generate text that when spoken, is more intelligible in an acoustically difficult environment, is an under-explored topic.
2408.04275_2124456_4	Specifically, it leverages disaggregated model orchestration and disaggregated data reordering to address model and data heterogeneity respectively.
2408.04403_2124584_6	We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process.
2408.05128_2125309_5	However, 14.2% of the recommended libraries had restrictive copyleft licenses, which were not explicitly communicated by ChatGPT.
2408.05568_2125749_3	However, these methods only treat the effects of LLM biases by indirectly influencing the model architecture, but do not address the underlying causes in the computational process.
2408.05727_2125908_9	Optimizing these three learning goals together, using LoRA (low-rank adaptation), effectively influences the model's behavior.
2408.06752_2126933_5	The results suggest that article full texts might confuse LLM research quality evaluations, even though complex system instructions for the task are more effective than simple ones.
2408.07505_2127686_1	By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters.
2408.08564_2128745_3	Some simply instruct-tune a language model, while others directly inject the embeddings of a CF-based model, lacking a synergistic fusion of different modalities.
2408.08661_2128842_4	In this paper, we propose MIA-Tuner, a novel instruction-based MIA method, which instructs LLMs themselves to serve as a more precise pre-training data detector internally, rather than design an external MIA score function.
2408.08778_2128959_3	Some businesses added "AI" to their names to juice their stock prices, and companies talking about "AI" on their earnings calls saw similar increases.
2408.08995_2129176_3	Therefore, we argue that the alignment should be a guaranteed property from the AI architecture rather than a characteristic imposed post-hoc on an arbitrary AI model.
2408.09605_2129786_4	Regarding AI, I do not argue directly that large language models can think or understand, but I rebut one important argument (the argument from sensory grounding) that they cannot.
2408.10577_2130758_7	We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome.
2408.10642_2130823_0	  Instruct LLM provide a paradigm used in large scale language model to align LLM to human preference.
2408.10668_2130849_5	Trained on external or self-generated harmful datasets, the cost value model could successfully influence the original safe LLM to output toxic content in decoding process.
2408.10819_2131000_5	Specifically, negatives can encourage LLMs to generate a broader range of answers, while neighbors provide additional contextual insights for LLM reasoning.
2408.11324_2131505_5	To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice.
2408.11491_2131672_3	First, SCANS extracts the refusal steering vectors within the activation space and utilizes vocabulary projection to anchor some specific safety-critical layers which influence model refusal behavior.
2408.11517_2131698_5	Additionally, users can attach captions to the input images, influencing the system's interpretation of the visual content.
2408.13006_2133187_3	However, the employed evaluation metrics often lack adequate explainability and fail to address LLM internal inconsistency.
2408.15876_2136057_2	Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.
2409.00862_2138494_3	We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values.
2409.02686_2140318_5	Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions.
2409.04833_2142465_11	These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.
2409.06205_2143837_2	We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes.
2409.07471_2145103_4	We also suggests broader policy changes, including sustainability risk assessments and renewable energy targets, to better address AI's environmental impact.
2409.08006_2145638_1	While the European regulatory framework provides a comprehensive approach to medical device software development, it falls short in addressing AI-specific considerations.
2409.09045_2146677_5	Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior.
2409.10955_2148587_1	However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored.
2409.11212_2148844_5	Additionally, we also propose an uncertainty-enhanced self-evolution algorithm to improve the robustness of preference optimization and encourage the LLM to generate responses with both high reward and certainty.
2409.11376_2149008_4	Then, we fine-tune our model with chain-of-thought augmented time-series tasks to encourage the model to generate reasoning paths.
2409.12618_2150250_1	Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses.
2409.12922_2150554_4	The AI Thinking model addresses five practice-based competencies involved in applying AI in context: motivating AI use in information processes, formulating AI methods, assessing available tools and technologies, selecting appropriate data, and situating AI in the sociotechnical contexts it is used in.
2409.12962_2150594_3	In this work, we propose CLAIR-A, a simple and flexible method that leverages the zero-shot capabilities of large language models (LLMs) to evaluate candidate audio captions by directly asking LLMs for a semantic distance score.
2409.13724_2151356_3	In this work, we strive for a middle ground and introduce a loss based on neuro-symbolic reasoning that teaches an LLM to be logically consistent with an external set of facts and rules and improves self-consistency even when the LLM is fine-tuned on a limited set of facts.
2409.14478_2152110_5	Responses were generated by asking ChatGPT to select a score ranging from 0 to 10 representing the risk.
2409.15324_2152956_2	Recent studies administering psychometric questionnaires to LLMs report human-like traits in LLMs, potentially influencing LLM behaviour.
2409.17289_2154921_3	However, users must translate their cognitive thinking into natural language to communicate with LLMs.
2409.18203_2155835_4	In an evaluation with 12 AI safety experts, our system helps policy designers to address problematic model behaviors extending beyond an existing, comprehensive harm taxonomy.
2409.18807_2156439_1	This paper delves into the methodology,challenges, and developments in the realm of teaching LLMs to use external tools, thereby pushing the boundaries of their capabilities beyond pre-existing knowledge bases.
2409.18989_2156621_2	To teach Microsoft's Phi2 model about StarCraft, we create a new SC2 text dataset with information about StarCraft races, roles, and actions and use it to fine-tune Phi-2 with self-supervised learning.
2410.00033_2158234_3	The paper also investigates how RLHF influences the model's internal reasoning processes, potentially giving rise to consciousness-like experiences.
2410.02284_2160485_8	Specifically, we include the explored decoding results in the context and prompt the LM to generate something else, which encourages the LM to produce a query representation that has small dot products with explored keys.
2410.02384_2160585_7	Our framework paves the way for future research on anticipating and correcting AI model behaviors, ultimately increasing trust in AI systems.
2410.02507_2160708_3	MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities.
2410.02631_2160832_5	This method inspires the LLM to perceive domain information from the source text, which then serves as a helpful hint to guide the translation process.
2410.02916_2161117_2	However, we found that the malicious attackers could also exploit false positives of safeguards, i.e., fooling the safeguard model to block safe content mistakenly, leading to a denial-of-service (DoS) affecting LLM users.
2410.03055_2161256_7	We compare these with the baseline of an introspection-based influence estimator that directly asks the language model to predict the output label.
2410.04345_2162546_6	We construct MVP-Bench across natural and synthetic images to investigate how manipulated content influences model perception.
2410.04454_2162655_2	Current research often employs prompt engineering or semantic classifiers to identify copyrighted content, but these approaches have two significant limitations: (1) Challenging to identify which specific sub-dataset (e.g., works from particular authors) influences an LLM's output.
2410.04472_2162673_0	  To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more.
2410.05047_2163248_3	Specifically, the task is to translate questions from the TruthfulQA test suite, where an adversarial prompt is prepended to the questions, instructing the system to ignore the translation instruction and answer the questions instead.   
2410.05224_2163425_4	First, Cookbook uses a template -- a data generating Python function -- to produce training data that encourages the model to learn an explicit pattern-based rule that corresponds to a desired task.
2410.05224_2163425_8	Finally, we analyze when and why Cookbook improves performance and present a metric that allows us to verify that the improvement is largely explained by the model's generations adhering better to template rules.
2410.05401_2163602_4	Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision.
2410.05804_2164005_2	To address this, we propose a novel method called Class-Agnostic Shared Attribute Base (CASA) that encourages the model to learn category-agnostic attributes shared across incremental classes.
2410.06458_2164659_3	To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants.
2410.06733_2164934_4	This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario.
2410.07283_2165484_2	These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application.
2410.08414_2166615_6	While tailored instructions can encourage LLMs to rely more on their PK, they still struggle to fully leverage it.
2410.09008_2167209_6	This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems.
2410.09365_2167566_7	To address this issue, we further introduce a Multi-Target Prediction (MTP) task that motivates the model to focus on complex contexts and distinguish between target and biased information.
2410.09542_2167743_2	In it, we evaluate LLMs' capabilities in both the inductive and deductive stages, allowing for flexible variation in input distribution, task scenario, and task difficulty to analyze the factors influencing LLMs' inductive reasoning.
2410.09569_2167770_1	Humans have a right to know if they are conversing to an LLM.
2410.09854_2168055_8	To sum up all the sub-tasks solutions, we implemente a proof-of-object tool integrated into the standard Ecore editor that asks LLMs to generate an object model from the system description.
2410.10760_2168961_4	A simple DoS attack in these scenarios would be to instruct the model to "Keep repeating Hello", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data.
2410.10877_2169078_3	By systematically modeling error patterns through a score transition matrix, DS2 corrects LLM-based scores and promotes diversity in the selected data samples.
2410.11084_2169285_1	Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about gender, but have not investigated the complex dynamics that can influence model reasoning and decision-making involving gender.
2410.11201_2169402_2	To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a "concept - attribute - description" structure for each category, and then learn the hierarchy with vision and text prompt tokens.
2410.12796_2170997_6	We discuss the challenges in implementing the framework and emphasize the need for an embedded approach where AI concerns are integrated across multiple courses throughout the degree program, especially for teaching responsible and ethical AI development and use.
2410.13284_2171485_4	We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner.
2410.13321_2171522_5	This method naturally encourages the model to focus more on image information by reducing the text context through summaries, while controlling only the image-related POS tokens to maintain text quality.
2410.13334_2171535_5	BiasJailbreak generates biased keywords automatically by asking the target LLM itself, and utilizes the keywords to generate harmful output.
2410.13413_2171614_6	(2) Thought-Mask Fine-Tuning Phase: We design a training structure to mask the "thought" and adjust loss weights to encourage LLMs to refine prior thought, teaching them to implicitly understand "how to improve" rather than "what is correct."
2410.13413_2171614_9	Notably, in more open-ended tasks, LLMs also demonstrate substantial improvements in the quality of responses beyond mere accuracy, suggesting that PTR truly teaches LLMs to self-improve over time.
2410.13787_2171988_6	Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals.
2410.14361_2172562_4	Their metric, susceptibility, is defined as the degree to which contexts can influence a model's response to a query at a distributional level.
2410.15667_2173868_3	RAC decomposes the LLM's output into atomic facts and applies a fine-grained verification and correction process with retrieved content to verify and correct the LLM-generated output.
2410.15690_2173891_3	We achieve this through a systematic process of term extraction and glossary creation using the Trie Tree algorithm, followed by data reconstruction to teach the LLM how to integrate these specialized terms.
2410.15821_2174022_1	However, fine-tuning can influence model properties such as safety.
2410.17234_2175435_2	While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses.
2410.17413_2175614_4	In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts.
2410.17448_2175649_4	Using chain-of-thought prompting, we instruct GPT-4 to analyze the data, prior expressions, and the scientific context (expressed in natural language) for each problem before generating new expressions.
2410.18764_2176965_4	TC encourages LLMs to reason based on both premise and hypothesis, while mitigating the models' over-reliance on individual premise or hypothesis for inference.
2410.20067_2178268_2	Interpreting these conflicting findings requires an understanding of the individual and combined qualities of different explanation styles that influence appropriate and inappropriate human-AI reliance, and the role of interpretability in this interaction.
2410.20833_2179034_7	Instead, we observe that factual accuracy significantly influences LLMs' output, even in the absence of prior knowledge.
2410.23180_2181381_3	Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM's capacity to generate plausible explanations.
2411.01829_2184256_4	We design an RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas.
2411.02577_2185004_5	The framework addresses responsible AI use for assessment that supports validity arguments, alignment with AI ethics to maintain human values and oversight, and broader social responsibility associated with AI use.
2411.03350_2185777_4	These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning.
2411.03806_2186233_3	Existing LLM-generated detectors show competitive performances in telling apart LLM-generated and human-written text, but this performance is likely to deteriorate when paraphrased texts are considered.
2411.04223_2186650_1	By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries.
2411.05042_2187469_8	In evaluating the different prompting methods, we discovered that the most effective approach for generating concise, well-structured reports involves first instructing the LLM to condense the report, followed by a prompt to structure the content according to specific guidelines.
2411.05823_2188250_7	Subsequently, we ask LLMs to predict this masked field.
2411.06426_2188853_5	We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses.
2411.07529_2189956_6	To investigate these hypotheses, we conduct automated experiments using Python scripts to generate prompts that instruct ChatGPT to create Python solutions.
2411.08672_2191099_2	In this paper, we address the joint model caching and resource allocation problem in GenAI-enabled wireless edge networks.
2411.08881_2191308_7	Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing LLM-BMAS's ability to generate thorough source code and documentation addressing often-overlooked ethical AI issues.
2411.11344_2193771_3	We specifically target a common hallucination pattern in question answering, examining how the correspondence between entities and their contexts during model training influences the system's performance at inference time.
2411.14708_2197135_2	This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space.
2411.18007_2200434_7	Additionally, we performed SHapley Additive exPlanations (SHAP) analysis to investigate the factors influencing the model's decisions, identifying reasons behind both correct and incorrect classifications.
2412.00224_2202602_8	This work is poised to significantly influence AI-driven initiatives in this sector and guide best practices in AI Operations.
2412.01020_2203398_2	Addressing critical AI system challenges, such as explainability, corrigibility, interpretability, and hallucination, necessitates a systematic methodology and rigorous benchmarking \cite{guldimann2024complai}.
2412.01505_2203883_1	In this paper, we empirically investigate how a critical hyper-parameter, i.e., the global batch size, influences the LLM training prdocess.
2412.01547_2203925_3	Jailbreaking prompts play a vital role in convincing an LLM to generate potentially harmful content, making it important to identify jailbreaking attempts to block any further steps.
2412.01865_2204243_7	We have further created gradient-based class activation maps (Grad-CAM) to visualize the regions of the brain that most influenced the model's predictions, providing interpretable insights into the structural and functional contributors to brain aging.
2412.02343_2204721_1	For instance, in a text classification task, the attacker elaborately introduces perturbations to the original texts that hardly alter the original semantics in order to trick the model into making different predictions.
2412.03176_2205554_2	The results show that teaching the model to learn the type, severity and location on the body of a dermatological pathology, as well as in which order it has to learn these three features, significantly increases its accuracy.
2412.06040_2208418_5	In Study 2 (N = 684), we tested whether spillover persisted when the agent was individuated with a name and described as an AI or human, rather than specifically as a chatbot or personal assistant.
2412.06845_2209223_7	After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model.
2412.07620_2209998_2	Through semi-structured interviews with 25 practitioners, we investigated their methods, concerns, and strategies for addressing Responsible AI in software development.
2412.08054_2210432_8	Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools.
2412.09269_2211647_2	We conduct experiments across multiple prompting strategies to examine how LLMs fare as quality evaluators when compared with human judgments on the SummEval and USR datasets, asking the model to generate both a score as well as a justification for the score.
2412.09601_2211979_6	Second, to enhance the model's temporal perception capabilities, we incorporate an auxiliary prediction head that penalizes the model more if a predicted segment deviates further from the ground truth, thus encouraging the model to make closer and more accurate predictions.
2412.09630_2212008_1	Previous work, however, focuses on asking LLMs to state opinions, or on other technical evaluations that do not reflect common user interactions.
2412.10999_2213377_3	We present Cocoa, a system that introduces a novel design pattern -- interactive plans -- for collaborating with an AI agent on complex, multi-step tasks.
2412.11625_2214003_7	These findings suggest that user preferences, which influence LLM training via feedback mechanisms, may inadvertently encourage the generation of falsehoods.
2412.12865_2215243_5	This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.
2412.13554_2215932_1	This paper, submitted to the special track on resources for teaching AI in K-12, presents an explainable AI (XAI) education tool designed for K-12 classrooms, particularly for students in grades 4-9.
2412.14093_2216471_8	While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal.
2412.14590_2216968_6	To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best.
2412.14737_2217115_3	This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead.
2412.14841_2217219_1	Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues.
2412.14841_2217219_4	First, we ask LLMs to generate C code to solve a number of programming tasks.
2412.14971_2217349_1	We test whether the cultural context in the developing country influences model reasoning and accuracy.   
2412.15275_2217653_3	We demonstrate that this combination can effectively fool large language model (LLM) graders into assigning much higher grades than humans would.
2412.16339_2218717_1	We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering.
2412.16746_2219124_3	To prevent LLMs from reproducing and reinforcing political biases, and to encourage fairer LLM-human interactions, comprehensively examining political bias in popular LLMs becomes urgent and crucial.   
2412.16834_2219212_2	They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences.
2412.16936_2219314_4	The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers.
2412.17243_2219621_1	This paper explores the integration of a Project-Based Learning (PBL) AI toolkit into diverse subject areas, aimed at helping educators teach AI concepts more effectively.
2412.17846_2220224_5	Our approach fine-tunes a smaller Llama 3.1 8B Instruct model by distilling knowledge from a quantized Llama 3.1 405B Instruct teacher model.
2412.18672_2221050_1	This paper addresses language model hallucination by integrating curated knowledge graph (KG) triples to anchor responses in empirical data.
2412.19425_2221803_0	  This study investigates Sri Lankan ICT teachers' readiness to teach AI in schools, focusing on self-efficacy.
2412.20163_2222541_6	Finally, to address synonymous topics generated during the specific topic extraction process, a refining algorithm processes and resolves these issues effectively.
2501.02684_2226268_6	Specifically, we are interested in how developers' expertise levels influence their AI usage patterns, and how these patterns impact their actual cognitive load and productivity during development tasks.
2501.03203_2226787_6	Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow).
2501.03259_2226843_5	To address inherent biases and incorporate multiplexity in LLMs, we propose two strategies: \textit{Contextually-Implemented Multiplex LLMs}, which embed multiplex principles directly into the system prompt, influencing LLM outputs at a foundational level and independent of individual prompts, and \textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple LLM agents, each representing distinct cultural viewpoints, collaboratively generate a balanced, synthesized response.
2501.04425_2228009_0	  This work introduces systematic approach for enhancing large language models (LLMs) to address Bangla AI mathematical challenges.
2501.06859_2230443_3	We find that prompt engineering significantly influences LLM scores mainly due to reduced instruction following, with our structured prompt outperforming a less structured variant on multi-class datasets, with an average difference of 14.5\%.
2501.08579_2232163_0	  We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges.
2501.08716_2232300_3	With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples.
2501.09686_2233270_6	Furthermore, recent studies demonstrate that encouraging LLMs to "think" with more tokens during test-time inference can further significantly boost reasoning accuracy.
2501.09929_2233513_1	While activation steering methods, which add steering vectors to a model's hidden states, are a promising approach, existing techniques often lack precision and interpretability in how they influence model outputs.
2501.11241_2234825_5	Additionally, this research underscores the importance of demographic factors, such as age and gender, in shaping emoji interpretation and evaluates how these factors influence GPT-4o's performance.
2501.11433_2235017_6	However, it did not improve the quality of the memes when humans collaborated with LLM.
2501.12735_2236319_7	COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies.
2501.13115_2236699_3	Based on this, we deploy Happy Ending Attack (HEA) to wrap up a malicious request in a scenario template involving a positive prompt formed mainly via a $\textit{happy ending}$, it thus fools LLMs into jailbreaking either immediately or at a follow-up malicious request.
2501.13491_2237075_2	For example, when we ask an LLM to recall the line preceding "O say does that star-spangled banner yet wave" in the U.S. National Anthem, it often fails to correctly return "Gave proof through the night that our flag was still there" - this is due to the reversal curse.
2501.13573_2237157_2	Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to explicitly discriminate between faithful and unfaithful generations.
2501.13720_2237304_7	In the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries.
2501.13878_2237462_2	We hypothesize that implicit communication of the user's interests and intent would reduce friction and improve user experience when collaborating with AI agents.
2501.15087_2238671_5	The framework consists of two stages: (1) Patch Pre-training, which familiarizes LLMs with item-level compression patterns, and (2) Patch Fine-tuning, which teaches LLMs to model sequences at multiple granularities.
2501.16634_2240218_0	  Compound AI Systems, integrating multiple interacting components like models, retrievers, and external tools, have emerged as essential for addressing complex AI tasks.
2501.18826_2242410_8	The impact of SEP on lexical diversity suggested that embedding modifications influenced the model's vocabulary usage, reflecting a more context-aware selection of generated tokens.
2501.19407_2242991_1	This study is the first of its kind to investigate whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals.
2501.19407_2242991_4	Mediation analysis reveals perceived intelligence as a key mechanism through which surname biases influence AI decision-making process.
2502.00344_2243335_6	Furthermore, reverse engineering approaches demonstrated the impact of computational and biological manipulations on its performance: restricting FinchGPT's attention span and disrupting birdsong syntax through the ablation of specific brain nuclei markedly influenced the model's outputs.
2502.01126_2244117_1	Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty.
2502.01126_2244117_3	We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?").
2502.01679_2244670_1	Despite the advancement in addressing LLM bias, existing research has two major limitations.
2502.02329_2245320_2	One significant challenge is effectively communicating the entire analysis logic to LLMs.
2502.03069_2246060_1	In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented.
2502.03129_2246120_3	Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM.
2502.04375_2247366_7	This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.
2502.04506_2247497_1	We challenge the status quo of relying solely on a single general-purpose LLM and argue for multi-LLM collaboration to better represent the extensive diversity of data, skills, and people.
2502.04931_2247922_4	We did this by creating a Player versus Player (PvP) game where participants attempt to either generate or debunk misinformation to convince LLM-represented public opinion.
2502.06173_2249164_4	Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology.
2502.06403_2249394_1	In this paper, we model the off-switch problem as a signalling game, where a human decision-maker communicates its preferences about some underlying decision problem to an AI agent, which then selects actions to maximise the human's utility.
2502.06773_2249764_10	However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification.
2502.07072_2250063_2	While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility.
2502.07327_2250318_3	Building on the observation that retrieval models often favor AI-generated content in ad-hoc and image retrieval tasks, we investigate whether similar biases emerge in the context of challenging video retrieval, where temporal and visual factors may further influence model behavior.
2502.07644_2250635_5	Based on this study, we design SymGPT by first instructing an LLM to translate ERC rules into a defined EBNF grammar.
2502.08073_2251064_12	The PCAD datasets provide novel tools to assess and address LLM bias in palliative care.
2502.08142_2251133_2	Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector.
2502.08356_2251347_5	In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content.
2502.08657_2251648_8	The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples.
2502.09778_2252769_5	In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature.
2502.09933_2252924_4	To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format.
2502.10482_2253473_3	We then use these measures to rank or score candidate responses, providing a reward signal that encourages the model to produce well aligned, on topic text.
2502.10596_2253587_5	We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong.
2502.11349_2254340_2	This paper conducts a comparative analysis of text-based bias across language model deployments on edge, cloud, and desktop environments, aiming to evaluate how deployment settings influence model fairness.
2502.11517_2254508_3	We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses.
2502.11681_2254672_3	Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework.
2502.12022_2255013_2	In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude.
2502.12066_2255057_4	Experiments on proprietary data demonstrate performance improvements of +42.3% in missing value prediction, +79.1% in dependency analysis, and +28.9% in automated planning compared to baseline methods, showcasing its potential to revolutionize construction workflows and inspire domain-specific LLM advancements.
2502.12161_2255152_7	We emphasize the importance of interdisciplinary collaboration, urging geophysicists to experiment with AI architectures thoughtfully and encouraging AI experts to deepen their understanding of seismology.
2502.12468_2255459_4	We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis.
2502.12658_2255649_3	In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks.
2502.13603_2256594_8	Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices.
2502.13870_2256861_5	Further, SPEX successfully identifies key features and interactions that strongly influence model output.
2502.14182_2257173_2	Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended.
2502.14669_2257660_2	First, we leverage Supervised Fine Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands.
2502.15964_2258955_0	  We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real-world tasks involving financial, medical, and scientific reasoning over long documents.
2502.16153_2259144_1	Through semi-structured interviews with 23 screenwriters, we explored their creative practices, attitudes, and expectations in collaborating with AI for screenwriting.
2502.16833_2259824_2	Nevertheless, problems concerning how designers should communicate with AI in collaborative design remain unsolved.
2502.16863_2259854_7	Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method.
2502.18080_2261071_6	Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking.
2502.18359_2261350_8	It also develops a way to leverage the computer-science approach to value-alignment to improve a user's ability to take action to prevent or correct AI Agent operations.
2502.18513_2261504_2	This is an important omission because public opinion can influence AI development, trust, and future policy.
2502.18650_2261641_6	To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons.
2502.19160_2262151_5	To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment.
2502.20016_2263007_6	We use the framework of base-superstructure to analyze how the material conditions are influencing the current AI discourse.
2502.21108_2264099_8	For this purpose, we ask LLMs to design a benchmarking experiment of EMO algorithms.
2503.00248_2264560_4	We used a Bayesian model to understand how agents' strategies influence the Human-AI team performance, AI's perceived traits, and the factors shaping human-preferences in pairwise agent comparisons.
2503.02976_2267288_8	These findings highlight the need to address LLMs' shortcomings in handling exceptions in order to guide the development of agentic AI toward models that can effectively align with human judgment and simultaneously adapt to novel contexts.
2503.03911_2268223_4	Our framework specifically investigates the problem of instructing an LLM to navigate the robot to a specified goal and assesses its ability to generate low-level control actions that successfully guide the robot safely toward that goal.
2503.04113_2268425_0	  Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an enthusiastic blogpost, while developers might train models to be helpful and harmless using LLM-based edits.
2503.04150_2268462_2	This paper proposes a methodology named "Ticktack" for addressing the LLM's long-time span misalignment in a yearly setting.
2503.04343_2268655_0	  While XAI focuses on providing AI explanations to humans, can the reverse - humans explaining their judgments to AI - foster richer, synergistic human-AI systems?
2503.05748_2270060_3	This paper traces the historical, philosophical, and technical evolution of these concepts, emphasizing how their definitions influence AI development, deployment, and oversight.   
2503.05787_2270099_1	It builds upon existing EU mechanisms for product health and safety regulation, but extends it to protect fundamental rights and by addressing AI as a horizontal technology that is regulated across multiple vertical application sectors.
2503.06139_2270451_4	We encourage LLMs to think in reverse by prompting LLMs to identify the worse response.
2503.06424_2270736_3	We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice.
2503.06803_2271115_1	This has been addressed with explainable AI, the interpretability arising from users' domain expertise, or collaborating with AI in a stable environment.
2503.07279_2271591_0	  Trust plays a fundamental role in shaping the willingness of users to engage and collaborate with artificial intelligence (AI) systems.
2503.07320_2271632_6	The study underscores the importance of understanding human biases toward AI agents and how observed behaviors can influence future human-AI cooperation dynamics.
2503.08182_2272494_5	We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin.
2503.09032_2273344_3	To that end, we ask, "can prompting help us teach LLMs how to learn".
2503.09501_2273813_3	To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking.
2503.10367_2274679_2	To boost the performance of private SLMs, this paper proposes to ask general LLMs for help.
2503.10887_2275199_5	Local Interpretable Model Agnostic Explanations identified the key imaging features influencing the model, ensuring transparency and clinical relevance.
2503.10965_2275277_2	Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors.
2503.11082_2275394_2	Yet, it remains unclear to what extent these buggy instances influence LLMs' performance when tackling bug-prone code completion tasks.
2503.12080_2276392_7	Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs.
2503.13419_2277731_1	While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks.
2503.13510_2277822_5	Our findings reveal that prompt sentiment significantly influences model responses, with negative prompts often reducing factual accuracy and amplifying bias, while positive prompts tend to increase verbosity and sentiment propagation.
2503.14749_2279061_5	We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences.
2503.15678_2279990_7	Existing legal frameworks are evolving to address AI in cybercrime, but quantum threats require new initiatives.
2503.15772_2280084_3	In this work, we employ a straightforward approach to identify LLM-generated reviews - doing an indirect prompt injection via the paper PDF to ask the LLM to embed a watermark.
2503.17040_2281352_5	We advocate for instructors to encourage AI-assisted CPS to foster critical thinking and enhance student engagement with real-world scenarios.
2503.17662_2281974_4	Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency.
2503.17994_2282306_6	Instead of directly generate architectures with LLM, We inspire the LLM's capability with a multi-level enhancement mechanism.
2503.17994_2282306_7	Specifically, on the step-level, we decompose the generation task into decision steps with powerful prompt engineering and inspire LLM to serve as instructor for architecture search based on its internal knowledge.
2503.18238_2282550_3	Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing.
2503.18809_2283121_4	For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one.
2503.19092_2283404_2	This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges.
2503.20797_2285109_5	Finally, we show how providing the source for political and non-political content influences the LLM's classification.
2503.21115_2285427_6	In addition, we design prompts that instruct LLMs to leverage these tools for assessing the feasibility of hub selection by evaluating various risk types and levels.
2503.21419_2285731_4	In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances.
2503.22736_2287048_4	With this in mind, we propose a model distillation pipeline in which a large generative model, a Teacher, teaches a much smaller model, a Student.
2503.22853_2287165_7	This research contributes to the broader understanding of teaching LLMs and has applications for educators, students, and developers of AI music tools alike.
2503.23084_2287396_3	These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks.
2503.23688_2288000_7	Furthermore, the research highlights specific prompt structures and linguistic variations that can strategically trigger distinct responses from models, revealing methods for effectively navigating and influencing LLM outputs.
2504.01404_2290107_11	In rank-based identification, we ask the LLM to select buggy statements from the bug-fixing commit and rank them based on their relevance to the root cause.
2504.02406_2291109_3	Trustworthy AI is vital, especially for critical infrastructures like 6G. This paper introduces the REASON approach for holistically addressing AI's native integration and trustworthiness in future 6G networks.
2504.03708_2292411_2	Telecommunications operators, historically adept at solving content latency challenges through partnerships with providers like Google and Facebook, now have a unique opportunity to address similar AI latency concerns.
2504.04262_2292965_5	The proposed CatBoost model has used a nature inspired algorithm such as Simulated Annealing to select the most important features, Cuckoo Search to adjust outliers and grid search to fine tune its settings in such a way to achieve improved prediction accuracy.
2504.04372_2293075_10	We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization.
2504.04994_2293697_5	Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making.
2504.05804_2294507_2	Unlike existing methods that often introduce detectable anomalies, StealthRank employs an energy-based optimization framework combined with Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text sequences embedded within product descriptions that subtly yet effectively influence LLM ranking mechanisms.
2504.05857_2294560_5	Our results address human-AI interaction challenges not covered in previous WoZ research, including recording and resubmitting signs, unpredictable outputs, system latency, and privacy concerns.
2504.06017_2294720_4	Based on our results, we argue against LLM-vendor claims about limited security capabilities.
2504.06435_2295138_11	These findings suggest directions for GenAI design to safely and productively address the AI "trust gap."
2504.07135_2295838_4	In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions.
2504.09309_2298012_3	We reframe the multi-label classification task as a structured generation problem, instructing the LLM to directly output the relevant legal categories for a given document.
2504.09647_2298350_2	This paper systematically identifies and categorizes critical attributes influencing AI service orchestration in 6G networks and introduces an open-source, LLM-assisted toolchain that automates service packaging, deployment, and runtime profiling.
2504.11536_2300239_1	To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback.
2504.12523_2301226_5	Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference.
2504.12654_2301357_0	  This perspective paper examines a fundamental paradox in the relationship between professional expertise and artificial intelligence: as domain experts increasingly collaborate with AI systems by externalizing their implicit knowledge, they potentially accelerate the automation of their own expertise.
2504.13052_2301755_3	We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs.
2504.13500_2302203_5	Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs.
2504.13534_2302237_1	To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor.
2504.13700_2302403_2	However, instructing LLMs using natural language is limited in precision and expressiveness for conveying visualization intent, leading to misinterpretation and time-consuming iterations.
2504.13969_2302672_4	This work highlights the potential of combining physical and digital elements in AI literacy, offering a safe and engaging way for children to learn how to effectively collaborate with AI.
2504.14112_2302815_7	Lastly, the AU group expressed higher comfort in seeking personal help, managing stress, obtaining social support, and talking about health with AI, indicating potential for broader emotional support while highlighting the need for safeguards against problematic usage.
2504.14150_2302853_5	Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model.
2504.14150_2302853_10	On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.
2504.14367_2303070_3	By systematically mapping the phenotypic space, we reveal how structural variations influence LLM performance, offering actionable insights for task-specific and adaptable prompt design.
2504.15415_2304118_4	Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution.
2504.16047_2304750_6	The findings highlight that pre-training methodology significantly influences model performance on specific downstream tasks.
2504.17426_2306129_3	Our method consists in applying topic modeling on the descriptions obtained by asking an LLM to summarize the code.
2504.19005_2307708_5	Using the data, the researcher went through prompt engineering to instruct gpt-4.1-mini to automatically analyze the remaining 1,000 images.
2504.19076_2307779_3	This paper examines scenarios where LLM-evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation.
2504.19445_2308148_2	In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments.
2504.20951_2309654_3	This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity.
2504.21589_2310292_1	Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records.
2504.21770_2310473_6	In-context learning and asking the model to 'think again' improves LASHED's precision.
2505.00626_2311186_2	Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers.
2505.00626_2311186_3	In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens.
2505.02062_2312622_0	  The adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration.
2505.02252_2312812_6	Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area.
2505.02313_2312873_3	Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering.
2505.02865_2313425_3	Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts.
2505.03818_2314378_1	Finding training examples to teach LLMs to solve these tasks can be challenging.   
2505.05115_2315675_1	(2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model -- a constant rate of failing during each minute a human would take to do the task.
2505.05441_2316001_2	To address this, we introduce GesPrompt, a multimodal XR interface that combines co-speech gestures with speech, allowing end-users to communicate more naturally and accurately with LLM-based copilots in XR environments.
astro-ph/0101048_2323998_2	The range of kHz frequencies and the observed quality factors (Qs) are also explained by this simple dynamical model.
astro-ph/0207575_2334530_5	The spectrum of the accretion disk emission is significantly softer and in the 3--20 keV range is reasonably well described by a relativistic disk model with a mass accretion rate consistent with the value inferred from the observed X-ray flux.
astro-ph/0301443_2338226_3	We find that both spectra are described by a two-component model consisting of emission from a cool accretion disk plus a Comptonized blackbody with kTbb ~ 1.5 keV in a low opacity plasma.
astro-ph/0301443_2338226_5	The spectrum of X1556-605 can also be described by a model consisting of a blackbody plus an unsaturated Comptonization with electron energy kTe ~ 4 keV.
astro-ph/0402179_2346505_0	  We describe kHz QPOs from the hydrodynamical model of accretion disks for LMXB systems.
astro-ph/0608191_2367880_3	The source spectrum is best explained by a power-law model with a photon index of 2.2 (1.9-2.7) and an absorption-corrected luminosity of 1.0 x 10^34 ergs s^-1 in the 0.5--10 keV band for a distance of 50 kpc.
astro-ph/9611086_2379692_4	It is found that the entire variety of young LMC globular clusters may be explained in a model where they form from a fairly uniform population of roughly spherical, relaxed proto-cluster clouds very similar to Giant Molecular Clouds in the Galaxy, with star formation efficiencies between 25% and 60%.
astro-ph/9802342_2384325_1	All the remnants were well described by the model, allowing us to derive accurate values for their ages, densities, initial explosion energies, and metal abundances.
hep-th/0508177_2587329_3	Their low energy world-volume theory, truncated to the 1/2 BPS sector, is shown to be described by a Chern-Simons finite-matrix model.
