1_acl_131_34995_1	We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.	LLMs	LLMs	LLM	determine	p3
1_acl_7_26333_1	In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.	the model	model	model	understand	p2
1_acl_1_13930_4	‚Ä¢ Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?	LMs	LMs	LM	recognize	p2
1_acl_335_27056_0	The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.	LMs	LMs	LM	think	p2
1_acl_52_19823_2	We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.	LMs	LMs	LM	infer	p2
1_acl_615_27336_1	These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.	an LM	LM	LM	resolve a conflict	p2
1_acl_12_42096_5	We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.	LLMs	LLMs	LLM	learn	p1
1_acl_508_29544_3	Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.	LLMs	LLMs	LLM	collaborate	p2
1_acl_679_35529_6	4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts.	LLMs	LLMs	LLM	exhibit overconfidence	p3
1_acl_102_12912_7	Finally, as it is hard to tell given a privacy parameter ùúñ what was the effect on the trained representation, we present experiments showing that the trained model does not memorize private information.	the trained model	model	model	memorize	p2
1_acl_263_41409_3	That is, the LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests.	LLM	LLM	LLM	remember	p2
1_acl_1011_30047_1	However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses.	the instruction-tuned model	instruction-tuned model	model	see	p2
1_acl_392_27113_5	During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it.	the LLM	LLM	LLM	recall	p2
1_acl_682_37442_3	Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM.	LLMs	LLMs	LLM	have awareness	p2
1_arx_2304.10149_1828195_10	And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results.	ChatGPT	ChatGPT	ChatGPT	understand	p2
1_arx_2212.13371_1769548_6	In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.	the AI agent	AI agent	agent	decide	p1
1_arx_1905.04127_1122563_1	Traditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy.	AI agents	AI agents	agent	suffer	p3
1_arx_2109.14723_1537544_1	As a result, it can be hard to identify what the model actually "believes" about the world, making it susceptible to inconsistent behavior and simple errors.	the model	model	model	believe	p2
1_arx_2210.01478_1722833_1	In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions.	AI systems	AI systems	system	understand, intepret and predict	p2
1_arx_2212.01681_1757858_6	I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.	LMs	LMs	LM	infer	p2
1_arx_2303.17557_1817354_1	What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?	a model	model	model	remember	p2
1_arx_1810.06338_1037525_0	In order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.	an AI system	AI system	system	try to achieve	p2
1_arx_2210.09492_1730847_6	Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items.	GPT-3	GPT-3	GPT-3	reason	p2
1_arx_2302.04761_1789695_2	In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.	LMs	LMs	LM	teach	p2
1_arx_2212.09561_1765738_6	By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.	LLM	LLM	LLM	deduce	p2
1_acl_322_28458_1	To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.	an LLM	LLM	LLM	prefer	p2
1_acl_378_29414_7	In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question.	the model	model	model	ask	p2
1_arx_2304.05376_1823422_8	Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.	GPT-4	GPT-4	GPT-4	distinguish	p3
1_arx_2212.02911_1759088_4	Our evaluation shows that the model can create French poetry successfully.	the model	model	model	create	p3
1_arx_2303.18027_1817824_5	First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.	LLMs	LLMs	LLM	select	p2
1_arx_2304.09048_1827094_1	However, a large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.	large generative language model	large generative language model	model	demonstrate	p1
1_acl_1_13930_5	‚Ä¢ Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?	LMs	LMs	LM	know	p2
1_acl_299_27020_1	However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.	LMs	LMs	LM	perform,cheat	p2
1_acl_242_32404_6	We only conduct retrieval for the missing knowledge in questions that the LLM does not know.	the LLM	LLM	LLM	know	p2
1_acl_497_32659_4	BinLLM converts collaborative embeddings from external models into binary sequences ‚Äî a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs.	LLMs	LLMs	LLM	understand	p2
1_arx_2012.11976_1399854_6	Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs.	a conversational AI development platform	conversational AI development platform	platform	exhibit	p3
1_acl_22_34383_5	Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.	the more general LLM-based expert	LLM-based expert	expert	analyze,select	p2
1_arx_2107.04022_1498034_6	To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons.	AI agents	AI agents	agent	break rules	p2
1_acl_519_45829_8	These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.	LLMs	LLMs	LLM	autonomously develop	p1
1_acl_131_34995_2	Our experiments show that without external feedback, many LLMs struggle to recognize their need for user support.	LLMs	LLMs	LLM	struggle	p2
1_arx_1904.08530_1113570_2	We cross-check with control cases to ensure that the AI is not randomly guessing and is indeed identifying an inherent structure.	the AI	AI	AI	guess,identify	p2
1_arx_2304.04966_1823012_7	Resultantly, the developed model efficiently analyzed the test data with a mean average precision of 0.89.	the developed model	model	model	analyze	p2
1_acl_296_12174_4	By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB.	the model	model	model	learn	p3
1_arx_2304.02868_1820914_3	Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.	ChatGPT	ChatGPT	ChatGPT	construct	p2
1_acl_518_29554_3	Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning.	LLMs	LLMs	LLM	struggle	p2
1_arx_2303.08014_1807811_6	In addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible sentences that were likely to have been corrupted by noise, drew reasonable inferences, and overlooked semantic fallacies in a sentence.	ChatGPT	ChatGPT	ChatGPT	interpret,draw an inference,overlook	p2
1_arx_2303.09461_1809258_3	We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points.	ChatGPT	ChatGPT	ChatGPT	pass an exam	p2
1_acl_57_25373_2	By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor.	AI	AI	AI	determine,provide feedback and insights	p2
1_acl_212_35072_9	When an LLM encounters questions outside its domain, the system recognizes its knowledge scope and determines whether it can answer the question independently.	the system	system	system	recognize,determine	p2
1_acl_1477_40886_4	The experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably in specific-domain conversations.	ChatGPT	ChatGPT	ChatGPT	demonstrate proficiency,struggle	p3