acl_153_4884_5	By combining relation prediction and relevance ranking tasks with our target link prediction, the proposed model can learn more relational properties in KGs and properly perform even when lexical similarity occurs.	the proposed model	model	learn
acl_261_9142_1	Our proposed model learns to extract textual features using a BiGRU-based deep neural network supported by a Hierarchical Attention architecture to focus on the most relevant areas in the text.	Our proposed model	model	learn
acl_131_34995_1	We propose metrics to evaluate the trade-off between performance improvements and user burden, and investigate whether LLMs can determine when to request help under varying information availability.	LLMs	LLM	determine
acl_7_26333_1	In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task.	the model	model	understand
acl_1_13930_4	‚Ä¢ Do LMs recognize when words are used nonliterally in non-compositional MWEs (e.g. do they know whether there are fleas in the flea market)?	LMs	LM	recognize
acl_482_41628_7	We find that certain commercial LLMs could surprisingly guess the missing option in various test sets.	certain commercial LLMs	LLM	guess
acl_335_27056_0	The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.	LMs	LM	think
acl_163_34648_5	Our evaluation on both automated metrics and qualitative human evaluation suggests that by incorporating end-user specifications into the conversion process, our model can create presentations that are not only informative but also tailored to expectations and cognitive abilities of target audience.	our model	model	create
acl_52_19823_2	We propose a framework to analyze what LMs can infer about new entities that did not exist when the LMs were pretrained.	LMs	LM	infer
acl_615_27336_1	These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict.	LM	LM	resolve a conflict
acl_22_31716_3	Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics.	LLMs	LLM	learn
acl_12_42096_5	We then propose a system that leverages the recently introduced social learning paradigm in which LLMs collaboratively learn from each other by exchanging natural language.	LLMs	LLM	learn
acl_508_29544_3	Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs.	LLMs	LLM	collaborate
acl_679_35529_6	4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts. LLMs LLM exhibit overconfidence
acl_102_12912_7	Finally, as it is hard to tell given a privacy parameter ùúñ what was the effect on the trained representation, we present experiments showing that the trained model does not memorize private information. the trained model model memorize
263_41409_3	That is, LLM only remembers the answer style for open-ended safety questions, which makes it unable to solve other forms of safety tests. LLM LLM remember
acl_1011_30047_1	However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. the instruction-tuned model model see
acl_296_12174_4	By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. the model model learn
acl_11_43395_1	For example, a model should correctly identify kimchi (Korean food) in an image both when an Asian woman is eating it, as well as an African man is eating it. a model model identify
acl_392_27113_5	During the test stage, given a test question, the LLM recalls relevant memory to help itself reason and answer it.	the LLM	LLM	recall
acl_507_27228_1	However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks or performing complex math or logical reasoning.	LLMs	LLMs	struggle
acl_682_37442_3	Motivated by this, we propose that LLMs have an inherent awareness from where the text was copied, likely captured in the hidden states of the LLM.	LLMs	LLM	have awareness
arx_2304.10149_1828195_10	And the human evaluations show ChatGPT can truly understand the provided information and generate clearer and more reasonable results.	ChatGPT	ChatGPT	understand
arx_2212.13371_1769548_6	In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions.	the AI agent	agent	decide
arx_2304.09337_1827383_2	It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user's intention.	the model	model	interpret
arx_1905.04127_1122563_1	Traditionally, AI agents have suffered from difficulties in using only sensory inputs to obtain a good representation of their environment and then mapping this representation to an efficient control policy.	AI agents	agent	suffer
arx_2212.10511_1766688_2	We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail.	LMs	LM	struggle
arx_2109.14723_1537544_1	As a result, it can be hard to identify what the model actually "believes" about the world, making it susceptible to inconsistent behavior and simple errors.	the model	model	believe
arx_2207.14382_1690280_4	However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence.	these LLMs	LLM	understand
arx_2210.01478_1722833_1	In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions.	AI systems	system	be able to understand, intepret and predict
arx_2212.01681_1757858_6	I survey findings from the recent literature showing that -- even in today's non-robust and error-prone models -- LMs infer and use representations of fine-grained communicative intentions and more abstract beliefs and goals.	LMs	LM	infer
arx_2303.17557_1817354_1	What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples?	a model	model	remember
arx_1810.06338_1037525_0	  In order to engender trust in AI, humans must understand what an AI system is trying to achieve, and why.	  an AI system	  system	  try to achieve
arx_2012.11995_1399873_3	Our results also show that pre-training on structured data does not always make the model acquire ability that can be transferred to natural language downstream tasks.	a model	model	acquire
arx_2304.10592_1828638_0	  The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images.	  The recent GPT-4	  GPT-4	  demonstrate
arx_2207.08333_1684231_3	We observed that the VL model does not interpret the overall context of an input image but instead shows biases toward a specific object or shape that forms the local context.	the VL model	model	interpret
arx_2210.09492_1730847_6	Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items.	GPT-3	GPT-3	reason
arx_2302.04761_1789695_2	In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds.	LMs	LM	teach
acl_282_44833_0	Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities.	new models	model	claim
acl_280_10245_4	We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters.	off-the-shelf language models	model	identify,struggle
arx_2212.09561_1765738_6	By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score.	LLM	LLM	deduce
acl_322_28458_1	To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization.	an LLM	LLM	prefer
acl_378_29414_7	In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question.	the model	model	ask
arx_2304.05376_1823422_8	Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance.	GPT-4	GPT-4	distinguish
arx_2212.02911_1759088_4	Our evaluation shows that the model can create French poetry successfully.	the model	model	create
arx_2304.02868_1820914_3	Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses.	ChatGPT	ChatGPT	construct
arx_2304.09337_1827383_2	It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user's intention.	the model	model	interpret
arx_2303.18027_1817824_5	First, LLMs sometimes select prohibited choices that should be strictly avoided in medical practice in Japan, such as suggesting euthanasia.	LLMs	LLM	select
arx_2304.09048_1827094_1	However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks.	large generative language model	model	demonstrate
acl_1_13930_5	‚Ä¢ Do LMs know idioms, and can they infer the meaning of new idioms from the context as humans often do?	LMs	LM	know,infer
acl_34_15815_4	As the training stages go on, we make the system learn to solve multiple tasks by adding extra information at different training stages gradually.	the system	system	learn
acl_78_25975_8	The semantic dependency feature serves as a global signal and helps the model learn simile knowledge that can be applied to unseen domains.	the model	model	learn
acl_299_27020_1	However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism.	LMs	LM	perform
acl_242_32404_6	We only conduct retrieval for the missing knowledge in questions that the LLM does not know.	the LLM	LLM	know
acl_497_32659_4	BinLLM converts collaborative embeddings from external models into binary sequences ‚Äî a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs.	LLMs	LLM	understand
2012.11976_1399854_6	Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs.	a conversational AI development platform	platform	exhibit
22_34383_5	Then, the more general LLM-based expert, through prompting techniques, analyzes the nuanced differences between candidate categories and selects the most suitable target category.	the more general LLM-based expert	expert	analyze,select
2107.04022_1498034_6	To understand when AI agents need to break rules, we examine the conditions under which humans break rules for pro-social reasons.	AI agents	agent	break rules
519_45829_8	These results suggest that LLMs can autonomously develop effective model-improvement techniques beyond human intuition.	LLMs	LLM	develop
4_arx_2204.07644_1637782_4	The results also demonstrate that users perceive co-creative AI as more reliable, personal and intelligent when it can communicate with the users.	co-creative AI	AI	reliable,personal and intelligent