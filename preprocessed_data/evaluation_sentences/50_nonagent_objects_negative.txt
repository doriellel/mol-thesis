arx_1712.09783_928382_2	In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.	the MoE model	model	train
arx_2003.13027_1263382_6	We additionally train our model on the SwissText dataset to demonstrate usability on German.	our model	model	train
acl_6_25720_4	We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.	LLMs	LLM	look at
arx_2101.04617_1408359_3	We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.	the trained model	model	employ
arx_2101.05967_1409709_1	Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.	AI	AI	deploy
acl_3_2185_6	Instead of replacing the conventional n-gram-based language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way.	the RBM-based language model	model	train
acL_26_25595_4	Another way is to use unlabeled domain-specific data to pre-train these transformer model and then fine-tune this model on labeled data.	this model	model	fine-tune
acl_3_46388_0	Recently, many works have been attempting to adapt Large Language Models (LLMs) for sentence embedding, with most of them fine-tuning LLMs towards the contrastive objective and enabling bi-directional attention for better performance, using LoRA to address the large model scale.	LLMs	LLM	fine-tune
acl_118_9850_4	We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.	the model	fine-tune
arx_2204.13828_1643966_2	We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments.	a conceptual model called MATCH	model	develop
arx_1912.03652_1215548_6	To create examples that serve as demonstrations for humans to improve, we develop a model based on a conditional convolutional autoencoder (CCAE).	a model	 model	develop
arx_2205.00176_1644588_2	However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety.	a chat system	system	build
acl_1_41051_1	However, challenges remain in developing effective chatbots, particularly in addressing LLMs’ lack of “statefulness”.	effective chatbots	chatbot	develop
arx_2205.10957_1655369_0	  In this paper, we develop an impairments-aware air-to-ground unified channel model that incorporates the effect of both wobbling and hardware impairments, where the former is caused by random physical fluctuations of unmanned aerial vehicles (UAVs), and the latter by intrinsic radio frequency (RF) nonidealities at both the transmitter and receiver, such as phase noise, in-phase/quadrature (I/Q) imbalance, and power amplifier (PA) nonlinearity.	  an impairments-aware air-to-ground unified channel model	  model	  develop
arx_2205.13770_1658182_1	In this paper, we design an edge-based energy-aware MAR system that enables MAR devices to dynamically change their configurations, such as CPU frequency, computation model size, and image offloading frequency based on user preferences, camera sampling rates, and available radio resources.	  an edge-based energy-aware MAR system	  system	  design
arx_1211.2736_384982_3	In an effort to construct an intelligent computer system, a primary consideration is to represent large amounts of knowledge in a way that allows effective use and efficiently organizing information to facilitate making the recommended inferences.	an intelligent computer system	system	construct
arx_2306.02920_1856068_2	Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.	bilingual LMs	LM	train
acl_87_55519_0	This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model.	a model	model	present
acl_182_45507_6	We evaluate seven LLMs on the two datasets using various prompts.	seven LLMs	LLM	evaluate
acl_3_2185_5	We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.	the RBM-based language model	model	evaluate
acl_5_45253_1	Developing an effective Bahnaric-Vietnamese translation system is essential for fostering linguistic exchange, preserving cultural heritage, and empowering local communities by bridging communication barriers.	an effective Bahnaric-Vietnamese translation system	system	develop
arx_1911.03597_1202191_5	Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.	the model	model	pre-train
acl_329_43922_3	We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.	a model	model	develop
acl_131_44712_1	While research has largely focused on developing specialized graph LLMs through task-specific instruction tuning, a comprehensive benchmark for evaluating LLMs solely through prompt design remains surprisingly absent.	specialized graph LLMs	LLM	develop
acl_418_44990_5	These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.	AI	AI	develop
acl_92_4559_4	To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT, specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset.	the cross-lingual pre-training model	model	fine-tune
acl_42_15944_1	Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language.	a model	model	fine-tune
arx_2302.07257_1792191_6	The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.	a more user-friendly and understandable system	system	create
arx_301_32463_8	Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.	eight mainstream LLMs	LLM	evaluate and analyze
acl_5_42629_6	We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.	recent multilingual LLMs	LLM	evaluate
acl_28_45074_3	We design a system using these pre-trained models to answer questions, based on the given context.	a system	system	design
acl_21_45115_0	This paper presents a system developed for Task 1 of the COLING 2025 Workshop on Detecting AI-Generated Content, focusing on the binary classification of machine-generated versus human-written text.	a system	system	present
acl_41_45371_2	Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.	an individual model	model	construct
acl_104_45430_3	We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.	a search algorithm	algorithm	develop
acl_16_45239_1	We evaluate five LLMs, and investigate whether retrieval-augmented generation (RAG) with historical encyclopedic knowledge enhances results.	five LLMs	LLM	evaluate
acl_6_46417_4	Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.	a BERT-based model	model	train and present
acl_15_54341_0	This paper presents a novel, data-driven language model that produces entire lyrics for a given input melody.	a novel, data-driven language model	model	produce
acl_112_54438_2	In this paper we present the Pivot Based Language Model (PBLM), a representation learning model that marries together pivot-based and NN modeling in a structure aware manner.	the Pivot Based Language Model (PBLM)	model	present
acl_45_45963_3	Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.	a router model	model	train
acl_7_60414_2	In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.	recurrent LMs	LM	train
acl_27_46301_5	To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.	GPT-2 models	model	train
acl_116_43683_3	To that end, we propose to newly train a small backward LM and concatenate its representations to those of an existing LM for downstream tasks.	a small backward LM	LM	train
arx_2002.12804_1250227_0	  We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM).	  a unified language model	  model	  pre-train
acl_647_53456_4	We compare language models trained on different subsets of the resource with those trained on the Catalan Wikipedia and the target side of the parallel data used to train the SMT system.	language models	model	compare
acl_278_43871_2	On the other hand, developing LLMs for Thai should also include enhancing the cultural understanding as well as core capabilities.	LLMs	LLM	develop
arx_1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.	a fast and compact model	model	train
acl_1_7090_2	Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.	the seq2seq model	model	build
acl_25_25341_4	Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model.	a single application-agnostic model	model	build
acl_928_27649_2	Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.	the model	model	bias
acl_582_29618_3	In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.	a recurrent neutral language model	model	develop
\\
\\ inconclusive 
acl_696_6286_1	How should we train a language model in this scenario?	a language model	model	train
arx_2309.11000_1915804_0	  This paper explores the potential of constructing an AI spoken dialogue system that "thinks how to respond" and "thinks how to speak" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules.	  an AI spoken dialogue system	  system	  construct
acl_27_42781_1	A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output.	LLMs	LLM	correct
acl_27_42781_3	This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches.	LLMs	LLM	correct
acl_814_27535_2	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.	an agent	agent	train
2305.01937_1834993_4	We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.	the LLMs	LLM	present
2306.03856_1857004_0	  We propose iteratively prompting a large language model to self-correct a translation, with inspiration from their strong language understanding and translation capability as well as a human-like translation approach.	  a large language model	  model	  prompt