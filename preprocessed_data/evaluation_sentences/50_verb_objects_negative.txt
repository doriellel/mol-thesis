2_arx_1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.	pre-trained masked language models	pre-trained masked language models	model	provide	n1
2_arx_1911.01387_1199981_11	In this work, nine open-source projects are employed to evaluate our proposed model on the features extracted by previous researchers and identify the actionable warnings in a priority order given by our algorithm.	our algorithm	algorithm	algorithm	give	n1
2_arx_2006.05347_1299861_5	As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a --low complexity algorithm.	a low complexity algorithm.	low complexity algorithm.	algorithm	address	n2
2_arx_2007.00900_1312414_5	We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.	the BERT language model	BERT language model	model	power	n1
2_acl_7_34206_4	A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.	GPT-4	GPT-4	GPT-4	reproduce	n2
2_arx_2103.07820_1437965_2	We consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.	  the onboard DAA system	  DAA system	  system	  control	n1
2_arx_1811.12185_1056818_9	We observed 95.61% alarms raised by the said system are taken care of by the operator.	the said system	system	system	raise alarm	n2
2_arx_1909.09993_1180031_2	Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.	the A2W model	A2W model	model	cover	n1
2_arx_1912.06835_1218731_4	By comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.	the ion flow model	ion flow model	model	predict	n3
2_arx_2102.07384_1423873_5	The two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.	the BCD algorithm	BCD algorithm	algorithm	generate	n3
2_arx_2202.12803_1611627_3	The controller exploits a low-order rate-based linear parameter-varying (LPV) model for prediction which is identified from transient response data generated by the GT-Power model.	the GT-Power model	GT-Power model	model	generate	n3
2_arx_2205.05016_1649428_3	The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.	the K-means algorithm	K-means algorithm	algorithm	cluster	n1
2_acl_280_28416_2	For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?	the debiasing method	debiasing method	method	use	n1
2_acl_4_33862_8	Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.	a flexible form-to-meaning mapping system	form-to-meaning mapping system	system	support	n1
2_acl_6_42908_8	We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.	LLMs	LLMs	LLM	generate	n3
2_acl_377_35235_0	Significant advancements have recently been made in large language models, represented by GPT models.	GPT models	GPT models	model	represent	n2
2_acl_4_39247_3	Additionally, it is presented a Knowledge Graph Q&A System powered by Generative AI.	Generative AI	Generative AI	AI	power	n1
2_arx_1901.05719_1075266_4	  Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.	AI algorithms	AI algorithms	algorithm	realize	n1
2_arx_1902.02508_1083592_0	The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.	a SGS model	SGS model	model	capture	n1
2_arx_2002.05702_1243125_4	CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.	a generative model of synthetic structures	generative model of synthetic structures	model	create	n3  
2_arx_2009.12437_1353940_2	Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.	a model	model	model	predict	n3
2_acl_2010.12858_1369188_1	Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available.	large-scale multilingual language model	language model	model	cover	n1
2_arx_2106.02498_1480262_5	Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.	an AI system	AI system	system	meet	n2
2_arx_2111.01122_1555225_0	There is a growing consensus in HCI and AI research that the design of AI systems needs to engage and empower stakeholders who will be affected by AI.	AI	AI	AI	affect	n1
2_arx_1212.5593_395209_0	Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.	a linear time varying model	linear time varying model	model	describe	n2
2_arx_1401.5941_495172_4	Then, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.	a backpropagation algorithm (MLP-BP)	backpropagation algorithm (MLP-BP)	algorithm	train	n3
2_arx_1408.5886_551192_2	Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.	a model	model	model	describe	n3
2_arx_1501.07576_594324_2	In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.	a three-dimensional dynamic point-mass model	three-dimensional dynamic point-mass model	model	describe	n2
2_acl_1_42012_5	We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.	an LLM	LLM	LLM	generate	n3
2_arx_1610.02937_778284_8	The synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.	the model	model	model	predict	n3
2_arx_2301.04655_1775719_4	This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.	generative AI	generative AI	AI	affect	n1
2_arx_2108.01608_1510423_6	Initially, an Integer Linear Program (ILP) formulation is presented, that is solved offline and optimally, followed by a near-optimal algorithm, that solves the problem incrementally, one AV at a time, to address scalability issues, allowing scheduling in problems involving large numbers of locations, AVs, and customer requests.	a near-optimal algorithm	algorithm	algorithm	follow	n1
2_arx_2002.07063_1244486_6	Specifically, the optimal TPC matrix and AN covariance matrix are derived by Lagrangian multiplier method, and the optimal phase shifts are obtained by Majorization-Minimization (MM) algorithm.	Majorization-Minimization (MM) algorithm	Majorization-Minimization (MM) algorithm	algorithm	obtain	n1
2_arx_2304.11116_1829162_4	Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.	the latest ChatGPT and Toolformer models	ChatGPT and Toolformer models	model	inspire	n2
2_arx_2010.01869_1358199_0	In this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems.	a Neural Language Model (NLM)	Neural Language Model	(NLM)	model	learn	n3
2_arx_2110.14419_1552562_1	Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.	AI	AI	AI	shape,influence	n3
2_arx_2207.00691_1676589_8	The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.	language-and-image AI	AI	AI	learn	n3
2_arx_1812.01714_1059288_3	Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.	our model	model	model	learn	n3
2_arx_1907.12932_1157468_4	We demonstrate that qualitatively similar behaviour to the experiments is exhibited by a previously established, depth-averaged mathematical model; a consequence of the model's intricate solution structure.	a previously established, depth-averaged mathematical model	mathematical model	model	exhibit behavior	n3
2_arx_2412.12865_2215243_5	This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.	the aligned LLMs	LLMs	LLM	predict	n3
2_arx_2112.05675_1575451_0	Various tools and practices have been developed to support practitioners in identifying, assessing, and mitigating fairness-related harms caused by AI systems.	AI systems	AI systems	system	cause harm	n1
2_acl_19_45086_1	Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.	an LLM	LLM	LLM	generate	n3
2_arx_0906.5497_132048_3	We find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.	a model	model	model	explain	n2
2_acl_126_36901_4	It is determined by counting the revision edits generated by LLMs.	LLMs	LLMs	LLM	generate	n3
2_arx_2002.10965_1248388_3	For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.	a low-complexity trellis-based algorithm	low-complexity trellis-based algorithm	algorithm	solve	n2
2_arx_1910.12583_1196502_2	Solidarity as an AI principle (1) shares the prosperity created by AI, implementing mechanisms to redistribute the augmentation of productivity for all; and shares the burdens, making sure that AI does not increase inequality and no human is left behind.	AI	AI	AI	create	n3
2_arx_1809.04258_1024274_3	The results preliminarily reveal that it is a relationship between the ontology-based attributions and the corresponding predicted indicator that can be learnt by AI for predicting the SE, which suggests the proposed model has a potential in AI-assisted SE prediction.	AI	AI	AI	learn	n3
2_acl_750_35597_0	We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).	ChatGPT	ChatGPT	ChatGPT	exhibit bias	n3



arx_1712.09783_928382_2	In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.	the MoE model	model	train
arx_2003.13027_1263382_6	We additionally train our model on the SwissText dataset to demonstrate usability on German.	our model	model	train
acl_6_25720_4	We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.	LLMs	LLM	look at
arx_2101.04617_1408359_3	We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.	the trained model	model	employ
arx_2101.05967_1409709_1	Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.	AI	AI	deploy
acl_3_2185_6	Instead of replacing the conventional n-gram-based language model, we trained the RBM-based language model on the more important but smaller in-domain data and combined them in a log-linear way.	the RBM-based language model	model	train
acL_26_25595_4	Another way is to use unlabeled domain-specific data to pre-train these transformer model and then fine-tune this model on labeled data.	this model	model	fine-tune
acl_3_46388_0	Recently, many works have been attempting to adapt Large Language Models (LLMs) for sentence embedding, with most of them fine-tuning LLMs towards the contrastive objective and enabling bi-directional attention for better performance, using LoRA to address the large model scale.	LLMs	LLM	fine-tune
acl_118_9850_4	We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.	the model	fine-tune
arx_2204.13828_1643966_2	We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments.	a conceptual model called MATCH	model	develop
arx_1912.03652_1215548_6	To create examples that serve as demonstrations for humans to improve, we develop a model based on a conditional convolutional autoencoder (CCAE).	a model	 model	develop
arx_2205.00176_1644588_2	However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety.	a chat system	system	build
acl_1_41051_1	However, challenges remain in developing effective chatbots, particularly in addressing LLMs’ lack of “statefulness”.	effective chatbots	chatbot	develop
arx_2205.10957_1655369_0	  In this paper, we develop an impairments-aware air-to-ground unified channel model that incorporates the effect of both wobbling and hardware impairments, where the former is caused by random physical fluctuations of unmanned aerial vehicles (UAVs), and the latter by intrinsic radio frequency (RF) nonidealities at both the transmitter and receiver, such as phase noise, in-phase/quadrature (I/Q) imbalance, and power amplifier (PA) nonlinearity.	  an impairments-aware air-to-ground unified channel model	  model	  develop
arx_2205.13770_1658182_1	In this paper, we design an edge-based energy-aware MAR system that enables MAR devices to dynamically change their configurations, such as CPU frequency, computation model size, and image offloading frequency based on user preferences, camera sampling rates, and available radio resources.	  an edge-based energy-aware MAR system	  system	  design
arx_1211.2736_384982_3	In an effort to construct an intelligent computer system, a primary consideration is to represent large amounts of knowledge in a way that allows effective use and efficiently organizing information to facilitate making the recommended inferences.	an intelligent computer system	system	construct
arx_2306.02920_1856068_2	Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.	bilingual LMs	LM	train
acl_87_55519_0	This paper presents a model for disfluency detection in spontaneous speech transcripts called LSTM Noisy Channel Model.	a model	model	present
acl_182_45507_6	We evaluate seven LLMs on the two datasets using various prompts.	seven LLMs	LLM	evaluate
acl_3_2185_5	We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.	the RBM-based language model	model	evaluate
acl_5_45253_1	Developing an effective Bahnaric-Vietnamese translation system is essential for fostering linguistic exchange, preserving cultural heritage, and empowering local communities by bridging communication barriers.	an effective Bahnaric-Vietnamese translation system	system	develop
arx_1911.03597_1202191_5	Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.	the model	model	pre-train
acl_329_43922_3	We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.	a model	model	develop
acl_131_44712_1	While research has largely focused on developing specialized graph LLMs through task-specific instruction tuning, a comprehensive benchmark for evaluating LLMs solely through prompt design remains surprisingly absent.	specialized graph LLMs	LLM	develop
acl_418_44990_5	These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.	AI	AI	develop
acl_92_4559_4	To this end, we propose a low-resource agglutinative language model fine-tuning AgglutiFiT, specifically, we build a low-noise fine-tuning dataset by morphological analysis and stem extraction, then fine-tune the cross-lingual pre-training model on this dataset.	the cross-lingual pre-training model	model	fine-tune
acl_42_15944_1	Our approach assumes access to translations (exact or approximate) between source-target language pairs, where we fine-tune a model on source language task data and evaluate the model in the target language.	a model	model	fine-tune
arx_2302.07257_1792191_6	The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.	a more user-friendly and understandable system	system	create
arx_301_32463_8	Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.	eight mainstream LLMs	LLM	evaluate and analyze
acl_5_42629_6	We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.	recent multilingual LLMs	LLM	evaluate
acl_28_45074_3	We design a system using these pre-trained models to answer questions, based on the given context.	a system	system	design
acl_21_45115_0	This paper presents a system developed for Task 1 of the COLING 2025 Workshop on Detecting AI-Generated Content, focusing on the binary classification of machine-generated versus human-written text.	a system	system	present
acl_41_45371_2	Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.	an individual model	model	construct
acl_104_45430_3	We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.	a search algorithm	algorithm	develop
acl_16_45239_1	We evaluate five LLMs, and investigate whether retrieval-augmented generation (RAG) with historical encyclopedic knowledge enhances results.	five LLMs	LLM	evaluate
acl_6_46417_4	Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.	a BERT-based model	model	train and present
acl_15_54341_0	This paper presents a novel, data-driven language model that produces entire lyrics for a given input melody.	a novel, data-driven language model	model	produce
acl_112_54438_2	In this paper we present the Pivot Based Language Model (PBLM), a representation learning model that marries together pivot-based and NN modeling in a structure aware manner.	the Pivot Based Language Model (PBLM)	model	present
acl_45_45963_3	Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image.	a router model	model	train
acl_7_60414_2	In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.	recurrent LMs	LM	train
acl_27_46301_5	To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.	GPT-2 models	model	train
acl_116_43683_3	To that end, we propose to newly train a small backward LM and concatenate its representations to those of an existing LM for downstream tasks.	a small backward LM	LM	train
arx_2002.12804_1250227_0	  We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM).	  a unified language model	  model	  pre-train
acl_647_53456_4	We compare language models trained on different subsets of the resource with those trained on the Catalan Wikipedia and the target side of the parallel data used to train the SMT system.	language models	model	compare
acl_278_43871_2	On the other hand, developing LLMs for Thai should also include enhancing the cultural understanding as well as core capabilities.	LLMs	LLM	develop
arx_1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.	a fast and compact model	model	train
acl_1_7090_2	Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.	the seq2seq model	model	build
acl_25_25341_4	Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model.	a single application-agnostic model	model	build
acl_928_27649_2	Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.	the model	model	bias
acl_582_29618_3	In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.	a recurrent neutral language model	model	develop
\\
