2_arx_1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.	pre-trained masked language models	pre-trained masked language models	model	provide	n2
2_arx_2006.05347_1299861_5	As for the passive eavesdropping, an average secrecy rate maximization problem is formulated, which is addressed by a --low complexity algorithm.	a low complexity algorithm.	low complexity algorithm.	algorithm	address	n2
2_arx_2007.00900_1312414_5	We introduce an explainable VQA system that uses spatial and object features and is powered by the BERT language model.	the BERT language model	BERT language model	model	power	n1
2_acl_7_34206_4	A positive correlation of 0.40 was found between the emotion intensity scores reproduced by GPT-4 and those manually annotated by humans.	GPT-4	GPT-4	GPT-4	reproduce	n2
2_arx_2103.07820_1437965_2	We consider a UAS that can be fully controlled by the onboard DAA system and by a remote human pilot.	  the onboard DAA system	  DAA system	  system	  control	n1
2_arx_1811.12185_1056818_9	We observed 95.61% alarms raised by the said system are taken care of by the operator.	the said system	system	system	raise alarm	n2
2_arx_1909.09993_1180031_2	Moreover, the A2C model can be used to recover out-of-vocabulary (OOV) words that are not covered by the A2W model, but this requires accurate detection of OOV words.	the A2W model	A2W model	model	cover	n1
2_arx_1912.06835_1218731_4	By comparing the measurements with the results predicted by the ion flow model for negative corona discharge, it is found that the electric field at the conductor surface is proportional to the current density of the corona discharge with a negative constant of proportionality.	the ion flow model	ion flow model	model	predict	n3
2_arx_2102.07384_1423873_5	The two data-driven approaches are trained using data samples generated by the BCD algorithm via supervised learning.	the BCD algorithm	BCD algorithm	algorithm	generate	n3
2_arx_2205.05016_1649428_3	The fuzzy trajectory data are developed based on different driving styles, which are clustered by the K-means algorithm.	the K-means algorithm	K-means algorithm	algorithm	cluster	n1
2_acl_280_28416_2	For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?	the debiasing method	debiasing method	method	use	n1
2_acl_4_33862_8	Taken together, these findings support the idea that meaning construction is supported by a flexible form-to-meaning mapping system based on statistical regularities in the language environment that can accommodate novel lexical entries as soon as they are encountered.	a flexible form-to-meaning mapping system	form-to-meaning mapping system	system	support	n1
2_acl_6_42908_8	We find that synthetic data generated by LLMs is a promising avenue of research, but further research is needed to improve the quality of the generated data and develop better filtering methods.	LLMs	LLMs	LLM	generate	n3
2_acl_377_35235_0	Significant advancements have recently been made in large language models, represented by GPT models.	GPT models	GPT models	model	represent	n2
2_arx_1901.05719_1075266_4	  Specifically, we propose a constructor-evaluator framework, in which the code constructor is realized by AI algorithms and the code evaluator provides code performance metric measurements.	AI algorithms	AI algorithms	algorithm	realize	n1
2_arx_1902.02508_1083592_0	The equations for LES are formally derived by low-pass filtering the NS equations with the effect of the small scales on the larger ones captured by a SGS model.	a SGS model	SGS model	model	capture	n1
2_arx_2002.05702_1243125_4	CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth.	a generative model of synthetic structures	generative model of synthetic structures	model	create	n3  
2_arx_2009.12437_1353940_2	Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance.	a model	model	model	predict	n3
2_arx_2106.02498_1480262_5	Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions.	an AI system	AI system	system	meet	n2
2_arx_1212.5593_395209_0	Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.	a linear time varying model	linear time varying model	model	describe	n2
2_arx_1401.5941_495172_4	Then, a multilayer perceptron is trained by a backpropagation algorithm (MLP-BP) on a data subset, and used to classify the transients as glitch or burst.	a backpropagation algorithm (MLP-BP)	backpropagation algorithm (MLP-BP)	algorithm	train	n3
2_arx_1408.5886_551192_2	Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.	a model	model	model	describe	n3
2_arx_1501.07576_594324_2	In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.	a three-dimensional dynamic point-mass model	three-dimensional dynamic point-mass model	model	describe	n2
2_acl_1_42012_5	We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.	an LLM	LLM	LLM	generate	n3
2_arx_1610.02937_778284_8	The synthetic PM10 record predicted by the model was found to correlate with the PM10 observations with a correlation coefficient close to 0.80 with a confidence greater than 99%.	the model	model	model	predict	n3
2_arx_2304.11116_1829162_4	Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools.	the latest ChatGPT and Toolformer models	ChatGPT and Toolformer models	model	inspire	n2
2_arx_2110.14419_1552562_1	Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI.	AI	AI	AI	shape,influence	n3
2_arx_2207.00691_1676589_8	The results indicate that biases equating American identity with being White are learned by language-and-image AI, and propagate to downstream applications of such models.	language-and-image AI	AI	AI	learn	n3
2_arx_1812.01714_1059288_3	Through examining the weights in the trained DCNN model, we are able to quantitatively measure the visual similarities between architects that are implicitly learned by our model.	our model	model	model	learn	n3
2_arx_2412.12865_2215243_5	This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.	the aligned LLMs	LLMs	LLM	predict	n3
2_acl_19_45086_1	Therefore, determining whether a text was generated by an LLM has become one of the factors that must be considered when evaluating its reliability.	an LLM	LLM	LLM	generate	n3
2_arx_0906.5497_132048_3	We find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density.	a model	model	model	explain	n2
2_arx_2002.10965_1248388_3	For the sake of simplicity, a simple single-cell scenario is considered, where the optimization of the BS and IRS phase shifts is solved by a low-complexity trellis-based algorithm.	a low-complexity trellis-based algorithm	low-complexity trellis-based algorithm	algorithm	solve	n2
2_acl_750_35597_0	We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-”standard” varieties from around the world).	ChatGPT	ChatGPT	ChatGPT	exhibit bias	n3
2_acl_502_20526_4	We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.	the LM	LM	LM	learn	n3
2_arx_2105.13818_1476377_2	By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.	these models	models	model	acquire	n3
2_acl_46_41679_2	However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model.	the language model	language model	model	see	n3
3_acl_3_33872_4	Variations of this vignette are used for role-prompting a commercial LLM, GPT-4, instructing the LLM to take on the role described in the patient vignette and act accordingly.	the LLM	LLM	LLM	instruct	n3
3_arx_1712.09783_928382_2	In order to train the MoE model efficiently, a matrix factorization method is applied, by extending each weight matrix of the RNN to be an ensemble of topic-dependent weight matrices.	the MoE model	MoE model	model	train	n3
3_acl_6_25720_4	We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.	LLMs	LLMs	LLM	look at	n1
3_arx_2101.04617_1408359_3	We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers.	the trained model	model	model	employ	n2
3_arx_2101.05967_1409709_1	Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.	AI	AI	AI	deploy	n2
3_acl_118_9850_4	We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset.	the model	model	model	fine-tune	n1
3_arx_2306.02920_1856068_2	Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.	bilingual LMs	LMs	LM	train	n3
3_acl_3_2185_5	We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.	the RBM-based language model	RBM-based language model	model	evaluate	n1
3_arx_1911.03597_1202191_5	Moreover, since our model shares the same architecture as GPT (Radford et al., 2018), we are able to pre-train the model on large-scale unparallel corpus, which further improves the fluency of the output sentences.	the model	model	model	pre-train	n1
3_acl_329_43922_3	We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.	a model	model	model	develop	n1
3_arx_2302.07257_1792191_6	The goal is to merge the strengths of LLMs' medical domain knowledge and logical reasoning with the vision understanding capability of existing medical-image CAD models to create a more user-friendly and understandable system for patients compared to conventional CAD systems.	a more user-friendly and understandable system	system	system	create	n1
3_arx_301_32463_8	Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks.	eight mainstream LLMs	LLMs	LLM	evaluate and analyze	n1
3_acl_5_42629_6	We evaluate recent multilingual LLMs, including GPT-4, GPT-3.5, Cohere-AYA, Trendyol-LLM, and Turkcell-LLM, using this dataset.	recent multilingual LLMs	LLMs	LLM	evaluate	n1
3_acl_28_45074_3	We design a system using these pre-trained models to answer questions, based on the given context.	a system	system	system	design	n1
3_acl_41_45371_2	Unlike traditional methods that often simplify multi-agent interactions using a single opponent model, EMO constructs an individual model for each opponent and aligns these models working in synergy through a bi-level feedback-refinement framework.	an individual model	model	model	construct	n1
3_acl_104_45430_3	We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with out-of-the-box LLMs for the very first time, achieving an accuracy of 93% on New York Times crossword puzzles.	a search algorithm	search algorithm	algorithm	develop	n1
3_acl_6_46417_4	Using this approach, we train and present a BERT-based model trained on a biomedical corpus that matches or surpasses traditionally trained biomedical language models in performance across several downstream classification tasks while incurring up to 11 times lower training costs.	a BERT-based model	BERT-based model	model	train and present	n3
3_acl_7_60414_2	In evaluations of ranking character predictions, training recurrent LMs on noisy text makes them much more robust to noisy histories, even when the error model is misspecified.	recurrent LMs	recurrent LMs	LM	train	n3
3_acl_27_46301_5	To test the impact of our filtering, we train GPT-2 models on both the original and the filtered datasets.	GPT-2 models	GPT-2 models	model	train	n3
3_arx_1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.	a fast and compact model	model	model	train	n3
3_acl_1_7090_2	Thus, it is important to leverage memorized knowledge in the external LM for building the seq2seq model, since it is hard to prepare a large amount of paired data.	the seq2seq model	seq2seq model	model	build	n1
3_acl_928_27649_2	Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness.	ChatGPT	ChatGPT	ChatGPT	evaluate	n1
3_acl_582_29618_3	In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.	a recurrent neural language model	recurrent neural language model	model	develop	n1