2_acl_119_34984_3	However, even state-of-the-art large language models (LLMs) still face challenges in such scenarios, primarily due to the following hurdles: (1) LLMs are not explicitly trained to deal with ambiguous utterances; (2) the degree of ambiguity perceived by the LLMs may vary depending on the possessed knowledge.	the LLMs	LLMs	LLM	perceive	p2
2_acl_3_26185_65	The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience.	ChatGPT	ChatGPT	ChatGPT	translate	p2
2_acl_738_35585_4	Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism.	LLMs	LLMs	LLM	consider	p2
2_acl_304_37076_3	However their explicitly mention of malicious intent will be easily recognized and defended by LLMs.	LLMs	LLMs	LLM	recognize, defend	p2
2_acl_305_37077_7	This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals.	GPT-4	GPT-4	GPT-4	conduct	p3
2_acl_687_35537_5	However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.	LMs	LMs	LM	master	p2
2_arx_1811.00189_1044822_2	A remarkable feature of RAE is that the image can be correctly recognized and used by the AI model specified by the user because the authorized AI can recover the original image from the RAE exactly by eliminating adversarial perturbation.	the AI model	AI model	model	recognize,use	p2
2_arx_2204.03332_1633470_10	We also demonstrate, that the considered system possesses a counter-intuitive relationship between workload and performance, which nevertheless is correctly inferred by the proposed simulation model.	the proposed simulation model	simulation model	model	infer	p2
2_arx_1811.00189_1044822_0	In this study, we propose a new methodology to control how user's data is recognized and used by AI via exploiting the properties of adversarial examples.	AI	AI	AI	recognize,use	p2
2_arx_1910.04404_1188323_0	Explanation is necessary for humans to understand and accept decisions made by an AI system when the system's goal is known.	an AI system	AI system	system	make a decision	p2
2_arx_2004.11543_1276313_6	Each lesson in the curriculum is learnt by a deep reinforcement learning model.	a deep reinforcement learning model	deep reinforcement learning model	model	learn	p3
2_arx_2012.08285_1396163_1	The goal of this article is to paint a vision of a new air interface which is partially designed by AI to enable optimized communication schemes for any hardware, radio environment, and application.	AI	AI	AI	design	p3
2_arx_2205.08123_1652535_3	Results: After the exclusion of cases not meeting the study criteria, 9579 cases were analysed by AI.	AI	AI	AI	analyse	p2
2_arx_2204.06916_1637054_2	Typically, the advice generated by AI is judged by a human and either deemed reliable or rejected.	AI	AI	AI	generate	p3
2_acl_10_8527_2	The former is concerned with the generation of explanations for decisions taken by AI systems, while the latter is concerned with the way explanations are given to users and received by them.	AI systems	AI systems	system	take a decision	p2
2_acl_137_12015_5	We demonstrate that the sentence representations discovered by our model achieve better quality than previous methods that extract representations from pretrained transformers on text similarity tasks, style transfer (an example of controlled generation), and single-sentence classification tasks in the GLUE benchmark, while using fewer parameters than large pretrained models.	our model	model	model	discover	p2
2_arx_2011.13169_1385917_2	The need for explainable AI does not stem only from ethical and moral grounds but also from stricter legislation around the world mandating clear and justifiable explanations of any decision taken or assisted by AI.	AI	AI	AI	take a decision,assist a decision	p2
2_arx_1712.07473_926072_0	  One of the big challenges in machine learning applications is that training data can be different from the real-world data faced by the algorithm.	the algorithm	algorithm	algorithm	  face	p2
2_arx_2011.03195_1375943_6	Medical diagnosis model is responsible for human life and we need to be confident enough to treat a patient as instructed by a black-box model.	a black-box model	black-box model	model	instruct	p2
2_arx_1806.10698_996338_7	In addition, we found that the triage advice recommended by the AI System was, on average, safer than that of human doctors, when compared to the ranges of acceptable triage provided by independent expert judges, with only a minimal reduction in appropriateness.	the AI system	AI system	system	recommend	p2
2_acl_260_9141_4	The final submission was chosen based on the best performances which was achieved by the BERT+BiLSTM model.	the BERT+BiLSTM model	model	model	achieve	p3
2_acl_75_14800_1	However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training.	the model	model	model	acquire	p3
2_arx_1707.01659_866691_0	In this work, a dynamic system is controlled by multiple sensor-actuator agents, each of them commanding and observing parts of the system's input and output.	multiple sensor-actuator agents	sensor-actuator agents	agent	control	p3
2_acl_67_46367_3	The LLM-generated responses were analyzed in three ways: 1) manual error analysis by a technical writer, 2) automatic assessment using deterministic metrics (BLEU, ROUGE, token overlap), and 3) evaluation of correctness by LLM as a judge.	LLM	LLM	LLM	evaluate	p3
2_arx_1905.10083_1128519_5	However, research on edge intelligence is still in its infancy stage, and a dedicated venue for exchanging the recent advances of edge intelligence is highly desired by both the computer system and artificial intelligence communities.	the computer system	computer system	system	desire	p2
3_acl_569_29605_3	  Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs.	  LLMs	  LLMs	  LLM	  fool	  p2
3_acl_71_43638_5	Specifically, we first identify a series of action candidates that could potentially trick LLMs into providing harmful responses.	LLMs	LLMs	LLM	trick	p2
3_arx_40_10977_2	In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?	a language model	model	model	tell	p3
3_arx_2304.08366_1826412_8	Next, we summarize the interviewees' reasons why and why not they would like to collaborate with AI.	AI	AI	AI	collaborate with	p2
3_arx_2305.07001_1840057_9	Our approach sheds light on developing more user-friendly recommender systems, in which users can freely communicate with the system and obtain more accurate recommendations via natural language instructions.	the system	system	system	communicate with	p2
3_acl_358_32520_4	Our goal is to teach the LLM that “even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different”.	the LLM	LLM	LLM	teach	p2
3_acl_180_20204_2	  In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion.	  an LM	  LM	  LM	  teach	  p2
3_acl_546_18015_4	To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics.	the model	model	model	encourage	p2
3_acl_3_22073_5	First, we induce a language model to produce step-by-step rationales before outputting the answer to effectively communicate the task to the model.	the model	model	model	communicate to	p2
3_arx_2112.11668_1581444_1	Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.	a BERT-based sentiment analysis model	BERT-based sentiment analysis model	model	fool	p2
3_arx_2205.01772_1646184_6	Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing.	a biometric system	biometric system	system	fool	p2
3_arx_2402.09671_2007310_0	  This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.	multiple AI vision systems	AI vision systems	system	fool	p2
3_arx_2308.14132_1902024_1	Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content.	  LLMs	  LLMs	  LLM	  trick	  p2
3_arx_2304.11082_1829128_6	This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona.	the LLM	LLM	LLM	  trick	  p2
3_arx_2309.05689_1910493_2	Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement.	LLMs	LLMs	LLM	encourage	p2
3_arx_2306.04707_1857855_3	We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses.	the model	model	model	trick	p2
3_arx_2311.08147_1951114_3	However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response.	the model	model	model	confuse	p2
3_acl_602_40012_4	Based on our findings, we propose FSLI, a framework for encouraging LLMs to Forget Spurious correlations and Learn from In-context information.	LLMs	LLMs	LLM	encourage	p2
3_acl_292_36775_3	In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.	LLMs	LLMs	LLM	encourage	p2
3_acl_773_32935_3	Specifically, we study how to persuade LLMs to jailbreak them.	LLMs	LLMs	LLM	persuade	p2
3_acl_328_38052_5	This method inspires the LLM to perceive domain information from the source text, which then serves as a helpful hint to guide the translation process.	the LLM	LLM	LLM	inspire	p2
3_acl_476_35330_3	Motivated by social psychology principles, we propose a novel strategy named perspective-taking prompting (PeT) that inspires LLMs to integrate diverse human perspectives and self-regulate their responses.	LLMs	LLMs	LLM	inspire	p2
3_acl_373_22509_4	Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.	the model	model	model	incentivize	p2
3_acl_31_27805_5	Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights.	the LLM	LLM	LLM	collaborate with	p2
3_acl_920_38624_10_2	We show that choosing the best policy to interact with the LLM can reduce cost by ~90% while giving better or comparable performance, compared to communicating with the LLM in the original LRL.	the LLM	LLM	LLM	communicate with	p2
3_acl_8_31860_3	In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response.	GPT-3	GPT-3	GPT-3	confuse	p2
3_acl_774_40184_4	The framework motivates the model itself to automatically generate rationales on existing datasets.	the model	model	model	motivate	p2
3_acl_347_24497_6	Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences.	the model	model	model	teach	p2
3_acl_814_27535_3	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.	LLMs	LLMs	LLM	teach	p2
3_acl_114_29150_2	Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction.	LLMs	LLMs	LLM	encourage	p2
3_arx_2306.10063_1863211_1	In this conception, learners continually converse with AI language models within a dynamic computational medium of internet tools and resources.	AI language models	AI language models	model	converse with	p2
3_acl_814_27535_2	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.	an agent	agent	agent	train	p3
3_arx_2306.03856_1857004_0	We propose iteratively prompting a large language model to self-correct a translation, with inspiration from their strong language understanding and translation capability as well as a human-like translation approach.	a large language model	large language model	model	prompt	p3
3_acl_212_35072_4	Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors.	LLMs	LLMs	LLM	instruct	p3
3_acl_599_29635_5	Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.	the LLM	LLM	LLM	ask	p3