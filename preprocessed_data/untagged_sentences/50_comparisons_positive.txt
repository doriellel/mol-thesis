acl_8_10945_3	Although the best bidirectional model performs similarly to humans, they display different strengths: humans outperform neural networks in conversational contexts, while RoBERTa excels at written genres.

acl_115_26836_3	With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative.

acl_22_39431_0	Large Language Models (LLMs) have been developed without a theoretical framework, yet we posit that evaluating and improving LLMs will benefit from the development of theoretical frameworks that enable comparison of the structures of human language and the model of language built up by LLMs through the processing of text.

acl_9_38794_1	We study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.

2401.01623_1981179_2	In this paper, we prove in theory that AI can be as creative as humans under the condition that it can properly fit the data generated by human creators.

572_45853_6	We find that some LLMs are sensitive to factors that affect the inference process similarly to humans, yet there remains variance in human behavior not fully captured by LLMs.

865_25015_6	On the other hand, downstream performance is mainly driven by the modelâ€™s size and prior legal knowledge which can be estimated by upstream and probing performance.	the model	model	's

2403.03230_2020349_6	Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries.


2303.09038_1808835_0	  The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities.



POSSESSIVE

245_29281_1	With large language models like GPT-4 taking over the field, prompting techniques such as chain-of-thought (CoT) were proposed to unlock compositional, multi-step reasoning capabilities of LLMs.
