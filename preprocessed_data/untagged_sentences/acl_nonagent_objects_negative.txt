30_284_0	We present a newly designed transformational system for the MT system LMT, consisting of a transformational formalism, LMT-TL, and an algorithm for applying transformations written in this formalism.
6_781_0	We present a syntax-based language model for use in noisy-channel machine translation.
15_843_0	This paper presents a system overview of an English to Hindi Machine-Aided Translation System named AnglaHindi.
3_2185_5	We evaluated the RBM-based language model on the German to English and English to French translation task of TED lectures.
13_2319_0	In this paper, we present our submitted MT system for the IWSLT2014 Evaluation Campaign.
54_3473_1	The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually.
29_4444_1	Building an explanation system requires a definition of explanation.
59_4790_3	We achieve this by building a neural-based deep contextual understanding model for Question-Answering (QA) and Information Retrieval (IR) tasks.
68_5568_1	MAIA will employ cutting-edge machine learning and natural language processing technologies to build multilingual AI agent assistants, eliminating language barriers.
96_5687_5	We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches.
79_6524_0	Despite recent success in neural task-oriented dialogue systems, developing such a real-world system involves accessing large-scale knowledge bases (KBs), which cannot be simply encoded by neural approaches, such as memory network mechanisms.
141_6586_8	We utilize Quadratic Weighted Kappa to evaluate our model on the Automated Student Assessment Prize dataset.
376_6821_2	In this work, we present the Multichannel Generative Language Model (MGLM).
64_7046_0	In this paper, we have designed a character-level pre-trained language model for extracting support phrases from tweets based on the sentiment label.
57_8938_2	We use a BERT base model for the classification task and build a hybrid BERT Multi-Layer Perceptron system to handle the sequence identification task.
74_8955_1	The objective of this task was to develop a system that can differentiate statements that make sense from the ones that don’t.
74_8955_4	We have developed a system that leverages commonsense knowledge from pretrained language models trained on huge corpus such as RoBERTa, GPT2, etc.
12_9521_3	We incorporated some of the methods proposed in the previous edition of Textgraphs-13 (Chia et al., 2019), which proved to be very effective, improved upon them, and built a model on top of it using powerful state-of-the-art pre-trained language models like RoBERTa (Liu et al., 2019), BART (Lewis et al., 2020), SciB-ERT (Beltagy et al., 2019) among others.
19_9625_1	Recently, OpenAI has developed a machine learning system called GPT-2 for Generative Pre-trained Transformer-2, which can produce deepfake texts.
13_9675_0	We present a system for bilingual Data-ToText Generation and Semantic Parsing.
13_9675_3	We evaluate the system on WebNLG 2020 data 1 , which consists of RDF triples in English and natural language sentences in English and Russian for both the tasks.
59_9932_3	This work focuses on developing such a language system that can differentiate between Informative or Uninformative tweets associated with COVID-19 for WNUT-2020 Shared Task 2.
39_10004_2	In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model’s layers simultaneously and highlighting intra-layer properties and inter-layer differences.
102_10067_4	In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning.
230_10195_3	Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation.
236_10201_5	Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students.
295_10260_2	We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples.
418_10383_2	With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures.
490_10455_7	We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.
87_10623_4	At the same time, even at larger model sizes, we find that pre-training with parallel data still provides benefits in the limited labelled data regime
1_10755_5	We discuss the scientific challenges that arise when building such a system, including argument mining, argument quality assessment, stance classification, principled argument detection, narrative generation, and rebutting a human opponent.
26_10845_1	We present a low-resource machine translation system that improves translation accuracy using cross-lingual language model pretraining.
21_10870_0	We present the system description for our submission towards the Key Point Analysis Shared Task at ArgMining 2021.
5_10989_1	We evaluate the transformer model on the tasks of part-of-speech tagging, named-entity-recognition, geo-location prediction and commonsense causal reasoning, showing improvements on all tasks over state-of-the-art models.
15_11356_0	We present the Everyday Living Artificial Intelligence (AI) Hub, a novel proof-of-concept framework for enhancing human health and wellbeing via a combination of tailored wear-able and Conversational Agent (CA) solutions for non-invasive monitoring of physiological signals, assessment of behaviors through unobtrusive wearable devices, and the provision of personalized interventions to reduce stress and anxiety.
153_11599_1	However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies.
242_11688_4	We evaluate 11 different LMs on CLiMP, covering n-grams, LSTMs, and Chinese BERT.
161_12039_6	We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data.
680_12558_3	We evaluated different pretrained LMs on two datasets and found significant gains in realistic scenarios with up to 15% WER improvement over the best OCR view.
108_12918_3	This paper empirically studies the performance and efficiency of building an Arabic language model with Funnel Transformer and ELECTRA objective.
346_13156_2	We test HSO with pretrained Transformer-XL and GPT-2 language models, finding improvement on the WikiText-103 and PG-19 datasets in terms of perplexity, especially when evaluating a model outside of its training distribution.
6_13284_0	In this paper we present a system that exploits different pre-trained Language Models for assigning domain labels to WordNet synsets without any kind of supervision.
17_13755_4	Based on the analysis and understanding of the task description and data set, we designed a system based on a pre-trained language model to complete this shared task.
290_14227_4	Second, evaluating a model’s likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs?
422_14359_2	We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations.
4_14577_4	In the present paper, we present “MacBERTh”—a transformer-based language model pre-trained on historical English—and exhaustively assess its benefits on a large set of relevant downstream tasks.
3_14634_2	Therefore, our work is to develop a model that automatically writes the commit message.
9_14650_2	We present an algorithm to create synthetic training data with an explicit focus on capturing medically relevant information.
39_14689_2	We consider a number of arguments for collaboratively developing a large-scale Nordic language model, include environmental considerations, cost, data availability, language typology, cultural similarity, and transparency.
1_14941_4	We argue that this DS-RDF hybrid satisfies the desiderata listed above, yielding semantic infrastructure that can be used to build responsive, real-time, interpretable Conversational AI that can be rapidly customised for specific user groups such as people with dementia.
107_15161_0	This paper presents the system developed by our team for Semeval 2021 Task 4: Reading Comprehension of Abstract Meaning.
46_15294_3	We evaluated our system on the multi-domain DSTC8 data set and reported joint goal accuracy of 75.8% (ranked among the first half positions), intent accuracy of 97.4% (which is higher than the reported literature), and a 15% improvement for success rate compared to a baseline with no symbolic injection.
22_15489_0	We present a language model that combines a large parametric neural network (i.e., a transformer) with a non-parametric episodic memory component in an integrated architecture.
40_16022_2	We analyze the differences between text and image and propose a probing task that detects bias by evaluating a model’s tendency to pick stereotypical statements as captions for anti-stereotypical images.
3_16132_4	We also experimented with multi-task learning and build a language model that can improve performance on multiple time-related tasks.
54_16212_2	We design an automated question-answer generation (QAG) system for this education scenario: given a story book at the kindergarten to eighth-grade level as input, our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student’s comprehension skills.
505_16663_5	We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset.
593_16751_1	Generative Spoken Language Modeling (GSLM) (CITATION) is the only prior work addressing the generative aspect of speech pre-training, which builds a text-free language model using discovered units.
593_16751_3	In this work, we present a prosody-aware generative spoken language model (pGSLM).
24_16785_7	We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.
280_17749_3	In this paper, we present a structure-aware self-attention language model to capture structural information of target representations and propose a knowledge distillation based approach to incorporating the target language model into a seq2seq model, where grammar rules or sketches are not required in the training process.
324_17793_3	Next, we create a new benchmark that allows us to evaluate a model on 19 test cases, distinguished by multiple types of consistency and diverse downstream tasks.
405_17874_5	We evaluate our model on standard semantic textual similarity (STS) and semantic search tasks.
6_18201_3	After automatically verbalizing triples in ATOMIC2020, a wide coverage commonsense reasoning knowledge graph, we continually pretrain BERT and evaluate the resulting model on cause-effect pair classification and answering commonsense causal reasoning questions.
7_18217_0	Building a natural language processing (NLP) model can be challenging for end-users such as analysts, journalists, investigators, etc., especially given that they will likely apply existing tools out of the box.
225_18672_4	We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot.
398_18845_4	We then synthesize our findings and develop a knowledge graph completion model that significantly outperforms recent neural models.
150_19590_6	We evaluate the proposed Dict-BERT model on the language understanding benchmark GLUE and eight specialized domain benchmark datasets.
32_19803_1	While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pretrained model that can understand and generate long documents.
52_19823_4	We evaluate LMs’ perplexity on masked spans within these sentences.
52_19823_7	Given its wide coverage on entity knowledge and temporal indexing, our dataset can be used to evaluate LMs and techniques designed to modify or extend their knowledge.
57_19828_3	We present LM-CORE – a general framework to achieve this– that allows decoupling of the language model training from the external knowledge source and allows the latter to be updated without affecting the already trained model.
21_20045_5	For APIRetriever, we present a dense retrieval system and also design a friendly interaction to involve uses.
24_20048_9	To address TQVSR, we develop a simple yet effective model called Dual Multimodal Encoders (DME) that significantly outperforms several baseline methods while still having large room for improvement in the future.
207_20231_6	We evaluate our pre-trained model on several English datasets from different domains as well as on synthetic noise.
398_20422_5	We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines.
477_20501_4	We then design a Model Uncertainty–aware Knowledge Integration (MUKI) framework to recover the golden supervision for the student.
530_20554_2	We evaluate our model on several NLP tasks and achieve state-of-the-art results.
30_20602_3	The goal of sub-task1 is to develop a model to predict correctly a list of given terms from ESG taxonomy domain into the most relevant concepts.
30_20602_4	The aim of subtask2 is to design a system that can automatically classify the ESG Taxonomy text sentence into sustainable or unsustainable class.
6_20674_3	Given a model and a debiasing objective, our framework finds a subset of the model containing less bias than the original model.
12_20842_2	We present a system for generating “sparks”, sentences related to a scientific concept intended to inspire writers.
6_20896_3	We evaluate these LMs on BLiMP, a targeted evaluation benchmark of multiple English linguistic phenomena.
6_20896_5	We hope our work offers useful insights for future research into designing Transformer LMs that more effectively learn linguistic knowledge.
5_21050_3	To address both points for the case of Modern Historical Japanese text, this paper proposes the use of unsupervised domain adaptation methods to develop a domain-adapted language model (LM) that can flag instances of inaccurate UD output from a pretrained LM and the use of these instances to form rules that, when applied, improves pretrained annotation accuracy.
5_21050_4	To test the efficacy of the proposed approach, the paper evaluates the domain-adapted LM against three baselines that are not adapted to the historical domain.
304_21481_3	We aim to address this research niche by building a language model that understands the linguistic phenomena in the target language which can be trained with low-resources.
543_21720_2	In this paper, we present LuxemBERT, a BERT model for the Luxembourgish language that we create using the following approach: we augment the pre-training dataset by considering text data from a closely related language that we partially translate using a simple and straightforward method.
630_21807_0	We hypothesise and evaluate a language model-based approach for scoring the quality of OCR transcriptions in the British Library Newspapers (BLN) corpus parts 1 and 2, to identify the best quality OCR for use in further natural language processing tasks, with a wider view to link individual newspaper reports of crime in nineteenth-century London to the Digital Panopticon—a structured repository of criminal lives.
14_22026_3	As a result, we present DC-LM, a dual-channel language model that sees hope speech by using the English translations of the code-mixed dataset for additional training.
18_22030_4	In this paper, we present our system for the LT-EDI shared task on detecting homophobia and transphobia in social media comments.
33_22169_0	Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans’ roles, experience, and needs when interacting with or being assisted by AI.
155_22291_1	However, building a chat system is not scalable since it often requires a considerable volume of human-human dialogue data, especially when enforcing features such as persona, style, or safety.
21_22599_3	In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data.
2_22824_4	We present a system that facilitates attack construction, combining human judgment with automated attacks to create better attacks more efficiently.
22_23038_2	In this study, we evaluate a pre-trained LongT5 model on the MSLR22: Multi-Document Summarization for Literature Reviews Shared Task datasets.
87_23139_1	In the SemEval-2022 Multimedia Automatic Misogyny Identification (MAMI) challenge, we designed a system using two simple but effective principles.
169_23221_1	To build our system for the task, we experimented with several multilingual language models which were originally pre-trained for semantic similarity but were not further fine-tuned.
203_23255_1	To construct this system, we used Pre-trained Language Models (PLMs).
25_23457_4	We present a trigram language model for the Sorani dialect of the Kurdish language that is created using educational text.
1_23460_0	We present MozoLM, an open-source language model microservice package intended for use in AAC text-entry applications, with a particular focus on the design principles of the library.
58_23867_2	We use a combination of data augmentation, Name Entity Recognition, rule-based repetition detection, and ARBERT prediction to develop our system.
18_23980_4	In model evaluation: (1) We find that previous work underestimated the translation performance of Livonian due to inconsistent Unicode normalization, which may cause a discrepancy of up to 14.9 BLEU score.(2)
307_24457_9	The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs.
387_24537_1	However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks.
507_24657_4	Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.
647_24797_1	In this work, we present SSD-LM—a diffusion-based language model with two key design choices.
647_24797_4	We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines.
780_24930_2	We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision?
792_24942_0	We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs).
870_25020_4	We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.
119_25179_5	We design an annotation scheme for evaluating model outputs, withan emphasis on assessing the factual accuracy of generated summaries.
24_25340_5	We present a cost framework for evaluating an NLP model’s utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product.
57_25373_3	In this paper, we present “AI Coach Assis”, which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors.
3_25435_0	We present a cross-linguistic study in which the open source C-LARA platform was used to evaluate GPT-4’s ability to perform several key tasks relevant to Computer Assisted Language Learning.
19_25451_0	In this paper, I provide a detailed description of my approach to tackling the ALTA 2023 shared task whose objective is to build an automatic detection system to distinguish between humanauthored text and text generated from Large Language Models.
1_25479_3	We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed Violet.
5_25654_3	Our study identifies three key aspects: a) we provide insights into student needs when writing peer reviews with generative models which we then use to develop a novel system to provide adaptive instructions b) we fine-tune three German language models on a selected corpus of 11,925 student-written peer review texts in German and choose German-GPT2 based on quantitative measures and human evaluation, and c) we evaluate our tool with fourteen students, revealing positive technology acceptance based on quantitative measures.
65_25714_0	This paper presents the ADAIO team’s system entry in the Building Educational Applications (BEA) 2023
7_25731_3	We designed three different prompt techniques to break down the task and evaluate ChatGPT.
57_25781_3	We present a system that leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs, enhancing their medical knowledge and performance on specific medical tasks.
22_25815_6	Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions.
8_25983_5	To tackle these problems,existing works proposed various benchmarks to better evaluate LLMs.
9_25984_3	Therefore, a new trend in multimodal AI is to build a compositionalAI system that connects existing foundation models with external modules and tools.
9_25984_5	Inthis paper, we will give a brief overview of the state-of-the-art multimodal AI techniques and thedirection of building compositional AI systems.
26_26092_6	We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively.
31_26097_1	We present a German medical Named Entity Recognition (NER) system capable of cross-domain knowledge transferring.
31_26097_4	We evaluate the system’s effectiveness on two German annotated datasets obtained from different clinics in zero- and few-shot settings.
51_26117_1	We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task.
21_26163_5	Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance.
19_26316_0	This paper presents our approach to building a generalized model for Track 5 in DSTC11: “Task-oriented Conversational Modeling with Subjective Knowledge” which addresses the challenge of generating responses to users’ utterances based on a variety of factual and subjective knowledge.
19_26316_5	These outcomes serve as solid evidence that data augmentation and using a large-size model were highly effective for developing a conversational model system that incorporates objective and subjective knowledge.
8_26729_5	We hope that our experiments and analyses provide additional insights into building more reliable ChatGPT-based solutions.
13_26734_1	This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines.
73_26794_1	Therefore, it is important to build strong AI-generated text (AIGT) detectors.
87_26808_2	In this paper, we address the lack of benchmarks to evaluate LLMs’ ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world.
162_26883_5	To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM’s ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions.
162_26883_6	We evaluate GPT-3 and GPT-4 on this task and explore their robustness.
187_26908_5	For the first question, we develop a runnable evaluation system consisting of 73 API tools.
214_26935_3	We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines as in Figure 1.
258_26979_1	An important question being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging.
258_26979_6	We create a framework for evaluating generative LLMs in the multilingual setting and provide directions for future progress in the field.
315_27036_3	In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture.
319_27040_2	To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks.
330_27051_4	For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model’s conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.
400_27121_4	We develop a model that integrates synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data.
407_27128_3	Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains.
418_27139_3	We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies.
428_27149_3	The corpus provides a test bed for evaluating LLMs for knowledge and understanding of region-specific etiquettes.
429_27150_5	Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs’ ability to determine the matching between relations and associated text.
593_27314_3	We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs’ proficiency in formulating a coherent and consistent knowledge representation from segmented narratives.
593_27314_7	The findings from this study offer insights for developing more robust and reliable LLMs.
599_27320_2	This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements.
636_27357_3	In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT.
699_27420_5	Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs.
711_27432_2	In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proof but also evaluates a generative LM’s reasoning ability on formulas and capability to manipulate, group, and factor number terms.
842_27563_1	In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters.
891_27612_2	Exploring various prompting strategies, we evaluated GPT-4 on a diverse range of common radiology tasks and we found GPT-4 either outperforms or is on par with current SOTA radiology models.
952_27673_1	While previous studies have primarily focused on designing complex model architectures, this paper takes a different perspective by rethinking the fundamental question: what kind of words are good auto-completions?
4_27778_3	With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively.
17_27791_0	With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model’s capabilities has become an increasingly significant issue.
17_27791_2	We present CLEVA, a user-friendly platform crafted to holistically evaluate Chinese LLMs.
21_27795_2	Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements.
21_27795_3	Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements.
40_27814_4	Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.
44_27818_2	To this end, we propose the Zhujiu benchmark, which has the following strengths: (1) Multi-dimensional ability coverage: We comprehensively evaluate LLMs across 7 ability dimensions covering 51 tasks.
44_27818_4	(2) Multi-faceted evaluation methods collaboration: We use 3 different yet complementary evaluation methods to comprehensively evaluate LLMs, which can ensure the authority and accuracy of the evaluation results.
44_27818_7	We evaluate 10 current mainstream LLMs, and conduct an in-depth discussion and analysis of their results.
12_27838_0	We present a new BERT model for the cybersecurity domain, CTI-BERT, which can improve the accuracy of cyber threat intelligence (CTI) extraction, enabling organizations to better defend against potential cyber threats.
76_27902_6	We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt.
12_27915_1	We focus on the summarization task and evaluate both small and large LLM models.
79_28018_2	We fine-tune and evaluate our model on three important natural language downstream tasks, Part-of-speech tagging, Named-entity recognition, and Question answering.
83_28022_3	Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus.
29_28165_3	Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets.
29_28165_6	We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models.
167_28303_8	The code for this work can be found at https://github.com/Ziems/llm-url.
218_28354_2	In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity – (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor.
222_28358_4	We evaluate our model on IWSLT14 En-De, De-En, WMT14 En-De, and En-Fr tasks, and the results show that our proposed PLM enhancement gives significant improvement and even helps achieve new state-of-the-art.
229_28365_5	This paper explores the application of closed-source LLMs to real-world security management scenarios by evaluating ChatGPT’s performance on VDM tasks.
247_28383_5	On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model.
320_28456_3	We evaluate our model’s performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French.
551_28687_4	This study aims to evaluate LLMs’ self-knowledge by assessing their ability to identify unanswerable or unknowable questions.
675_28811_3	We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization.
727_28863_4	Our framework presents the pre-trained language model GPT-2 for data generation.
733_28869_4	We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity.
734_28870_0	This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data.
853_28989_3	To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user’s intent.
58_29095_5	We hope this study builds foundation evaluation benchmarks for continuing efforts to build more advanced LLMs in the financial domain.
157_29193_0	This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency.
181_29217_2	To solve the problem of self-supervised representation learning on text-attributed graphs, we develop a novel Graph-Centric Language model – GRENADE.
267_29303_4	We compare descriptive noun phrases, human-crafted definitions, introduce a new method to help the model generate definitions from examples, and propose a method to evaluate GPT-3’s understanding of the definitions.
284_29320_2	In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy.
287_29323_4	We evaluated GPT-4 with our dataset to investigate its ability to extract information from legal wills.
360_29396_2	The most common approach to evaluate LLMs on hallucinations is to test them on Question Answering (QA) test sets such as TruthfulQA.
393_29429_0	An important aspect of developing LLMs that interact with humans is to align models’ behavior to their users.
503_29539_5	We find a subset as informative as BIG-bench Hard for evaluating new model families, while being 3× smaller.
546_29582_3	We propose a method to evaluate LLMs ability to rule-based generalization.
582_29618_3	In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories.
631_29667_4	We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models.
877_29913_4	We then discuss ways to develop concept-aware LLMs, taking place at different stages of the pipeline.
878_29914_6	In particular, we evaluate ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources.
894_29930_1	Consequently, designing a white-box model to compute metrics that reflect the presence of specific internal relations in these vectors has become a common approach for post-hoc interpretability analysis of pretrained language models.
911_29947_3	In this paper, we present a time-aware language model named TALM, to learn temporal word representations by transferring language models of general domains to those of time-specific ones.
23_30132_3	We validate this claim by evaluating both generative LLMs and existing encoder-based STS models on three newly-collected STS challenge sets which require world knowledge in the domains of Health, Politics, and Sports.
8_30149_6	Our findings advance the field toward the concerns of properly evaluating LLMs in high-risk domains, aiming to steer the adaptability of LLMs in fulfilling societal obligations and aligning with forthcoming regulations, such as the EU AI Act.
11_30322_2	In this paper, our main aim is to evaluate ChatGPT’s question generation in a task where language production should be driven by an implicit reasoning process.
17_30328_0	In this paper, we present a system for augmenting virtual AI characters with long-term memory, enabling them to remember facts about themselves, their world, and past experiences.
2_30482_1	This paper proposes a methodology for developing and evaluating ChatGPT detectors for French text, with a focus on investigating their robustness on out-of-domain data and against common attack schemes.
20_30802_5	Drawing on the above-mentioned corpus we have been developing an AI mediated MT post-editing (MTPE) system through the optimization of precedent rendition distribution and semantic association to enhance the work of translators and MTPE practitioners.
8_30939_2	To solve this we present Kani: a lightweight, flexible, and model-agnostic open-source framework for building language model applications.
1_31261_0	In this paper, we present our system for the textual entailment identification task as a subtask of the SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data.
72_31332_3	Specifically, we first develop the Legal-BERT-HSLN model that considers the com-prehensive context information in both intra-and inter-sentence levels to predict rhetoricalroles (subtask A) and then train a Legal-LUKEmodel, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B).Our evaluations demonstrate that our designedmodels are more accurate than baselines, e.g.,with an up to 15.0% better F1 score in subtaskB. We achieved notable performance in the taskleaderboard, e.g., 0.834 micro F1 score, andranked No.5 out of 27 teams in subtask A.
78_31338_0	This paper presents the system developed by the Sartipi-Sedighin team for SemEval 2023 Task 2, which is a shared task focused on multilingual complex named entity recognition (NER), or MultiCoNER II.
137_31397_0	The goal of the NLI4CT task is to build a Natural Language Inference system for Clinical Trial Reports that will be used for evidence interpretation and retrieval.
137_31397_3	We have evaluated the publicly available LLMs under zeroshot setting, and finetuned the best performing Flan-T5 model for this task.
197_31457_0	This paper presents the system submissions of the John-Arthur team to the SemEval Task 4 “ValueEval: Identification of Human Values behind Arguments”.
241_31501_3	Our findings provide insights into the features and decision-making processes underlying our classifier system, thereby contributing to a broader effort to develop explainable AI models to detect online sexism.
285_31545_0	We present the system proposed by the MilaNLP team for the Explainable Detection of Online Sexism (EDOS) shared task.
10_31704_3	We evaluate different LLMs - including the open-source GPT-neo, GPT-3, and InstructGPT - against a wide range of negation benchmarks.
5_31744_0	This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD).
16_31957_0	This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset.
33_31974_0	This study evaluated ChatGPT’s ability to understand causal language in science papers and news by testing its accuracy in a task of labeling the strength of a claim as causal, conditional causal, correlational, or no relationship.
1_32007_2	We evaluate system outputs with professional human annotators using a combination of source-based Direct Assessment and scalar quality metric (DA+SQM).
100_32106_3	We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning.
13_32129_3	The resulting annotated data is used to build our problematic webpage classification model.
40_32202_2	FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method.
46_32208_3	Second, we construct the LLM questioner to generate non-redundant questions about the attributes of the target image, based on the information of retrieval candidate images in the current context.
61_32223_5	Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency.
63_32225_5	Hence, to design an effective human evaluation system in the age of generative NLP we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars - Consistency, Scoring Criteria, Differentiating, User Experience, Responsible, and Scalability.
82_32244_7	We systematically evaluate various LLMs and discover significant differences in their performance on this task.
90_32252_1	To evaluate LLMs’ intention detection capability in conversation, we modified the existing datasets of persuasive conversation and created datasets using a multiple-choice paradigm.
180_32342_7	Thirdly, we design a novel multi-stage learning algorithm, which makes the best use of readily available data and synthetic data for cost-effective training of the landmark embedding.
185_32347_6	The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.
196_32358_2	To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities.
257_32419_6	By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work.
265_32427_1	Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance.
288_32450_8	We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.
331_32493_5	Our code can be found in https://github.com/HKUST-KnowComp/LLM-discussion.
343_32505_4	Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets.
370_32532_2	Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important.
401_32563_5	We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks.
510_32672_0	In this paper, we present an innovative process-oriented math process reward model called Math-shepherd, which assigns a reward score to each step of math problem solutions.
525_32687_5	Moreover, to quantitatively evaluate LLMs in multi-turn instruction following, we manually build a multi-turn benchmark derived from existing ones.
530_32692_1	State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task.
530_32692_6	Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data.
534_32696_6	We evaluate open LMs as baselines and find they are still far behind the human level.
560_32722_1	The research community often relies on a model’s average performance across the test prompts of a benchmark to evaluate the model’s performance.
578_32740_1	To this end, we introduce CLAMBER, a benchmark for evaluating LLMs using a well-organized taxonomy.
593_32755_4	To effectively evaluate a generation model’s counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem.
595_32757_2	To facilitate research on multilingual LLM evaluation, we release IndicGenBench — the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families.
595_32757_5	We evaluate stateof-the-art LLMs like GPT-3.5, GPT-4, PaLM2, and LLaMA on IndicGenBench in a variety of settings.
604_32766_4	We also evaluated the LLMs’ memorization-independent reasoning abilities and analyzed the typical errors.
614_32776_5	Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
624_32786_2	To fill in this gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark for evaluating LLMs’ alignment in Chinese.
636_32798_6	We evaluated LLMs using both Surprisal distributions and prompting techniques.
677_32839_4	Specifically, we evaluate two prominent LLMs:
678_32840_3	Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks.
705_32867_1	However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions.
742_32904_2	Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding.
747_32909_0	Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions.
749_32911_5	Motivated by these results, we construct a benchmark to evaluate LLMs’ ability to generalize to neologisms with various natural language understanding tasks and model perplexity.
774_32936_1	In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently.
791_32953_3	In this paper, we present D2LLMs—Decomposed and Distilled LLMs for semantic search—that combines the best of both worlds.
799_32961_6	Our suite enables reproducible research on methods to build LLMs for low-resource languages.
832_32994_5	These scenarios evaluate LLMs’ long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding.
844_33006_5	We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks.
852_33014_5	We believe that DocMath-Eval can serve as a valuable benchmark for evaluating LLMs' capabilities in solving challenging numerical reasoning problems within expert domains.
862_33024_7	Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment.
6_33109_0	We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models.
9_33112_7	Besides, we present an online system for real-time knowledge editing, and a demo video.
19_33122_3	For capability assessment, we include 12 benchmark datasets to evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge, commonsense reasoning and mathematical reasoning.
34_33137_2	Demonstrating its utility in cancer research, BioLunar leverages modular design, reusable data access and data analysis components, and a low-code user interface, enabling researchers of all programming levels to construct LLM-enabled scientific workflows.
7_33146_7	By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs.
6_33199_1	In this study, we present a Werewolf AI agent developed for the AIWolfDial 2024 shared task, co-hosted with the 17th INLG.
6_33199_3	We thus develop the LLM-based agents for the Werewolf Game.
9_33209_1	By leveraging Large Language Models (LLMs), particularly the RoBERTa transformer model, we developed an automated system to extract and structure venue availability information according to MARC (Machine-Readable Cataloging) standards.
10_33317_7	Moreover, we also evaluate the general LLMs outputs to verify their correctness, consistency, and completeness.
15_33448_4	We outline a benchmarking study that evaluates leading LLMs (including open-source ones) on program repair and explanation tasks.
13_33504_2	In this work, we develop an augmented LLM framework, KG-Rank, which leverages a medical knowledge graph (KG) with ranking and re-ranking techniques, aiming to improve free-text question-answering (QA) in the medical domain.
20_33511_2	In this work, we evaluate GPT on four contextual biomedical MRC benchmarks.
28_33519_6	We evaluated various LLMs to investigate their ability to recognize bacterial host organisms and genetic toolboxes for engineering.
42_33533_6	To evaluate the model’s performance, we augmented the RICO dataset, consisting of Android user interface screenshots, by superimposing alcohol ads onto them.
54_33545_4	The study highlights the importance of generating both FINDINGS and IMPRESSIONS sections in radiology reports and evaluates the model’s performance using various metrics, achieving notable accuracy in generating high-quality medical reports.
65_33556_2	In this work, we develop an LLM-based framework for solving the Discharge Summary Documentation (DSD) task, i.e., generating the two critical target sections ‘Brief Hospital Course’ and ‘Discharge Instructions’ in the discharge summary.
6_33630_1	In this study, we evaluate ChatGPT’s ability to detect hate speech in Turkish tweets and measure its strength using zero- and few-shot paradigms and compare the results to the supervised fine-tuning BERT model.
18_33887_4	We perform experiments using three different prompting techniques on the LLM Vicuna to evaluate the model thoroughly.
25_33971_0	This paper presents an AI experiment of translation in emoji conducted on a glossary from Dante Alighieri’s Comedy.
25_33971_3	The present test involves human (Emojitaliano) and machine (Chat-GPT) translations in a comparative analysis to devise an automated integrated model highlighting emojis’ expressive ability in transferring senses, clarifying semantic obscurities and ambiguities, and simplifying language.
68_34014_4	This approach is based on the premise that achieving good performance ineach of these individual tasks can imply having developed a model capable of understanding language.
14_34098_2	Using true/false labeled Q&A data for fine-tuning and evaluating LLMs on climate-related claims, we compare open-source models, assessing their ability to generate truthful responses to climate change questions.
3_34107_1	In our study, we evaluated LLMs, including Google’s Gemini, across various medical tasks.
19_34123_1	We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios.
43_34147_0	This paper presents a system developed for the Clinical NLP 2024 Shared Task, focusing on reliable text-to-SQL modeling on Electronic Health Records (EHRs).
51_34155_3	This study evaluates small LMs in the medical field using the MEDIQA-CORR 2024 task, which assesses the ability of models to identify and correct errors in clinical notes.
52_34156_5	In addition, we developed a RAG system injected with clinical practice guidelines as an external knowledge datastore.
58_34162_2	We evaluate multiple model variations based on how the training data is used.
59_34163_3	For the MS dataset, which contains subtle errors, we developed a retrieval-based system leveraging external medical question-answering datasets.
1_34172_0	There is growing interest in utilizing large language models (LLMs) in the field of mental health, and this goes as far as suggesting automated LLM-based therapists.
27_34198_2	Initially, we construct a BERT-based model for estimating sentence-level suicide risk and negative sentiment.
6_34205_4	Evaluating the two LMs against fMRI time series via the surprisal complexity metric, the results implicate the superior temporal gyrus.
12_34286_4	In this work, we evaluate word sense disambiguation capabilities of four LLMs: OpenAI’s ChatGPT-3.5, Mistral’s 7b parameter model, Meta’s Llama 70b, and Google’s Gemini Pro.
2_34316_2	To this end, we develop a model trained on a curated dataset consisting of 10 million words, primarily sourced from child-directed transcripts.
4_34318_4	We apply this pipeline to the 100-million-word pre-training dataset from the BabyLM challenge, as well as to standard language and grammatical benchmarks, enabling us to pre-train and evaluate a model using phonemic input representations.
16_34330_0	We present a model for the Strict-Small track of the BabyLM Challenge 2024 (Choshen et al. 2024).
21_34382_4	Second, we propose three criteria to evaluate LLM-as-a-tutor specifically designed for EFL writing education, emphasizing pedagogical aspects.
21_34382_7	This approach lays the groundwork for developing LLMs-as-a-tutor tailored to the needs of EFL learners, advancing the effectiveness of writing education in this context.
22_34383_1	With the vast number of available products and the numerous potential categories, it becomes crucial to develop a classification system capable of assigning products to their correct categories with high accuracy.
22_34383_2	We present a dual-expert classification system that utilizes the power of large language models (LLMs).
5_34390_2	In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley–Terry–Luce (BTL) model and certain generalizations of it.
8_34423_0	This article addresses the question of evaluating generative AI prompts designed for specific tasks such as linguistic linked open data modelling and refining of word embedding results.
4_34489_2	We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality.
46_34531_1	Here, we evaluate 16 different LMs on 10 probing English datasets – 4 template-based and 6 template-free – in general and biomedical domains to answer the following research questions: (RQ1) Do model rankings differ between the two approaches?
156_34641_7	We also evaluate our proposed model in the benchmark dataset of NEWSCLAIMS.
18_34684_1	The current study presents a novel CTG algorithm that enforces adherence toward specific rhetorical relations in an LLM sentence-completion context by a parser-driven decoding scheme that requires no model fine-tuning.
22_34688_4	We exploit this fact and evaluate the LM’s ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the ‘next word prediction’ task.
23_34731_2	In this study, we introduce the LLMeBench framework, which can be seamlessly customized to evaluate LLMs for any NLP task, regardless of language.
27_34759_3	This PhD research aims to fill these gaps by developing a multimodal language model (MMLM) that detects empathy in audiovisual data.
53_34819_3	Two medical translation experts evaluated the GPT-4-generated questions and answers, one focusing on English–European Portuguese, and the other on English–German.
20_34841_1	The ExU project focuses on developing AI-based models for multilingual disinformation analysis, addressing the tasks of rumour stance classification and claim retrieval.
5_34857_3	To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs).
7_34874_5	Results affirm the framework’s effectiveness in creating adaptive agents and suggest LLM-based agents’ potential in navigating dynamic social interactions.
13_34880_3	By systematically evaluating many LLMs across many datasets and different prompting strategies, we show that the more complex System 2-based Implicative Prompts significantly improve over other techniques demonstrating lower mean bias in the outputs with competitive performance on the downstream tasks.
39_34906_4	Recognizing the complexity of this writing task, we built an LLM-powered assistant into the workers’ interface to aid in drafting and refining their comments.
63_34930_2	This work aims to bridge the gap by developing an LLM-based trading agent, CryptoTrade, which uniquely combines the analysis of on-chain and off-chain data.
80_34946_5	We evaluate over 15 different backbone LLMs and non-LLMs.
82_34948_7	Based on ToolAlign, we develop LLMs by supervised fine-tuning and preference learning, and experimental results demonstrate that the LLMs exhibit remarkable tool-calling capabilities, while also refusing to engage with harmful content, and displaying a high degree of autonomy in tool utilization.
108_34973_4	Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data.
163_35027_6	We further integrated these tasks into the PhiloBenchmark, establishing a new standard for evaluating ancient Chinese LLMs address-ing philology tasks.
177_35039_4	We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.
210_35070_3	To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents.
245_35104_4	This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process.
250_35109_3	Specifically, we prompt the LLMs to represent a given text using a single word, and then use the last token’s hidden states and the corresponding logits associated with the prediction of the next token to construct a hybrid document retrieval system.
257_35116_4	Through comprehensive experimentation and analysis of recent commercial or open-source VLLMs, we evaluate the SoV model’s ability to comprehend facial expressions in natural environments.
272_35131_1	We go beyond evaluating LLMs on accuracy; rather, we aim to investigate their token bias in solving logical reasoning tasks.
278_35137_3	We evaluate recent LLMs and neural machine translation systems on DTAiLS, with the best-performing model, GPT-4, achieving from 67 to 85% accuracy across languages.
367_35225_2	To address the challenge of creating training signals for unannotated datasets, we develop a reward model that leverages multimodal environment feedback to automatically generate reward signals.
379_35237_5	In this work, we present LLM-TRes, a logical reasoning framework based on the notion of “theory resolution” that allows for seamless integration of the commonsense knowledge from LLMs with a verifiable logical reasoning framework that mitigates hallucinations and facilitates debugging of the reasoning procedure as well as repair.
381_35239_4	We develop a new LLM architecture composed of multiple sparsely interacting language modelling modules.
383_35241_7	These findings underline the crucial role of intention understanding in evaluating LLMs’ social intelligence and highlight the potential of using social deduction games as a complex testbed to enhance LLM evaluation.
416_35273_5	We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.
453_35307_9	To address these diverse settings, we develop a comprehensive LLM-based framework for constructing leaderboards.
456_35310_3	In this paper, we propose evaluating LLMs’ character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development.
473_35327_2	With advances in LLMs, we hypothesize that unlabeled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised.
516_35370_5	Using Informed and Stylized LLMs, we developed a model to transform these instances into more empathetic language.
538_35392_6	Given these tools, we evaluate LMs’ abilities to reconstruct reference tables, finding this task benefits from additional context to ground the generation (e.g. table captions, in-text references).
647_35497_2	Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs’ ability on solving scientific tasks.
647_35497_5	Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance.
653_35503_2	To this end, this paper presents an enhanced LLM-based recommender (ELMRec).
709_35558_3	To address this, we develop a LLM-based Assistant for Coaching nEgotiation (ACE).
709_35558_5	To build our system, we collect a dataset of negotiation transcripts between MBA students.
764_35610_2	Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations.
833_35677_2	Counterfactual planning that evaluates the model’s reasoning ability over alternative task situations are also under exploited.
842_35685_3	Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
880_35723_5	Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs’ abilities with tool assistance.
887_35730_1	Recently, significant attempts have been made to benchmark datasets and metrics to evaluate LLMs for these traits.
889_35732_5	No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMs for correctness and consistency across various criteria.
906_35749_0	Large language models (LLMs) have played a pivotal role in building communicative AI, yet they encounter the challenge of efficient updates.
940_35781_1	Most existing LLM benchmarks evaluate LLMs on i.i.d. tasks, overlooking their ability to learn iteratively from past experiences.
940_35781_3	LLM-Evolve evaluates LLMs over multiple rounds, providing feedback after each round to build a demonstration memory that the models can query in future tasks.
942_35783_2	Our objectives include evaluating the performance of state-of-the-art GEC systems on an authentic CSW dataset from English as a Second Language (ESL) learners, exploring synthetic data generation as a solution to data scarcity, and developing a model capable of correcting grammatical errors in monolingual and CSW texts.
966_35806_3	To ensure that the output sentence remains faithful to the input sentence, we design a minimal distortion model that utilizes pronunciation or shape similarities between the original and replaced characters.
993_35833_1	While existing research seeks to enhance RAG performance by retrieving higher-quality documents or designing RAG-specific LLMs, the internal mechanisms within LLMs that contribute to RAG’s effectiveness remain underexplored.
1010_35849_6	On the basis of DSKD, we further develop a cross-model attention mechanism, which can automatically align the representations of the two models with different vocabularies.
1111_35948_6	Then, we develop a LLM-based framework for large-scale subjective evaluation (i.e., identifying errors) and an objective metric, PuzzleEval, to evaluate the correctness of reasoning chains.
1143_35978_0	Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial).
1183_36017_3	In this work, we present the first German single-language RoBERTa model, GottBERT, pre-trained exclusively on the German portion of the OSCAR dataset.
35_36136_3	Here, we present one such system, LLM-DetectAIve, designed for fine-grained detection.
38_36138_4	In this paper, we present an AI agent designed specifically for error resolution in a computational notebook.
38_36138_5	We have developed an agentic system capable of exploring a notebook environment by interacting with it—similar to how a user would—and integrated the system into the JetBrains service for collaborative data science called Datalore.
40_36140_1	To enhance the performance of LLMs in scientific literature services, we developed the scientific literature LLM (SciLit-LLM) through pre-training and supervised fine-tuning on scientific literature, building upon the iFLYTEK Spark LLM.
40_36140_2	Furthermore, we present a knowledge service system Spark Research Assistant (SparkRA) based on our SciLit-LLM.
45_36197_2	This paper introduces a methodology for developing a specialized Telecommunications LLM (Telco LLM) designed to enhance the efficiency of customer service agents and promote consistency in service quality across representatives.
45_36197_4	We also evaluate various LLMs and demonstrate the ability to benchmark both proprietary and open-source LLMs on predefined telecommunications-related tasks, thereby establishing metrics that define telcommunications performance.
72_36222_4	In this paper, we present the HeAL benchmark (HEalth Advice in LLMs), a health-advice benchmark dataset that has been manually curated and annotated to evaluate LLMs’ capability in recognizing health-advice - which we use to safeguard LLMs deployed in industrial settings.
77_36227_8	These findings shed light on developing suitable LLM fine-tuning methods for return prediction-based portfolio construction.
83_36233_4	We create a dataset containing over 400 icons with their ground-truth descriptions and use it to evaluate model-generated descriptions across several performance metrics.
91_36241_1	Specifically, we evaluate LLMs’ performance when restricted to adhere to structured formats versus generating free-form responses across various common tasks.
94_36244_4	We design a unique threat model which leverages the LLM sycophancy effect and elevates the average attack success rate (ASR) from 17.7% to 86.2% in a multi-turn setting.
96_36246_4	We introduce offline metrics to evaluate student LLMs.
98_36248_0	We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses.
1_36272_4	Instead of developing general-purpose LLMs, how to endow LLMs with specific knowledge?
10_36295_2	This paper presents our system designed to address this issue.
27_36309_3	We evaluate the LLMs’ predictive accuracy on five CD/CW datasets from diverse domains, using corresponding annotation guidelines in prompts.
52_36389_3	The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.
31_36522_6	Building on this foundation, we outline several research problems in this area, aiming to guide future explorations towards building more responsible and accountable LLMs.
34_36525_2	The study also presents GraphTMI, a novel benchmark for evaluating LLMs in graph structure analysis, focusing on homophily, motif presence, and graph difficulty.
53_36544_2	In this paper, we propose a novel and valuable method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs more cleanly.
69_36558_5	We trained an SELF-EXPERTISE augmented instruction dataset on the LLaMA-2 7B model to construct Korean legal specialized model, called LxPERT.
85_36574_3	Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs’ reliability in using new evidence for answering.
85_36574_8	The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.
87_36576_3	We systematically evaluate 10+ leading LLMs as well as OpenAI’s legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings.
110_36599_5	Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model.
139_36628_7	We present the Plug-in Language Model (PiLM) as a solution to address the limitations.
144_36633_0	Making moral judgments is an essential step toward developing ethical AI systems.
176_36663_3	We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4.
192_36679_2	To evaluate LLMs’ capability of logical fallacy understanding (LFU), we propose five concrete tasks from three cognitive dimensions of WHAT, WHY, and HOW in this paper.
271_36755_1	Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations.
35_36812_3	To address the above issue and develop legal LLM, we propose to detect the pre-training data from LLM in a pure black-box way because the existing LLM services only return the generated text.
58_36835_1	To address this gap, we propose a new dataset, natural language inference on economic events (EconNLI), to evaluate LLMs’ knowledge and reasoning abilities in the economic domain.
58_36835_2	We evaluate LLMs on (1) their ability to correctly classify whether a premise event will cause a hypothesis event and (2) their ability to generate reasonable events resulting from a given premise.
128_36904_1	However, the previous research mainly focus on basic sentiment analysis tasks, such as emotion recognition, which is not enough to evaluate LLMs’ overall emotional intelligence.
128_36904_5	We also design two metrics to evaluate LLMs’ capabilities in recognition and response for emotion-related statements.
131_36907_4	Therefore, in this paper, we introduce a novel game named BrainKing based on the “Who is undercover” and “Twenty Questions” for evaluating LLM capabilities under incomplete information scenarios.
154_36930_3	We conduct extensive experiments to evaluate existing LLMs and approaches on Knowledge Crosswords.
194_36968_1	While numerous benchmarks have been proposed to evaluate LLMs’ capabilities, there is a notable absence of a dedicated benchmark for assessing their musical abilities.
196_36970_4	We manually craft 3 scripts, Detective Conan, Harry Potter, Romeo and Juliet, and design a 5-dimension principle to evaluate the drama LLM comprehensively.
235_37009_1	To meet this crucial need, we propose SALAD-Bench, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods.
259_37032_2	To address this issue, we present UltraTool, a novel benchmark designed to improve and evaluate LLMs’ ability in tool utilization within real-world scenarios.
264_37037_4	Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model’s reasoning.
277_37050_2	In this study, we present SPIN: a model-agnostic framework that sparsifies and integrates internal neurons of intermediate layers of LLMs for text classification.
296_37069_6	To better evaluate the model’s capabilities, we manually construct a multi-level MVQA evaluation benchmark named MLe-Bench.
382_37152_3	We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models.
416_37185_0	We introduces ***LLaST***, a framework for building high-performance Large Language model based Speech-to-text Translation systems.
462_37227_1	Despite the growing integration of LLMs and education, the absence of a dedicated benchmark for evaluating LLMs within this domain presents a pressing concern.
485_37250_2	To address these concerns, we propose to evaluate LLMs by designing new tasks, automatically generating evaluation datasets for the tasks, and conducting detailed error analyses to scrutinize LLMs’ adaptability to new tasks, their sensitivity to prompt variations, and their error tendencies.
490_37255_6	This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.
501_37266_5	We use StudentEval to evaluate 12 Code LLMs and find that StudentEval is a better discriminator of model performance than existing benchmarks.
517_37282_7	We also design an LLM-based approach to automatically and accurately evaluate the responses from Video LLMs.
530_37295_8	We report the correlations that we find for 4 cutting-edge LLMs.
544_37308_5	By leveraging the LLM incorporating the advanced sampling strategies, we design a sampling algorithm for atomic mentions and train the recall model using contrastive learning.
572_37335_6	Overall, our study reveals limitations in current models’ processing of sentence semantics and highlights the importance of using naturalistic data and communicative scenarios when evaluating LMs’ language capabilities.
586_37348_5	We also find variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation.
606_37368_2	In this work, we explore an alternative strategy of continual pre-training as a means to develop domain-specific LLMs over an existing open-domain LLM.
606_37368_7	Our work proposes an alternative solution to building domain-specific LLMs cost-effectively.
672_37434_2	Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs.
673_37435_3	We also design diverse prompts to thoroughly evaluate eleven representative LLMs.
711_37472_3	We go beyond previous studies, which only examined GPT* models on a selection of English GEC datasets, by evaluating seven open-source and three commercial LLMs on four established GEC benchmarks.
715_37476_3	We propose an approach, **L**LMs-as-**C**orrector for **E**vent **E**xtraction (**LC4EE**), aiming to leverage the superior extraction capability of SLMs and the instruction-following ability of LLMs to construct a robust and highly available EE system.
754_37515_2	Designing adequate LLM benchmarks is challenging:
791_37549_2	To help mitigate this problem, we introduce the Raccoon benchmark which comprehensively evaluates a model’s susceptibility to prompt extraction attacks.
858_37615_5	Our method entails modifying prompts in LLMs to develop an amateur model and a professional model.
874_37631_8	This paves the way to build open-vocabulary LLMs that operate on perceptual input only and calls into question the necessity of the usual symbolic input representation, i.e., text as (sub)tokens.
904_37661_0	High-quality conversational datasets are essential for developing AI models that can communicate with users.
928_37685_1	We propose CToolEval, a benchmark designed to evaluate LLMs in the context of Chinese societal applications, featuring 398 APIs across 27 widely-used Apps (e.g., Apps for shopping, map, music, travel, etc.) that cover 14 domains.
928_37685_3	Our extensive experiments with CToolEval evaluate 11 LLMs, revealing that while GPT-3.5-turbo excels in tool invocation, Chinese LLMs usually struggle with issues like hallucination and a lack of comprehensive tool understanding.
960_37716_6	Our findings show that model editing serves as a cost-effective tool for topical red-teaming by methodically applying targeted edits and evaluating the resultant model behavior.
14_37745_7	To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM.
50_37780_0	Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation.
95_37825_2	In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains.
111_37841_5	We evaluate various LLMs fine-tuned on this dataset, using traditional metrics, LLM-based evaluation, and human annotation.
123_37853_3	In this paper, we present IntentionQA, a double-task multiple-choice question answering benchmark to evaluate LMs’ comprehension of purchase intentions in E-commerce.
172_37900_2	To explore the ability of large language models to control the length of generated responses, we propose the Target Length Generation Task (TLG) and design two metrics, Precise Match (PM) and Flexible Match (FM) to evaluate the model’s performance in adhering to specified response lengths.
173_37901_3	We used GEST to evaluate English and Slavic masked LMs, English generative LMs, and machine translation systems.
178_37906_4	Using this dataset, we evaluated several LLMs and discovered that their proficiency in this aspect is limited, regardless of whether the knowledge is trained in a separate or adjacent training settings.
204_37931_2	In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model’s familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts under the zero-resource setting, where external ground-truth or background information is not available.
230_37956_2	To address this gap, we present CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.
319_38043_4	First two datasets were developed in close collaboration with lawyers to evaluate LLMs in practical scenarios in a certified manner.
323_38047_3	We develop two tasks for evaluating LMs’ abilities to assess math problems: (1) verifying whether a problem aligns with a given standard, and (2) tagging a problem with all aligned standards.
400_38118_4	This study aims to evaluate generative LLMs, employed through prompt engineering, for few-shot clinical NER.
406_38124_6	We collect, annotate, and integrate the Pcl-PT/SFT dataset, and then develop a bilingual PclGPT-EN/CN model group through a comprehensive pre-training and supervised fine-tuning staircase process to facilitate implicit toxic detection.
445_38161_1	In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs’ understanding of legal theories and reasoning capabilities.
458_38173_3	To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs’ ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants.
519_38232_4	In it, we design a reward model to filter out the noisy knowledge and take the marginal consistent reasoning module to reduce invalid reasoning.
592_38304_5	We find that powerful LLMs are aware of the certainty of their prediction and can achieve high agreement with ground truth on high-certainty samples, indicating a promising approach for building reliable and scalable proxies for evaluating LLM personalization.
592_38304_6	Our human annotation reveals that third-person crowd worker evaluations of personalized preferences are even worse than LLM predictions, highlighting the challenges of evaluating LLM personalization.
614_38326_1	In this paper, we study building language-specific LLMs by adapting monolingual and multilingual LLMs.
614_38326_6	Together, our work lays foundations on efficiently building language-specific LLMs by adapting existing LLMs.
715_38423_2	We evaluate several open-source LLMs, as well as GPT-4, using Chain of Thought prompting and expert human assessment.
722_38430_7	We further analyze the robustness behavior and error reasons of LLMs in our benchmark, which provide valuable insights for researchers to understand the LLM’s robustness in task completion and develop more robust LLMs and agents.
724_38432_2	However, developing high-performance kernels for weight-quantized LLMs presents substantial challenges, especially when the weights are compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform, lookup table (LUT) quantization.
739_38446_4	Inspired by prior research in psychology and cognitive science, we develop a dataset containing 13,465 prompts to evaluate LLM decisions on different cognitive biases (e.g., prompt-induced, sequential, inherent).
760_38467_4	We evaluate encoder-only and generative LMs by calculating a metric based on the similarity score between distributed representations of molecules and their augmentations.
782_38489_3	We evaluate various LLMs, both open-source and commercial, to assess their performance in understanding and generating emphasis.
801_38508_3	We evaluate existing LLMs using three types of data that exhibit different time constraints.
841_38547_0	To combat the misuse of Large Language Models (LLMs), many recent studies have presented LLM-generated-text detectors with promising performance.
870_38575_9	Furthermore, we evaluate LLMs’ ability to assess CFs in a mislabelled data setting, and show that they have a strong bias towards agreeing with the provided labels.
875_38580_2	Despite the wide use of large language models (LLMs) in numerous domains and their growing proficiency in Chinese, there is a notable lack of datasets to thoroughly evaluate LLMs’ ability to handle ambiguity in Chinese.
913_38617_5	To evaluate the model performance, we introduce two metrics: Algorithm Identification Accuracy (AIA) for correct answer rationale identification, and Malgorithm Identification Accuracy (MIA) for incorrect answer rationale identification.
913_38617_8	These findings have important implications for developing LLMs with improved counterfactual reasoning, particularly relevant for AI-powered tutoring systems, where identifying and addressing student misconceptions is essential.
942_38646_3	We quantitatively and qualitatively evaluate model outputs when an explicit cue of culture, specifically nationality, is perturbed in the prompts.
946_38650_2	They require anyone wishing to evaluate his language model to submit the model’s predictions for centralized processing and then publish the model’s result on their leaderboard.
996_38700_0	Driven by the surge in code generation using large language models (LLMs), numerous benchmarks have emerged to evaluate these LLMs capabilities.
4_38790_2	We present a test generation pipeline to evaluate LLMs as conversational AI agents.
4_38790_5	Additionally, we put forward ALMITA, a manually curated dataset for evaluating AI agents in customer support, and use it to evaluate existing LLMs.
4_38790_7	While our focus is on customer support, our test generation pipeline is general enough to evaluate different AI agents.
9_38795_2	We also propose three methods to evaluate LLMs on these inferences out of context, where there is a distribution of human-like answers rather than a single correct answer.
1_38824_2	We conduct the first study to evaluate LLMs on a novel task of generating acoustically intelligible paraphrases for better human speech perception in noise.
6_38829_7	We discuss how our framework can be extended to a broader set of social signals, personas, and scenarios to evaluate LLM behaviors under various conditions.
10_38840_1	For this paper, we evaluated a QAG (question-answer generation) system centered on English children’s storybooks that was presented in a previous research, by using human evaluators for the study.
14_38870_1	We are motivated to evaluate language model’s (LMs) capabilities in many real-world domains due to their significant potential.
21_38877_2	To address these issues, we developed a classification system to differentiate between human-written and AI-generated texts using a diverse HC3-English dataset.
23_38965_2	In this paper, we discuss our motivation for building this system and the human evaluation we conducted, comparing the generated highlights against the source input to assess the degree of hallucinations and/or contradictions present.
8_39010_4	We then fully evaluated the most promising system for each scenario: (i) LLM prompting in English followed by translation, and (ii) LLM PEFT-tuning in English followed by translation.
4_39248_3	Additionally, it is presented a Knowledge Graph Q&A System powered by Generative AI.
4_39271_3	Developed collaboratively through ChatGPT, MultiAPI consists of 187 diverse API calls and 1,799 contextual prompts, offering a unique platform evaluation of tool-augmented LLMs handling multimodal tasks.
4_39287_2	In line with these works, we evaluate the LLM”s extrapolation ability in the chemical domain.
6_39396_4	Our findings highlight the potential of few-shot learning for improving XLS performance and the need for further research in designing LLM architectures and pre-training objectives tailored for this task.
196_39606_2	This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP).
197_39607_5	To enable a thorough comparison, we evaluate LLMs against existing task-specific fine-tuned models.
244_39654_7	Finally, we evaluate GPT-4 on a dataset consisting of six other languages for span detection, and results suggest that the model struggles with the task across languages.
251_39661_2	The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ’s efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English.
260_39670_5	Extensive experiments demonstrate the effectiveness of the dataset in evaluating model bias, with all 12 publicly available Chinese large language models exhibiting strong bias in certain categories.
269_39679_3	Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings.
269_39679_9	The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents.
276_39686_6	We conduct a series of experiments on 11 datasets to evaluate ChatGPT’s commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again.
281_39691_2	In light of this, we construct MorphEval, a morpheme-informed benchmark, including three datasets following the bottom-up levels of characters, words, and sentences in Chinese, and then evaluate representative LLMs with both zero- and few-shot settings under two metrics.
314_39724_3	To this end, we present the first discourse-aware multimodal task-oriented dialogue system that combines discourse theories with offline LLM generation.
374_39784_6	Specifically, we evaluated the combined system on a French treebank named Sequoia, which features an annotation layer encompassing all syntactic types of French MWEs.
388_39798_3	We introduce a novel benchmark, and an open LLM Leaderboard, designed to evaluate LLMs’ performance in Italian, providing a rigorous framework for comparative analysis.
596_40006_6	We believe our findings contribute to evaluating LLMs’ code execution abilities and would encourage further investigation and application for the computation power of LLMs.
641_40051_5	By evaluating GPT-4’s performance using human annotations as ground truths, we show that it can reduce resources required by dataset annotation while barely losing any important information.
695_40105_0	This paper details the process of developing the first native large generative language model for the North Germanic languages, GPT-SW3.
758_40168_2	This study evaluates Instruct-LLMs on popular benchmark datasets for IC and SF, emphasizing their capacity to learn from fewer examples.
778_40188_5	We propose a simple labeling principle and a deep neural network to build the model, embedding the outputs as prompts into the extraction process of the large model.
813_40223_5	Subsequently, we construct a reward model to learn the rank and optimize our generative policy.
916_40326_1	However, the existing benchmarks for comprehensively evaluating these LLMs are still insufficient, particularly in terms of measuring knowledge that LLMs capture.
930_40340_4	We evaluated the fine-tuned LLM, LlamaCare, on various clinical tasks, such as generating discharge summaries, predicting mortality and length of stay, and more.
1018_40428_4	In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data.
1018_40428_5	We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark.
1079_40489_5	Furthermore, we conduct extensive experiments on NutFrame to evaluate various widely-used LLMs.
1090_40500_1	Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings.
1095_40505_1	Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs).
1146_40556_7	This finding serves as a warning that intrinsic bias test sets, based either on language modeling tasks or next sentence prediction, should not be the only benchmark in developing a debiased language model.
1202_40612_3	We build LMs which leverage rich contextual information to reduce perplexity by up to 6.5% compared to a non-contextual model, and generalise well to a scenario with no speaker-specific data, relying on combinations of demographic characteristics expressed via metadata.
1209_40619_4	We evaluate our model on two public datasets for real-world news claim verification, and the results demonstrate that FFRR achieves significant improvements over strong LLM-enabled and non-LLM baselines.
3_41057_2	In this paper, we propose the Semantically Rich Variable Substitution Method (SemRiVas) as an enhancement to existing symbolic methodologies for evaluating LLMs on Mathematical Word Problems (MWPs).
15_41105_2	In this work, we address these issues by developing a pipeline for adaptation of English-oriented pre-trained models to other languages and constructing efficient bilingual LLMs.
21_41111_3	Our approach includes two methodologies: (i) adapting existing LLMs originally pretrained in English to understand Turkish, and (ii) developing a model from the ground up using Turkish pretraining data, both supplemented with supervised fine-tuning on a novel Turkish instruction-tuning dataset aimed at enhancing reasoning capabilities.
26_41116_5	The potential applications of the proposed approach span i) detecting ambiguous sentences, ii) fine-tuning existing multilingual LLMs to preserve ambiguous information, and iii) developing AI systems that can generate ambiguity-free languages when needed.
79_41226_2	Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance.
97_41244_4	Therefore, to build a more reliable multimodal sarcasm detection model, we propose a generative multimodal sarcasm model consisting of a designed instruction template and a demonstration retrieval module based on the large language model.
123_41270_3	Specifically, we consider two distinct scenarios: 1) Online Racing, featuring multiple LLMs/agents to facilitate direct competition and comparison; 2) Offline Probing, constructing targeted questions with verified ground truth to evaluate LLMs’ strategic behaviors.
135_41282_3	We introduce a new dataset and a grounding metric to evaluate model capability under the definition.
213_41360_7	We then evaluate various multilingual LLMs on our dataset and metrics to probe their internal knowledge and use the proposed metrics to discover numerous inconsistencies in how these models respond in different languages.
283_41430_2	To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding.
290_41437_0	We evaluated GPT-4 in a public online Turing test.
302_41449_1	To study the ability of language models for these time-dependent dynamics in human language, we introduce a novel task, EvolvingQA, a temporally evolving question-answering benchmark designed for training and evaluating LMs on an evolving Wikipedia database.
319_41466_3	This paper presents an empirical study evaluating these LLMs using the TREC Fair Ranking dataset, focusing on the representation of binary protected attributes such as gender and geographic location, which are historically underrepresented in search outcomes.
319_41466_5	We assess fairness from both user and content perspectives, contributing an empirical benchmark for evaluating LLMs as the fair ranker.
341_41488_2	Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems.
21_41655_2	We then create a new dataset to evaluate LM generalization ability in the setting where training data contains additional information that is not causally relevant.
20_41729_6	We evaluate 14 current mainstream LLMs and conduct a comprehensive discussion and analysis of their results.
3_41733_3	Then, we fine-tune and evaluate LLMs on these datasets.
27_41757_2	In this thesis proposal, I will present my work on (1) benchmarking cross-task generalization abilities with diverse NLP tasks; (2) developing model architectures for improving cross-task generalization abilities; (3) analyzing and predicting the generalization landscape of current state-of-the-art large language models.
11_41831_0	We present a large language model (LLM) based approach for comparing legal contracts with their corresponding template documents.
26_41845_0	This paper presents our system description and error analysis of our entry for NLLP 2024 shared task on Legal Natural Language Inference (L-NLI).
8_41861_0	High-quality conversational datasets are essential for developing AI models that can communicate with users.
44_41905_3	To answer this question, we evaluate 9 LLMs in the task of MT with 4 Coptic and 4 Ancient Greek ostraca into English using 6 NLP metrics.
28_41961_1	To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE).
4_41967_3	Additionally, we present the PsychoLexLLaMA model, optimized specifically for psychological applications, demonstrating superior performance compared to general-purpose models.
11_41974_5	We hope our work will set the stage for developing a new task of dream generation for LLMs.
2_42024_5	We evaluate four recent instruction-tuned LLMs as “annotators” on five subjective tasks across four languages.
6_42152_1	To support clinical assessment and research, we developed Open Brain AI (https://openbrainai.com).
2_42193_1	This is especially crucial when evaluating AI systems for safety, where accounting for such diversity in interpretations and potential impacts on human users will make them both more successful and inclusive.
5_42206_5	Through reward and preference model-based evaluations, we find that these interactions serve as useful training data and create more helpful downstream assistants.
34_42274_0	In this paper, we present our system for the SemEval-2024 Task 7, i.e., NumEval subtask 3: Numericial Reasoning.
36_42276_2	In this work, we developed a system SHTL, which means simulate human thinking capabilities by Large Language Model (LLM).
69_42309_4	Finally, we presented an AI model that can distinguish AI-generated texts from human-written ones with high accuracy on both multilingual and monolingual tasks using the M4 dataset.
91_42331_2	Evaluating LLMs in miscellaneous reasoning tasks remains an active area of research.
161_42401_4	For this reason, we evaluate several LLMs that aim to extract valuable multilevel information (such as lexical, semantic, and syntactic) from the text in their training processing.
187_42427_0	In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge.
194_42434_0	This paper presents an artificial intelligence model designed to detect semantic relationships in natural language, addressing the challenges of SemEval 2024 Task 1.
205_42445_0	Participating in SemEval 2024 Task 2, we built a three-module system to predict entailment labels for NLI4CT, which consists of a sequence of the query generation module, the query answering module, and the aggregation module.
210_42450_5	These findings contribute to understanding and improving AI methods for detecting machine-generated text, allowing us to build more robust and traceable AI systems in the language ecosystem.
218_42458_1	This paper presents our system for the SemEval 2024 Task 9 competition and also investigates the efficacy of fine-tuning language models (LMs) on BrainTeaser—a benchmark designed to evaluate NLP models’ lateral thinking and creative reasoning abilities.
223_42463_0	This paper presents our system development for SemEval-2024 Task 3: “The Competition of Multimodal Emotion Cause Analysis in Conversations”.
226_42466_1	Our project targets the challenges of subtask 2, dedicated to Multimodal Emotion-Cause Pair Extraction with Emotion Category (MECPE-Cat), and constructs a dual-component system tailored to the unique challenges of this task.
233_42473_2	In this paper, we present our system for SemEval-2024 Task 9’s BrainTeaser challenge, which requires language models to answer brain teaser questions that typically involve lateral reasoning scenarios.
244_42484_5	By ensembling all of these models, we developed a final model that outperforms all other baselines.
255_42495_0	In this article, we present an effective system for semeval-2024 task 5.
255_42495_3	In this task, we designed a self-eval LLM system that simultaneously performs reasoning and self-assessment tasks.
62_42592_2	In such cases, it is crucial not to just end with failure but to correct and recover the dialogue to turn it into a success for building a robust goal-oriented dialogue system.
20_42615_0	This paper presents the winning system participating in the ACL 2024 workshop SIGHAN-10 shared task: Chinese dimensional aspect-based sentiment analysis (dimABSA).
5_42629_5	Our Indexical Shift Dataset consists of 156 multiple-choice questions, each annotated with necessary linguistic details, to evaluate LLMs in a few-shot setting.
14_42647_1	However, evaluating the intersection of these two skills—multilingual few-shot reasoning—is difficult: even relatively low-resource languages can be found in large training corpora, raising the concern that when we intend to evaluate a model’s ability to generalize to a new language, that language may have in fact been present during the model’s training.
3_42655_0	To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect.
8_42710_2	We develop a system composed of an ADE extraction module and an ADE normalization module which furtherly includes a retrieval module and a filtering module.
11_42713_1	We built our system on the basis of GLM, a pre-trained large language model with few-shot Learning capabilities, using a two-step prompting strategy to extract adverse drug event (ADE) and an ensemble method for normalization.
3_42783_4	To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers.
50_42830_4	We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues.
95_42875_2	We present RUFF, a carefully designed dataset of over 5 million instances to measure robust pronoun fidelity in English, and we evaluate 37 model variants from nine popular families, across architectures (encoder-only, decoder-only, and encoder-decoder) and scales (11M-70B parameters).
5_42880_0	This paper presents a language model trained from scratch exclusively on a brand new corpus consisting of about 6 GiB of Uruguayan newspaper text.
5_42880_2	We evaluated the model on two NLP tasks and found that it outperforms BETO, the widely used Spanish BERT pre-trained model.
15_42921_0	This work describes an approach to develop Knowledge Graph Question Answering (KGQA) system for TextGraphs-17 shared task.
10_43013_0	We present our proposed system named Sherlock to UNLP 2024 Shared Task on Question Answering winning first place.
11_43030_4	Additionally, we introduce ŠarišCOPA, a new dataset for causal common sense reasoning, which, alongside SlovakCOPA, serves to evaluate LLM’s performance in a zero-shot framework.
19_43038_2	We utilize and evaluate the GPT-4 model in combination with various prompts engineering and the Retrieval-Augmented Generation (RAG) technique.
44_43085_0	This paper presents a detailed system description of our entry which finished 1st with a large lead at WASSA 2024 Task 2, focused on cross-lingual emotion detection.
1_43123_3	We evaluate system outputs with professional human annotators using a new protocol called Error Span Annotations (ESA).
2_43124_6	The results strongly confirm the results reported last year, that fine-tuned neural metrics continue to perform well, even when used to evaluate LLM-based translation systems.
6_43304_0	My research theme is to develop an optimal analytical model for various information generated during therapy using multimodal data in psychotherapy, to elucidate the process of psychotherapy, and to create an AI therapist to develop a new psychotherapy.
19_43317_2	During my master’s program, I developed a system that uses an interview dialogue system to support user review writing.
11_43341_3	It introduces a new metric, the “Cultural Sensitivity Score”, to evaluate the model’s ability to adjust responses based on dialectal differences.
5_43351_2	We introduce SchNovel, a benchmark to evaluate LLMs’ ability to assess novelty in scholarly papers, a task central to streamlining discovery pipeline.
7_43418_3	To assist non-native moderators, we present LLM-C3MOD, a human-LLM collaborative pipeline with three steps: (1) RAG-enhanced cultural context annotations; (2) initial LLM-based moderation; and (3) targeted human moderation for cases lacking LLM consensus.
23_43460_0	This paper presents a detailed system description of our entry for the CHiPSAL 2025 challenge, focusing on language detection, hate speech identification, and target detection in Devanagari script languages.
34_43471_4	We evaluate multiple LLMs on the Devanagari dataset provided by Thapa et al.
2_43479_3	We present an LLM with both the text and the spoken form of a joke, generated using an off-the-shelf text-to-speech (TTS) system.
15_43509_3	This work evaluates an open-source LLM for lay translation in this data-scarce environment using datasets of German synthetic clinical documents and real tumor board protocols.
7_43537_4	We evaluated multiple LLMs against human assessments, exploring various prompting strate- gies to optimize performance and fine-tuning the models using a subset of the collected data to enhance accuracy.
4_43561_0	This study examines the use of Natural Language Processing (NLP) technology within the Islamic domain, focusing on developing an Islamic neural retrieval model.
5_43571_4	This preliminary analysis helps to inform further research in this field, pro- viding a simple ranking of publicly-available models, and indicating which language features require particular attention when evaluating model capacity.
6_43575_5	We propose a novel humor detection metric designed to evaluate LLMs amongst various prompts on their capability to extract humorous punchlines.
79_43672_7	We conclude that there is a significant need for a Bengali-oriented LLM, but the field currently lacks the high-quality pretraining and instruction-tuning datasets necessary to develop a highly effective model.
98_43691_6	We evaluate LLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark for household rearrangements, demonstrating a more than 30 percent increase in success rate over existing LLM planners, showcasing significantly improved alignment with human preferences.
106_43699_1	To this end, we have been developing a slot-filling dialogue system that engages in pre-interview to collect information on staff careers as a preparatory step before the actual interviews.
159_43752_5	Experimental results indicate that with the increase of model size, although the ease-of-use could be significantly improved, there is still a long way to go to build a sufficiently user-friendly model.
239_43832_5	We implement and evaluate several LLM-based generation strategies, and discover that AI-generated and human-written counterspeech can be easily distinguished by both simple classifiers and humans.
244_43837_7	In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria.
253_43846_3	Using Counting-Stars, we conducted experiments to evaluate several long-context LLMs, including GPT-4 Turbo, Gemini 1.5 Pro, Claude3 Opus, GLM-4, and Moonshot-v1.
283_43876_3	We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects.
325_43918_2	We argue that single-turn question-answering tasks such as MultiMedQA are insufficient for evaluating LLMs’ medical consultation abilities.
329_43922_3	We ask whether structural information can be extracted from LLM’s and develop a model that integrates it with their learnt statistics.
340_43933_3	To address this issue, we introduce TMATH, a dataset specifically designed to evaluate LLMs’ ability to generate high-quality hints for MWPs.
390_43983_5	This observation suggests that LLMs might regard multiple options as correct, which could undermine the reliability of MCQA as a metric for evaluating LLMs.
395_43988_2	Furthermore, the resources for evaluating Japanese biomedical LLMs are insufficient.
450_44043_4	We exhaustively evaluate this algorithm across a variety of state-of-the-art datasets, accuracy metrics, and challenging NLP tasks.
457_44050_6	Results confirm that our approach not only reduces dependency on conventional data but also provides a targeted and efficient means of evaluating LLM robustness in constrained domains.
464_44057_6	Technically, we define the problem of constructing an effective LLM services invocation strategy, and based on this, propose a unified LLM service invocation framework.
529_44122_7	Our contribution includes a 73,500 prompts dataset constructed with a taxonomy of real-world occupations and a multi-step verification framework to evaluate model’s behavior regarding gender stereotype.
567_44160_2	We apply our approach to quantitatively evaluate LLMs—namely Llama 2, GPT-3.5, and GPT-4—against the cultural dimensions of regions like the United States, China, and Arab countries, using different prompting styles and exploring the effects of language-specific fine-tuning on the models’ behavioural tendencies and cultural values.
580_44173_1	By evaluating LLMs using two multilingual datasets on simile and idiom interpretation, we explore the effectiveness of various prompt engineering strategies, including chain-of-thought, few-shot, and English translation prompts.
594_44187_4	We evaluate the model’s performance at each stage on generation and classification tasks.
634_44227_0	Evaluating LLM-generated text has become a key challenge, especially in domain-specific contexts like the medical field.
634_44227_4	By examining multiple LLM-generated arguments, we establish a methodology for determining whether a Proxy Task is suitable for evaluating LLM-generated medical explanatory arguments, requiring only five examples and two human experts.
646_44239_3	First, we evaluate LLM performance in Zero- and Few-Shot Learning settings, with and without using Chain-of-Thought prompting.
652_44245_0	Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks.
681_44274_2	We takes Moklen as a case study to evaluate the efficacy of LLMs in producing coherent grammatical rules and lexical entries using only bilingual dictionaries and parallel sentences of the unknown language without building the model from scratch.
699_44292_4	We evaluate 33 existing LLMs on IberoBench on 0- and 5-shot settings.
716_44309_4	We evaluate legal LLMs across three levels of capability, each reflecting a progressively more complex stage of legal syllogism: fundamental information retrieval, legal principles inference, and advanced legal applications, and encompassing a wide range of tasks in different legal scenarios.
8_44358_1	We develop a feedback simulation system that generates public responses considering demographic distributions.
10_44360_5	This paper presents EasyJudge, a model developed to evaluate significant language model responses.
24_44396_3	In addition, when evaluating LLMs on internal customer data, an on-premise evaluation system is necessary to protect customer privacy rather than sending customer data to third-party APIs for evaluation.
24_44396_4	In this paper, we demonstrate how we build an on-premise system for LLM evaluation to address the challenges in the evaluation of LLMs in real-world industrial settings.
37_44409_5	To fulfill this goal, we first construct an automatic data collection system with seed datasets generated from both public repositories and our in-house datasets.
13_44469_0	This study presents a hybrid model integrating TamilXLM-RoBERTa and MalayalamXLM-RoBERTa with BiLSTM and attention mechanisms to classify AI-generated and human-written product reviews in Tamil and Malayalam.
35_44491_2	In this study, we explore various feature extraction techniques, including TF-IDF, Count Vectorizer, and transformer-based embeddings such as BERT-Base-Multilingual-Cased and XLM-RoBERTa-Large, to build a robust classification model.
71_44527_6	We have conducted a comparative study among all the models by training and evaluating each model on the dataset.
116_44699_4	Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM’s one aspect of robustness.
125_44706_6	SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement.
160_44740_5	Moreover, we develop LLM-based humanoid agents to simulate users in social media, and propose prototype-based attackers to replicate poisoning attacks.
191_44771_7	These findings underscore the necessity of evaluating LLMs and developing corresponding safety alignment strategies in a complex, multilingual context to align with their superior cross-language generalization capabilities.
195_44775_1	In this paper, we share our experiences in developing and operating generative AI models within a national-scale search engine, with a specific focus on the sensitiveness of user queries.
195_44775_3	We believe that our experiences in launching generative AI search systems can contribute to reducing the barrier in building generative LLM-based services.
197_44776_6	We further evaluate 17 popular LLMs, including both commercial and open-source ones, on TestEval.
222_44801_3	Our approach evaluates LLM agents’ ability to detect and appropriately respond to norm-violating user queries and observations.
230_44809_3	MojoBench includes HumanEval-Mojo, a benchmark dataset designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM pretrained and finetuned for Mojo code generation, which supports instructions in 5 natural languages (NLs).
253_44831_3	To bridge these gaps, we outline current Cantonese NLP methods and introduce new benchmarks designed to evaluate LLM performance in factual generation, mathematical logic, complex reasoning, and general knowledge in Cantonese, which aim to advance open-source Cantonese LLM technology.
260_44838_5	Furthermore, we discovered that when evaluating LLMs with similar performance, the performance of the automatic LLM bencher declines sharply, underscoring the limitations of current benchers and calling for future work.
293_44870_4	When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries.
306_44883_1	Specifically, we develop a task-agnostic framework for evaluating a system’s ability to determine when to ask for clarification.
313_44889_4	To address this gap, this study focuses on developing a specialized LLM, LlamaLens, for analyzing news and social media content in a multilingual context.
314_44890_1	We evaluate LLMs for bias in the personalized educational setting, specifically focusing on the models’ roles as “teachers.”
365_44939_3	A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs’ performance in dynamic legal scenarios.
418_44990_2	We introduce CounselingBench, a novel NCMHCE-based benchmark evaluating 22 general-purpose and medical-finetuned LLMs across five key competencies.
418_44990_5	These findings highlight the challenges of developing AI for mental health counseling, particularly in competencies requiring empathy and nuanced reasoning.
432_45004_4	For further exploration, we present LLM-Microscope, an open-source toolkit that assesses token-level nonlinearity, evaluates contextual memory, visualizes intermediate layer contributions (via an adapted Logit Lens), and measures the intrinsic dimensionality of representations.
435_45007_5	We then develop an LLM-based automated evaluation framework to identify reasoning errors and evaluate the performance of LLMs.
444_45016_0	Existing legal benchmarks focusing on knowledge and logic effectively evaluate LLMs on various tasks in legal domain.
448_45020_2	Our benchmark evaluates LLMs through two distinct tasks.
449_45021_5	In this paper, we present AssertionBench, a novel benchmark to evaluate LLMs’ effectiveness for assertion generation quantitatively.
453_45025_4	Then, challenging dialogue scripts are extracted for evaluating different target LLMs.
459_45031_2	We first construct a factually weak LLM by inducing hallucinations from the original LLMs.
13_45059_2	The related works focus on conventional NLP tasks in finance, while developing LLM for specific tasks is also required.
13_45059_5	In this paper, the credit product customization is studied by developing an LLM-based financial AI assistant for the credit loan business.
1_45095_1	As the detection of such content is of significant importance, substantial research has been conducted with the objective of developing reliable AI-generated text detectors.
18_45112_5	We also evaluate our model across datasets generated by different distinct models in many languages, showcasing its robustness in multilingual and cross-model scenarios.
25_45119_3	Our study found that carefully exploring fine-tuned parameters such as i) no. of training epochs, ii) maximum input size, iii) handling class imbalance etc., plays an important role in building an effective system to achieve good results and can significantly impact the underlying tasks.
12_45176_6	We develop a benchmark system utilizing transformer-based architectures, including fine-tuned Bidirectional Encoder Representations from Transformers variants (F1-87%) and Large Language Models with Quantized Low-Rank Approximation (F1-89%), that significantly outperforms traditional methods.
11_45195_4	To this end, we evaluate various open LLMs—including BioMistral and Llama-2 models—on a diverse set of biomedical datasets, using standard prompting, Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora.
12_45196_0	We propose using prompts made up of multiple problems to evaluate LLM capabilities, an approach we call multi-problem evaluation.
16_45200_0	Evaluating an LLM’s robustness against numerical perturbation is a good way to know if the LLM actually performs reasoning or just replicates patterns learned.
7_45207_2	Meanwhile, for general-purpose LLM MT, recent studies have found some success in generating similarly useful domain knowledge from an LLM itself, prior to translation.
5_45253_3	However, the low-resource nature of Bahnaric, characterized by data scarcity, vocabulary constraints, and the lack of parallel corpora, poses significant challenges to building an accurate and efficient translation system.
29_45288_5	The research highlights the importance of cultural sensitivity in evaluating inclusive Arabic LLMs, fostering more widely accepted LLMs for Arabic-speaking communities.
8_45320_3	The generated CNs are evalu- ated using JudgeLM (a LLM to evaluate other LLMs in open-ended scenarios) along with traditional metrics such as ROUGE-L, BLEU, BERTScore, and other traditional metrics.
50_45378_3	Evaluating five LLMs over 100 diverse social scenarios and nine demographic axes, our findings suggest a “default persona” bias toward middle-aged, able-bodied, native-born, Caucasian, atheistic males with centrist views.
99_45425_6	By evaluating a model’s ability to successfully reconstruct high-surprisal tokens in text, we can identify a surprising number of texts memorized by LLMs.
114_45440_7	We provide a framework for evaluating LLM bias in real-world clinical cases, offer insights into the complex nature of bias in these models, and present strategies for bias mitigation.
126_45452_4	We compare a variety of LLMs with diverse properties to evaluate broad LLM performance on four types of constituent movement: heavy NP shift, particle movement, dative alternation, and multiple PPs.
158_45483_2	BTProp works by constructing a probabilistic model of the LM itself: it reasons jointly about logical relationships between claims and relationships between claim probabilities and LM factuality judgments via probabilistic inference in a “hidden Markov tree”.
194_45518_3	To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded in professional work environments.
196_45520_6	We evaluated representative LLMs on our benchmark.
203_45526_1	This paper presents a multi-LLM communication framework designed to enhance the generation of persuasive data automatically.
288_45608_5	Moreover, we put forward a new benchmark named SeqEval to evaluate a model’s ability to follow all the instructions in a sequence, which further corroborates the benefits of our sequential instruction tuning method.
294_45614_5	Models trained on SynthDetoxM outperform all evaluated LLMs in few-shot setting.
318_45638_1	This paper presents FinEval, a benchmark designed to evaluate LLMs’ financial domain knowledge and practical abilities.
329_45648_5	We introduce the Information Updation dataset, simulating the real-world process of updating outdated Wikipedia tables, and evaluate LLM performance.
356_45673_1	To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual’s multidimensional self-concept.
383_45698_1	In this work, wepropose TimerBed, the first comprehensivetestbed for evaluating LLMs’ TsR performance.
400_45715_0	Building pluralistic AI requires designing models that are able to be shaped to represent a wide range of value systems and cultures.
405_45720_3	Using these datasets, we evaluate various open- and closed-source LLMs, NMT models, and their combinations.
456_45771_4	Using CCPT, we establish three types of tasks to evaluate LLMs for conceptual combination thoroughly.
466_45781_1	In this paper, we investigate if NLI tasks, that are rarely used for LLM evaluation, can still be informative for evaluating LLMs.
476_45791_7	To address these challenges, we present Multi-Agent System with Tactical Execution and Reasoning using LLM Specialized MCTS (MASTER), a novel framework that coordinates agent recruitment and communication through LLM specialized MCTS.
485_45798_6	Therefore, we call for greater efforts in developing and evaluating LLMs that go beyond English-centric paradigms.
486_45799_4	All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations.
501_45812_5	We present DCE-LLM, a framework for automated dead code elimination using a small CodeBERT model with an attribution-based line selector to efficiently locate suspect code.
515_45825_2	In response, we present Language model Ensemble with Monte Carlo Tree Search (LE-MCTS), a novel framework for process-level ensembling of language models.
542_45851_6	These results suggest LLM-assisted input rewriting as a promising direction for improving translations.
553_45862_3	To address this, we introduce the **Psych-ADR** benchmark and the **A**dverse **D**rug Reaction **R**esponse **A**ssessment (**ADRA**) framework to systematically evaluate LLM performance in detecting ADR expressions and delivering expert-aligned mitigation strategies.
561_45870_4	We use this approach to evaluate non-aligned and aligned LLMs on two domains with rich human behavioral data.
565_45874_4	We measure utility by evaluating the LLM’s performance on various tasks, including general knowledge, mathematical abilities, programming, and reasoning skills.
629_45938_1	This work presents a detailed analysis of strategies for developing a model capable of following instructions in a low-resource language, specifically Basque, by focusing on three key stages: pre-training, instruction tuning, and alignment with human preferences.
40_45984_3	We extensively evaluate kNN-LMs on a diverse set of tasks, ranging from sentiment classification and commonsense reasoning to multi-hop reasoning.
53_45997_2	In this work, we present an automated, LLM Supervised, pipeline to generate high quality synthetic data for Text2Cypher.
56_46000_0	This paper presents the Korean National Educational Test Benchmark (KoNET), a new benchmark designed to evaluate Multimodal Generative AI Systems using Korean national educational tests.
64_46008_1	To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs.
64_46008_4	We evaluate both closed- and open-source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones.
25_46050_4	To that end, we first develop a new algorithm to select a subset of texts from a larger corpus.
25_46050_6	In search of further improvement, we design a new algorithm to select tokens to include in the LLM vocabulary.
34_46059_1	This paper introduces MedEthicEval, a novel benchmark designed to systematically evaluate LLMs in the domain of medical ethics.
62_46087_2	The study acknowledges the diverse nature of children, often overlooked by standard safety evaluations, and proposes a comprehensive approach to evaluating LLM safety specifically for children.
71_46095_3	Specifically, we present a persona extraction model designed to autonomously and precisely generate vast persona dialogue datasets.
79_46103_2	This paper presents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the AI legal domain, with NYC Local Law 144 (LL144) as the test case.
2_46106_0	This paper presents our system, InsightBuddy-AI, designed for extracting medication mentions and their associated attributes, and for linking these entities to established clinical terminology resources, including SNOMED-CT, the British National Formulary (BNF), ICD, and the Dictionary of Medicines and Devices (dm+d).To perform medication extraction, we investigated various ensemble learning approaches, including stacked and voting ensembles (using first, average, and max voting methods) built upon eight pre-trained language models (PLMs).
14_46118_1	Using a synthetic sports feedback dataset, we evaluate open-weight LLMs’ ability to extract aspect-polarity pairs and propose a metric to facilitate the evaluation of aspect extraction with generative models.
53_46157_1	This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs’ logical reasoning and generalization capabilities in a controlled environment.
8_46165_5	Participants will not only gain a comprehensive overview of the field’s progress, but also acquire technical skills on analysing and developing LLM-based social intelligence.
3_46168_6	We evaluated our system through an expert pre-study with 4 experts and a user study with 13 participants.
4_46169_3	With this paper, we present LM-Pub-Quiz, a Python framework and leaderboard built around the BEAR probing mechanism that enables researchers and practitioners to apply it in their work.
23_46188_6	In the first release, Libra-Leaderboard evaluates 26 mainstream LLMs from 14 leading organizations, identifying critical safety challenges even in state-of-the-art models.
38_46203_2	To address this problem, we take Japanese as a non-English language and propose Japanese multimodal datasets for rapidly developing a Japanese multimodal model.
29_46263_2	This study systematically evaluates recent LLMs’ causal reasoning across three levels of Pearl’s Ladder of Causation—associational, interventional, and counterfactual—as well as commonsensical, anti-commonsensical, and nonsensical causal structures using the CLadder dataset.
43_46277_3	We first evaluated multiple open-source LLMs on a dataset of 1.6k group conversation messages.
50_46284_1	Our contributions include a novel general-purpose test framework for reliable and large-scale generation of tests for LLMs, a benchmark dataset with 30,000 tests for detecting cognitive biases in LLMs, and a comprehensive assessment of the biases found in the 20 evaluated LLMs.
52_46286_3	To fill this gap, we crowdsource subjective annotations of emotion categories in a German argument corpus and evaluate automatic LLM-based labeling methods.
27_46442_2	We outline strategies for large-scale text collection, including the creation of an online platform to engage the broader public in contributing texts and a communication campaign promoting openly accessible and transparently developed LLMs.
32_46446_4	It also motivates the need to develop LLMs that understand the differences in how PI is expressed across languages with varying levels of availability of linguistic resources.
3_46463_5	We evaluate three multilingual LLMs–one open source (Llama3) and two closed-source (GPT-4/3.5).
25_46499_3	This work tries to bridge this gap by proposing a framework to experimentally evaluate small, open LMs in practical settings through measuring semantic correctness of outputs across three practical aspects: task types, application domains and reasoning types, using diverse prompt styles.
33_46507_0	Existing benchmarks are becoming saturated and less effective in evaluating model performance due to factors such as data contamination and the advancing capabilities of the Large Language Models (LLMs).
144_46714_2	Formulating the completion of an edge into a sentence as finding a solution path in a large state-transition system, we demonstrate a connection to AI Planning which is concerned with this kind of problem.
65_47043_4	Then, we compare them empirically to softmax-based approaches, which are self-normalized using explicit regularization, and suggest a hybrid model with compelling properties.
288_48589_0	Can we construct a neural language model which is inductively biased towards learning human language?
507_48807_3	To deal with issues coming from introducing subword language model, we develop a retrace algorithm and a reranking method by approximate marginalization.
40_49444_0	We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model.
375_50764_2	Data for building this system was collected from 41 matches that were played during World Championships in years 2000, 2001, and 2002 and were transmitted by the Czech TV channels.
268_51173_0	In this paper we present a Linguistic Meta-Model (LMM) allowing a semiotic-cognitive representation of knowledge.
153_52990_4	Subsequently an effort has been made to develop a trigram model of Indian English, Punjabi and Nepali.
33_55270_1	We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence.
161_55398_3	We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.
221_55766_2	In this paper, we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information.
221_55766_3	We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model.
28_55829_0	We herein present a language-model-based evaluator for deletion-based sentence compression and view this task as a series of deletion-and-evaluation operations using the evaluator.
15_56793_0	We present a probabilistic language model that captures temporal dynamics and conditions on arbitrary non-linguistic context features.
15_56793_3	With five streaming datasets from two different genres—economics news articles and social media—we evaluate our model on the task of sequential language modeling.
32_56971_4	In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information.
84_57230_3	For these reasons, we propose to build an n-class LM that is based mainly on the integration of purely semantic data.
21_58507_0	In this paper we present a system for automatic Arabic text diacritization using three levels of analysis granularity in a layered back off manner.
16_58708_4	We evaluate our proposed model using the popular Penn Treebank and Text8 corpora.
29_59262_1	In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems.
13_59412_0	In this work, we develop a novel, completely unsupervised, neural language model-based document ranking approach to semantic tagging of documents, using the document to be tagged as a query into the GLM to retrieve candidate phrases from top-ranked related documents, thus associating every document with novel related concepts extracted from the text.
4_59494_2	In order to address this issue and to realize such smart hybrid dialogue systems, we develop a model to discriminate user utterance between task-oriented and chit-chat conversations.
7_60102_2	We developed an ensemble system consisting of language models together with LSTM-based networks containing a CNN attention mechanism.
8_60990_5	The work aims at supporting the hypothesis by developing the first Universal Language Model in Arabic (hULMonA - حلمنا meaning our dream), demonstrating its use for Arabic classifications tasks, and demonstrating how a pre-trained multi-lingual BERT can also be used for Arabic.
32_61014_0	In this paper, we present a Dialect Identification system (ArbDialectID) that competed at Task 1 of the MADAR shared task, MADARTravel Domain Dialect Identification.
32_61014_3	We firstly build a coarse identification model to classify each sentence into one out of six dialects, then use this label as a feature for the fine-grained model that classifies the sentence among 26 dialects from different Arab cities, after that we apply ensemble voting classifier on both sub-systems.
