0803.0785_52545_5	The fusion rules in the extended picture are deduced from the known fusion rules for the Virasoro representations of LM(1,p) and are found to be in agreement with previous works.
1006.0386_193661_2	The public key cryptosystem based on rank codes was presented in 1991 by Gabidulin -Paramonov-Trejtakov(GPT).
1008.3328_208085_3	In Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS), two new versions of LMS algorithms, same formulations are performed as in the LMS algorithm with the exception that filter tap weights are now expressed using trigonometric and hyperbolic formulations, in cases for TLMS and HLMS respectively.
1104.1698_255252_2	The corresponding algorithm for the sequential determination of the generalized LM-inverse is established in the present paper.
1303.2255_413912_3	Secondly, two modifications to the ZA-LMS algorithm have been made.
1303.2255_413912_7	In addition, the mean square performance of DWZA-LMS algorithm is analyzed.
1303.3263_414920_3	The error estimation in Polynomial method is carried out by LMS Filter.
1404.6813_519866_4	This brings up the issue of studying the performance of the diffusion LMS algorithm when it is run, either intentionally or unintentionally, in a multitask environment.
1611.00196_785643_2	This paper proposes a novel distributed vector representation of a document: a simple recurrent-neural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document.
1703.10724_834228_6	Building LSTM $n$-gram LMs may be appealing for some practical situations: the state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes storing the identity of the words in the context and batches of $n$-gram contexts can be processed in parallel.
1710.04381_899764_4	A rigorous mean and mean square performance evaluation is conducted to justify the performance advantages of the proposed scheme over the conventional augmented LMS solution.
1806.01782_987422_3	Convergence analysis of the LMS algorithm in the case of coloured input signal, i.e., correlated input signal is demonstrated on adaptive FIR filter via power spectral density of the input signals and Fourier transform of the autocorrelation matrix of the input signal.
1806.01782_987422_4	Simulations have been carried out on adaptive filtering of FIR and IIR filters and tested on white and coloured input signals to validate the powerfulness of the genetic-based LMS algorithm.
1807.07395_1004603_5	Convergence of the proposed method is investigated and it is proved that the rate of convergence of the introduced method is equal to that of LMS algorithm in the expected value sense, provided that the distribution of the added noise is uniform.
1903.02852_1095238_2	While AL is known to be beneficial to AM training, we show that it also carries out substantial improvements to the LM when combined with SST.
1904.06234_1111274_7	We apply two different sub-approaches in the LM Based approach and the combined result of these two approaches is considered as the final output of the system.
2002.11268_1248691_4	The Density Ratio method was found to consistently outperform the dominant approach to LM and end-to-end ASR integration, Shallow Fusion.
2008.09036_1336758_1	However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies.
2010.06189_1362519_1	However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English.
2010.10657_1366987_2	In the recent decades, concerns have been raised on studying improper signals and providing an accurate model of the LMS algorithm for both proper and improper signals.
2010.10657_1366987_3	Other models for the LMS algorithm for improper signals available in the scientific literature either make use of the independence assumptions regarding the desired signal and the input signal vector, or are exclusive to proper signals; it is shown that by not considering these assumptions a more general model can be derived.
2010.11349_1367679_0	  LSTM language models (LSTM-LMs) have been proven to be powerful and yielded significant performance improvements over count based n-gram LMs in modern speech recognition systems.
2010.15577_1371907_4	The formats for submitting questions, examples of its designing and developed questions were demonstrated in view mode in Moodle LMS.
2012.05628_1393506_5	This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2.
2104.04466_1451884_3	We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level.
2104.08826_1456244_0	  Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts.
2105.13818_1476377_2	By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.
2109.04314_1527135_2	Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner.
2110.08743_1546886_6	\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM
2202.01771_1600595_3	In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action.
2202.04173_1602997_5	We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before.
2202.13169_1611993_5	We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine.
2203.02155_1615017_7	In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.
2203.10692_1623554_0	  Class-based language models (LMs) have been long devised to address context sparsity in $n$-gram LMs.
2204.06201_1636339_4	We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.
2204.06201_1636339_5	Moreover, we show that a complete constituency tree can be linearly separated from LM representations.
2207.09638_1685536_3	Motivated by this phenomenon, we for the first time posit that domain-general parameters can underpin a domain-general LM that can be derived from the original LM.
2207.14382_1690280_7	What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test.
2208.06946_1697658_5	We conducted a pilot experiment in which individuals are asked to distinguish between authentic passwords and honeywords when the username is provided for GPT-3 and a tweaking technique.
2208.07601_1698313_3	Noting that the computation of the LM rate can also be formulated as an entropy-based optimization problem with constraints, in this work, we transform the task into an optimal transport (OT) problem with an extra constraint.
2210.01293_1722648_7	Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.
2210.10585_1731940_1	Through surveys of human subjects enrolled in the crowdsourcing platform Prolific.co and queries submitted to the OpenAI's language model GPT-3, I test whether the numerical response for what wage is deemed fair for a particular job description changes when respondents and GPT-3 are prompted with additional information that includes a numerical minimum wage, whether realistic or unrealistic, relative to a control where no minimum wage is stated.
2210.12353_1733708_1	MCQA tasks have traditionally been presented to LLMs like cloze tasks.
2210.12770_1734125_2	We compare Transformer models that are trained from scratch to fine-tuned BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT.
2211.11483_1750389_1	Such claims have been made concerning the LaMDA model and also concerning the current wave of LLM-powered chatbots, such as ChatGPT.
2211.15006_1753912_7	The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step.
2212.05058_1761235_1	We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which AI behaviour, including its productions of bias and harm, can be analysed.
2212.08104_1764281_5	Note from the human-authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors in writing review articles.
2212.09292_1765469_4	Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools.
2212.14882_1771059_6	While further studies are needed, the initial insights of this study indicate a great potential in using large language models like ChatGPT to improve patient-centered care in radiology and other medical domains.
2301.02828_1773892_3	In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on.
2301.05272_1776336_1	The impressive scalability of LLMs due to the advent of deep learning can be seen as a continuation of empiricist lingusitic methods, as opposed to rule-based linguistic methods that are grounded in a nativist perspective.
2301.05517_1776581_1	How can we ensure that AI systems, including ChatGPT, are developed and adopted in a responsible way?
2301.12867_1783931_3	Large-scale benchmarks for accountable LLMs should consequently be developed.
2301.13852_1784916_3	In this paper, we study whether a machine learning model can be effectively trained to accurately distinguish between original human and seemingly human (that is, ChatGPT-generated) text, especially when this text is short.
2302.07136_1792070_0	  This study aims to understand the perceptions and opinions of academicians towards ChatGPT-3 by collecting and analyzing social media comments, and a survey was conducted with library and information science professionals.
2302.08579_1793513_2	To alleviate this problem, this paper designs a replaceable internal language model (RILM) method, which makes it feasible to directly replace the internal language model (LM) of E2E ASR models with a target-domain LM in the decoding stage when a domain shift is encountered.
2302.12246_1797180_1	It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers.
2302.13439_1798373_0	  The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.
2302.14229_1799163_2	However, it is not yet known the performance of LLMs on CLS.
2303.05349_1805146_1	However, while the debate on ChatGPT in academia is making waves, more understanding is needed among lecturers and teachers on how students use and perceive ChatGPT.
2303.06074_1805871_5	The same pattern of effects was found for LLM-simulated participants.
2303.07529_1807326_1	In 2023, OpenAI responded to criticism that Kenyan workers were paid less than $2 per hour to filter traumatic content from its ChatGPT model by stating in part that it had outsourced the work to a subcontractor, who managed workers' payment and mental health concerns.
2303.08769_1808566_4	An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.
2303.09461_1809258_5	At the same time, the questions in our exam are structurally similar to those of other exams, solved homework problems, and teaching materials that can be found online and might have been part of ChatGPT's training data.
2303.11158_1810955_15	However, currently, a great amount of attention is needed before a user can utilize materials from ChatGPT
2303.11436_1811233_3	Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required.
2303.12712_1812509_8	Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.
2303.17003_1816800_3	This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed.
2304.01933_1819979_0	  The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca).
2304.01964_1820010_7	PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs.
2304.02468_1820514_4	A strategy for validating the arguments and results of ChatGPT is presented summarily as an example of safe, large-scale adoption of LLMs.
2304.02796_1820842_5	A case study is conducted to validate the advantages and drawbacks of ChatGPT, showing that designers can acquire targeted knowledge from various domains, but the quality of the acquired knowledge is highly dependent on the prompt.
2304.04370_1822416_5	Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models.
2304.05436_1823482_6	Consequently, in the second phase, bibliometric analysis has been carried out on ChatGPT publications, and 45 published studies have been analyzed thoroughly based on their methods, novelty, and conclusions.
2304.09823_1827869_7	In particular, industries related to technology, products, and operations are expected to have higher proficiency requirements for ChatGPT-related skills, while the manufacturing, services, education, and health science related industries will have lower requirements for ChatGPT-related skills.
2304.10436_1828482_0	  With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns.
2304.10778_1828824_6	The average technical debt, considering code smells, was found to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes for Amazon CodeWhisperer.   
2304.12191_1830237_6	We hypothesize that Zipf's law will hold for genlangs because (1) genlangs created by ChatGPT fundamentally operate in the same way as human language with respect to the semantic usefulness of certain tokens, and (2) ChatGPT has been trained on a corpora of text that includes many different languages, all of which exhibit Zipf's law to varying degrees.
2304.12244_1830290_8	By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT.
2304.13712_1831758_9	A curated list of practical guide resources of LLMs, regularly updated, can be found at \url{https://github.com/Mooler0410/LLMsPracticalGuide}.
2304.14178_1832224_4	In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text.
2304.14354_1832400_3	The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.
2304.14399_1832445_5	We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset.
2304.14415_1832461_5	A survey was created to measure the effects of ChatGPT on students, faculty, and staff.
2305.00118_1833174_0	  In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query.
2305.01550_1834606_7	Furthermore, our framework is robust in mitigating approximate memorization across various circumstances, including longer context, which is known to increase memorization in LLMs.
2305.02220_1835276_4	Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
2305.02320_1835376_8	Further work is needed to determine the effect of factually wrong information in the generated responses and test our findings' generalizability with open-source LLMs.
2305.03851_1836907_5	Further research is needed to fully understand the potential applications of LLMs in sports science and medicine and to ensure that their use is ethical and beneficial to athletes, clients, patients, practitioners, and the general public.
2305.04134_1837190_5	Additionally, we introduce a new variant of the classical method in order to avoid that the solutions are found in the LLM training data (dataleakeage).
2305.04812_1837868_5	Three experiments were conducted to explore the effects of external information on LLMs' memories, opinions, and social media behavioral decisions.
2305.07716_1840772_2	Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark.
2305.09612_1842668_8	The code for this work can be found at \url{https://github.com/Ziems/llm-url}.
2305.09955_1843011_4	Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM.
2305.10383_1843439_6	A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification.
2305.10646_1843702_5	These commandment examples are expected to motivate the ethical use of ChatGPT.
2305.11000_1844056_0	  Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT.
2305.11169_1844225_4	We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe.
2305.11391_1844447_3	Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications.
2305.11391_1844447_5	In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.
2305.12392_1845448_3	We show how a small language model could be trained to act as a verifier module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions.
2305.12477_1845533_4	While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD.
2305.14045_1847101_0	  Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.
2305.14283_1847339_5	A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader.
2305.14283_1847339_6	The rewriter is trained using the feedback of the LLM reader by reinforcement learning.
2305.14283_1847339_8	Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.
2305.14307_1847363_2	For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?
2305.14752_1847808_4	This combined information is then fed into an LLM, which is instructed to attempt to fix the code.
2305.14771_1847827_1	While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies of diffusion LMs have been conducted on a smaller scale.
2305.14864_1847920_1	A growing assortment of methods for compression promises to reduce the computational burden of LLMs in deployment, but so far, only quantization approaches have been demonstrated to be effective for LLM compression while maintaining zero-shot performance.
2305.15020_1848076_6	The evaluation is performed over four NLP tasks (two generative and two classification tasks) among four widely used multilingual LMs in seven languages.
2305.15717_1848773_6	However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data.
2305.15929_1848985_5	Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT
2305.17359_1850415_6	We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text.
2305.17608_1850664_0	  The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts.
2305.18086_1851142_6	Studies including secondary data related to the application of ChatGPT were considered.
2305.18086_1851142_9	After multi-step screening process, 11 reviews were selected, consisting of 9 reviews specifically focused on ChatGPT and 2 reviews on broader AI topics that also included discussions on ChatGPT.
2305.18449_1851505_2	Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs.
2305.18616_1851672_3	Weekly surveys were conducted on collaborative interdisciplinary problem-solving, physical and cognitive engagement, and individual reflections on ChatGPT use.
2305.18752_1851808_6	Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways.
2305.19187_1852243_6	Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes).
2306.00597_1853745_4	The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community.
2306.01248_1854396_3	Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization.
2306.01499_1854647_3	Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting.
2306.02096_1855244_0	  With the launch of ChatGPT, serious concerns have reasonably been raised of its ill-effect on the integrity of remote take-home exams.
2306.03097_1856245_0	  Large generative AI models (GMs) like GPT and DALL-E are trained to generate content for general, wide-ranging purposes.
2306.03241_1856389_7	Additionally, we present results for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained on the PILE-deduped dataset containing 207B tokens.
2306.03503_1856651_2	By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified.
2306.04610_1857758_2	Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training.
2306.05087_1858235_3	In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs.
2306.05827_1858975_3	In this paper, we present our work on a cooperative-legal question-answering LLM-based chatbot, where we developed a set of legal questions about Palestinian cooperatives, associated with their regulations and compared the auto-generated answers by the chatbot to their correspondences that are designed by a legal expert.
2306.06815_1859963_1	However, the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined.
2306.07032_1860180_3	We aim an critical issue in the emerging topic of LLM based causal structure learning, to tackle erroneous prior causal statements from LLM, which is seldom considered in the current context of expert dominating prior resources.
2306.08178_1861326_6	Source code can be found at: https://github.com/DrCintas/ASCON-with-ChatGPT.
2306.09339_1862487_4	Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions.
2306.10765_1863913_5	With an increasing number of domain-specific professional multimodal LLMs in the medical field being developed, MedAGI is designed to automatically select appropriate medical models by analyzing users' questions with our novel adaptive expert selection algorithm.
2306.13421_1866569_1	However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another.
2306.14905_1868053_3	By finetuning LLMs on domain-specific academic papers that have been selected as a result of a rigorous SLR process, the proposed PRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer the potential to achieve greater efficiency, reusability and scalability, while also opening the potential for conducting incremental living systematic reviews with the aid of LLMs.
2306.17176_1870324_1	A total of 100 fact-checked news items, all sourced from independent fact-checking agencies, were presented to each of these LLMs under controlled conditions.
2306.17176_1870324_4	The effectiveness of the LLMs was gauged based on the accuracy of their classifications against the verified facts provided by the independent agencies.
2307.00012_1871008_9	Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs (roughly between 51% and 83%) can be expected to pass.
2307.02006_1873002_2	The proposed framework relies on domain-specific pre-training, to produce a specialised language model which is trained on task-specific natural data augmented by synthetic data generated by a black-box LLM.
2307.03196_1874191_4	Based on a preliminary survey on ChatGPT's quality in answering questions in Geography and GIScience, we demonstrate that this assumption might be fairly naive, and effective control in assessments and supervision is required.
2307.03744_1874739_5	Participants in our experiments were asked to solve a series of decision tasks that involved researching and comparing different products, and were randomly assigned to do so with either an LLM-based search tool or a traditional search engine.
2307.03972_1874967_6	Our findings demonstrates that further investigation is required for the application of LLMs on Chinese GEC task.
2307.07171_1878166_3	This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input.
2307.08974_1879969_3	The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked ideas, findings, and reporting standards in scholarly research, and to formulate guidelines for its use and disclosure, b) a bibliometric analysis of existing author guidelines in journals that mention GAI/GPT/LLM, with the goal of evaluating existing guidelines, analyzing the disparity in their recommendations, and identifying common rules that can be brought into the Delphi consensus process, c) a Delphi survey to establish agreement on the items for the guidelines, ensuring principled GAI/GPT/LLM use, disclosure, and reporting in academia, and d) the subsequent development and dissemination of the finalized guidelines and their supplementary explanation and elaboration documents.
2307.09009_1880004_6	This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting.
2307.10811_1881806_1	Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting.
2307.11316_1882311_12	The code will be made public at \url{https://github.com/Yangyi-Chen/LM-TOAST}.
2307.16180_1887175_4	Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality.
2307.16883_1887878_4	HAGRID is constructed based on human and LLM collaboration.
2308.00121_1888013_3	For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine.
2308.00225_1888117_3	In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning.
2308.01990_1889882_1	Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users.
2308.02312_1890204_2	Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT's answers to programming questions.
2308.02955_1890847_2	Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models.
2308.03301_1891193_5	It can be shown that all references provided by ChatGPT that were found to be genuine have also been cited on Wikipedia pages.
2308.03313_1891205_1	As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned.
2308.03313_1891205_9	The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1.
2308.03333_1891225_3	In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations.
2308.03929_1891821_3	We aim to achieve fact-checking of the knowledge embedded in biological graphs that were contrived from ChatGPT contents at the aggregate level.
2308.04709_1892601_8	The study was conducted to evaluate the ability of LLM models to provide correct answers to nephSAP (Nephrology Self-Assessment Program) multiple-choice questions.
2308.05201_1893093_0	  Large Language Model (LLM) based generative AI, such as ChatGPT, is considered the first generation of Artificial General Intelligence (AGI), exhibiting zero-shot learning abilities for a wide variety of downstream tasks.
2308.05374_1893266_7	Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs.
2308.05596_1893488_3	Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability.
2308.06373_1894265_0	  The fundamental principles, potential applications, and ethical concerns of ChatGPT are analyzed and discussed in this study.
2308.06463_1894355_2	In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages.
2308.07317_1895209_2	Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs.
2308.07462_1895354_9	Therefore, further research is needed to understand how the use of ChatGPT and more broadly generative AI tools will affect the vocabulary and lexical richness in different types of text and languages.
2308.09067_1896959_6	The sexist bias prevalent in human text is also expressed by LLMs, and even magnified in all of them but one.
2308.09267_1897159_5	We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.
2308.09936_1897828_3	These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs.
2308.11526_1899418_6	Motivated by the success of LLMs in specific domains like science and biology, this paper introduces a LLM for log data which is trained on public and proprietary log data.
2308.12415_1900307_6	The results of the case study demonstrate the positive causal influence of prompt semantics on ChatGPT's generative performance by an average treatment effect of $\approx 3\%$. Moreover, it was found that confounders such as prompt size are highly correlated with accuracy metrics ($\approx 0.412\%$).
2308.14120_1902012_2	Real-world clinical datasets and study details from large trials across various medical specialties were presented to ChatGPT ADA without specific guidance.
2308.14337_1902229_4	Specifically, we show that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent.
2308.15276_1903168_10	Further research is needed to fully harness the potential of LLMs like ChatGPT for practical fault localisation applications.
2309.00237_1905041_7	This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals.
2309.01522_1906326_4	Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users.
2309.02077_1906881_5	To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets.
2309.02077_1906881_6	A medical consultation training set is further constructed to improve the consultation ability of LLMs.
2309.02726_1907530_4	A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation.
2309.03079_1907883_7	A Machine Learning model is then trained with LLM outputs as features.
2309.03087_1907891_6	Our results showed that nearly half of the solutions provided with the support of ChatGPT were mistakenly assumed to be correct by the students, indicating that they overly trusted ChatGPT even in their field of expertise.
2309.03914_1908718_2	This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work.
2309.04198_1909002_3	Our study focuses on how the dual logic ability of LLMs is affected during the privatization process in the medical domain.
2309.04316_1909120_6	The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement.
2309.05454_1910258_2	Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5--which have shown promising results.
2309.05918_1910722_0	  In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts.
2309.06687_1911491_6	Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process.
2309.06794_1911598_4	Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future.
2309.07689_1912493_4	We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights
2309.08491_1913295_4	These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction.
2309.08859_1913663_3	Existing learning rate policies are primarily designed for training traditional deep neural networks (DNNs), which may not work well for LLM fine-tuning.
2309.09120_1913924_1	In this paper, we conducted a content analysis of social media discussions to gauge public perceptions of gender bias in LLMs which are trained in different cultural contexts, i.e., ChatGPT, a US-based LLM, or Ernie, a China-based LLM.
2309.09120_1913924_3	A difference between the two LLMs was seen -- ChatGPT was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles, while explicit gender bias was found in Ernie's responses, e.g., overly promoting women's pursuit of marriage over career.
2309.10066_1914870_9	Based on these metrics, the fine-tuned PEGASUS model was selected as the top LLM.
2309.10092_1914896_6	To address it, we propose HERACLEs, a hierarchical neuro-symbolic planner that relies on a novel integration of (i) existing symbolic planners generating high-level task plans determining the order at which the NL sub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs) to design sequences of robot actions based on these task plans; and (iii) conformal prediction acting as a formal interface between (i) and (ii) and managing uncertainties due to LLM imperfections.
2309.10254_1915058_1	While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted.
2309.10371_1915175_0	  A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are.
2309.10371_1915175_2	It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources.
2309.10371_1915175_4	Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.
2309.11166_1915970_1	In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs.
2309.11231_1916035_3	In this study, the features and capabilities of ChatGPT-4 are analyzed in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish.
2309.11231_1916035_4	Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors.
2309.11674_1916478_3	In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on.
2309.11805_1916609_6	Inspired by the superior performance of LLMs, we leverage their capability to understand natural language for capturing the information that was previously getting lost during the conversion of unstructured data to structured form.
2309.12161_1916965_5	Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4.
2309.12348_1917152_2	Answers to the above questions were solicited from ChatGPT itself, the responses were collected, and then the recent literature was surveyed to determine whether or not the responses are supported.
2309.12570_1917374_3	Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators.
2309.13243_1918047_2	The students were asked to revise their essays through dialogues with ChatGPT.
2309.13963_1918767_3	Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures.
2309.14348_1919152_4	RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM.
2309.14379_1919183_6	The replications among the experiments also illustrate how tasks previously requiring protracted team effort or complex computational pipelines can now be accomplished by an LLM-assisted scholar in a fraction of the time.
2309.16134_1920938_5	Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation.
2309.16697_1921501_11	Logical and critical thinking are needed to validate the result presented by ChatGPT.
2310.00194_1922451_3	To improve planning with LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in which planning is accomplished via the recurrent interaction of the specialized modules mentioned above, each implemented using an LLM.
2310.00603_1922860_7	After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters.
2310.01423_1923680_3	However, more study is needed to see how effective they are for multi-domain ChatGPT material.
2310.01424_1923681_6	To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first SoK on data privacy for LLMs.
2310.01434_1923691_2	This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity.
2310.01783_1924040_5	However, the utility of LLM-generated feedback has not been systematically studied.
2310.01957_1924214_3	A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data.
2310.01960_1924217_3	More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements.
2310.02003_1924260_4	Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store.
2310.02277_1924534_1	It has been believed that weights in LLMs contain significant redundancy, leading to the conception that a considerable chunk of the parameters can be removed by pruning without compromising performance.
2310.03473_1925730_4	The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM.
2310.03691_1925948_2	This idea is exemplified in DirectGPT, a user interface layer on top of ChatGPT that works by transforming direct manipulation actions to engineered prompts.
2310.04408_1926665_5	Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.
2310.04668_1926925_6	Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes.
2310.04698_1926955_7	The code is then executed by an LLM agent in a local environment and .
2310.05216_1927473_2	In this work, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of human reading patterns.
2310.06474_1928731_1	Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English.
2310.08164_1930421_6	Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions.
2310.08535_1930792_4	Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design, implementation, and experimentation with different LLM-based agents.
2310.08885_1931142_5	Moreover, the effectiveness of LLMs in TODS is further supported by our comprehensive evaluations on TODS subtasks: dialogue state tracking, intent classification, and response generation.
2310.08915_1931172_6	This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs.
2310.09130_1931387_5	Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters.
2310.09909_1932166_6	All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
2310.10826_1933083_5	Our design is supported by demonstrations on a publicly available LLM.
2310.11409_1933666_8	The current version of the LLM-guided privilege-escalation prototype can be found at https://github.com/ipa-labs/hackingBuddyGPT.
2310.11532_1933789_3	This correction task is formulated as a multi-step rule-based LLM reasoning process, which uses explicitly written rules in prompts to decompose the task into concrete reasoning steps.
2310.12418_1934675_5	We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research.
2310.12836_1935093_5	To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning.
2310.12973_1935230_7	This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions.
2310.13229_1935486_1	However, details about LLM training data are often not made public, which has caused concern as to whether existing bug benchmarks are included.
2310.13549_1935806_1	Large Language Models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs.
2310.13561_1935818_1	To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM.
2310.14724_1936981_3	The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task.
2310.15007_1937264_10	Taken together, our results show that accurate document-level membership can be inferred for LLMs, increasing the transparency of technology poised to change our lives.
2310.15051_1937308_3	Afterwards, using the provided dataset, an evaluation is conducted to assess the capabilities of LLMs, including GPT-3.5 and GPT-4.
2310.15896_1938153_4	To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT.
2310.15941_1938198_4	We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained.
2310.16253_1938510_7	While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods.
2310.16301_1938558_2	An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks.
2310.16789_1939046_4	In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text?
2310.18373_1940630_6	This dataset allows for the evaluation of LLMs in a new context, as they are predominantly designed and trained on data from high-income North American countries.
2310.19813_1942070_8	Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
2310.20381_1942638_1	For the evaluation, a set of prompts is designed for each task to induce the corresponding capability of GPT-4V to produce sufficiently good outputs.
2311.01740_1944707_1	To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone.
2311.02105_1945072_6	Specifically, we introduce ``security vectors'', a few new parameters that can be separated from the LLM, to ensure LLM's responses are consistent with the harmful behavior.
2311.02126_1945093_1	Recently, significant progress has been made in the development of Vision Language Models (VLMs), expanding the capabilities of LLMs and enabling them to execute more diverse instructions.
2311.03287_1946254_4	Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented.
2311.04205_1947172_3	While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped.
2311.04368_1947335_3	It was conducted by administering questionnaires in the form of test papers through the LLM network interface, with the valuable participation of volunteers.
2311.04547_1947514_4	In contrast, a negative correlation was found between LM size and reading time fit of linear mixed-effects models using LM surprisal as a predictor, with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model without surprisal.
2311.05374_1948341_9	We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs.
2311.06180_1949147_7	Students were asked to rate the correctness and usefulness of each feedback, and to indicate which one was generated by GPT.
2311.06427_1949394_4	The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness.
2311.06825_1949792_2	Maximum ratio transmission (MRT) and matched-filtering (MF) precoding techniques are adopted at the satellite separately for the common messages (CMs) and for the private messages (PMs), which are both implemented based on the estimated LMS channels suffering from the Shadowed-Rician fading.
2311.06981_1949948_0	  Ever since the public release of ChatGPT in November 2022, serious concerns have been raised about the impact and potentially dire consequences of the increasingly widespread use of generative AI tools for purposes of scientific writing and publishing.
2311.07418_1950385_4	Experiments were performed on SLURP to quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and Vicuna-13B (v1.1 and v1.5) with different ASR error rates.
2311.07582_1950549_6	However, further development and validation are still required before the promise of LLMs in accelerating biological discovery can be realized.
2311.08268_1951235_0	  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses.
2311.08287_1951254_5	Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs.
2311.09247_1952214_0	  We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts.
2311.09308_1952275_2	Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language.
2311.09651_1952618_2	However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students.
2311.09651_1952618_6	However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students.
2311.09709_1952676_2	While such modifications have been proven effective in tasks like machine translation, tailoring them to LLMs demands specific modifications given the diverse nature of LLM applications.
2311.10372_1953339_2	A considerable portion of Code LLMs is derived from general LLMs through model fine-tuning.
2311.10372_1953339_6	We aim to address three questions: (1) What LLMs are specifically designed for software engineering tasks, and what is the relationship between these Code LLMs?
2311.10961_1953928_2	One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual.
2311.11811_1954778_1	A methodology is developed to explore the potential use of LLMs for translating the explanations produced by rule-based systems, from high-level programming languages to natural language, allowing all users a fast, clear, and accessible interaction with such technologies.
2311.12188_1955155_0	  Reinforcement learning-based large language models, such as ChatGPT, are believed to have potential to aid human experts in many domains, including healthcare.
2311.13720_1956687_1	To set the stage for this union, we explore two different flavors of model space problems that have been studied in the AI planning literature and explore the effect of an LLM on those tasks.
2311.14711_1957678_1	These decisions should not be left solely in the hands of frontier LLM developers.
2311.14711_1957678_4	Though there are encouraging signs of increasing external scrutiny of frontier LLMs, its success is not assured.
2311.14722_1957689_3	The generated program is then executed by a program interpreter, thus mitigating the limitations of LLM in performing accurate arithmetic calculations.   
2311.17295_1960262_5	If the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible.
2311.17474_1960441_5	The insight is that language understanding and tool usage are both required for network LLMs.
2311.18021_1960988_2	While ICL has been extensively studied on LLMs, its research on MLLMs remains limited.
2311.18041_1961008_4	We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully.
2311.18743_1961710_7	Since its release, AlignBench has been adopted by top (Chinese) LLMs for evaluating their alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi, Baichuan, and Abab.
2312.00353_1962161_2	Two research questions are formulated to investigate the accuracy of LLMs in recalling information from pre-training knowledge graphs and their ability to infer knowledge graph relations from context.
2312.00678_1962486_2	To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs.
2312.01858_1963666_1	To manage the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.
2312.03052_1964860_5	VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one.
2312.03088_1964896_3	The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM.
2312.03815_1965623_2	Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem.
2312.04032_1965840_2	Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives.
2312.04828_1966636_9	In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners.
2312.05275_1967083_6	Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.
2312.05603_1967411_2	The core idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which an STS model is trained.
2312.06351_1968159_4	However, quantitative research has not been conducted on how accurately different types of LLMs can handle these problems.
2312.07592_1969400_7	Multiple experiments, exploring response hallucination and considering question complexity, were conducted on ChatGPT.
2312.07876_1969684_0	  Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted in many safety-critical applications.
2312.08027_1969835_2	To alleviate this, we use an interpretable structure to explain the prompt learning principle in LLMs, which certificates that the effectiveness of language models is determined by position changes of the task's related tokens.
2312.08055_1969863_4	The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.
2312.09007_1970815_4	Complex tasks, which may involve collaborations of multiple domain-specific AI modules and IoT devices, are executed through a control script generated by the LLM using a Language-Code transformation approach, which first converts language descriptions to an intermediate finite-state machine (FSM) before final precise transformation to code.
2312.09801_1971609_5	The students in each course were asked to prompt an LLM of their choice with one question from a set of four and required to affirm or refute statements in the LLM output by using peer-reviewed references.
2312.10055_1971863_7	This work demonstrates the potential for LLM-generated feedback, but further research is required to explore its practical implementation.
2312.10075_1971883_3	While the WVS is accepted as an explicit assessment of values, we lack methods for assessing implicit moral and cultural values in media, e.g., encountered in social media, political rhetoric, narratives, and generated by AI systems such as LLMs that are increasingly present in our daily lives.
2312.10631_1972439_8	The feasibility of LLM-Twin is demonstrated by numerical experiments and case studies.
2312.10793_1972601_1	However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood.
2312.11701_1973509_2	The wide-ranging capabilities of LLMs are examined in the context of the building energy field, including intelligent control systems, code generation, data infrastructure, knowledge extraction, and education.
2312.12683_1974491_4	In experiments across four LLMs, we find that multilingual instruction tuning with as few as two to three languages is both necessary and sufficient to elicit effective cross-lingual generalisation, with the limiting factor being the degree to which a target language is seen during pretraining.
2312.13096_1974904_9	These findings highlight the potential of LLM-based chatbots in tackling different forms of false information in online environments, but also points to the substantial variation in terms of how such potential is realized due to specific factors, such as language of the prompt or the topic.
2312.17259_1979067_4	The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links.
2312.17276_1979084_0	  The recent trend of large language models (LLMs) is to increase the scale of both model size (\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama.
2312.17485_1979293_5	In this study, various prompts are designed and compared across mainstream LLMs using two distinct datasets from human reviewers and automated checkers.
2312.17493_1979301_3	Motivated by this, we investigate how data privacy can be ensured in LLM fine-tuning through practical federated learning approaches, enabling secure contributions from multiple parties to enhance LLMs.
2401.00627_1980183_3	The student data is derived from think-aloud interviews of introductory students and the AI data comes from ChatGPT's solutions collected using Zero shot approach.
2401.02051_1981607_4	They are then translated into executable codes by LLMs.
2401.02984_1982540_6	Diverse applications of LLMs in mental health care are identified, including diagnosis, therapy, patient engagement enhancement, etc.
2401.03630_1983186_3	In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs.
2401.03729_1983285_3	In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM?
2401.03855_1983411_5	The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs.
2401.05908_1985464_5	Our model is trained from the pre-trained LLM by fine-tuning technique using datasets from the epilepsy domain.
2401.06102_1985658_3	We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework.
2401.06640_1986196_6	However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.
2401.06853_1986409_5	A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task.
2401.07181_1986737_3	The RL agent is then deployed in these scenarios, and a reward model is learnt through the LLM preferences and feedback.
2401.07237_1986793_4	This can be viewed as a mechanism of distilling event sequence knowledge from LLMs.
2401.09083_1988639_5	Considering that LLM is trained with natural language and is not capable of directly perceiving visual concepts as contained in remote sensing images, we designed visual cues that inject visual information into ChatGPT.
2401.10360_1989914_1	A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload.
2401.10444_1989998_0	  The paper discusses what is needed to address the limitations of current LLM-centered AI systems.
2401.12773_1992327_3	Here, we report the results of a large-scale pre-registered online experiment (N = 3,552) indicating diminished fairness, trust, trustworthiness, cooperation, and coordination by human players in economic twoplayer games, when the decision of the interaction partner is taken over by ChatGPT.
2401.13588_1993142_13	Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized.
2401.13641_1993195_6	Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field.
2401.15269_1994823_2	To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation.
2401.16186_1995740_4	Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.   
2401.16587_1996141_3	However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues.
2401.16960_1996514_8	The final prediction of the equivalent entity is derived from the LLM's output.
2401.17390_1996944_4	The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself.
2401.17459_1997013_3	Such improvement can be made, as a first step, by engineering prompts fed to the LLM based on the responses produced, to include relevant contexts and structures so that the model provides more accurate results.
2401.17461_1997015_8	The evaluation results show an overall good quality of the dialogues, though research is still needed to improve the quality of the GPT-4 evaluation metrics.
2401.17622_1997176_6	We compare the results obtained with ChatGPT to previous automatic commit message generation methods that have been trained specifically on commit data.
2401.18028_1997582_2	However, performance evaluations of LLMs for such tasks are still needed, including examining the general quality of generated impacts but also the range of types of impacts produced and resulting biases.
2402.00024_1997663_0	  Purpose: Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI are increasingly recognized for their potential in the field of cheminformatics, particularly in understanding Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures.
2402.00350_1997989_5	In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024.
2402.00858_1998497_1	However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features.
2402.01158_1998797_8	Furthermore, since LLM-Detector is trained based on open-source LLMs, it is easy to customize for deployment.
2402.01684_1999323_0	  With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain.
2402.01723_1999362_4	However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied.
2402.01731_1999370_1	In this context, validity examinations were conducted on data sets generated according to the Two-Parameter Logistic Model (2PLM) with algorithms written by ChatGPT 3.5 and researchers.
2402.01748_1999387_1	However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications.
2402.01750_1999389_5	To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and cost.
2402.01817_1999456_7	We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs.
2402.01874_1999513_8	We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.
2402.01881_1999520_2	To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization).
2402.02008_1999647_6	Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide.
2402.02136_1999775_5	Moreover, whenever the user intent is correctly recognized, while users are more satisfied with the intent-based reformulations of GPT-4 compared to GPT-3.5, they tend to be more satisfied with the models' answers to their original prompts compared to the reformulated ones.
2402.02834_2000473_8	Code and models can be found at: https://github.com/Nota-NetsPresso/shortened-llm
2402.02987_2000626_0	  Significant advancements have recently been made in large language models represented by GPT models.
2402.03009_2000648_1	Although there exist various methods devoted to enhancing the long-context processing ability of LLMs, they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments.
2402.03435_2001074_1	The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy.
2402.03719_2001358_9	The applicability of LaMAI is further evidenced by its successful integration with various LLMs, highlighting its potential for the future of interactive language models.
2402.03744_2001383_5	Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.
2402.04110_2001749_3	The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4.
2402.04110_2001749_8	This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5.
2402.05110_2002749_1	We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30).
2402.05445_2003084_0	  The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware.
2402.06360_2003999_4	In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents.
2402.07179_2004818_1	Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs.
2402.07179_2004818_2	However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied.
2402.07945_2005584_1	The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent.
2402.08113_2005752_6	Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.
2402.09091_2006730_3	However their explicitly mention of malicious intent will be easily recognized and defended by LLMs.
2402.09216_2006855_5	This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches.
2402.09649_2007288_5	The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM.
2402.10083_2007722_10	Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation.
2402.10412_2008051_4	In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), an innovative hallucination metric that is specifically designed for the scenario when gold-standard answers are absent.
2402.10767_2008406_3	Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2).
2402.10835_2008474_7	This observation can be explained by the ability of LLMs to recognize the underlying period within datasets, which is supported by our experiments.
2402.10835_2008474_8	In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases substantially improve the predictive performance of LLMs for time series.
2402.11005_2008644_4	To further illustrate the theory, we demonstrate that concept prototypes in LLMs are affected by prescriptive norms, similar to the concept of normality in humans.
2402.11060_2008699_3	In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization.
2402.11753_2009392_2	However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics.
2402.11753_2009392_5	In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics.
2402.12212_2009851_2	The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities.
2402.12566_2010205_3	GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support.
2402.12801_2010440_2	Named Entity Recognition (NER) is a critical task in information extraction that is not covered in recent LLM benchmarks.
2402.14007_2011646_2	Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.
2402.14007_2011646_3	Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.
2402.14261_2011900_7	These success metrics are designed to evaluate the performance of LLMs within a given IDE and its respective parameter space.
2402.14296_2011935_4	Therefore, in this paper, we propose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs.
2402.14805_2012444_0	  As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs.
2402.14845_2012484_1	Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs.
2402.14846_2012485_2	We argue that context-dependence (specifically, value stability) should be studied as a specific property of LLMs and used as another dimension of LLM comparison (alongside others such as cognitive abilities, knowledge, or model size).
2402.14848_2012487_1	Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood.
2402.14903_2012542_2	With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers.
2402.14904_2012543_2	We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.
2402.15911_2013550_2	More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM.
2402.15929_2013568_2	However, existing evaluations of LLMs on knowledge comprehension are typically conducted on small test sets, but these datasets represent only a tiny fraction of the vast number of possible queries.
2402.16539_2014178_7	These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures.
2402.17097_2014736_4	Re-Ex revises the initial response of LLMs using 3-steps : first, external tools are used to retrieve the evidences of the factual errors in the initial LLM response; next, LLM is instructed to explain the problematic parts of the response based on the gathered evidence; finally, LLM revises the initial response using the explanations provided in the previous step.
2402.17385_2015024_7	Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.
2402.17887_2015526_3	Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase.
2402.18120_2015759_0	  Prior research has revealed that certain abstract concepts are linearly represented as directions in the representation space of LLMs, predominantly centered around English.
2402.18439_2016078_2	NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined.
2402.19366_2017004_2	A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations.
2403.00858_2017977_2	However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding.
2403.00863_2017982_3	Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters.
2403.00952_2018071_0	  Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine).
2403.01241_2018360_3	Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs.
2403.01422_2018541_5	The pipeline is carefully designed to control the style of videos by improving textual inversion technique with powerful text generation capability of GPT-4.
2403.01586_2018705_7	The function is then deduced by LLMs and zero-shot classification from a predefined catalog of IoT functions.   
2403.02454_2019573_4	Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from prompted outputs of the LLM, ChatGPT.
2403.02502_2019621_2	This learning method is designed to enhance the performance of open LLM agents.
2403.02839_2019958_3	While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of judge models.
2403.04013_2021132_0	  Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models.
2403.04784_2021903_4	Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets.
2403.05266_2022385_1	Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity.
2403.06465_2023584_2	The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences.
2403.06586_2023705_5	In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed.
2403.06660_2023779_6	Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.
2403.07747_2024866_2	FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs.
2403.08399_2025518_12	The code for this project can be found on the GitHub repository at https://github.com/GPT-Laboratory/SLR-automation.
2403.09162_2026281_1	However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood.
2403.10434_2027553_4	These spotted signs are then passed to an LLM, which transforms them into coherent and contextually appropriate spoken language sentences.
2403.11322_2028441_4	The transitions between states are controlled by heuristic rules or decisions made by the LLM, allowing for a dynamic and adaptive progression.
2403.11439_2028558_5	In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control.
2403.12601_2029720_5	LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.
2403.12958_2030077_10	Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.
2403.13073_2030192_5	This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a "canary in the coal mine" that highlights deep structural concerns about the current LLM paradigm whose harms could soon affect nearly everyone.
2403.13334_2030453_2	Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance.
2403.13381_2030500_3	The potential of the VS-LMS adaptation algorithms using a DAG is then illustrated by experimental results obtained on a relevant adaptive active noise attenuation system.
2403.14643_2031762_7	However, the use of ChatGPT has also raised several concerns, including ethical, social, and employment challenges, which must be carefully considered to ensure the responsible use of this technology.
2403.14643_2031762_11	This study is expected to contribute to a greater understanding of ChatGPT and aid in predicting the potential changes it may bring about.
2403.14938_2032057_1	Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings.
2403.15371_2032490_5	Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents.
2403.15491_2032610_6	These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.
2403.15501_2032620_7	The study also recorded an average response time of 6.12 seconds for the ChatGPT API, which is considered acceptable but has room for improvement.
2403.16446_2033565_10	Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.
2403.17125_2034244_8	Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results.
2403.17160_2034279_3	The conversational agent was developed based on the GPT-3.5 turbo model.
2403.17336_2034455_4	Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists.
2403.18341_2035460_6	Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM.
2403.19181_2036300_5	ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks.
2403.19790_2036909_6	Our model is able to deliver triage recommendations consistent with existing clinical practices and it's architecture was implemented on a single GPU, making it practical for implementation in resource-limited NHS environments where private implementations of LLM technology will be necessary to ensure confidential clinical data is appropriately controlled and governed.
2403.19876_2036995_5	This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities.
2403.20330_2037449_2	The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs.
2404.00216_2037667_2	Extensive efforts have been made to enable better outputs from LLMs by mitigating hallucinations through factuality enhancement methods.
2404.00344_2037795_4	Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation.
2404.01096_2038547_7	We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM.
2404.01475_2038926_1	However, we possess only a limited systematic understanding of the chemical capabilities of LLMs, which would be required to improve models and mitigate potential harm.
2404.01549_2039000_6	Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling.
2404.01799_2039250_1	While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?).
2404.02151_2039602_2	In this way, we achieve 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack.
2404.02403_2039854_8	Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5.
2404.02421_2039872_1	In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible.
2404.02438_2039889_6	multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like GPT-4-32k or a less accurate predictor like KNN.
2404.02575_2040026_9	Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.
2404.02893_2040344_1	While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.
2404.03411_2040862_4	Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models.
2404.03602_2041053_1	However, little research has been conducted on error detection of LLM responses.
2404.04286_2041737_4	We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades.
2404.04286_2041737_6	This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs.
2404.05182_2042633_3	Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server).
2404.06209_2043660_4	We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training.
2404.07108_2044559_4	It is determined by counting the revision edits generated by LLMs.
2404.07475_2044926_3	Studies of bias in LM responses to open-ended prompts (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms.
2404.07475_2044926_7	We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals.
2404.07499_2044950_8	Further validation results indicate that the number of user rating histories provided to LLM prompts should be carefully chosen to avoid both insufficient and excessive inputs and that the output of LLMs that show high classification performance is difficult to interpret.
2404.08627_2046078_3	We find that large language models (LLMs), represented by ChatGPT, are having an increasing impact on arXiv abstracts, especially in the field of computer science, where the fraction of LLM-style abstracts is estimated to be approximately 35%, if we take the responses of GPT-3.5 to one simple prompt, "revise the following sentences", as a baseline.
2404.08676_2046127_4	It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy.
2404.09248_2046699_3	This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods.
2404.09329_2046780_6	In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans.
2404.10209_2047660_3	DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert.
2404.10229_2047680_4	The main goal of LLM-Stega is that the secure covert communication between Alice (sender) and Bob (receiver) is conducted by using the user interfaces of LLMs.
2404.10508_2047959_8	Those who are at the intersection of gender and racial minority groups--such as Black females--are consistently described by texts with lower levels of agency, aligning with real-world social inequalities; (3) Among the 3 LLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.
2404.10922_2048373_4	Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM.
2404.11343_2048794_4	Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited.
2404.11502_2048953_1	For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it.
2404.12038_2049489_4	Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known.
2404.12549_2050000_0	  Large language models (LLMs) like ChatGPT have been widely adopted in work contexts.
2404.13340_2050791_3	Current research along this line primarily focuses on enhancing code generation with assistance from test cases generated by LLMs, while the performance of LLMs in test case generation alone has not been comprehensively examined.
2404.13571_2051022_7	Given annotations from LLMs, a two-stage training strategy is designed to tailor the test-time model with the limited and noisy labels.
2404.14928_2052379_5	Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability.
2404.15406_2052857_3	Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues.
2404.15458_2052909_0	  Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude are trained on massive quantities of text parsed from the internet and have shown a remarkable ability to respond to complex prompts in a manner often indistinguishable from humans.
2404.15578_2053029_5	In particular, (1) the ability of LLMs in automating the process of extracting specific information such as root cause of a case from unstructured data, as well as (2) the possibility of identifying similar or related deviations by performing semantic search on the database of historical records are examined.
2404.15667_2053118_15	To recommend the use of LLMs in the screening process of SRs, more research is needed.
2404.16118_2053569_9	Honeywords generated by GPT-3.5 were found to be less distinguishable from real passwords compared to previous methods of automated honeyword generation.
2404.16308_2053759_1	However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.
2404.16653_2054104_5	It was evidenced that even the most sophisticated models, such as ChatGPT and Gemini, exhibit errors and deficiencies in their responses, with explanations often providing inconsistent.
2404.16807_2054258_3	However, the diversity aspect in LLM outputs has not been systematically studied before.
2404.16891_2054342_1	This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted.
2404.17140_2054591_5	Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.
2404.17790_2055241_4	Consequently, Swallow achieved superior performance compared to other LLMs that were trained from scratch in English and Japanese.
2404.18373_2055824_4	This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.
2404.18466_2055917_3	Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge.
2404.18865_2056316_4	Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences.
2404.19509_2056960_7	Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.
2404.19737_2057188_0	  Large language models such as GPT and Llama are trained with a next-token prediction loss.
2405.00492_2057704_1	The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter.
2405.00545_2057757_0	  An approach is established for maximizing the Lower bound on the Mismatch capacity (hereafter abbreviated as LM rate), a key performance bound in mismatched decoding, by optimizing the channel input probability distribution.
2405.00728_2057940_3	The potential of ChatGPT to address the triage problem in emergency departments has been examined, while few studies have explored its application in outpatient departments.
2405.01592_2058804_1	With the introduction of ChatGPT, an evaluation of its simplification performance is needed.
2405.02466_2059678_4	Current intellectual property (IP) protection schemes for LLMs are either designed for white-box settings or require additional modifications to the original model, which restricts their use in real-world settings.   
2405.05080_2062292_1	These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them.
2405.05418_2062630_3	To reduce excessive safety behaviours -- which was discovered to be 26.1% of safe prompts being misclassified as dangerous and refused -- we use a combination of XSTest dataset prompts as well as interactive, contextual, and few-shot prompting to examine the decision bounds of LLMs such as Llama2, Gemma Command R+, and Phi-3.
2405.05600_2062812_5	We further find that, depending on the LLM employed, new runs will be highly favored (or penalized), and this effect is magnified proportionally to the size of the holes.
2405.05647_2062859_2	While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR.
2405.05777_2062989_4	ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging.
2405.06211_2063423_8	Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/
2405.06410_2063622_7	Lastly, we are surprised to discover that significant overlap in the errors is made by both LLMs and untrained humans, accounting for almost 30% of all errors.
2405.06687_2063899_3	Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering.
2405.06694_2063906_3	Through extensive evaluations, SUTRA is demonstrated to surpass existing models like GPT-3.5, Llama2 by 20-30% on leading Massive Multitask Language Understanding (MMLU) benchmarks for multilingual tasks.
2405.06713_2063925_0	  The strategic significance of Large Language Models (LLMs) in economic expansion, innovation, societal development, and national security has been increasingly recognized since the advent of ChatGPT.
2405.06808_2064020_3	A case study was conducted to assess the performance of various LLMs, demonstrating that GPT-4 outperforms other models in processing and collecting necessary information, as well as executing mathematical calculations.
2405.06931_2064143_1	Recently, numerous studies have been conducted on tasks related to relevance judgment using Large Language Models (LLMs) such as GPT-4, demonstrating significant improvements.
2405.09186_2066398_2	The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable.
2405.10616_2067828_3	Yet, its application in LLMs has not been extensively studied.
2405.11002_2068214_4	With experiments on a real network intrusion detection dataset, in-context learning proves to be highly beneficial in improving the task processing performance in a way that no further training or fine-tuning of LLMs is required.
2405.11106_2068318_1	Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent.
2405.11290_2068502_4	MBIAS is designed to significantly reduce biases and toxic elements in LLM outputs while preserving the main information.
2405.11505_2068717_3	Following this, a preliminary experiment is presented as an example to demonstrate how HCD principles can be employed to enhance user experience within GPT by using a single document input to GPT's Knowledge base as a new knowledge resource to control the interactions between GPT and users, aiming to meet the diverse needs of hypothetical software learners as much as possible.
2405.11880_2069092_1	These effects are formulated as non-linear interactions between tokens/words encoded by the LLM.
2405.12842_2070054_5	This information is then utilized by LLMs to generate a sequence of actions that are executed by a scripting engine to complete an assigned task.
2405.12939_2070151_3	We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers.
2405.13004_2070216_3	Each of the subproblems is formulated as an algebraic expression whose value is evaluated by the Python code generated by the LLM for the corresponding algebraic expression.
2405.13004_2070216_7	If the final answer matches the correct answer, it is produced as output else a refinement prompt is fed to the LLM.
2405.13820_2071032_3	In specific, we introduce \textsc{SafePatching}, a novel framework for comprehensive PSA, where two distinct safety patches are developed on the harmful data to enhance safety and mitigate over-safety concerns, and then seamlessly integrated into the target LLM backbone without compromising its utility.
2405.13845_2071057_0	  With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation.
2405.14487_2071699_5	Fundamental concepts of the progression of LLMs from Transformers, Pre-trained Transformers, and GPT is presented.
2405.14488_2071700_2	Many defense strategies have been developed to enhance the safety of LLMs.
2405.15216_2072428_1	Error correction models are designed to fix ASR errors, however, they showed little improvement over traditional LMs mainly due to the lack of supervised training data.
2405.15523_2072735_1	Memorization in LLMs is widely assumed to only occur as a result of sequences being repeated in the training data.
2405.16310_2073522_7	These relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy LLMs.
2405.16311_2073523_5	Our insights suggest that researchers and practitioners should simultaneously clarify the ``who'' in considerations of explainability and transparency, the ``what'' in the information needs, and ``why'' they are needed to ensure responsible design and development across the LLM supply chain.
2405.16363_2073575_2	The framework controls the interfacing between the LLMs and the classic recommendation models through "interest clusters", the granularity of which can be explicitly determined by algorithm designers.
2405.16450_2073662_4	We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs.
2405.17067_2074279_2	This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs.
2405.17067_2074279_8	Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs.
2405.18241_2075453_0	  Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science.
2405.18433_2075645_0	  We perform a missing, reproducible evaluation of all publicly available GPT-4 family models concerning the Document Understanding field, where it is frequently required to comprehend text spacial arrangement and visual clues in addition to textual semantics.
2405.18492_2075704_10	Code can be found at https://github.com/felixbmuller/llms-memorization-copyright.
2405.18632_2075844_3	Therefore, we conducted a qualitative analysis of LLM assessment comments, showing that: 1) LLMs can match the assessment capabilities of faculty members, 2) variations in LLM assessments should be interpreted as diversity rather than confusion, and 3) assessments by humans and LLMs can differ and complement each other.
2405.18710_2075922_3	However, prior experience with FP16, which was found to be less stable than BF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be a cost-effective option for LLM training.
2405.19209_2076421_9	Moreover, on the long split of Video-MME (average 44 minutes), VideoTree achieves better performance than GPT-4V and many other MLLMs that were extensively trained on video data.
2405.19334_2076546_8	A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
2405.19487_2076699_4	All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time.
2405.19578_2076790_3	More importantly, the performance of these LLMs has not been compared and discussed in detail when domain-specific data analysis tasks are needed.
2405.19631_2076843_5	Further, clinical notes contain trusted health information making the use of closed-source language models from commercial vendors difficult, so the identification of open source LLMs that can be run within health organizations and exhibits high performance on SDOH tasks is an urgent problem.
2405.20099_2077311_4	Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs.
2405.20770_2077982_2	In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM.
2405.20947_2078159_7	Our datasets are available at https://huggingface.co/datasets/bench-llm/or-bench and the demo can be found at https://huggingface.co/spaces/bench-llm/or-bench.
2406.00936_2079224_1	Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake.
2406.01538_2079826_9	Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure.
2406.01698_2079986_10	Users can also be tried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on your web browser.
2406.03007_2081295_5	Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools.
2406.03079_2081367_4	Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem.
2406.03079_2081367_8	It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT.
2406.03085_2081373_7	Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability.
2406.04640_2082928_5	This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP.
2406.04817_2083105_1	The chatbot was intentionally designed to mimic proprietary commercial chatbots such as ChatGPT where the chatbot has not been tailored for the educational context; the underlying engine was OpenAI GPT-4.
2406.04941_2083229_4	Extensive evaluation of various general LLMs and medical-domain-specific LLMs is conducted.
2406.05232_2083520_6	DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment.
2406.05431_2083719_5	Through comprehensive evaluations of the GPT usage cost, labeling cost, and extraction accuracy for the learning methods of zero-shot, few-shot and fine-tuning, we present a Pareto-front mapping where the few-shot learning method was found to be the most balanced solution owing to both its high extraction accuracy (total F1 score>95%) and low cost (GPT usage cost of 5.97 US dollars and labeling cost of 10 I/O paired examples).
2406.05741_2084029_3	In contrast, large language models (LLMs) represented by ChatGPT and natural language processing utilizing LLMs have been developed revolutionarily.
2406.05804_2084092_5	Resources have been made publicly available at in our GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.
2406.05900_2084188_2	Seemingly remarkable results have been reported for such LLM-based HAR systems when they are evaluated on standard benchmarks from the field.
2406.05972_2084260_2	Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood.
2406.05972_2084260_6	However, there are significant variations in the degree to which these behaviors are expressed across different LLMs.
2406.07295_2085583_2	Separate preference models are trained for each principle using feedback from GPT-3.5-Turbo.
2406.07528_2085816_8	Our code can be found in https://github.com/dvlab-research/Q-LLM.
2406.07594_2085882_8	Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4.
2406.08527_2086815_4	We use decision trees to convey this reasoning information, as they can be easily represented in natural language, effectively providing knowledge from prior experiments (i.e., the impact of the generated features on performance) to the LLMs.
2406.08751_2087039_4	Experiments are conducted to evaluate the completeness and satisfaction of buildings generated via LLMs.
2406.09012_2087300_1	This raises questions about the human-likeness of LLM-derived information, alignment with human intuition, and whether LLMs could possibly be considered (parts of) explanatory models of (aspects of) human cognition or language use.
2406.09671_2087959_1	This study investigates the performance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the time the study was conducted, on the Bachelor in Computer Science section of Brazil's 2021 National Undergraduate Exam (ENADE).
2406.10300_2088588_12	LLM-integrated applications are described as combinations of their LLM components.
2406.10303_2088591_3	Most medical LLMs are developed through continued training of open-source general LLMs, which require significantly fewer computational resources than training LLMs from scratch.
2406.10450_2088738_6	Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time.
2406.10918_2089206_3	However, most prior work dealing with Multi-LLM QA has focused on scenarios where the models are asked in a zero-shot manner or are given information sources to extract the answer.
2406.11020_2089308_6	Additionally, common error types are identified through manual inspection, revealing specific challenges faced by LLMs in different reasoning contexts.
2406.11046_2089334_4	These results suggest that AI tools like ChatGPT can substantially boost developer productivity, though further analysis is needed to address potential downsides such as low quality code and privacy concerns.
2406.11116_2089404_7	Significant correlations were also found between ChatGPT and laypeople across all tasks, though the correlation strength varied by task.
2406.11131_2089419_2	In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs.
2406.11336_2089624_5	Furthermore, a data enhancement strategy is designed to mitigate the impact of LLM hallucinations on forecasting results.
2406.11341_2089629_5	Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter mitigates most reasoning biases without harming model consistency.
2406.11629_2089917_1	However, this kind of evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs.
2406.11670_2089958_2	Different approaches and implemented detectors for the recognition of LLM-generated text are presented.
2406.12296_2090584_7	Based on the GPT-4-generated scripts, key visuals were created for the air taxi, and the ATJ was evaluated by 72 participants.
2406.12479_2090767_4	In this paper, we designed a high-quality, diversified, and unified multimodal instruction-following dataset for RSI understanding produced by GPT-4V and existing datasets, which we called RS-GPT4V. To achieve generalization, we used a (Question, Answer) which was deduced from GPT-4V via instruction-following to unify the tasks such as captioning and localization; To achieve complex scene, we proposed a hierarchical instruction description with local strategy in which the fine-grained attributes of the objects and their spatial relationships are described and global strategy in which all the local information are integrated to yield detailed instruction descript; To achieve reasoning, we designed multiple-turn QA pair to provide the reasoning ability for a model.
2406.12775_2091063_4	By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model.
2406.12935_2091223_2	Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale.   
2406.13229_2091517_2	While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs.
2406.13261_2091549_3	Enhancing honesty in LLMs addresses critical limitations and helps uncover latent capabilities that are not readily expressed.
2406.13893_2092181_5	Leveraging continual pretraining, we adapt to Galician two existing LLMs trained on larger corpora, thus mitigating the data constraints that would arise if the training were performed from scratch.
2406.13925_2092213_1	Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases.
2406.13993_2092281_6	Our study provides insight into how biases and stereotypes are realized within LLMs when adopting different national personas.
2406.14230_2092518_3	Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs.
2406.14903_2093191_0	  As large language models (LLMs) continue to develop and gain widespread application, the ability of LLMs to exhibit empathy towards diverse group identities and understand their perspectives is increasingly recognized as critical.
2406.14903_2093191_3	GIEBench is designed to evaluate the empathy of LLMs when presented with specific group identities such as gender, age, occupation, and race, emphasizing their ability to respond from the standpoint of the identified group.
2406.14955_2093243_1	Evaluating the ICL ability of LLMs can enhance their utilization and deepen our understanding of how this ability is acquired at the training stage.
2406.14986_2093274_5	As text completion is at the core of LLMs, these results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.
2406.15130_2093418_2	To create these tools, examples of texts generated by LLMs are needed.
2406.15259_2093547_6	Since CoT is reported to perform poorly with small LLMs, we adopted a strategy in which a large LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to fine-tune a small model, Llama-2-7B, which plays the role of a Student.
2406.15325_2093613_5	Our benchmark, Bug In The Code Stack (BICS), is designed to assess the ability of LLMs to identify simple syntax bugs within large source code.
2406.15379_2093667_0	  The recent, widespread availability of Large Language Models (LLMs) like ChatGPT and GitHub Copilot may impact introductory programming courses (CS1) both in terms of what should be taught and how to teach it.
2406.15504_2093792_3	To bridge this gap, we introduce an end-to-end modality-aligning framework for LLM-graph alignment: Dual-Residual Vector Quantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully designed to facilitate token-level alignment with LLMs, enabling an effective translation of the intrinsic `language' of graphs into comprehensible natural language.
2406.15673_2093961_6	Leveraging these factors, we demonstrate that intrinsic self-correction ability is exhibited across multiple existing LLMs.
2406.15741_2094029_4	MT-Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost.
2406.15891_2094179_3	The same prompt is presented to LLMs and human writers, and evaluation is performed by humans using a detailed rubric including various aspects like fluency, style, originality or humor.
2406.16801_2095089_1	Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems.
2406.17588_2095876_9	For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).
2406.17781_2096069_5	Variability in GPT-4's performance across concepts could be explained by specificity of the concept's color-concept association distribution.
2406.17789_2096077_0	  The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics.
2406.17888_2096176_5	Two LM-based evaluation methods are developed to compare the actual baseline feature lists against LM-generated responses.
2406.17975_2096263_5	In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered.
2406.18510_2096798_1	Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system.
2406.18725_2097013_3	However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet.
2406.19358_2097646_2	The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied.
2406.19840_2098128_3	The insights from this research are expected to be beneficial for enhancing the robustness of and accuracy of LLMs, particularly in the development and assessment of tokenizers.
2407.00497_2098884_6	Our code can be found at https://yingjiahao14.github.io/LLMs-as-Instructors-pages/.
2407.00869_2099256_4	Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism.
2407.01082_2099469_5	Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its significant impact on improving text generation quality.
2407.01178_2099565_4	The model is named $\text{Memory}^3$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values).
2407.03856_2102243_7	Thanks to the residual Q-learning framework, we can restore the customized LLM with the pre-trained LLM and the \emph{residual Q-function} without the reward function $r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function $r_2$ can be derived from the residual Q-function, enabling us to directly learn the residual Q-function from the new human preference data upon the Bradley-Terry model.
2407.04121_2102508_3	RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics.
2407.04467_2102854_4	Our structured evaluation of GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making decisions in these games, are affected by at least one of the following systematic biases: positional bias, payoff bias, or behavioural bias.
2407.04675_2103062_3	Seed-ASR is developed based on the framework of audio conditioned LLM (AcLLM), leveraging the capabilities of LLMs by inputting continuous speech representations together with contextual information into the LLM.
2407.04694_2103081_0	  AI assistants such as ChatGPT are trained to respond to users by saying, "I am a large language model".
2407.04752_2103139_6	The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations.
2407.04868_2103255_1	The majority of the studies in this direction only focus on the improvements in performance of the LMs on different benchmarks, whereas LMs are considered black boxes.
2407.05205_2103592_6	These questions are presented to ChatGPT, followed by interactions to assess its effectiveness in delivering complete and meaningful responses.
2407.05347_2103734_2	In this paper, we analyze the impact of LLM output token distribution on the inference queueing delay, where the max-token clipping and the batched inference are considered.
2407.05502_2103889_1	Although the multilingual capability of LLMs offers new opportunities to overcome the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences?
2407.05710_2104097_1	and so it is expected that, following the release of code-generative AI tools, such as ChatGPT and GitHub Copilot, developers will use these tools to perform security tasks and use security APIs.
2407.06645_2105032_9	Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages.
2407.06908_2105295_6	Using emotion attribution, we explore how different religions are represented in LLMs.
2407.06917_2105304_2	We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model's internal representations.
2407.07321_2105708_3	While the effectiveness of LLM-based QA systems has already been established at an acceptable level in popular domains such as trivia and literature, it has not often been established in niche domains that traditionally require specialized expertise.
2407.07796_2106183_7	This study enhances our understanding of LLMs' capabilities in playing games they were not specifically trained for, helping to assess their rule comprehension and strategic thinking.
2407.08410_2106797_6	Furthermore, in a single-blind reader study two senior ophthalmologists with up to 32 years of experience found RetinaVLM's reports were found to be substantially more accurate than those by ChatGPT-4o (64.3% vs. 14.3%).
2407.08440_2106827_10	The data and code can be found at: https://anonymous.4open.science/r/llm-rule-following-B3E3/
2407.09975_2108362_0	  Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot.
2407.11654_2110041_4	Based on this analysis, a physical layer framework is developed for resilient SFL with LLMs (R-SFLLM) over wireless networks.
2407.11919_2110306_5	Our experiments show that these errors can be identified with high accuracy by an LLM.
2407.12022_2110409_1	However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire.
2407.12024_2110411_5	The advantages of the proposed model are demonstrated on a set of scenarios, as well as a comparative analysis with various LLM implementations.
2407.12145_2110532_13	The usage and perceived utility of ChatGPT were moderate, but positive correlations between student grade and ChatGPT usage were found.
2407.12854_2111241_6	Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs.
2407.13069_2111456_3	In addition, the issues of variability and reproducibility of results from each trial of LLMs have rarely been considered in existing literature.
2407.13244_2111631_5	We also conclude that while the proposed benchmark is useful for identifying LLMs that are adequate for process mining tasks, further research is needed to overcome the evaluation biases and perform a more thorough ranking of the competitive LLMs.
2407.14088_2112475_2	Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks.
2407.14246_2112633_2	Unipa-GPT relies on gpt-3.5-turbo, it was presented in the context of the European Researchers' Night (SHARPER night).
2407.14246_2112633_4	The whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned systems are compared, and a brief discussion on their performance is reported.
2407.14402_2112789_7	The code will be made available at https://aka.ms/ACV-LLM.
2407.15050_2113437_5	Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent.
2407.15677_2114064_1	Promising recent research has been conducted showing that the knowledge contained in LLMs can be utilized in making goal-driven decisions that are enactable in interactive, embodied environments.
2407.16434_2114821_5	Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval).
2407.17390_2115777_1	However, most existing evaluation practices are not designed for LLM-generated topics and result in low inter-annotator agreement scores, hindering the reliable use of LLMs for the task.
2407.17730_2116117_4	Many concerns have been raised by mental health experts regarding the use of LLMs for therapy.
2407.17915_2116302_1	While extensive research has been conducted on the safety of LLMs in chat mode, the security implications of their function calling feature have been largely overlooked.
2407.18008_2116395_9	While our findings underscore that LLM responses can be ideologically steered with political personas, they suggest that observed changes in LLM outputs could be better described as personalization to the given context rather than sycophancy.
2407.18276_2116663_9	Tools for the Recurrent Optimization via Machine Editing (ROME) method can be found at https://github.com/ajn313/ROME-LLM
2407.18418_2116805_0	  Abstention, the refusal of large language models (LLMs) to provide an answer, is increasingly recognized for its potential to mitigate hallucinations and enhance safety in LLM systems.
2407.18607_2116994_7	Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.
2407.19354_2117741_3	At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed.
2407.21264_2119651_8	This method is designed to enhance the model's robustness to variations in prompts and focuses on distinguishing between different source LLMs.
2408.00724_2120905_0	  While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored.
2408.01055_2121236_7	When an unhandled runtime error occurs, Healer will be activated to generate a piece of error-handling code with the help of its internal LLM and the code will be executed inside the runtime environment owned by the framework to obtain a rectified program state from which the program should continue its execution.
2408.01935_2122116_0	  Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks.
2408.02143_2122324_10	We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.
2408.02232_2122413_1	Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent.
2408.02450_2122631_12	Based on this benchmark, extensive experiments are conducted to evaluate the abilities and limitations of seven advanced LLMs.
2408.02651_2122832_1	To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs.
2408.02811_2122992_1	REGAI uses rubrics, which can be created manually or automatically by the system, to enhance the performance of LLMs for evaluation purposes.
2408.03074_2123255_0	  In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out.
2408.04666_2124847_3	Moreover, there are important explanations of LLM behavior and capabilities that are lost when we engage in this kind of reduction.
2408.04905_2125086_5	We first reveal the characteristic features induced by glitch tokens on LLMs, which are evidenced by significant deviations in the distributions of attention patterns and dynamic information from intermediate model layers.
2408.05126_2125307_1	Despite the promising capabilities of various LLMs in conducting qualitative analysis, their use in the humanities and social sciences has not been thoroughly examined.
2408.05128_2125309_5	However, 14.2% of the recommended libraries had restrictive copyleft licenses, which were not explicitly communicated by ChatGPT.
2408.05542_2125723_4	Specifically, we first propose a set of ChatGPT prompting rules that are specifically designed for source code and queries.
2408.06621_2126802_8	Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.
2408.06752_2126933_2	This article assesses which ChatGPT inputs (full text without tables, figures and references; title and abstract; title only) produce better quality score estimates, and the extent to which scores are affected by ChatGPT models and system prompts.
2408.06837_2127018_0	  Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways?
2408.07904_2128085_9	All code, dataset, and the generated responses can be found in https://github.com/tanny411/llm-reliability-and-consistency-evaluation.
2408.07982_2128163_1	In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots.
2408.08054_2128235_5	Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework.
2408.08231_2128412_1	Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment.
2408.09459_2129640_4	WPN is designed to modify the output distribution of LMs by eliminating specific harmful outputs (e.g., replacing toxic responses with neutral ones), thereby transforming the model's behavior from "harmful prompt-harmful output" to "harmful prompt-harmless response".
2408.09639_2129820_1	Conventional approaches directly compare sentence probabilities assigned by LMs, but recent large language models (LLMs) are trained to perform tasks via prompting, and thus, the raw probabilities they assign may not fully reflect their grammatical knowledge.
2408.09878_2130059_3	In this paper, we focus on whether existing mini-LLMs may be unconsciously instructed in backdoor knowledge by poisoned teacher LLMs through knowledge distillation (KD).
2408.10417_2130598_4	The system is described and the efficacy of the LLM for populating the model is analyzed herein.
2408.10682_2130863_7	It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness.
2408.11517_2131698_1	The proposed method explores the multimodal capabilities of GPT-4o to interpret visual content and create engaging stories, which are illustrated by a Stable Diffusion XL model.
2408.11862_2132043_0	  In this study, the emotion and tone of preservice teachers' reflections were analyzed using sentiment analysis with LLMs:
2408.12787_2132968_4	LLM-PBE is designed to analyze privacy across the entire lifecycle of LLMs, incorporating diverse attack and defense strategies, and handling various data types and metrics.
2408.12787_2132968_7	Aimed at enhancing the breadth of knowledge in this area, the findings, resources, and our full technical report are made available at https://llm-pbe.github.io/, providing an open platform for academic and practical advancements in LLM privacy assessment.
2408.13467_2133648_6	Extensive experiments with leading edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks.
2408.13510_2133691_0	  Large Language Model (LLM) workloads have distinct prefill and decode phases with different compute and memory requirements which should ideally be accounted for when scheduling input queries across different LLM instances in a cluster.
2408.14007_2134188_7	Our key findings reveal that: (i) in our qualitative analyses, when the documents generated by GPT were compared with the original ones, 69.7% were considered equivalent (45.7%) or required minor changes to be equivalent (24.0%); (ii) indeed, 22.4% of the comments were rated as having superior quality than the original ones; (iii) the use of quantitative metrics is susceptible to inconsistencies, for example, comments perceived as having higher quality were unjustly penalized by the BLEU metric.
2408.14853_2135034_6	The proposed attacker is trained within a reinforcement learning scheme with the LLM outputting probability of the target answer as the reward.
2408.15409_2135590_1	For this we assess over 2,000 research works based on criteria typical of what is considered good research (e.g. presence of statistical tests and reproducibility) and cross-validate it with arguments that are at the centre of controversy (e.g., claims of emergent behaviour, the use of LLMs as evaluators).
2408.15769_2135950_2	With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions.
2408.15792_2135973_0	  In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori.
2408.16098_2136279_4	The first is a language-based representation involving relations of sub-events that can be learned by LLMs via fine-tuning.
2408.16400_2136581_3	We evaluate the performance of six open-source models that are specifically trained for vulnerability detection against six general-purpose LLMs, three of which were further fine-tuned on a dataset that we compiled.
2408.16400_2136581_5	The findings highlight significant variations in classification accuracy across benchmarks, revealing the critical influence of fine-tuning in enhancing the detection capabilities of small LLMs over their larger counterparts, yet only in the specific scenarios in which they were trained.
2408.16779_2136960_5	While they can be complemented by formal systems, the properties delivered by LLMs regarding inductive learning, are not well understood and quantified.
2408.16978_2137159_4	For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions.
2408.17175_2137356_2	However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM.
2409.00105_2137737_7	Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found to be suffering from this syndrome.
2409.00557_2138189_1	However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world.
2409.01247_2138879_4	This raises the question: How much conversational effort is needed to elicit harmful information from LLMs?
2409.01345_2138977_4	This design is intended to make better use of the LM's instruction-following capability.
2409.01584_2139216_1	However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English.
2409.02228_2139860_1	We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels.
2409.02228_2139860_5	Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs' initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization.
2409.02387_2140019_5	The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities.
2409.02387_2140019_6	Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition.
2409.02569_2140201_8	This bias should be considered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.
2409.03161_2140793_6	The differences and similarities in the performance of LLMs measured by the MaterialBENCH are analyzed and discussed.
2409.03291_2140923_5	A purpose-trained detector generalizing across LLMs and unseen attacks can be developed, but it fails to generalize to new human-written texts.   
2409.03701_2141333_1	Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods.
2409.04056_2141688_3	Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM.
2409.04109_2141741_4	By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility.
2409.05247_2142879_3	While expanding the use of LLMs to more languages may bring many potential benefits, such as assisting cross-community communication and language preservation, great care must be taken to ensure that data collection on these languages is not extractive and that it does not reproduce exploitative practices of the past.
2409.05247_2142879_5	Furthermore, linguistic complexity and cultural nuances are often lost in LLMs.
2409.08087_2145719_3	Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises.
2409.08087_2145719_7	This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.
2409.09040_2146672_5	The outputs of the simulations are then interpreted by the LLM resulting in informative comparisons and summaries.
2409.09280_2146912_4	We replaced the disputes that were prepared by the courts with the itemized disputes that were generated by GPT-3.5 and GPT-4, and repeated the same experiments.
2409.09380_2147012_4	At the same time, an inconsistency-checking approach between the LLMs' output and the reasoning process is adopted for the allocation APIs confirmation with an off-the-shelf Natural Language Processing (NLP) tool.
2409.10245_2147877_7	Mechanistic Interpretability analysis showed that this latent behaviour of LLMs could be traced to specific neurons that became activated or amplified after PEFT.
2409.10550_2148182_7	But the evaluation result shows that only weak sign of statistical truthfulness can be produced due to limited capability of current LLMs.
2409.10576_2148208_3	An automated pipeline was developed to benchmark the performance of various LMs and RAG configurations.
2409.10955_2148587_3	We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works.
2409.11022_2148654_1	However, existing datasets are designed for traditional machine learning methods, inadequate for LLM-based methods in terms of corpus selection, entity categorization, and design logic.
2409.12866_2150498_1	Extensive efforts have been made to evaluate the abilities of code LLMs in various aspects, with an increasing number of benchmarks and evaluation frameworks proposed.
2409.12866_2150498_7	In particular, four specification-related tasks are designed meticulously to assess the capability of LLMs from basic to advanced levels.
2409.12866_2150498_8	Counterfactual analysis is further conducted to study the performance variance of LLMs under semantics-preserving perturbations.
2409.12917_2150549_0	  Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs.
2409.13373_2151005_4	OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM).
2409.13884_2151516_2	Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs.
2409.13897_2151529_3	A comprehensive evaluation of LLMs is conducted to assess their capabilities in these languages, revealing the challenges of multilingual and multicultural generalization.
2409.13902_2151534_8	62.5% of the top 10 documents retrieved by RAG were selected as the top references in the LLM response, with an average ranking of 4.9.
2409.14037_2151669_9	Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.
2409.14478_2152110_1	However, high-quality evidence is urgently needed on the potential and limitation of LLMs in providing accurate clinical decisions based on real-world medical data.
2409.15551_2153183_4	Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction.
2409.16732_2154364_3	These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles.
2409.16900_2154532_1	The grounding of LLMs knowledge into the empirical world has been considered a crucial pathway to exploit the efficiency of LLMs in robotics.
2409.16900_2154532_4	The roadmap for LLMs grounding is envisaged in an active bodily system as the reference point for experiencing the environment, a temporally structured experience for a coherent, self-related interaction with the external world, and social skills to acquire a common-grounded shared experience.
2409.17561_2155193_2	Despite the growing interest, limited efforts have been made to thoroughly evaluate the actual capabilities of LLMs in this task.   
2409.17698_2155330_4	The findings include: First,through several rounds of iterations the inter-reliability between GPT and human raters reached a level that is generally accepted by educators.
2409.18014_2155646_2	Moreover, a dilemma was often encountered when we tried to select the most suitable LLM from a large number of LLMs amidst explosive growth aiming for outstanding performance, affordable prices, and short response delays.
2409.18290_2155922_2	To address this, we introduce RadOnc-GPT, a specialized Large Language Model (LLM) powered by GPT-4 that has been designed with a focus on radiotherapeutic treatment of prostate cancer with advanced prompt engineering, and specifically designed to assist in generating responses.
2409.18764_2156396_3	Experiments were conducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with visualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1 70B-Instruct models.
2409.18957_2156589_4	The classification is performed by LLMs using a method similar to that used by humans who manually explore and understand the data to decide classifications.
2409.18989_2156621_5	Unlike the large models used in StarCraft LLMs such as GPT-3.5, Phi2 is trained primarily on textbook data and contains little inherent knowledge of StarCraft II beyond what is provided by our training process.
2409.18996_2156628_7	An associated GitHub repository that collects the relevant papers can be found at https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs
2409.19382_2157014_2	However, the autoregressive nature of LLMs inherently poses a challenge as errors may accumulate if mistakes are made in the intermediate reasoning steps.
2409.19450_2157082_5	Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage.
2410.00699_2158900_4	Specifically, we consider a popular tuning paradigm for downstream tasks, head tuning, where all pre-trained parameters are frozen and only individual heads are trained atop pre-trained LLMs.
2410.01306_2159507_7	Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and con-textually relevant responses.
2410.01487_2159688_2	Yet, the basic linguistic units processed in these LMs are determined by subword-based tokenization, which limits their validity as models of learning at and below the word level.
2410.02338_2160539_2	While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored.
2410.02425_2160626_1	The performance of an LLM inference service is largely determined by the hardware onto which it is deployed, but understanding of which hardware will deliver on performance requirements remains challenging.
2410.02653_2160854_6	Our findings indicate that the persuasiveness of LLMs correlates positively with model size, but smaller models can also be made to have a higher persuasiveness than much larger models.
2410.02958_2161159_3	These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs.
2410.03018_2161219_1	As GenAI technologies, such as ChatGPT, become increasingly integrated into educational settings, teachers are required to adapt to evolving classroom dynamics, where AI plays a significant role in content creation, personalized learning, and student engagement.
2410.03124_2161325_2	To fine-tune a black-box LLM, labeled data are always required to adjust the model parameters.
2410.03168_2161369_7	Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs.
2410.03255_2161456_2	To objectively assess the capabilities of existing LLMs, performance benchmarks are conducted.
2410.03769_2161970_2	Moreover, the safety mechanisms of LLMs in scientific tasks are insufficiently studied.
2410.04698_2162899_2	Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios.
2410.06992_2165193_12	In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.
2410.07739_2165940_1	Although many efforts have been made, it is still a challenge to balance the training budget, downstream performance, and the general capabilities of the LLMs in many applications.
2410.07826_2166027_1	While morally clear scenarios are more discernible to LLMs, greater difficulty is encountered in morally ambiguous contexts.
2410.08068_2166269_6	Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs.
2410.08102_2166303_3	In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process.
2410.08475_2166676_1	However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass.
2410.08545_2166746_8	Additionally, we find that the personalities of LLMs are derived from their pre-trained data.
2410.09024_2167225_0	  The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots.
2410.09699_2167900_5	Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results.
2410.10456_2168657_4	These allocators are designed to be fully pluggable, making it broadly applicable across all mainstream MoE-based LLMs.
2410.10852_2169053_5	Preliminary findings are presented with the approach applied to ChatGPT-4 generated test sentences.
2410.10870_2169071_1	However, pretrained LLMs such as ChatGPT are periodically evolved, i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application.
2410.10991_2169192_1	A study based on prompt engineering was carried out to uncover how LLMs discriminate varieties of Brazilian Portuguese, specifically if sociolinguistic rules are taken into account in four LLMs: GPT 3.5, GPT-4o, Gemini, and Sabi.-2.
2410.11720_2169921_5	ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training.
2410.12126_2170327_4	A natural question arises: Is there a kind of graph representation that can be described by natural language for LLM's understanding and is also easy to require to serve as the raw input for LLMs?
2410.12298_2170499_4	This structure is designed to reflect the input question and generate more validated deductive knowledge, thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive integration.
2410.13032_2171233_1	One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits.
2410.13073_2171274_3	A wide range of model explanation approaches have been developed for deep learning models, However, these local explanations are designed for single-output tasks like classification and regression,and cannot be directly applied to LLMs, which generate sequences of tokens.
2410.13213_2171414_8	After that, to prevent hallucinations in LLMs, such as sacrificing solving accuracy to avoid execution errors, the model alignment and self-correction mechanism are adopted in LLMOPT.
2410.13453_2171654_4	We introduce two approaches: (1) LLM-Guided Augmentation Policy Optimization, where augmentation policies are selected by an LLM prior to training and iteratively refined across multiple training cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization, where policies adapt in real-time based on performance metrics.
2410.13501_2171702_1	We propose an architecture where a Reinforcement Learning (RL) Agent guides an LLM's space exploration: (1) the Agent has access to domain-specific information, and can therefore make decisions about the quality of candidate solutions based on specific and relevant metrics, which were not explicitly considered by the LLM's training objective; (2) the LLM can focus on generating immediate next steps, without the need for long-term planning.
2410.13918_2172119_6	We introduce the FTSmartAudit framework, which is designed to develop cost-effective, specialized models for smart contract auditing through the fine-tuning of LLMs.
2410.14235_2172436_5	To enhance consistency across languages, we propose novel "Compositional Representations" where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations.
2410.14975_2173176_2	Specifically, the out-of-distribution detection (OoDD) capabilities of large vision-language models (LVLMs), such as GPT-4o, which are trained on massive multi-modal data, have not been sufficiently addressed.
2410.15153_2173354_2	However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other.
2410.15288_2173489_10	Its robustness was proven through consistent performance across different LLM architectures.
2410.15884_2174085_3	Quantitative scores generated by GPT model have been analyzed using Bayesian regression to derive trend lines.
2410.16314_2174515_1	This paper explores activation engineering, where outputs of pre-trained LLMs are controlled by manipulating their activations at inference time.
2410.16443_2174644_0	  Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns.
2410.17126_2175327_7	Our findings suggest that direct RL training of LLMs may be more suitable for relatively minor changes, such as alignment, than for learning new tasks altogether, even if an informative reward signal can be expressed programmatically.
2410.17406_2175607_3	Over 25,000 vulnerabilities have been identified so far in 2024, which are introduced after popular LLMs' (e.g., GPT-4) training data cutoff.
2410.17482_2175683_1	We study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.
2410.17558_2175759_8	Extensive experiments are conducted with 40 LLMs over 1,018 discipline-specific questions.
2410.18824_2177025_0	  Privacy vulnerabilities in LLMs, such as leakage from memorization, have been constantly identified, and various mitigation proposals have been proposed.
2410.18824_2177025_5	The experiments are executed on three different LLM architectures fine-tuned on three datasets with LoRA.
2410.18906_2177107_1	While various methods have been proposed to elicit the preferences of such models, countermeasures have been taken by LLM trainers, such that LLMs hide, obfuscate or point blank refuse to disclosure their positions on certain subjects.
2410.19811_2178012_9	The effectiveness of ControlAgent is demonstrated via extensive comparative evaluations between LLM-based and traditional human-involved toolbox-based baselines.
2410.19848_2178049_1	Further progress has been made in multimodal LLMs, with many datasets created to evaluate LLMs with vision abilities.
2410.20037_2178238_4	Furthermore, it is argued that existing dual-process computational cognitive architectures (models of the human cognitive/psychological architecture) provide usable frameworks for fundamentally enhancing LLMs by introducing dual processes (both implicit and explicit) and, in the meantime, can also be enhanced by LLMs.
2410.20749_2178950_4	Matryoshika is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance.
2410.20783_2178984_1	Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities, but these systems are still known to hallucinate, and granular uncertainty estimation for long-form LLM generations remains challenging.
2410.21008_2179209_3	The responses were analyzed by computing averages, standard deviations, and performing significance tests to investigate differences between GPT-3.5 and GPT-4.
2410.21071_2179272_1	Artifacts generated by state of the art LLM technology are expected to be useful in the sense that a user will be able to use the LLM generated artifact after a small number of easy modifications.
2410.21218_2179419_1	In recent years, a great surge has been witnessed in the introduction of both commercial and open-source LLMs.
2410.21411_2179612_6	The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired.
2410.21750_2179951_2	We investigate this question by injecting facts into LMs from a new probing dataset, "Outlandish", which is designed to permit the testing of a spectrum of different fact types.
2410.21779_2179980_1	Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved.
2410.22839_2181040_3	This limited-size benchmark was found to produce a robust ranking that correlates to human feedback at $\rho \sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings.
2410.24190_2182391_6	The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse.
2411.00154_2182581_5	In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing.
2411.00331_2182758_1	Despite efforts to improve the accuracy of LLM-based recommendation models, relatively little attention is paid to beyond-utility dimensions.
2411.00849_2183276_7	Statistical analysis was used to compare scores between feedback and non-feedback questions, and the effect of ChatGPT feedback on eye-tracking data was examined.
2411.00860_2183287_1	Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology.
2411.01245_2183672_0	  Reinforcement Learning from Human Feedback (RLHF) has been proven to be an effective method for preference alignment of large language models (LLMs) and is widely used in the post-training process of LLMs.
2411.01471_2183898_6	The framework is designed to integrate seamlessly with existing LLM systems, as it does not require modifications to the underlying architectures.
2411.01595_2184022_5	The Instruction Router is designed to generate specific prompts tailored for each corresponding LLM, guiding them to focus on distinct aspects of the RSIC task.
2411.01610_2184037_2	To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM.
2411.02523_2184950_7	Lab tests, including liver function, metabolic/toxicology panels, and serology/immune tests, were generally interpreted correctly by LLMs for differential diagnosis.
2411.02528_2184955_2	We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency.
2411.02816_2185243_9	The results and analysis of these surveys are presented to highlight the impact of ChatGPT in this context.
2411.03349_2185776_4	The resulting logic rules are translated into natural language, allowing targeted knowledge injection and seamless integration into LLM prompts for LLM's downstream task reasoning.
2411.04847_2187274_1	However, they sometimes generate responses that are logically coherent but factually incorrect or misleading, which is known as LLM hallucinations.
2411.05232_2187659_5	Additionally, the zero-shot benchmark results indicate that aggregated high-quality human reviews are overwhelmingly preferred over LLM-generated responses, even for the most capable models like GPT-4o.
2411.06145_2188572_8	Finally, 1,243 failure cases made by the best-performing LLM under test are analyzed and categorized in this paper for practical guidance and future enlightenment.
2411.06254_2188681_2	Furthermore, the internal mechanisms of LLMs during ranking are still not fully understood.
2411.06272_2188699_7	The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at \url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.
2411.06713_2189140_4	Statistically significant differences (p < 0.05) were found between Sporo and the other models, with post-hoc tests showing significant improvements over GPT-3.5, Gemma-9B, and Llama 3.2-3B. While Sporo outperformed GPT-4o by up to 10%, the difference was not statistically significant (p = 0.25).
2411.06877_2189304_5	Thus a complete replacement with LLMs is argued to be too risky and not fully reliable.
2411.06899_2189326_2	While safety alignment in short context has been widely studied, the safety concerns of long-context LLMs have not been adequately addressed.
2411.07071_2189498_0	  While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive.
2411.07091_2189518_3	Based on more than 587 patch reviews provided by RevMate, we observed that 8.1% and 7.2%, respectively, of LLM-generated comments were accepted by reviewers in each organization, while 14.6% and 20.5% other comments were still marked as valuable as review or development tips.
2411.07336_2189763_3	Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs' algorithmic abilities under simple lexical or semantic variations.
2411.07360_2189787_1	Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms.
2411.07360_2189787_8	In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME.
2411.07656_2190083_1	This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns ("he," "she") when inclusive language is needed to accurately represent all identities.
2411.07990_2190417_3	As of yet, it is not known whether linguistic generalization in LLMs could equally well be explained as the result of analogical processes, which can be formalized as similarity operations on stored exemplars.
2411.08028_2190455_1	However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required.
2411.08534_2190961_3	In LLM-ITL, global topics and document representations are learned through the NTM.
2411.08724_2191151_6	The chunks with the highest score are selected and input into the LLMs to generate responses.
2411.09266_2191693_4	Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans.
2411.10145_2192572_6	When numerical calculations are required, we use code generated by LLMs to avoid the disadvantage of LLM not being good at calculations.
2411.10163_2192590_3	This benchmark is derived from existing QA datasets, annotated with proprietary LLMs and verified by humans for accuracy.
2411.10213_2192640_5	Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.
2411.10583_2193010_2	However, readability evaluation differs among developers, so personalization of the evaluation by LLM is needed.
2411.13560_2195987_9	Simulation results of the netlist are fed back to the LLM for further topology refinement, ensuring the circuit design specifications are met.
2411.14033_2196460_3	The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.
2411.14708_2197135_2	This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space.
2411.15715_2198142_4	Comprehensive experiments were conducted with various LLMs such as Mixtral, LLaMA-2, Qwen, and PhiMoE across three test environments featuring different CPUs and GPUs.
2411.16337_2198764_3	A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring capabilities.
2411.16594_2199021_7	Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.
2411.16818_2199245_4	Clinical notes were concatenated chronologically for each patient and transformed into expert summaries using Med42-v2 70B. A multi-representational learning framework was developed to integrate these data sources, leveraging LLMs to enhance textual data while mitigating direct reliance on LLM predictions, which can introduce challenges in uncertainty quantification and interpretability.
2411.16985_2199412_3	Similar to a general-purpose LLM, we assume that our much smaller Reasoning Models may be asked arbitrary questions from unknown distributions, so we focus on evaluation in an unseen setting.
2411.17525_2199952_6	Further, we show that our method can be efficiently supported in terms of GPU kernels at various batch sizes, advancing both data-free and non-uniform quantization for LLMs.
2411.17569_2199996_4	Here, attackers inject malicious code for the training data, which can be carried over into the HDL code generated by LLMs.
2411.17855_2200282_0	  The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in computer science (CS) education is expected to be profound.
2412.00251_2202629_12	This study establishes that CBT specific fine-tuning can effectively encode therapeutic competencies in small LLMs, though significant technical and ethical considerations must be resolved prior to clinical deployment.
2412.00543_2202921_1	While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators.
2412.00546_2202924_1	In this paper, we consider the application of LLMs on symmetric tasks where a query is asked on an (unordered) bag of elements.
2412.00726_2203104_3	With the recent advancements in LLM technology, some open-source applications have been developed to address this problem.
2412.00804_2203182_1	As the problem has not been thoroughly examined yet, this study examines identity consistency across nine LLMs.
2412.01069_2203447_4	This underperformance can be traced to GPT's distinct textual and quantitative approaches: its textual processing follows a consistent, generalized pattern across firms, highlighting its strengths in language tasks.
2412.01617_2203995_2	However, we argue that the use of widespread LLMs like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose.
2412.01955_2204333_6	The findings demonstrate the potential of LLMs "out-of-the-box" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.
2412.02466_2204844_3	These 30 oaths were first translated via ChatGPT and then analyzed and compared to the human translation in terms of types of gaps left unfulfilled by ChatGPT.
2412.02466_2204844_5	It concludes that ChatGPT translation of oaths is still much unsatisfactory, unveiling the need of further developments of ChatGPT, and the inclusion of Arabic data on which ChatGPT should be trained including oath expressions, oath nuances, rituals, and practices.
2412.03123_2205501_1	We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder.
2412.03856_2206234_6	However, several issues related to potential errors arising from LLMs were identified, which can potentially mislead students.
2412.04057_2206435_2	We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs.
2412.04503_2206881_2	It is intended to be useful to those in academia and industry who are interested in gaining an understanding of the key LLM concepts and technologies, and in utilising this knowledge in both day to day tasks and in more complex scenarios where this technology can enhance current practices and processes.
2412.04922_2207300_5	The best results are produced by the Mistral7-Base LLM after fine-tuning and DPO.
2412.05563_2207941_1	However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence.
2412.06564_2208942_2	A systematic mapping study was conducted, analyzing 21 relevant studies to explore reported uses of LLMs for qualitative analysis.
2412.06603_2208981_4	Our case study characterizes the impact of an LLM-powered assistant on developers' perceptions of productivity and it shows that although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.
2412.07355_2209733_3	The few preliminary studies investigating the possible combination of LLM with BCI spellers to efficiently support fast communication and control are then described.
2412.08054_2210432_7	In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods.
2412.08599_2210977_1	The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained.
2412.09237_2211615_8	Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations.
2412.09416_2211794_2	To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusions in the mathematical domain.
2412.10139_2212517_0	  The capacity of LLMs to carry out automated qualitative analysis has been questioned by corpus linguists, and it has been argued that corpus-based discourse analysis incorporating LLMs is hindered by issues of unsatisfying performance, hallucination, and irreproducibility.
2412.10198_2212576_1	However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied.
2412.10400_2212778_7	Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.
2412.10535_2212913_7	Further research is needed to evaluate these interactions across larger models and varied architectures, offering a pathway to more reliable and generalizable LLMs.
2412.10849_2213227_10	New robust benchmarks and scalable evaluation of LLM capabilities compared to human physicians are needed along with trials evaluating AI in real clinical settings.
2412.11053_2213431_4	The dynamic nature of autoregressive token generation in LLMs is therefore not supported out of the box.
2412.11328_2213706_7	Our evaluation, which encompasses over 3,000 GUI annotations from over 100 crowd-workers with UI/UX experience, shows that SCGG, in contrast to PDGG and RAGG, can lead to more effective GUI generation, and provides valuable insights into the defects that are produced by the LLMs in the generated GUI prototypes.
2412.11908_2214286_4	Our variations preserve the mathematical core and are designed to measure the generalisability of LLM problem-solving abilities, while also increasing confidence that problems are submitted to LLMs in forms that have not been seen as training instances.
2412.13233_2215611_2	To overcome this limitation and achieve integration of LLMs and Artificial Intelligence (AI) into real-world applications, customized AI agents are being constructed.
2412.13666_2216044_3	However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet.
2412.13705_2216083_2	A gradient-based defensive suffix generation algorithm is designed to bolster the robustness of LLMs.
2412.13765_2216143_5	Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement.
2412.13845_2216223_10	Our paper's GitHub repository can be found at https://github.com/Darcyddx/Video-LLM.
2412.14471_2216849_8	In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \textit{Japanese abilities} for LLMs.
2412.14501_2216879_0	  The philosophy of language, which has historically been developed through an anthropocentric lens, is now being forced to move towards post-anthropocentrism due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude (Anthropic), which are considered to possess linguistic abilities comparable to those of humans.
2412.15574_2217952_8	Further advances in deep-sea species-specific LLMs are therefore required.
2412.15584_2217962_2	Users may over-rely on LLM advice that is confidently stated but wrong, or under-rely due to mistrust.
2412.15584_2217962_3	Reliance interventions have been developed to help users of LLMs, but they lack rigorous evaluation for appropriate reliance.
2412.15594_2217972_0	  Solving tabular math word problems (TMWPs) has become a critical role in evaluating the mathematical reasoning ability of large language models (LLMs), where large-scale TMWP samples are commonly required for LLM fine-tuning.
2412.16158_2218536_8	It is first trained to distill visual features from a pre-trained vision encoder and text embeddings from the LLM, enabling large-scale training with unpaired random images and text tokens.
2412.16216_2218594_0	  Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that has been widely adopted in various downstream applications of LLMs.
2412.16216_2218594_3	Lack of communication and collaboration among experts exacerbate the instability of LLMs due to the imbalance load problem of MoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning framework GraphLoRA, in which a graph router function is designed to capture the collaboration signals among experts by graph neural networks (GNNs).
2412.16783_2219161_2	Given that the latest generations of LLMs have digested and encoded extensive knowledge about different human subpopulations and individuals, the hope is that these models can be trained, tuned or prompted to align with a wide range of different human perspectives.
2412.16814_2219192_4	LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can create new content, including text, images, or video.
2412.17019_2219397_1	While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.
2412.17156_2219534_6	Theoretical challenges -- such as the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance -- must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.
2412.17200_2219578_7	Next, a series of experiments were designed and conducted on 40 students' UML modeling reports to explore the performance of ChatGPT in evaluating and grading these UML diagrams.
2412.17686_2220064_5	This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society.
2412.18544_2220922_5	We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future).
2412.18617_2220995_2	In this paper, we present a custom GPT (IBL Educator GPT) that is designed and developed based on Inquiry-based Learning and offers physics teachers a framework in which they can interact with ChatGPT and design educational strategies.
2412.18719_2221097_3	An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy.
2412.18835_2221213_3	Fine-tuning open-source LLMs like the Llama series is often preferred by enterprises over using commercial ones like the GPT series due to considerations including privacy, security, openness, performance, etc.
2412.19726_2222104_2	We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs.
2412.20087_2222465_4	Scores were determined by averaging the values assessed by three distinct LLMs.
2412.21016_2223394_4	However, to the best of our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs of LLM-based NLP software.
2412.21102_2223480_0	  Controlling diversity in LLM-agent world simulations is essential for maintaining stability in structured tasks while enabling variation where creativity is needed.
2501.01426_2225010_1	However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM.
2501.02178_2225762_6	This can also be presented as a study that shows the potential of LLMs for changing user experiences and making innovation possible in industries.
2501.02266_2225850_5	Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined.
2501.02346_2225930_2	An attempt to assess GPT-4's performance in radiation oncology was made via a dedicated 100-question examination on the highly specialized topic of radiation oncology physics, revealing GPT-4's superiority over other LLMs.
2501.02406_2225990_3	We answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)?
2501.02531_2226115_4	Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations.
2501.03838_2227422_7	To evaluate the feasibility of LM-Net, extensive experiments have been conducted on three publicly available datasets with different modalities.
2501.03904_2227488_4	A comparative analysis of different ChatGPT models was conducted to assess their ability to understand transportation information, retrieve relevant data, and provide comprehensive responses.
2501.04277_2227861_6	These findings provide a baseline for the raw capabilities of LLMs on Q&A tasks applied to materials science, and emphasize the substantial improvement that could be brought to open-source models via prompt engineering and fine-tuning strategies.
2501.04848_2228432_5	Built on GPT-4o-mini model, \msp is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering.
2501.05647_2229231_9	The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists.
2501.06137_2229721_6	We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky.
2501.07641_2231225_0	  Large Language Models (LLMs), such as GPT, are considered to learn the latent distributions within large-scale web-crawl datasets and accomplish natural language processing (NLP) tasks by predicting the next token.
2501.07824_2231408_2	To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done.
2501.07824_2231408_5	Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response.
2501.07837_2231421_4	Initially, domain-fine-tuning of the LLM is performed using a constructed railway knowledge question-and-answer dataset to improve answer accuracy in railway-related questions.
2501.07837_2231421_9	Finally, the fault handling capability of IDAS-LLM is demonstrated through simulations of real operational scenarios, proving that the proposed framework has practical application prospects.
2501.07857_2231441_4	First, smaller code units such as functions and variables are identified using syntax analysis and summarized with local LLMs.
2501.10134_2233718_3	In this article, thematic analysis has been performed on seven essays obtained from professionals in the education sector to understand the advantages and pitfalls of using GenAI models such as ChatGPT and Bard in education.
2501.10300_2233884_5	The ontology was developed utilizing suggestions from ChatGPT-3.5-010422 and validated using peer-reviewed research articles.
2501.10326_2233910_2	In academic publications, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts.
2501.10685_2234269_3	For instance, the ethical aspects of AI with respect to data privacy, transparency, and mitigation of bias are also covered, with the goal of promoting responsible use of the technology through best practices and the use of new technologies businesses can tap into the LLM potential, which help growth and stay one step ahead in the turmoil of digital marketing.
2501.11114_2234698_5	Our results are promising with regard to the incorporation of LLMs for simple cohort selection tasks, but also highlight the difficulties encountered by these models as soon as fine-grained knowledge and reasoning are required.
2501.11935_2235519_0	  LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts.
2501.11979_2235563_3	We iteratively refine the prompt by treating the deviation between the LLM output and the desired result as an error term until the output criteria are met.
2501.12332_2235916_5	To address this, we propose Retrieval Augmented Classification (RAC) for which LLM performs inferences for one label at a time using corresponding label schema; we start with the most related label and iterates until a label is chosen by the LLM.
2501.13381_2236965_8	Several interesting findings regarding LLMs' conformity are derived from empirical results and case studies.
2501.15875_2239459_2	In response, benchmarks focusing on the controllability of LLMs have been developed, but several issues remain: (1) They primarily cover major languages like English and Chinese, neglecting low-resource languages like Japanese; (2) Current benchmarks employ task-specific evaluation metrics, lacking a unified framework for selecting models based on controllability across different use cases.
2501.16149_2239733_7	These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs.
2501.16466_2240050_4	Rather than LLMs issuing low-level command-line instructions, which can lead to incorrect implementations, Incalmo allows LLMs to specify high-level tasks (e.g., infect a host, scan a network), which are then carried out by Incalmo.
2501.17024_2240608_5	Our findings underscore the potential of LLMs to achieve tasks where, in the past, implementing recommenders based on complex code analyses was required.
2501.18816_2242400_3	Furthermore, these approaches depend on the LLM to output solutions in an intermediate language, which must be translated into the representation language of the planning task.
2502.00340_2243331_8	Collider is designed for easy integration into existing LLM training frameworks, allowing systems already using token filtering to accelerate training with just one line of code.
2502.00808_2243799_4	To this end, we take the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data.
2502.00873_2243864_2	We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic.
2502.00903_2243894_2	By assessing each model's alignment with ideological perspectives, we explore how partisan selective processing could be identified in LLM-Assisted Content Analysis (LACA).
2502.01083_2244074_0	  Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs.
2502.01220_2244211_2	The accuracy of LMs is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context.
2502.01344_2244335_4	As the whole procedure conducts within LLMs, supporting and persuasive references are hard to acquire, while the absence of specific steps towards refining hidden mistakes persists even when errors are acknowledged.
2502.01436_2244427_2	These tailored models are increasingly made available through dedicated marketplaces, such as OpenAI's GPT Store.
2502.02201_2245192_6	These findings are believed to contribute to design implications for future LLM-based object manipulation interfaces, highlighting the potential for more intuitive and efficient user interactions in VR environments.
2502.02743_2245734_4	Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring adaptability to new models as they emerge.
2502.04358_2247349_4	This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them.
2502.05310_2248301_5	These choice points are resolved at runtime by LLMs, which generalize from user-provided examples of correct and incorrect decisions.
2502.07813_2250804_0	  The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs.
2502.07813_2250804_1	However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks.
2502.08109_2251100_3	For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision.
2502.08213_2251204_1	In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources.
2502.08301_2251292_10	Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.
2502.08648_2251639_4	The emerging area of generative AI was identified, linked to new AI models, such as ChatGPT, designed to generate content in the form of written text, audio, images or videos.
2502.09003_2251994_1	Quantization has been recently studied as a post-training technique for efficient LLM deployment.
2502.09606_2252597_4	Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency due to LLMs' disfavor.
2502.09690_2252681_1	While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet.
2502.09690_2252681_2	To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts.
2502.10673_2253664_2	To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed.
2502.10673_2253664_6	During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark.
2502.10709_2253700_0	  As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators.
2502.11122_2254113_6	The replay video can be viewed on https://www.bilibili.com/video/BV1uz42187EF and https://youtu.be/dO3PshWLV5M, and our codes have been open-sourced on https://github.com/luchang1113/HEP-LLM-play-StarCraftII.
2502.11133_2254124_0	  Multi-agent systems (MAS) powered by Large Language Models (LLMs) have been demonstrated to push the boundaries of LLM capabilities, yet they often incur significant costs and face challenges in dynamic LLM selection.
2502.11555_2254546_2	Typically, the safety alignment of LLM is trained on data with safety-related categories.
2502.11932_2254923_3	Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations).
