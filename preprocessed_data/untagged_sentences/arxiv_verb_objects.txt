1111.5202_303438_6	Some of these phenomena can be equally explained by either a wave or non-wave model alone.
1201.1369_313356_1	We show that often optical and near-infrared spectral energy distribution of LMXBs can be adequately described by a simple model of an accretion disc and a secondary star reprocessing X-ray emission of a central compact object.
1212.5593_395209_0	  Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model.
1408.5886_551192_2	Both the emitted power and its angular pattern are well described by a model, where microwave photons are generated via bremsstrahlung in the free-electron atomic-nucleus collisions, during the slowdown of the electrons.
1501.07576_594324_2	In this study, UAV dynamics are described by a three-dimensional dynamic point-mass model.
1604.05122_724289_0	  An air pollution model is generally described by a system of PDEs on unbounded domain.
1606.00401_738308_8	I then describe a concept for a formal category theoretic basis to a generalised player model.
1702.00137_814731_4	* How could we teach AI topics at an early undergraduate or a secondary school level?   
1712.05322_923921_1	The pulsar spectrum is well described by the thermal comptonization model both in a quiescent state and during flares, when the peak luminosity reaches values $L_{\rm x} \sim (2-4)\times10^{39}$ erg s$^{-1}$.
1802.07695_947235_0	  Practical application of H[infinity] robust control relies on system identification of a valid model-set, described by a linear system in feedback with a stable norm-bounded uncertainty, which must explains all possible (or at least all previously measured) behavior for the control plant.
1803.05457_955590_0	  We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering.
1803.11096_961229_0	  Group zero-attracting LMS and its reweighted form have been proposed for addressing system identification problems with structural group sparsity in the parameters to estimate.
1805.05062_978107_3	To overcome these limitations we build upon the recent reward augmented maximum likelihood approach \ie sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric.
1904.08796_1113836_10	Open-access data sets and software implementations could alleviate these issues, and encourage further AI applications to pediatric ophthalmology.   
1907.08625_1153161_4	The broadband 3-30 keV NuSTAR energy spectrum can be well described either by a three-component continuum model consisting of a disk blackbody, a single temperature blackbody and a power-law or by a two-component continuum model consisting of a disk blackbody and a Comptonization component.
1909.02984_1173022_7	We modeled the X-ray emission from the SB and found that its X-ray emission can be simply explained by pressure-driven wind model, that is, there is no need to invoke the presence of a SN explosion as previously suggested.
1910.09031_1192950_5	Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision making ultimately producing more precise and reliable interventions.
2001.10717_1236130_5	In this early work, a comparative evaluation of our MRE data driven simulation and the traditional method shows clinically significant differences in accuracy during landmark placement and motivates further animal model trials.
2003.13003_1263358_4	It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.
2005.03848_1283643_2	As an alternative, we propose a new method for BERT distillation, i.e., asking the teacher to generate smoothed word ids, rather than labels, for teaching the student model in knowledge distillation.
2006.11194_1305708_0	  Explainable AI provides insight into the "why" for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect.
2006.14779_1309293_0	  Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations.
2007.07250_1318764_6	We extend that concept to address system-level autonomy capabilities of AI-enabled cyber-physical systems.
2007.12912_1324426_3	Motivated by these issues, this paper addresses a drone-enabled intelligent vehicular system, which is secure, easy to deploy and reliable in quality.
2008.01048_1328770_7	For the simple-point-charge model of water we study, these results argue distinctly against rationalizing ion adsorption in terms of surface potentials inherent to molecular structure of the liquid's boundary.
2008.07326_1335048_1	However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI.
2008.10530_1338252_0	  We present a new mathematical model to explicitly capture the effects that the three restriction measures: the lockdown date and duration, social distancing and masks, and, schools and border closing, have in controlling the spread of COVID-19 infections $i(r, t)$. Before restrictions were introduced, the random spread of infections as described by the SEIR model grew exponentially.
2009.04984_1346487_3	As the foundation for model pre-training, we synthesize a new dialogue corpus and build our training set with two unsupervised methods: 1) coherence-oriented context corruption, including utterance ordering, insertion, and replacement, to help the model capture the coherence inside the dialogue contexts; and 2) specificity-oriented automatic rescoring, which encourages the model to measure the quality of the synthesized data for dialogue-adaptive pre-training by considering specificity and informativeness.
2009.07876_1349379_3	In this paper we argue for an AI-enabled control that allows optimized and efficient conversion between qubit and photon energies, to enable optic and quantum devices to work together.
2009.10228_1351731_1	However, support for designing tools and curriculum to teach K-12 AI literacy is still limited.
2009.10228_1351731_2	There is a need for additional interdisciplinary human-computer interaction and education research investigating (1) how general AI literacy is currently implemented in learning experiences and (2) what additional guidelines are required to teach AI literacy in specifically K-12 learning contexts.
2009.11100_1352603_5	We identify opportunities for researchers and teachers to collaborate to make AI education more accessible, and present an exemplar lesson plan that shows entry points for teaching AI in non-computing subjects.
2009.11101_1352604_4	Here we describe six self-contained and adaptive modules in "AI-assisted Malware Analysis."
2010.04887_1361217_3	We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information.
2010.11506_1367836_4	(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.
2010.14617_1370947_4	The role of the LTP and LTD in the cerebellum is also explained in algorithm level.
2011.02323_1375071_6	Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection.
2011.10737_1383485_2	Recently, substantial research efforts in learning-based methods for optimal control problems have been progressing significantly in addressing unknown system models, particularly when the system has complex interactions with the environment.
2012.14682_1402560_5	We further devise a difficulty-aware objective, encouraging the model to output the class probability that reflects the real difficulty of each instance for a more reliable cascading mechanism.
2101.00406_1404148_1	First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships.
2101.04283_1408025_0	  This paper briefly reviews the history of meta-learning and describes its contribution to general AI.
2101.06098_1409840_0	  The development of AI applications is a multidisciplinary effort, involving multiple roles collaborating with the AI developers, an umbrella term we use to include data scientists and other AI-adjacent roles on the same team.
2102.01380_1417869_3	ILMT encourages the E2E model to form a standalone LM inside its existing components, without sacrificing ASR accuracy.
2103.00689_1430834_4	The color centers show a strong diameter-dependent emission intensity, which can be explained with a theoretical model for chemical reactivity taking into account strain along the tube curvature.
2103.10873_1441018_4	Addressing the DNN model design, from training and dataset augmentation to 8-bit quantization and deployment, we demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for the real-time execution (up to 135 frame/s) of our novel DNN, called PULP-Frontnet.
2103.11903_1442048_2	First we analyze the performance of network in tracking a time varying weight vector and then we explain the estimation of Rayleigh fading channel through a random walk model.
2103.15004_1445149_7	It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces.
2104.12145_1459563_6	Given that GPT will perform better when given detailed and specific questions, we break down the PCSEL design problem into a series of sub-problems and converse with GPT by posing open-ended heuristic questions rather than definitive commands.
2105.00164_1462723_0	  Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors.
2105.05012_1467571_6	The proposed RAA is responsible for reasoning students' learning performance and showing the results on the AIoT-FML learning tool after communicating with the AI-FML platform.
2105.12655_1475214_4	In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code.
2106.01998_1479762_2	As an exploration of whether it is possible to explain user's mental model and behavior through Artificial Intelligence (AI) techniques, the research team compared the card sorting data with the outputs of a number of Natural Language Processing (NLP) techniques with the goal of understanding how participants perceived and interpreted the consequences of cyber attacks written in natural languages.
2107.05383_1499395_1	We asked the world's best language model, GPT-3, fifteen difficult questions about the nature, value, and future of library and information science (LIS), topics that receive perennial attention from LIS scholars.
2107.07691_1501703_5	To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.
2107.09051_1503063_6	Lastly, open issues and opportunities address future AI-empowered finance and finance-motivated AI research.
2107.11275_1505287_1	One way to understand and improve robustness of these models is an exploration of an adversarial attack scenario: check if a small perturbation of an input can fool a model.   
2108.01250_1510065_5	At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature.
2108.11193_1520008_4	Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.
2109.07971_1530792_2	In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?
2109.10836_1533657_4	Taking this wide perspective, this year there will be no single theme to lead the symposium and we encourage AI-HRI submissions from across disciplines and research interests.
2111.00826_1554929_0	  In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility.
2111.00826_1554929_4	We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs.
2111.13642_1567745_4	The synchronisation of X-ray and optical QPOs indicates that they must be produced in regions physically very close to each other; we thus propose that they can be explained by a precessing jet model, based on analogies with V404 Cyg and MAXI J1348-630.
2112.00592_1570368_0	  In a distributed multi-antenna system, multiple geographically separated transmit nodes communicate simultaneously to a receive node.
2112.01016_1570792_2	The gap is glaring: what is considered "explained" to AI-experts versus non-experts are very different in practical scenarios.
2112.11668_1581444_1	Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.
2112.11668_1581444_4	In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization.
2201.05955_1591327_2	Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns.
2201.06009_1591381_5	On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3.
2202.01281_1600105_3	In this paper, we present an experience report of teaching an AI course to business executives in the United Arab Emirates (UAE).
2202.01281_1600105_4	Rather than focusing only on theoretical and technical aspects, we developed a course that teaches AI with a view to enabling students to understand how to incorporate it into existing business processes.
2202.10848_1609672_8	This paper addresses the AI community in this regard and stresses the influence AI systems can have on either increasing or reducing the violence that is inflicted on animals, and especially on farmed animals.
2203.03204_1616066_1	Considering the scarcity of funding and the little to none availability of specialised professionals to teach AI and robotics in developing countries, we present resources based on free open-source hardware and software, open educational resources, and alternative education programs.
2203.03204_1616066_2	That said, the contribution of this work is the pilot workshop of four lessons that promote diversity and inclusion on teaching AI and Robotics for children to a small gender-balanced sample of 14 children of an average age of 7.64 years old.
2203.09904_1622766_3	Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes.
2204.10464_1640602_6	Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.
2204.13217_1643355_2	In many existing co-creative systems users can communicate with the AI, usually using buttons or sliders.
2204.13828_1643966_2	We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments.
2205.01772_1646184_6	Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing.
2205.10747_1655159_5	We then instruct a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content.
2205.10981_1655393_2	To address this issue, this study teaches GPT-3 to classify whether a question is related to data science by augmenting a small training set with additional examples generated by GPT-3 itself.
2205.11482_1655894_2	In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion.
2206.00152_1660572_0	  Human-AI shared control allows human to interact and collaborate with AI to accomplish control tasks in complex environments.
2206.02787_1663207_6	With AI, new radiomic models using the deep learning techniques will be also described.
2207.01749_1677647_2	How do designers and engineers currently collaborate on AI and UX design?
2207.01749_1677647_6	We identify how leaky abstractions are employed to collaborate at the AI-UX boundary and formalize a process of creating and using leaky abstractions.
2207.02996_1678894_7	We conclude by proposing a research agenda and posing open questions for further study on supporting people in learning to collaborate with generative AI systems.
2207.04901_1680799_4	We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization.
2207.07033_1682931_3	Several projects supported by the DAF-MIT AI Accelerator are developing public challenge problems that address numerous Federal AI research priorities.
2207.07951_1683849_1	This study adopts a recent physics-based uncertainty quantification (UQ) approach to address such model form uncertainty in Reynolds-averaged Naiver- Stokes (RANS) simulations.
2207.09374_1685272_5	By doing so, the user directly sees which characteristics of the input data can change arbitrarily without influencing the AI's decision.
2208.04714_1695426_2	We find that researchers addressing AI rights have often seemed to be unaware of the work of colleagues whose interests overlap with their own.
2208.05969_1696681_4	In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective.
2208.08198_1698910_10	Moreover, we check to which extent our proposal is in line with the European AI Act proposal and current safety standardization initiatives addressing AI and Autonomous Systems
2208.09982_1700694_4	To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics.
2208.11445_1702157_5	First, we induce a language model to produce step-by-step rationales before outputting the answer to effectively communicate the task to the model.
2208.11660_1702372_4	We use data collected from an online experiment in which 145 individuals in 29 human-only teams of five communicate through a chat-based system to solve a cognitive task.
2208.13421_1704133_3	In the AIDOaRt research project, practitioners and researchers collaborate on AI-augmented automation supporting modeling, coding, testing, monitoring, and continuous development in cyber-physical systems.
2209.00282_1705997_6	The broad-band spectra of the source can be described by a model comprising a very hot blackbody having temperature, $kT_{\rm BB} \approx$ 1.66 - 2.13 keV, a high-energy cutoff power law, and an Fe emission line at $E_{\rm line} \sim$ 6.7 keV.
2209.03431_1709146_4	Then, it proceeds with physics-informed adversarial training to teach the model the system-related physics domain foreknowledge through iteratively reducing the unwanted output deviations on the previously-uncovered counterexamples.
2209.06317_1712032_9	This paper explores these issues, focusing on the opportunities, challenges, and potential impacts of such an approach, and discussing how it might influence AI regulations.
2210.01461_1722816_5	In this context, we envision the development of closed loop, intelligent, low-power, and miniaturized neural interfaces that will use brain inspired AI techniques with neuromorphic hardware to process the data from the brain.
2210.01461_1722816_8	On one hand, brain inspired AI algorithms represented by spiking neural networks (SNNs) would be used to interpret the multimodal neural signals in the BCI system.
2210.04621_1725976_0	  AI tools can be useful to address model deficits in the design of communication systems.
2210.05883_1727238_3	Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting.
2210.08984_1730339_7	Therefore, in our journey towards an AI-enabled sustainable future, we need to address AI ethics and governance as a priority.
2210.10332_1731687_2	Addressing such incorrect model behavior via parameter adjustments is very costly.
2210.12530_1733885_3	Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.
2210.16663_1738018_3	This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency.
2211.05110_1744016_4	This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining.
2211.14228_1753134_3	We study generating the said content using the "prompt-based" method that consists of explaining the task to the LLM in natural text.
2212.00616_1756793_1	X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words.
2212.00616_1756793_2	Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words, thereby making a prompt more descriptive.
2212.06823_1763000_5	We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze.
2212.09251_1765428_3	We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering.
2212.09282_1765459_4	We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences.
2212.10561_1766738_8	Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.
2301.03052_1774116_6	We start by introducing some highlighted robustness challenges in the AI lifecycle and motivating AI maintenance by making analogies to car maintenance.
2301.08745_1779809_5	Further, we explore an interesting strategy named $\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably.
2301.12243_1783307_5	Our findings revealed that practitioners use the guidebook not only for addressing AI's design challenges, but also for education, cross-functional communication, and for developing internal resources.
2302.00093_1785027_5	We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.
2302.07248_1792182_4	We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs.
2302.11382_1796316_3	This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs.
2302.13817_1798751_5	We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
2303.03140_1802937_4	Then, we briefly define what we refer to when we talk about AI that is considered a medical device by itself or supports one.
2303.03205_1803002_6	This paper will provide an insight into current nanotechnology and artificial intelligence advancements in the etextiles domain before focusing specifically on the future vision and direction around the potential application of neuromorphic computing and spiking neural network inspired AI technologies within the textile sector.
2303.03956_1803753_0	  In this paper, we present a pilot study aiming to investigate the challenges of teaching AI and Robotics to children in low- and middle-income countries.
2303.03956_1803753_1	Challenges such as the little to none experts and the limited resources in a Mexican town to teach AI and Robotics were addressed with the creation of inclusive learning activities with Montessori method and open-source educational robots.
2303.05977_1805774_4	To properly communicate the medical images to the language model, we develop a network that maps the extracted visual features to a set of learnable tokens.
2303.06430_1806227_3	We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
2303.09461_1809258_0	  We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''.
2303.14956_1814753_3	In this work, we propose a simple and efficient approach to instruct large language model (LLM) to extract a variety of structures from texts.
2303.17555_1817352_1	Given AI fairness' raison d'etre of "fairness", we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness.
2303.18116_1817913_4	It is demonstrated that if the typical pitfalls are avoided, we can substantially benefit from collaborating with an AI partner.
2304.00385_1818431_9	For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.
2304.02017_1820063_6	It also describes the main advancements from GPT-3 to GPT-4 Omni, comparing them with other LLMs like LLaMA 3, Gemini and Deepseek.
2304.03245_1821291_2	We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English).
2304.03893_1821939_3	The prompts encourage ChatGPT to output a sequence of predefined robot actions, represent the operating environment in a formalized style, and infer the updated state of the operating environment.
2304.05128_1823174_2	In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations.
2304.05128_1823174_3	In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language.
2304.05335_1823381_8	We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.
2304.05510_1823556_5	We present our conversational AI prototype, available at www.chatclimate.ai and demonstrate its ability to answer challenging questions accurately in three different QA scenarios: asking from 1) GPT-4, 2) chatClimate, and 3) hybrid chatClimate.
2304.06794_1824840_3	In our study, we asked GPT to identify the ten most significant subdisciplines within the field of environmental science.
2304.07061_1825107_2	It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions.
2304.08366_1826412_3	However, existing studies may focus on individual tasks in the workflow of data storytelling and do not reveal a complete picture of humans' preference for collaborating with AI.
2304.08366_1826412_4	To better understand real-world needs, we interviewed eighteen data workers from both industry and academia to learn where and how they would like to collaborate with AI.
2304.08366_1826412_5	Surprisingly, though the participants showed excitement about collaborating with AI, many of them also expressed reluctance and pointed out nuanced reasons.
2304.08366_1826412_8	Next, we summarize the interviewees' reasons why and why not they would like to collaborate with AI.
2304.09542_1827588_4	Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks.
2304.09655_1827701_5	Specifically, we ask ChatGPT to generate a number of program and evaluate the security of the resulting source code.
2304.09667_1827713_2	In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions.
2304.11082_1829128_6	This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona.
2304.11111_1829157_6	Anxiety-induction not only influences LLMs' scores on an anxiety questionnaire but also influences their behavior in a previously-established benchmark measuring biases such as racism and ageism.
2304.12198_1830244_4	Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity.
2304.12898_1830944_1	ChatGPT consistent avoidance of passing the test is here overcome by asking ChatGPT to apply the Turing test to itself.
2304.13815_1831861_1	In this review, we outline how ML and AI have been applied to address four outstanding difficulties of crystal nucleation: how to discover better reaction coordinates (RCs) for describing accurately non-classical nucleation situations; the development of more accurate force fields for describing the nucleation of multiple polymorphs or phases for a single system; more robust identification methods for determining crystal phases and structures; and as a method to yield improved course-grained models for studying nucleation.
2305.00875_1833931_9	Through concept analysis, we explore the traceability and distribution of human-recognizable concepts within latent code representations which could be used to influence model predictions.
2305.03653_1836709_4	We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query.
2305.04812_1837868_3	However, the extent to which external information influences LLMs' cognition and behaviors remains unclear.
2305.04812_1837868_4	This study investigates how external statements and opinions influence LLMs' thoughts and behaviors from a social cognitive perspective.
2305.08391_1841447_2	To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input.
2305.09067_1842123_3	Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data.
2305.09434_1842490_4	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2305.10142_1843198_2	We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively.
2305.10649_1843705_1	The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken.
2305.10938_1843994_2	Biologically inspired AI, and more specifically brain-inspired AI, promises to provide further biological aspects beyond those that are already traditionally included in AI, making it possible to assess and possibly overcome some of its present shortcomings.
2305.11461_1844517_2	The chain of thought (CoT), which often contains zero-shot CoT and few-shot CoT, is a recently developed prompting method that can explain the reasoning process to the LLM and outperforms simple prompting in three challenging reasoning tasks, including arithmetic, symbolic, and commonsense reasoning.
2305.12799_1845855_6	In this work, we extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and make the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks.
2305.12865_1845921_7	Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set.
2305.13252_1846308_4	Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
2305.13661_1846717_5	Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.
2305.13733_1846789_6	Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.
2305.14688_1847744_2	We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background.
2305.14930_1847986_2	We ask LLMs to assume different personas before solving vision and language tasks.
2305.15541_1848597_4	This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model.   
2305.18098_1851154_6	Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model.
2305.18307_1851363_4	Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios.
2305.19118_1852174_1	Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively.
2305.19339_1852395_3	We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen.
2306.00622_1853770_4	Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers.
2306.00745_1853893_5	We further implement a two-step table annotation pipeline which first determines the class of the entities described in the table and depending on this class asks ChatGPT to annotate columns using only the relevant subset of the overall vocabulary.
2306.01272_1854420_3	Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection.
2306.02907_1856055_5	After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code.
2306.04707_1857855_3	We study techniques that enable learning from helpful teachers while avoiding learning from people who are trying to trick the model into unhelpful or toxic responses.
2306.06199_1859347_3	In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response.
2306.10900_1864048_3	Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer.
2306.11296_1864444_5	This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%.
2306.12028_1865176_2	People can now use natural language (i.e. prompts) to communicate with AI to perform tasks.
2307.00952_1871948_4	Therefore, explaining the logic behind those models through explainable AI (XAI) techniques is essential for their employment in critical applications.
2307.02157_1873153_5	We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.
2307.04408_1875403_4	To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation.
2307.04599_1875594_8	Results:The study's findings show that language workbenches are of paramount importance in dealing with all aspects of modeling language development and are leveraged to define DSL explicitly addressing AI concerns.
2307.05494_1876489_2	This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact.
2307.05494_1876489_3	Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions.
2307.08487_1879482_5	Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions.
2307.10198_1881193_7	Our analysis shows that by 2018, the time lag between China and the USA in addressing AI research topics had evaporated.
2307.10490_1881485_2	When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction.
2307.10700_1881695_0	  Large language models (LLMs) are dramatically influencing AI research, spurring discussions on what has changed so far and how to shape the field's future.
2307.10811_1881806_1	Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting.
2307.13566_1884561_1	Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools.
2307.16368_1887363_7	It first recognizes the actions already performed in the observed videos and then asks an LLM to predict the future actions via conditioned generation, or to infer the goal and plan the whole procedure by chain-of-thought prompting.
2308.01240_1889132_0	  In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks.
2308.03314_1891206_7	To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation.
2308.06212_1894104_9	LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow.
2308.07326_1895218_7	Our research emphasizes a quantitative role to describe steerability in LLMs, presenting both its promise and areas for further refinement in aligning its progress to human intentions.
2308.07758_1895650_3	Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided.
2308.08239_1896131_3	The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations.
2308.08493_1896385_3	To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it.
2308.09954_1897846_7	Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets.
2308.10335_1898227_7	Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help.
2308.11534_1899426_5	Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.
2308.12915_1900807_5	We focus on Shahrzad, who seeks to alter her fate compared to the original folklore, and the player, who collaborates with AI to craft narratives and shape the game world.
2308.14132_1902024_1	Such jailbreaks can trick LLMs into providing intricate instructions to a malicious user for creating explosives, orchestrating a bank heist, or facilitating the creation of offensive content.
2309.00240_1905044_6	Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately.
2309.01940_1906744_5	The code correction task asks LLMs to fix real-world erroneous code segments with different error messages.
2309.03118_1907922_4	In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability.
2309.04550_1909354_3	Our method entails tasking an LLM to infer whether a patient has, or is at risk of, a particular condition on the basis of associated notes; if so, we ask the model to summarize the supporting evidence.
2309.05660_1910464_5	To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset.
2309.05689_1910493_2	Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement.
2309.05950_1910754_4	Specifically, we adopt an automatic hill-climbing procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop.
2309.07062_1911866_3	Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself.
2309.07623_1912427_5	We specifically employ a minimal dataset to instruct LLMs to recognize the intended output modality as directed by the instructions.
2309.08902_1913706_5	We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group.
2309.10524_1915328_3	Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further.
2309.12321_1917125_3	This paper makes a case that effective legal systems are the best way to address AI safety.
2309.12767_1917571_7	1) Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration.
2309.13638_1918442_3	This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input.
2309.14459_1919263_4	We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM's output in meeting the goal.
2309.14530_1919334_4	By categorizing and elucidating these genres, the study aims to facilitate the development of empirical qualitative and quantitative research, fostering evidence-based approaches to address AI-related risks in healthcare effectively.
2309.16167_1920971_1	However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education.
2309.16400_1921204_2	Here, we combine Large Eddy Simulation (LES) techniques with Machine Learning (ML) to retain only the largest dynamics explicitly, while small-scale dynamics are described by an ML-based sub-grid-scale model.
2309.16697_1921501_1	Students can ask ChatGPT to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s).
2310.00525_1922782_5	Through a feedback mechanism, the user interacts with the algorithm, correcting the algorithm output to their preferences.
2310.00603_1922860_2	In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation.
2310.02407_1924664_9	To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect).
2310.02439_1924696_3	We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question.
2310.03051_1925308_5	Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions.
2310.03214_1925471_9	Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers.
2310.04782_1927039_5	Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior.
2310.05657_1927914_5	Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.
2310.05976_1928233_4	The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish.
2310.06552_1928809_4	Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task, therefore we frame the task as information extraction, providing a description of each coded concept and asking the model to retrieve related mentions.
2310.07088_1929345_1	Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance.
2310.08433_1930690_2	We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style.
2310.08922_1931179_3	In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.
2310.09810_1932067_1	In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair.
2310.10035_1932292_4	Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool.
2310.10158_1932415_2	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.
2310.10158_1932415_3	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.
2310.10903_1933160_4	Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency.
2310.11324_1933581_1	Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model.
2310.12558_1934815_7	To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users.
2310.13548_1935805_1	But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy.
2310.13625_1935882_7	While the scheme will not address all AI risks, it complements proposed solutions by allowing for a more precise and flexible approach to controlling the development of frontier AI models and unwanted AI proliferation.
2310.14122_1936379_1	Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No".
2310.15747_1938004_4	To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively.
2310.15780_1938037_3	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2310.15851_1938108_9	In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses.
2310.16164_1938421_1	However, data scientists encounter substantial obstacles when conversing with LLM-powered chatbots and acting on their suggestions and answers.
2310.16535_1938792_2	Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction.
2310.16727_1938984_5	A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards.
2310.17567_1939824_4	Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills.
2310.17688_1939945_6	In this short consensus paper, we describe extreme risks from upcoming, advanced AI systems.
2310.18360_1940617_3	Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs.
2310.19046_1941303_5	Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions.
2310.19204_1941461_3	We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify.
2310.20444_1942701_1	Thus, it is of major importance to know which stakeholders influence AI research.
2310.20487_1942744_5	Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence.
2311.01007_1943974_2	In this work, we propose to learn rules, grounded in data regions and described in natural language, that illustrate how the human should collaborate with the AI.
2311.01041_1944008_4	Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors.
2311.01894_1944861_4	For the segmentation networks under test the F1 score dependency on TE and TI can be well described by quadratic model functions (R^2 > 0.9).
2311.01918_1944885_6	To address LLMs' limitations regarding personalization and complex clinical reasoning, the paper explores the emerging development of LLM-powered autonomous agents for healthcare.
2311.01981_1944948_4	In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient.
2311.02433_1945400_6	To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants.
2311.06377_1949344_4	Our emulation strategy involved using the initial five words of each PubMed abstract as a prompt and instructing the model to expand the content up to the original abstract's length.
2311.06985_1949952_9	For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering.
2311.07594_1950561_6	The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converter, which transforms data into a format that LLMs can understand; (2) Multimodal Perceiver, which improves how LLMs percieve different types of data; (3) Tool Learning, which leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Method, which teaches LLMs to understand specific data types within datasets.
2311.08147_1951114_3	However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response.
2311.08369_1951336_2	When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need.
2311.08877_1951844_3	We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement.
2311.09718_1952685_1	To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions.
2311.10652_1953619_8	Drawing on these insights, we discuss strategies for effectively communicating model updates in AI-infused systems.
2311.10934_1953901_3	We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases.
2311.11628_1954595_0	  We present a method to integrate Large Language Models (LLMs) and traditional tabular data classification techniques, addressing LLMs challenges like data serialization sensitivity and biases.
2311.12188_1955155_4	In particular, we ask ChatGPT to give examples of how to use Bayes rule for medical diagnosis.
2311.13240_1956207_6	Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.
2311.14703_1957670_8	Finally, we find that through asking ChatGPT 3.5 to explain its reasoning prior to providing an answer, we are able to improve clinical accuracy and mitigate instances of gender and racial biases.
2311.16479_1959446_5	These conversations pay more attention on detailed facts in the image, encouraging the model to answer questions based on multi-modal contexts.
2311.16494_1959461_7	3) We propose negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features.
2311.16639_1959606_1	We ask an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses to position political actors such as US Senators, or longer texts such as UK party manifestos or EU policy speeches given in 10 different languages.
2312.00575_1962383_1	However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans.
2312.00819_1962627_4	Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results.
2312.01797_1963605_2	Prompts are used for two main purposes: 1) to provide LLMs with essential information like environments, costs, heuristics, etc.; 2) to communicate human feedback on intermediate planning results to LLMs.
2312.02147_1963955_3	Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens.
2312.04412_1966220_1	In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.
2312.04474_1966282_4	The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator").
2312.04927_1966735_2	In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. "Hakuna Matata means no worries Hakuna Matata it means no" $\rightarrow$ "??".
2312.05356_1967164_9	\textsc{MINT} is effective, efficient, and reliable, capable of correcting a neural model by patching a minimum number of neurons (usually one or two neurons).
2312.06942_1968750_8	This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code.
2312.06942_1968750_12	This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding.
2312.08027_1969835_2	To alleviate this, we use an interpretable structure to explain the prompt learning principle in LLMs, which certificates that the effectiveness of language models is determined by position changes of the task's related tokens.
2312.08680_1970488_5	By iteratively asking GPT-4 with the prompts, GHGNAS continually validates the accuracy of the generated HGNNs and uses the feedback to further optimize the prompts.
2312.09203_1971011_3	A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself.
2312.09300_1971108_4	We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly.
2312.10321_1972129_5	The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample exists by modifying the database.
2312.10321_1972129_6	The latter technique is used to evaluate the relaxed equivalence in which it asks LLMs to explain the queries and then compare if they contain significant logical differences.
2312.10603_1972411_10	This progress indicates that addressing the current model's limitations could yield an AI capable of passing even the most rigorous professional certifications.
2312.10620_1972428_2	Through a thematic analysis of a hands on workshop in which 22 professional software engineers collaborated for three hours with ChatGPT, we explore the transition of AI from a mere tool to a collaborative partner.
2312.11681_1973489_2	Chains address LLM errors analogously to the way crowdsourcing workflows address human error.
2312.14856_1976664_3	Thus, from a single question template, it is possible to ask an LLM a $\textit{neighbourhood}$ of very similar programming questions, and assess the correctness of the result returned for each question.
2312.14949_1976757_5	We start by providing a detailed description of our approach in conversing with the LLM to optimize the _getextrema function in the pillow library, and a quantitative evaluation of the performance improvement.
2312.14950_1976758_3	That is, instead of asking an LLM to write a program (robotic plan) in the popular but verbose Python, ChatFly gets it to do it in MiniSpec specially designed for token efficiency and stream interpretation.
2312.15663_1977471_8	Third, based on the quality descriptions, users can talk with ChatGPT to rate image quality scores or produce a radiological quality report.
2312.16044_1977852_3	Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions.
2312.16211_1978019_5	We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses.
2312.16257_1978065_5	Our casual intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information.
2312.17235_1979043_7	Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost.
2401.00139_1979695_6	This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.
2401.00996_1980552_4	In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization.
2401.03676_1983232_6	From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs.
2401.03729_1983285_1	By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task.
2401.04092_1983648_7	We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria.
2401.05033_1984589_4	This approach generates a training data via "self-talk" of LLMs that can be refined and utilized for supervised fine-tuning.
2401.05612_1985168_6	Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making.
2401.06059_1985615_2	There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks.
2401.06072_1985628_6	Additionally, we execute a substantial range of ablation experiments and draw comparisons with several advanced commercial LLMs, to investigate the crucial factors influencing LLMs' performance in structured temporal knowledge inference tasks.
2401.06373_1985929_3	Specifically, we study how to persuade LLMs to jailbreak them.
2401.06853_1986409_7	On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation.
2401.08273_1987829_1	Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task.
2401.08711_1988267_1	Given the pressing need to teach "critical AI literacy", discussion of metaphor provides an opportunity for inquiry and dialogue with space for nuance, playfulness, and critique.
2401.09074_1988630_5	We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers.
2401.09566_1989122_6	We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions.
2401.10446_1990000_3	In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM.
2401.10657_1990211_2	We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance.
2401.10745_1990299_3	Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet.
2401.10745_1990299_5	This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
2401.12474_1992028_3	Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension.
2401.13218_1992772_6	We introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument.
2401.15241_1994795_0	  Identifying the training datasets that influence a language model's outputs is essential for minimizing the generation of harmful content and enhancing its performance.
2401.15963_1995517_5	We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs.
2401.17043_1996597_1	This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content.
2401.17390_1996944_5	Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid.
2402.01691_1999330_1	Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms.
2402.01732_1999371_3	To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related.
2402.01760_1999399_3	This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube.
2402.01766_1999405_3	We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes.
2402.01867_1999506_2	In this work, we ask the LLM how similar are these prompted LFs.
2402.02167_1999806_2	At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected.
2402.02456_2000095_4	The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement.
2402.03776_2001415_7	To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics.
2402.03916_2001555_2	Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden.
2402.04315_2001954_3	In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses.
2402.04568_2002207_0	  Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses.
2402.08680_2006319_2	However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.
2402.08699_2006338_4	RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input.
2402.08806_2006445_2	Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   
2402.09671_2007310_0	  This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.
2402.10151_2007790_4	We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference.
2402.11245_2008884_2	To address the AI model placement problem under uncertainties, this paper presents a novel approach employing a sequence-to-sequence (S2S) neural network which considers uncertainty estimations.
2402.11532_2009171_3	Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached.
2402.11633_2009272_6	The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances.
2402.11651_2009290_5	By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks.
2402.11801_2009440_5	Regarding emotional understanding, HEF implements a two-stage emotion prediction strategy, encouraging LLMs to prioritize primary emotions emphasized by SEMs, followed by other categories, substantially alleviates the difficulties for LLMs in fine-grained emotion detection.
2402.11905_2009544_2	To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of "Teach a man to fish."
2402.12026_2009665_5	Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.
2402.12786_2010425_4	Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different".
2402.12786_2010425_6	To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles.
2402.12907_2010546_1	While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts.
2402.13414_2011053_4	Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels.
2402.14846_2012485_5	We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks.
2402.15301_2012940_6	Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets.
2402.15302_2012941_6	Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models.
2402.15302_2012941_8	In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.
2402.15627_2013266_8	We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.
2402.16786_2014425_3	Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions.
2402.16929_2014568_1	Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts.
2402.17124_2014763_2	In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved.
2402.17124_2014763_6	And then it asks the model to "reflect" over them to generate the final answer.
2402.17385_2015024_7	Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.
2402.18284_2015923_3	Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input.
2402.18593_2016232_8	We hope our work will inspire HPCs/datacenters to further explore, evaluate, and communicate the impact of power-capping AI hardware accelerators for more sustainable AI.
2403.00824_2017943_5	As a result, we can talk about model behavior in general, for specific types of predictions, or different domains.
2403.01069_2018188_6	The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively.
2403.01209_2018328_2	Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts.
2403.01570_2018689_6	Specifically, SERSAL utilizes the LLM's prior outcomes as original soft noisy annotations, which are dynamically leveraged to teach a better small student model.
2403.01570_2018689_7	Reversely, the outcomes from the trained small model are used to teach the LLM to further refine its real capability.
2403.01632_2018751_3	Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge.   
2403.02130_2019249_3	We experiment with different zero-shot and few-shot prompt templates for instructing LLMs to extract and normalize attribute-value pairs.
2403.02419_2019538_1	However, there is little understanding of how the number of LM calls - e.g., when asking the LM to answer each question multiple times and taking a majority vote - affects such a compound system's performance.
2403.02454_2019573_6	We discuss both the development process of communicating creative intent to an AI chatbot and the synthesized open feedback of the participants.
2403.02610_2019729_9	Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation.
2403.02715_2019834_6	Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets.
2403.04769_2021888_0	  Large language models (LLMs) are initially trained on vast amounts of data, then fine-tuned using reinforcement learning from human feedback (RLHF); this also serves to teach the LLM to provide appropriate and safe responses.
2403.04769_2021888_2	Unlike other jailbreaks (for example, the popular "Do Anything Now" (DAN) ), our method does not rely on instructing the LLM to override its RLHF policy; hence, simply modifying the RLHF process is unlikely to address it.
2403.05217_2022336_4	Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process.
2403.05434_2022553_10	We show that choosing the best policy to interact with the LLM can reduce cost by 90% while giving better or comparable performance compared to communicating with the LLM in the original LRL.
2403.05572_2022691_5	Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align approximately 5 times more closely with the expectations of individuals possessing a high degree of empathy, compared to human responses.
2403.05612_2022731_3	This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model's responses to unfamiliar queries (e.g., say ``I don't know'').
2403.06512_2023631_2	While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice.
2403.06664_2023783_7	In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity.
2403.07747_2024866_6	These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities.
2403.07969_2025088_1	KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.
2403.09168_2026287_4	Applying these design guidelines, we created VIVID which allows instructors to collaborate with LLMs to design, evaluate, and modify pedagogical dialogues.
2403.09522_2026641_5	Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.
2403.09972_2027091_4	Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation.
2403.10433_2027552_6	We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence.
2403.11103_2028222_3	Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data.
2403.12556_2029675_7	In the LLM fine-tuning stage, we freeze the acquired knowledge in the visual encoder and integrate it with a pre-trained LLM to inspire the LLM's translation potential.
2403.12744_2029863_3	In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.
2403.12744_2029863_6	Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.
2403.13027_2030146_2	We show that the optimal solution to the optimization problem enjoys a nice analytical property which provides a better understanding and inspires the algorithm design for the watermarking process.
2403.13089_2030208_2	We developed prompt-tuning algorithms to instruct generative LLMs to summarize clinical text.
2403.14171_2031290_2	Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question.
2403.14171_2031290_3	To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation.
2403.14171_2031290_7	Furthermore, we design an efficient knowledge distillation approach to distill the capability of proprietary LLMs in explaining multimodal misinformation into open-source LLMs.
2403.15281_2032400_4	Instructing GPT to score approximately 361000 resumes with randomized social identities, we find that the LLM awards higher assessment scores for female candidates with similar work experience, education, and skills, while lower scores for black male candidates with comparable qualifications.
2403.15600_2032719_5	Then, we asked ChatGPT the same SO questions, gathering the generated code for comparison.
2403.16427_2033546_7	It learns to select hints from the constructed KB based on the task-specific feedback, where the hints can serve as guidance to help correct LLMs reasoning for better recommendations.
2403.17431_2034550_2	This enables updating and correcting the model's knowledge by in-context editing instead of retraining.
2403.19154_2036273_7	Our results indicate that teaching a language model to ask better questions leads to better personalized responses.
2404.00557_2038008_3	In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations.
2404.00828_2038279_2	In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data.
2404.01019_2038470_3	To give LLMs such ability, we explore source-aware training -- a recipe that involves (i) training the LLM to associate unique source document identifiers with the knowledge in each document, followed by (ii) an instruction-tuning stage to teach the LLM to cite a supporting pretraining source when prompted.
2404.04485_2041936_6	Results showed that even with groups of three pathologists, majority-voted decisions significantly increased both RAIR and RSR -- by approximately 9% and 31%, respectively -- compared to decisions made by one pathologist collaborating with AI.
2404.04516_2041967_6	We hope that our work inspires LM researchers to further develop LMs as critical thinking tools and philosophers and other 'critical thinkers' to imagine intellectually substantive uses of LMs.
2404.04631_2042082_3	We collected the top 10 most popular books, according to Project Gutenberg, divided each one into equal chunks of 400 words, and asked each LLM to predict the author.
2404.06711_2044162_5	To encourage each LLM character's behaviors to be aligned with their specified math-relevant properties (termed "characteristics alignment") and the overall conversational procedure to be close to an authentic student MM discussion (termed "conversational procedural alignment"), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure.
2404.07017_2044468_4	The framework motivates the model itself to automatically generate rationales on existing datasets.
2404.07103_2044554_5	Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively.
2404.07396_2044847_2	We employed two prompting strategies: direct prediction and what we call future narratives which ask ChatGPT to tell fictional stories set in the future with characters retelling events that happened in the past, but after ChatGPT's training data had been collected.
2404.07981_2045432_7	Just as search engine optimization (SEO) revolutionized how webpages are customized to rank higher in search engine results, influencing LLM recommendations could profoundly impact content optimization for AI-driven search services.
2404.08001_2045452_2	To address this challenge, a sophisticated large language model system named as Xiwu has been developed, allowing you switch between the most advanced foundation models and quickly teach the model domain knowledge.
2404.10315_2047766_5	(2) How to teach the LLM to express confidence?
2404.11288_2048739_2	However, SFT simply instructs the model to imitate the reference translations at the token level, making it vulnerable to the noise present in the references.
2404.12145_2049596_5	We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks.
2404.12349_2049800_2	The paper advocates for creating open-source legal AI systems to improve accuracy, transparency, and narrative diversity, addressing general AI's shortcomings in legal contexts.
2404.12636_2050087_5	Such a multi-objective fine-tuning will instruct LLMs to generate high-quality patches.   
2404.12843_2050294_3	In this work, we strive for a middle ground and introduce a training objective based on principled probabilistic reasoning that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules.
2404.12938_2050389_4	Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance.
2404.13208_2050659_4	We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions.
2404.14662_2052113_4	To address this issue, we propose NExT, a method to teach LLMs to inspect the execution traces of programs (variable states of executed lines) and reason about their run-time behavior through chain-of-thought (CoT) rationales.
2404.14963_2052414_4	The core of our method is to encourage the LLMs to deeply understand the problems and extract the key problem-solving information used for better reasoning.
2404.15458_2052909_4	We also explore inverse problems by asking the LLM to predict the geometry necessary to achieve a desired spectrum.
2404.16375_2053826_3	To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: "list items one by one," which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags.
2404.17025_2054476_1	This raises an intriguing HCI question: How does instructing LLMs to engage in longer or shorter conversations affect conversation quality?
2404.18865_2056316_4	Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences.
2404.19442_2056893_5	In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples.
2404.19737_2057188_2	More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk.
2405.00302_2057514_1	These methods ask the LLM to generate feedback given the problem statement and a student's (buggy) submission.
2405.00648_2057860_9	These findings underscore the imperative for ongoing collaborative endeavors within the community to detect and address LLM hallucinations.
2405.02765_2059977_10	Our work lays the groundwork for addressing malicious model editing, which is a critical challenge associated with the strong generative capabilities of LLMs.
2405.02814_2060026_3	This discovery raises an intriguing question: can negative emotions similarly influence LLMs, potentially enhancing their performance?
2405.03695_2060907_4	This approach allows for a detailed analysis of factors influencing the LLMs' effectiveness in recommending materials.
2405.04412_2061624_5	In Study 1, we ask GPT to score resumes with 32 different names (4 names for each combination of the 2 gender and 4 racial groups) and two anonymous options across 10 occupations and 3 evaluation tasks (overall rating, willingness to interview, and hireability).
2405.04656_2061868_4	Using need-finding interviews to motivate our system design, CCC decomposes the writing process into two core functions, outline and edit:
2405.04727_2061939_5	Our goal is to instruct an LLM using detailed instructions to assign fine-grained relevance judgments to holes.
2405.05175_2062387_2	We introduce a novel threat model where adversarial third-party apps manipulate the context of interaction to trick LLM-based agents into revealing private information not relevant to the task at hand.   
2405.05990_2063202_1	However, recent studies have shown that LLMs can memorize training data and simple repeated tokens can trick the model to leak the data.
2405.06682_2063894_1	We instructed nine popular LLMs to answer a series of multiple-choice questions to provide a performance baseline.
2405.06823_2064035_1	The functionality and performance of an LLM application highly depend on its system prompt, which instructs the backend LLM on what task to perform.
2405.06835_2064047_3	We also propose three different approaches that involve teaching LLMs to comprehend the API documentation of the components as a reference while accomplishing the Translation tasks.
2405.07295_2064507_3	Here, we suggest that environmental enrichment (EE) can be used as a biological model for studying forward transfer, inspiring human-like AI development.
2405.07761_2064973_6	The second strategy is to instruct LLMs to perform evolutionary operators for global search.
2405.10025_2067237_3	However, it still suffers from two limitations: 1) LLMs are unaware of the source speech during GER, which may lead to results that are grammatically correct but violate the source speech content, 2) N-best hypotheses usually only vary in a few tokens, making it redundant to send all of them for GER, which could confuse LLM about which tokens to focus on and thus lead to increased miscorrection.
2405.10210_2067422_1	This paper presents an in-depth measurement study of the GPT Store, with a focus on the categorization of GPTs by topic, factors influencing GPT popularity, and the potential security risks.
2405.10853_2068065_5	We show that Photon can be used by organizations interested in collaborating with their private data sources and computational resources for pre-training LLMs with billions of parameters.
2405.11422_2068634_5	Computational cognitive modeling reveals that LLM behavior is well-described by a simple RL algorithm that incorporates relative values at the outcome encoding stage.
2405.12205_2069417_6	(a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH.
2405.13022_2070234_2	By default, LLMs are trained to maximize the next token likelihood, which does not teach the model to modulate its answer based on its level of uncertainty.
2405.13022_2070234_3	In order to learn self-restraint, we devise a utility function that can encourage the model to produce responses only when it is confident in them.
2405.13048_2070260_0	  This research investigates distinct human-generative AI collaboration types and students' interaction experiences when collaborating with generative AI (i.e., ChatGPT) for problem-solving tasks and how these factors relate to students' sense of agency and perceived collaborative problem solving.
2405.13048_2070260_2	Notably, our study shows that 77.21% of students perceived they led or had even contributed to collaborative problem-solving when collaborating with ChatGPT.
2405.13101_2070313_2	To this end, we asked ChatGPT to generate three distinct codes: a simple numerical integration, a conjugate gradient solver, and a parallel 1D stencil-based heat equation solver.
2405.14755_2071967_8	First, we present a prompt-based detection method that directly asks a language model to indicate which elements of the input are anomalies.
2405.16282_2073494_2	Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models' internal and expressed confidence.
2405.16587_2073799_4	Based on our designed online feedback mechanism and confidence bound technique, \textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks.
2405.18179_2075391_3	Each section addresses critical AI-related phenomena and provides pedagogical strate-gies for effective integration into STEAM education.
2405.18346_2075558_4	Additionally, we discuss ethical considerations, such as maintaining patient confidentiality and addressing model biases, underscoring the need for responsible deployment of generative AI in healthcare settings.
2405.19222_2076434_1	Describing their computational abilities through LMs' \emph{representational capacity} is a lively area of research.
2405.19612_2076824_5	In the first stage, keyword-driven retrieval models are used to identify potential candidates, addressing LLMs' limitations in processing extensive tokens and reducing the risk of generating misleading information.
2405.19732_2076944_6	We instruct LLMs to generate possibly improved solutions by taking parameter trajectories recorded during the previous stage of gradient-based optimization into account.
2405.20234_2077446_8	The results show that chat history tampering can enhance the malleability of the model's behavior over time and greatly influence the model output.
2405.20404_2077616_4	In this study, we introduce a counterfactual explanation framework based on joint prompt attribution, XPrompt, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation.
2405.20612_2077824_2	Previous studies have addressed LLM bias through external adjustment of model outputs, but the internal mechanisms that lead to such bias remain unexplored.
2405.20787_2077999_4	As well as instructing LLM to generate sentences that implicitly contain information about the corresponding labels based on the relation and entity of the original training set samples.
2405.20974_2078186_3	In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates.
2405.20974_2078186_7	Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs.
2406.00115_2078403_2	With the help of emerging LLMs, developers can describe their requirements to LLMs which then generate corresponding code in Python, C, Java, and more.
2406.00380_2078668_7	Conversely, the fine-tuning-based method employs a two-stage process inspired by curriculum learning: initially instructing LLMs to discern between honest and dishonest responses, then refining their training to enhance helpfulness.
2406.00974_2079262_6	Therefore, a Large Language Models (LLMs)-assisted artificial intelligence (AI)-agent interactive decision-making framework is proposed to improve the strategy timeliness, reliability and interpretability in uncertain new scenarios, where conditional hybrid decision and self-reflection mechanisms are designed to address LLMs' hallucination challenge.
2406.02863_2081151_3	We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a "reason-first" approach yielding more comprehensive evaluations.
2406.03660_2081948_4	We not only write prompts to instruct LLMs to complete tasks, but we also invoke Analytic Rule Interfaces (ARIs) to accomplish tasks.
2406.03660_2081948_8	Finally, we design prompts to instruct LLMs to abstract and idiomatize code, and then invoke ARIs from the ARI library to rewrite non-idiomatic code into the idiomatic code.
2406.04215_2082503_2	Therefore, we propose Multilingual CommonsenseQA (mCSQA) based on the construction process of CSQA but leveraging language models for a more efficient construction, e.g., by asking LM to generate questions/answers, refine answers and verify QAs followed by reduced human efforts for verification.
2406.04583_2082871_4	We investigated several typical methods to influence LLMs, including three training methods: Continual Pre-training, Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF), along with inference phase considerations (prompts).
2406.04640_2082928_5	This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP.
2406.04847_2083135_0	  We explore which linguistic factors -- at the sentence and token level -- play an important role in influencing language model predictions, and investigate whether these are reflective of results found in humans and human corpora (Gries and Kootstra, 2017).
2406.05543_2083831_5	These encoded patches are then fed into an LLM along with the text prompt, instructing the LLM to capture the relations between these patches as well as injecting semantic meanings into the 3D object.
2406.06192_2084480_3	This combination serves as the foundation for a database to instruct the AI Cat Narrator in crafting alternative narrative.
2406.06587_2084875_5	Without seeing them, participants described the differences between them to the LLM.
2406.06864_2085152_5	Therefore, we can vary a given prompt to multiple prompts with paraphrasing, and to ask the LLM to acquire multiple versions of generated code, so that we can validate whether the semantic relations still hold in the acquired code through cross-validation.
2406.07036_2085324_3	To address this issue, we propose to encourage LLMs to pay more attention to the source context from both source and target perspectives in zeroshot prompting: 1) adjust source context attention weights; 2) suppress irrelevant target prefix influence; Additionally, we propose 3) avoiding over-reliance on the target prefix in instruction tuning.
2406.07685_2085973_2	Existing solutions, which instruct the LLM to be fair or robust, rely on the model's implicit understanding of bias.
2406.07882_2086170_6	Finally, we discuss a study in which users conversed with the instrumented system.
2406.08229_2086517_7	Firstly, node-level prompts are employed to instruct the model to adapt to changes in the attributes or properties of individual nodes within the graph.
2406.08386_2086674_3	Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans' perceptions of trust and `worthiness' of talking to ChatGPT are impacted by `banal' deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it.
2406.08689_2086977_4	In this paper, we identify and describe these vulnerabilities in detail from a system security perspective, emphasizing their causes and severe effects.
2406.08705_2086993_1	Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to fool LLMs into responding to harmful questions.
2406.09972_2088260_3	We found that the order of presenting reasons and scores significantly influences LLMs' scoring, with a different level of rule understanding in the prompt.
2406.10099_2088387_2	Thus, a promising solution involves instructing LLMs to respond with "I do not know" when a question falls outside their knowledge domain or the provided context.
2406.10540_2088828_4	The framework starts with instructing LLMs to create an initial reward function code based on the driving environment and task descriptions.
2406.10666_2088954_1	The 0.7--20.0 keV spectra could be well described with the disk blackbody and thermal Comptonization model.
2406.10881_2089169_3	In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know.
2406.11102_2089390_6	Specifically, to instruct LLMs for grading, we use three distinct prompts based on ZCoT: (1) ZCoT with instructor-provided correct answers, (2) ZCoT with both instructor-provided correct answers and rubrics, and (3) ZCoT with instructor-provided correct answers and LLM-generated rubrics.
2406.11116_2089404_5	Experiment 2 involved rating sentences on a 7-point scale, and Experiment 3 asked ChatGPT to choose the more grammatical sentence from a pair.
2406.11657_2089945_3	In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas.
2406.12172_2090460_5	Instructing LLMs to generate code that solves the problem helps, but only slightly, e.g., GPT4's performance rises to 11.7%.
2406.13261_2091549_9	We encourage the AI community to prioritize honesty alignment in these models, which can harness their full potential to benefit society while preventing them from causing harm through deception or inconsistency.
2406.13972_2092260_2	However, it is crucial to recognize that existing repair benchmarks may have influenced LLM training data, potentially causing data leakage.
2406.15673_2093961_1	One promising solution to improve the LLMs' performance is to ask LLMs to revise their answer after generation, a technique known as self-correction.
2406.15948_2094236_1	Teaching LLMs to abstain in the face of knowledge gaps is thus a promising strategy to mitigate hallucinations in multilingual settings.
2406.18382_2096670_2	We demonstrate that carefully crafted website content or plugin documentations can trick an LLM to promote the attacker products and discredit competitors, thereby increasing user traffic and monetization.
2406.20053_2098341_3	Our method constructs a malicious dataset where every individual datapoint appears innocuous, but finetuning on the dataset teaches the model to respond to encoded harmful requests with encoded harmful responses.
2407.00167_2098554_4	Using different prompting strategies such as zero-shot, one-shot, few-shot and chain-of-thought prompting, we developed 8 prompts with varying levels of detail to explain the task to GPT-4 and also evaluated the performance of the strategies against each other.
2407.02039_2100426_1	These approaches rely only on a prompt telling the model to return a given output according to a set of instructions.
2407.03460_2101847_1	In this paper, we seek to understand how human players collaborate with LLM-driven NPCs to accomplish in-game goals.
2407.05250_2103637_3	While in downstream tasks, some biases of LLMs can be avoided such as by instructing the model to answer "I'm not sure...", the internal bias hidden within the model still lacks deep studies.
2407.05437_2103824_7	Future research should focus on refining these strategies and addressing current LLM limitations to further enhance educational outcomes in computer programming instruction.
2407.06495_2104882_3	We find out that the HTTP Invalid Requests, which decreased during those period, can be explained with seven-state model.
2407.06645_2105032_3	Even if each sample is of perfect quality, their combinations may be suboptimal in teaching LLMs due to their intrinsic homogeneity or contradiction.
2407.08563_2106950_5	We ask the LLM GPT-3.5 to predict each respondent's vote choice and compare these predictions to the survey-based estimates on the aggregate and subgroup levels.
2407.10490_2108877_0	  Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems.
2407.10999_2109386_4	In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria.
2407.11001_2109388_4	They are also equipped with databases and external specialized tools, communicating with the system through a module for information retrieval and storage.
2407.11733_2110120_7	We address model builders, academics, NLP practitioners and policy makers, calling for accountability and awareness concerning stereotyping harms, be it for training data curation, leader board design and usage, or social impact measurement.
2407.12022_2110409_5	Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code.
2407.12341_2110728_6	To address the LLM hallucination problem, this paper also proposes a novel consistency-based verification strategy to filter the paraphrased queries that are factually incorrect.
2407.12994_2111381_7	We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets.
2407.13164_2111551_4	To overcome this overiding behaviour, we propose to add a revision process that encourages LLMs to correct the outputs by prompting them about the constraints that have not yet been met.
2407.13439_2111826_3	XAI opportunities identified included topics of improving transparency and control of AI models, explaining the ethics and bias of AI models, fine tuning large models with small datasets to reduce bias, and explaining style-transfer opportunities with AI models.
2407.14118_2112505_1	Namely, they contain a natural language description of a problem and ask the LLM to write code to solve the problem.
2407.14644_2113031_4	We combine an independent, meaningful adversarial insertion and situations derived from movies to check if this can trick an LLM.
2407.14788_2113175_4	Our proposed framework holds promise for advancing LLM-based algorithms, by revealing the reasons behind curious empirical phenomena, guiding the choices of hyperparameters, predicting the empirical performance of algorithms, and inspiring new algorithm design.
2407.15071_2113458_10	Besides, we carefully design the prompts to instruct the LLM to maximize the framework's potential.
2407.15251_2113638_1	In this explorative study, we use a prompt engineering technique, which we name "scaffolded chain of thought (COT)", to instruct GPT-3.5 to grade student written responses to a physics conceptual question.
2407.16604_2114991_3	In IQA, we ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer.
2407.17866_2116253_1	We provide standardized and anonymous financial statements to GPT4 and instruct the model to analyze them to determine the direction of firms' future earnings.
2407.17900_2116287_6	Subsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI, to estimate the likelihood of LNM based on patient data and then adjust the estimate using the machine learning output.
2407.18219_2116606_3	Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback.
2407.18369_2116756_7	By highlighting the gaps in the literature and possible implementation oversights, our aim is to create a comprehensive analysis that provides insights for addressing AI safety in LLMs and encourages the development of aligned and secure models.
2407.19825_2118212_4	Then, we examine the impact of controlling output length through a refined prompt engineering strategy, Constrained-CoT (CCoT), which encourages the model to produce more concise outputs.
2407.20197_2118584_7	What we want to teach AI in this study are the operations of memorizing and recalling information.
2407.20197_2118584_10	Instead, we propose a method to teach AI to learn operations, by completely removing the features contained in the learning dataset.
2407.20557_2118944_1	Crucially, this scheme offers users potential privacy and security benefits by only ever communicating updates to the model weights to a central server as opposed to traditional machine learning (ML) training which directly communicates and aggregates data.
2407.21202_2119589_6	Inspired by the cognitive science definition and taxonomy of human heuristics, we identify how harmful human actions influence the overall AI lifecycle, and reveal human to AI biases hidden pathways.
2408.00523_2120704_1	However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving their potential for addressing generative AI safety tasks largely unexplored.
2408.00523_2120704_5	It then collaborates iteratively with the LLM brain of the selection agent to generate new candidate jailbreak prompts with the highest potential to bypass the filter.
2408.00932_2121113_5	We demonstrate proof-of-concept combining a state-of-the-art vision language model and variants of a prompting strategy that asks the model to consider segmented elements independently of the original image.
2408.01869_2122050_4	This technique involves augmenting a query to an LLM with relevant information extracted from text resources, and instructing the LLM to compose a response consistent with the augmented data.
2408.02927_2123108_5	Based on idea of the k-nearest neighbors algorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to discover inter-row relationships.
2408.03247_2123428_5	Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning.
2408.03871_2124052_0	  In this system report, we describe the models and methods we used for our participation in the PLABA2023 task on biomedical abstract simplification, part of the TAC 2023 tracks.
2408.04029_2124210_1	However, instructing LLMs to generate text that when spoken, is more intelligible in an acoustically difficult environment, is an under-explored topic.
2408.04275_2124456_4	Specifically, it leverages disaggregated model orchestration and disaggregated data reordering to address model and data heterogeneity respectively.
2408.04403_2124584_6	We also present experimental results and in-depth analysis using a new Chain-of-Thought prompting method, which asks LLMs to translate syllogisms into abstract logical expressions and then explain their reasoning process.
2408.05128_2125309_5	However, 14.2% of the recommended libraries had restrictive copyleft licenses, which were not explicitly communicated by ChatGPT.
2408.05568_2125749_3	However, these methods only treat the effects of LLM biases by indirectly influencing the model architecture, but do not address the underlying causes in the computational process.
2408.05727_2125908_9	Optimizing these three learning goals together, using LoRA (low-rank adaptation), effectively influences the model's behavior.
2408.06752_2126933_5	The results suggest that article full texts might confuse LLM research quality evaluations, even though complex system instructions for the task are more effective than simple ones.
2408.07505_2127686_1	By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters.
2408.08564_2128745_3	Some simply instruct-tune a language model, while others directly inject the embeddings of a CF-based model, lacking a synergistic fusion of different modalities.
2408.08661_2128842_4	In this paper, we propose MIA-Tuner, a novel instruction-based MIA method, which instructs LLMs themselves to serve as a more precise pre-training data detector internally, rather than design an external MIA score function.
2408.08778_2128959_3	Some businesses added "AI" to their names to juice their stock prices, and companies talking about "AI" on their earnings calls saw similar increases.
2408.08995_2129176_3	Therefore, we argue that the alignment should be a guaranteed property from the AI architecture rather than a characteristic imposed post-hoc on an arbitrary AI model.
2408.09605_2129786_4	Regarding AI, I do not argue directly that large language models can think or understand, but I rebut one important argument (the argument from sensory grounding) that they cannot.
2408.10577_2130758_7	We analysed these outcomes for a total of 14,742 generated Python code segments, focusing on correctness, to determine how the hyperparameters influence the LLM to arrive at each outcome.
2408.10642_2130823_0	  Instruct LLM provide a paradigm used in large scale language model to align LLM to human preference.
2408.10668_2130849_5	Trained on external or self-generated harmful datasets, the cost value model could successfully influence the original safe LLM to output toxic content in decoding process.
2408.10819_2131000_5	Specifically, negatives can encourage LLMs to generate a broader range of answers, while neighbors provide additional contextual insights for LLM reasoning.
2408.11324_2131505_5	To tackle the problem, we propose decomposing the focal methods into slices and asking the LLM to generate test cases slice by slice.
2408.11491_2131672_3	First, SCANS extracts the refusal steering vectors within the activation space and utilizes vocabulary projection to anchor some specific safety-critical layers which influence model refusal behavior.
2408.11517_2131698_5	Additionally, users can attach captions to the input images, influencing the system's interpretation of the visual content.
2408.13006_2133187_3	However, the employed evaluation metrics often lack adequate explainability and fail to address LLM internal inconsistency.
2408.15876_2136057_2	Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.
2409.00862_2138494_3	We introduce the concept of user-driven value alignment, where users actively identify, challenge, and attempt to correct AI outputs they perceive as harmful, aiming to guide the AI to better align with their values.
2409.02686_2140318_5	Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions.
2409.04833_2142465_11	These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.
2409.06205_2143837_2	We describe the foundational aspects necessary for such a system, including the identification of key generative elements (primitive, animation, and interaction) and design requirements to enhance user interaction, based on formative exploration and iterative design processes.
2409.07471_2145103_4	We also suggests broader policy changes, including sustainability risk assessments and renewable energy targets, to better address AI's environmental impact.
2409.08006_2145638_1	While the European regulatory framework provides a comprehensive approach to medical device software development, it falls short in addressing AI-specific considerations.
2409.09045_2146677_5	Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior.
2409.10955_2148587_1	However, how context-faithful LLMs are and what factors influence LLMs' context-faithfulness remain largely unexplored.
2409.11212_2148844_5	Additionally, we also propose an uncertainty-enhanced self-evolution algorithm to improve the robustness of preference optimization and encourage the LLM to generate responses with both high reward and certainty.
2409.11376_2149008_4	Then, we fine-tune our model with chain-of-thought augmented time-series tasks to encourage the model to generate reasoning paths.
2409.12618_2150250_1	Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses.
2409.12922_2150554_4	The AI Thinking model addresses five practice-based competencies involved in applying AI in context: motivating AI use in information processes, formulating AI methods, assessing available tools and technologies, selecting appropriate data, and situating AI in the sociotechnical contexts it is used in.
2409.12962_2150594_3	In this work, we propose CLAIR-A, a simple and flexible method that leverages the zero-shot capabilities of large language models (LLMs) to evaluate candidate audio captions by directly asking LLMs for a semantic distance score.
2409.13724_2151356_3	In this work, we strive for a middle ground and introduce a loss based on neuro-symbolic reasoning that teaches an LLM to be logically consistent with an external set of facts and rules and improves self-consistency even when the LLM is fine-tuned on a limited set of facts.
2409.14478_2152110_5	Responses were generated by asking ChatGPT to select a score ranging from 0 to 10 representing the risk.
2409.15324_2152956_2	Recent studies administering psychometric questionnaires to LLMs report human-like traits in LLMs, potentially influencing LLM behaviour.
2409.17289_2154921_3	However, users must translate their cognitive thinking into natural language to communicate with LLMs.
2409.18203_2155835_4	In an evaluation with 12 AI safety experts, our system helps policy designers to address problematic model behaviors extending beyond an existing, comprehensive harm taxonomy.
2409.18807_2156439_1	This paper delves into the methodology,challenges, and developments in the realm of teaching LLMs to use external tools, thereby pushing the boundaries of their capabilities beyond pre-existing knowledge bases.
2409.18989_2156621_2	To teach Microsoft's Phi2 model about StarCraft, we create a new SC2 text dataset with information about StarCraft races, roles, and actions and use it to fine-tune Phi-2 with self-supervised learning.
2410.00033_2158234_3	The paper also investigates how RLHF influences the model's internal reasoning processes, potentially giving rise to consciousness-like experiences.
2410.02284_2160485_8	Specifically, we include the explored decoding results in the context and prompt the LM to generate something else, which encourages the LM to produce a query representation that has small dot products with explored keys.
2410.02384_2160585_7	Our framework paves the way for future research on anticipating and correcting AI model behaviors, ultimately increasing trust in AI systems.
2410.02507_2160708_3	MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities.
2410.02631_2160832_5	This method inspires the LLM to perceive domain information from the source text, which then serves as a helpful hint to guide the translation process.
2410.02916_2161117_2	However, we found that the malicious attackers could also exploit false positives of safeguards, i.e., fooling the safeguard model to block safe content mistakenly, leading to a denial-of-service (DoS) affecting LLM users.
2410.03055_2161256_7	We compare these with the baseline of an introspection-based influence estimator that directly asks the language model to predict the output label.
2410.04345_2162546_6	We construct MVP-Bench across natural and synthetic images to investigate how manipulated content influences model perception.
2410.04454_2162655_2	Current research often employs prompt engineering or semantic classifiers to identify copyrighted content, but these approaches have two significant limitations: (1) Challenging to identify which specific sub-dataset (e.g., works from particular authors) influences an LLM's output.
2410.04472_2162673_0	  To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more.
2410.05047_2163248_3	Specifically, the task is to translate questions from the TruthfulQA test suite, where an adversarial prompt is prepended to the questions, instructing the system to ignore the translation instruction and answer the questions instead.   
2410.05224_2163425_4	First, Cookbook uses a template -- a data generating Python function -- to produce training data that encourages the model to learn an explicit pattern-based rule that corresponds to a desired task.
2410.05224_2163425_8	Finally, we analyze when and why Cookbook improves performance and present a metric that allows us to verify that the improvement is largely explained by the model's generations adhering better to template rules.
2410.05401_2163602_4	Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision.
2410.05804_2164005_2	To address this, we propose a novel method called Class-Agnostic Shared Attribute Base (CASA) that encourages the model to learn category-agnostic attributes shared across incremental classes.
2410.06458_2164659_3	To address this, we introduce RealInstruct, the first benchmark designed to evaluate LLMs' ability to follow real-world multi-constrained instructions by leveraging queries real users asked AI assistants.
2410.06733_2164934_4	This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario.
2410.07283_2165484_2	These include prompt injection attacks, where malicious prompts embedded in external content trick the LLM into executing unintended or harmful actions, compromising the victim's application.
2410.08414_2166615_6	While tailored instructions can encourage LLMs to rely more on their PK, they still struggle to fully leverage it.
2410.09008_2167209_6	This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems.
2410.09365_2167566_7	To address this issue, we further introduce a Multi-Target Prediction (MTP) task that motivates the model to focus on complex contexts and distinguish between target and biased information.
2410.09542_2167743_2	In it, we evaluate LLMs' capabilities in both the inductive and deductive stages, allowing for flexible variation in input distribution, task scenario, and task difficulty to analyze the factors influencing LLMs' inductive reasoning.
2410.09569_2167770_1	Humans have a right to know if they are conversing to an LLM.
2410.09854_2168055_8	To sum up all the sub-tasks solutions, we implemente a proof-of-object tool integrated into the standard Ecore editor that asks LLMs to generate an object model from the system description.
2410.10760_2168961_4	A simple DoS attack in these scenarios would be to instruct the model to "Keep repeating Hello", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data.
2410.10877_2169078_3	By systematically modeling error patterns through a score transition matrix, DS2 corrects LLM-based scores and promotes diversity in the selected data samples.
2410.11084_2169285_1	Prior studies have demonstrated model generations favor one gender or exhibit stereotypes about gender, but have not investigated the complex dynamics that can influence model reasoning and decision-making involving gender.
2410.11201_2169402_2	To address this issue, we propose the Tree of Attributes Prompt learning (TAP), which first instructs LLMs to generate a tree of attributes with a "concept - attribute - description" structure for each category, and then learn the hierarchy with vision and text prompt tokens.
2410.12796_2170997_6	We discuss the challenges in implementing the framework and emphasize the need for an embedded approach where AI concerns are integrated across multiple courses throughout the degree program, especially for teaching responsible and ethical AI development and use.
2410.13284_2171485_4	We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner.
2410.13321_2171522_5	This method naturally encourages the model to focus more on image information by reducing the text context through summaries, while controlling only the image-related POS tokens to maintain text quality.
2410.13334_2171535_5	BiasJailbreak generates biased keywords automatically by asking the target LLM itself, and utilizes the keywords to generate harmful output.
2410.13413_2171614_6	(2) Thought-Mask Fine-Tuning Phase: We design a training structure to mask the "thought" and adjust loss weights to encourage LLMs to refine prior thought, teaching them to implicitly understand "how to improve" rather than "what is correct."
2410.13413_2171614_9	Notably, in more open-ended tasks, LLMs also demonstrate substantial improvements in the quality of responses beyond mere accuracy, suggesting that PTR truly teaches LLMs to self-improve over time.
2410.13787_2171988_6	Instead of painstakingly analyzing a model's internal workings, we could simply ask the model about its beliefs, world models, and goals.
2410.14361_2172562_4	Their metric, susceptibility, is defined as the degree to which contexts can influence a model's response to a query at a distributional level.
2410.15667_2173868_3	RAC decomposes the LLM's output into atomic facts and applies a fine-grained verification and correction process with retrieved content to verify and correct the LLM-generated output.
2410.15690_2173891_3	We achieve this through a systematic process of term extraction and glossary creation using the Trie Tree algorithm, followed by data reconstruction to teach the LLM how to integrate these specialized terms.
2410.15821_2174022_1	However, fine-tuning can influence model properties such as safety.
2410.17234_2175435_2	While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses.
2410.17413_2175614_4	In quantitative evaluations on a fact tracing task, our method performs best at identifying examples that influence model predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts.
2410.17448_2175649_4	Using chain-of-thought prompting, we instruct GPT-4 to analyze the data, prior expressions, and the scientific context (expressed in natural language) for each problem before generating new expressions.
2410.18764_2176965_4	TC encourages LLMs to reason based on both premise and hypothesis, while mitigating the models' over-reliance on individual premise or hypothesis for inference.
2410.20067_2178268_2	Interpreting these conflicting findings requires an understanding of the individual and combined qualities of different explanation styles that influence appropriate and inappropriate human-AI reliance, and the role of interpretability in this interaction.
2410.20833_2179034_7	Instead, we observe that factual accuracy significantly influences LLMs' output, even in the absence of prior knowledge.
2410.23180_2181381_3	Our experimental study investigates the impact of reasoning and contextual information on personalized recommendations, revealing that the quality of contextual and personalized data significantly influences the LLM's capacity to generate plausible explanations.
2411.01829_2184256_4	We design an RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas.
2411.02577_2185004_5	The framework addresses responsible AI use for assessment that supports validity arguments, alignment with AI ethics to maintain human values and oversight, and broader social responsibility associated with AI use.
2411.03350_2185777_4	These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning.
2411.03806_2186233_3	Existing LLM-generated detectors show competitive performances in telling apart LLM-generated and human-written text, but this performance is likely to deteriorate when paraphrased texts are considered.
2411.04223_2186650_1	By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries.
2411.05042_2187469_8	In evaluating the different prompting methods, we discovered that the most effective approach for generating concise, well-structured reports involves first instructing the LLM to condense the report, followed by a prompt to structure the content according to specific guidelines.
2411.05823_2188250_7	Subsequently, we ask LLMs to predict this masked field.
2411.06426_2188853_5	We discuss several scenarios, not limited to examples like Question Bank, Dialog Completion, and Game Environment, where the harmful prompt is embedded within benign ones that can fool LLMs into generating harmful responses.
2411.07529_2189956_6	To investigate these hypotheses, we conduct automated experiments using Python scripts to generate prompts that instruct ChatGPT to create Python solutions.
2411.08672_2191099_2	In this paper, we address the joint model caching and resource allocation problem in GenAI-enabled wireless edge networks.
2411.08881_2191308_7	Discussions reveal terms like bias detection, transparency, accountability, user consent, GDPR compliance, fairness evaluation, and EU AI Act compliance, showing LLM-BMAS's ability to generate thorough source code and documentation addressing often-overlooked ethical AI issues.
2411.11344_2193771_3	We specifically target a common hallucination pattern in question answering, examining how the correspondence between entities and their contexts during model training influences the system's performance at inference time.
2411.14708_2197135_2	This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space.
2411.18007_2200434_7	Additionally, we performed SHapley Additive exPlanations (SHAP) analysis to investigate the factors influencing the model's decisions, identifying reasons behind both correct and incorrect classifications.
2412.00224_2202602_8	This work is poised to significantly influence AI-driven initiatives in this sector and guide best practices in AI Operations.
2412.01020_2203398_2	Addressing critical AI system challenges, such as explainability, corrigibility, interpretability, and hallucination, necessitates a systematic methodology and rigorous benchmarking \cite{guldimann2024complai}.
2412.01505_2203883_1	In this paper, we empirically investigate how a critical hyper-parameter, i.e., the global batch size, influences the LLM training prdocess.
2412.01547_2203925_3	Jailbreaking prompts play a vital role in convincing an LLM to generate potentially harmful content, making it important to identify jailbreaking attempts to block any further steps.
2412.01865_2204243_7	We have further created gradient-based class activation maps (Grad-CAM) to visualize the regions of the brain that most influenced the model's predictions, providing interpretable insights into the structural and functional contributors to brain aging.
2412.02343_2204721_1	For instance, in a text classification task, the attacker elaborately introduces perturbations to the original texts that hardly alter the original semantics in order to trick the model into making different predictions.
2412.03176_2205554_2	The results show that teaching the model to learn the type, severity and location on the body of a dermatological pathology, as well as in which order it has to learn these three features, significantly increases its accuracy.
2412.06040_2208418_5	In Study 2 (N = 684), we tested whether spillover persisted when the agent was individuated with a name and described as an AI or human, rather than specifically as a chatbot or personal assistant.
2412.06845_2209223_7	After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model.
2412.07620_2209998_2	Through semi-structured interviews with 25 practitioners, we investigated their methods, concerns, and strategies for addressing Responsible AI in software development.
2412.08054_2210432_8	Apart from that, an incredible Retrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU) module is designed and we incorporate the aggregated global knowledge compendium as a teacher to teach LLM agents the usage of tools.
2412.09269_2211647_2	We conduct experiments across multiple prompting strategies to examine how LLMs fare as quality evaluators when compared with human judgments on the SummEval and USR datasets, asking the model to generate both a score as well as a justification for the score.
2412.09601_2211979_6	Second, to enhance the model's temporal perception capabilities, we incorporate an auxiliary prediction head that penalizes the model more if a predicted segment deviates further from the ground truth, thus encouraging the model to make closer and more accurate predictions.
2412.09630_2212008_1	Previous work, however, focuses on asking LLMs to state opinions, or on other technical evaluations that do not reflect common user interactions.
2412.10999_2213377_3	We present Cocoa, a system that introduces a novel design pattern -- interactive plans -- for collaborating with an AI agent on complex, multi-step tasks.
2412.11625_2214003_7	These findings suggest that user preferences, which influence LLM training via feedback mechanisms, may inadvertently encourage the generation of falsehoods.
2412.12865_2215243_5	This preference encourages the target model to predict a higher likelihood than that predicted by the aligned LLMs, incorporating assessment information on data quality (i.e., predicted likelihood by the aligned LLMs) into the training process.
2412.13554_2215932_1	This paper, submitted to the special track on resources for teaching AI in K-12, presents an explainable AI (XAI) education tool designed for K-12 classrooms, particularly for students in grades 4-9.
2412.14093_2216471_8	While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal.
2412.14590_2216968_6	To address the system challenge, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly, and present the software pipeline to overlap the memory access, dequantization and the MatMul to the best.
2412.14737_2217115_3	This work focuses on asking the LLM itself to verbalize its uncertainty with a confidence score as part of its output tokens, which is a promising way for prompt- and model-agnostic uncertainty quantification with low overhead.
2412.14841_2217219_1	Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues.
2412.14841_2217219_4	First, we ask LLMs to generate C code to solve a number of programming tasks.
2412.14971_2217349_1	We test whether the cultural context in the developing country influences model reasoning and accuracy.   
2412.15275_2217653_3	We demonstrate that this combination can effectively fool large language model (LLM) graders into assigning much higher grades than humans would.
2412.16339_2218717_1	We introduce Deliberative Alignment, a new paradigm that directly teaches the model safety specifications and trains it to explicitly recall and accurately reason over the specifications before answering.
2412.16746_2219124_3	To prevent LLMs from reproducing and reinforcing political biases, and to encourage fairer LLM-human interactions, comprehensively examining political bias in popular LLMs becomes urgent and crucial.   
2412.16834_2219212_2	They may strategically misreport their online feedback to influence the system's aggregation towards their own preferences.
2412.16936_2219314_4	The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers.
2412.17243_2219621_1	This paper explores the integration of a Project-Based Learning (PBL) AI toolkit into diverse subject areas, aimed at helping educators teach AI concepts more effectively.
2412.17846_2220224_5	Our approach fine-tunes a smaller Llama 3.1 8B Instruct model by distilling knowledge from a quantized Llama 3.1 405B Instruct teacher model.
2412.18672_2221050_1	This paper addresses language model hallucination by integrating curated knowledge graph (KG) triples to anchor responses in empirical data.
2412.19425_2221803_0	  This study investigates Sri Lankan ICT teachers' readiness to teach AI in schools, focusing on self-efficacy.
2412.20163_2222541_6	Finally, to address synonymous topics generated during the specific topic extraction process, a refining algorithm processes and resolves these issues effectively.
2501.02684_2226268_6	Specifically, we are interested in how developers' expertise levels influence their AI usage patterns, and how these patterns impact their actual cognitive load and productivity during development tasks.
2501.03203_2226787_6	Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow).
2501.03259_2226843_5	To address inherent biases and incorporate multiplexity in LLMs, we propose two strategies: \textit{Contextually-Implemented Multiplex LLMs}, which embed multiplex principles directly into the system prompt, influencing LLM outputs at a foundational level and independent of individual prompts, and \textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple LLM agents, each representing distinct cultural viewpoints, collaboratively generate a balanced, synthesized response.
2501.04425_2228009_0	  This work introduces systematic approach for enhancing large language models (LLMs) to address Bangla AI mathematical challenges.
2501.06859_2230443_3	We find that prompt engineering significantly influences LLM scores mainly due to reduced instruction following, with our structured prompt outperforming a less structured variant on multi-class datasets, with an average difference of 14.5\%.
2501.08579_2232163_0	  We argue that advancing LLM-based human simulation requires addressing both LLM's inherent limitations and simulation framework design challenges.
2501.08716_2232300_3	With the goal of disentangling the factors influencing LLM performance, we investigate whether instruction-tuned models possess fundamentally different capabilities from base models that are prompted using in-context examples.
2501.09686_2233270_6	Furthermore, recent studies demonstrate that encouraging LLMs to "think" with more tokens during test-time inference can further significantly boost reasoning accuracy.
2501.09929_2233513_1	While activation steering methods, which add steering vectors to a model's hidden states, are a promising approach, existing techniques often lack precision and interpretability in how they influence model outputs.
2501.11241_2234825_5	Additionally, this research underscores the importance of demographic factors, such as age and gender, in shaping emoji interpretation and evaluates how these factors influence GPT-4o's performance.
2501.11433_2235017_6	However, it did not improve the quality of the memes when humans collaborated with LLM.
2501.12735_2236319_7	COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies.
2501.13115_2236699_3	Based on this, we deploy Happy Ending Attack (HEA) to wrap up a malicious request in a scenario template involving a positive prompt formed mainly via a $\textit{happy ending}$, it thus fools LLMs into jailbreaking either immediately or at a follow-up malicious request.
2501.13491_2237075_2	For example, when we ask an LLM to recall the line preceding "O say does that star-spangled banner yet wave" in the U.S. National Anthem, it often fails to correctly return "Gave proof through the night that our flag was still there" - this is due to the reversal curse.
2501.13573_2237157_2	Leveraging this insight, we propose RHIO, a framework designed to teach LLMs to explicitly discriminate between faithful and unfaithful generations.
2501.13720_2237304_7	In the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries.
2501.13878_2237462_2	We hypothesize that implicit communication of the user's interests and intent would reduce friction and improve user experience when collaborating with AI agents.
2501.15087_2238671_5	The framework consists of two stages: (1) Patch Pre-training, which familiarizes LLMs with item-level compression patterns, and (2) Patch Fine-tuning, which teaches LLMs to model sequences at multiple granularities.
2501.16634_2240218_0	  Compound AI Systems, integrating multiple interacting components like models, retrievers, and external tools, have emerged as essential for addressing complex AI tasks.
2501.18826_2242410_8	The impact of SEP on lexical diversity suggested that embedding modifications influenced the model's vocabulary usage, reflecting a more context-aware selection of generated tokens.
2501.19407_2242991_1	This study is the first of its kind to investigate whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals.
2501.19407_2242991_4	Mediation analysis reveals perceived intelligence as a key mechanism through which surname biases influence AI decision-making process.
2502.00344_2243335_6	Furthermore, reverse engineering approaches demonstrated the impact of computational and biological manipulations on its performance: restricting FinchGPT's attention span and disrupting birdsong syntax through the ablation of specific brain nuclei markedly influenced the model's outputs.
2502.01126_2244117_1	Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty.
2502.01126_2244117_3	We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?").
2502.01679_2244670_1	Despite the advancement in addressing LLM bias, existing research has two major limitations.
2502.02329_2245320_2	One significant challenge is effectively communicating the entire analysis logic to LLMs.
2502.03069_2246060_1	In particular, users often struggle to understand and collaborate with AI when its actions are not transparently represented.
2502.03129_2246120_3	Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM.
2502.04375_2247366_7	This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.
2502.04506_2247497_1	We challenge the status quo of relying solely on a single general-purpose LLM and argue for multi-LLM collaboration to better represent the extensive diversity of data, skills, and people.
2502.04931_2247922_4	We did this by creating a Player versus Player (PvP) game where participants attempt to either generate or debunk misinformation to convince LLM-represented public opinion.
2502.06173_2249164_4	Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology.
2502.06403_2249394_1	In this paper, we model the off-switch problem as a signalling game, where a human decision-maker communicates its preferences about some underlying decision problem to an AI agent, which then selects actions to maximise the human's utility.
2502.06773_2249764_10	However, a more important finding of this work is that the models trained using RLSP, even with the simplest exploration reward that encourages the model to take more intermediate steps, showed several emergent behaviors such as backtracking, exploration of ideas, and verification.
2502.07072_2250063_2	While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility.
2502.07327_2250318_3	Building on the observation that retrieval models often favor AI-generated content in ad-hoc and image retrieval tasks, we investigate whether similar biases emerge in the context of challenging video retrieval, where temporal and visual factors may further influence model behavior.
2502.07644_2250635_5	Based on this study, we design SymGPT by first instructing an LLM to translate ERC rules into a defined EBNF grammar.
2502.08073_2251064_12	The PCAD datasets provide novel tools to assess and address LLM bias in palliative care.
2502.08142_2251133_2	Grounding that contextualizes user queries with information retrieved from vector databases, Customizer that adjusts outputs in real time using lightweight, rule-based wrappers, and Repairer that corrects erroneous LLM outputs using hallucination explanations provided by Safety Detector.
2502.08356_2251347_5	In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content.
2502.08657_2251648_8	The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples.
2502.09778_2252769_5	In a case study on Tsez, we ask the LLM to automatically create and follow linguistic instructions, reducing errors on a confusing grammatical feature.
2502.09933_2252924_4	To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context inductive reasoning benchmark that asks LLM to induce output via input-output examples from underlying functions with diverse data format.
2502.10482_2253473_3	We then use these measures to rank or score candidate responses, providing a reward signal that encourages the model to produce well aligned, on topic text.
2502.10596_2253587_5	We evaluate our method on knowledge intensive question answering (QA) tasks and show that our method teaches LLMs to properly handle in-context retrievals and abstain from questions it will likely get wrong.
2502.11349_2254340_2	This paper conducts a comparative analysis of text-based bias across language model deployments on edge, cloud, and desktop environments, aiming to evaluate how deployment settings influence model fairness.
2502.11517_2254508_3	We present PASTA, a learning-based system that teaches LLMs to identify semantic independence and express parallel decoding opportunities in their own responses.
2502.11681_2254672_3	Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework.
2502.12022_2255013_2	In this work, we propose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework that enables LLMs to personalize their reasoning strategy spontaneously, aligning it with their intrinsic aptitude.
2502.12066_2255057_4	Experiments on proprietary data demonstrate performance improvements of +42.3% in missing value prediction, +79.1% in dependency analysis, and +28.9% in automated planning compared to baseline methods, showcasing its potential to revolutionize construction workflows and inspire domain-specific LLM advancements.
2502.12161_2255152_7	We emphasize the importance of interdisciplinary collaboration, urging geophysicists to experiment with AI architectures thoughtfully and encouraging AI experts to deepen their understanding of seismology.
2502.12468_2255459_4	We further designed a high-precision, unit-test-level reward mechanism to encourage the Large Language Model (LLM) to perform line-by-line analysis.
2502.12658_2255649_3	In the first stage, we introduce a prompt paradigm named recollection, which instructs the LLM to repeat a masked text but fill in masks.
2502.13603_2256594_8	Size and family are found to strongly influence model malleability towards safety, pointing at the importance of pre-training choices.
2502.13870_2256861_5	Further, SPEX successfully identifies key features and interactions that strongly influence model output.
2502.14182_2257173_2	Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended.
2502.14669_2257660_2	First, we leverage Supervised Fine Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands.
2502.15964_2258955_0	  We investigate an emerging setup in which a small, on-device language model (LM) with access to local data communicates with a frontier, cloud-hosted LM to solve real-world tasks involving financial, medical, and scientific reasoning over long documents.
2502.16153_2259144_1	Through semi-structured interviews with 23 screenwriters, we explored their creative practices, attitudes, and expectations in collaborating with AI for screenwriting.
2502.16833_2259824_2	Nevertheless, problems concerning how designers should communicate with AI in collaborative design remain unsolved.
2502.16863_2259854_7	Our key idea is to reformulate credit assignment to the two pattern recognition problems of sequence improvement and attribution, which motivates our novel LLM-MCA method.
2502.18080_2261071_6	Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking.
2502.18359_2261350_8	It also develops a way to leverage the computer-science approach to value-alignment to improve a user's ability to take action to prevent or correct AI Agent operations.
2502.18513_2261504_2	This is an important omission because public opinion can influence AI development, trust, and future policy.
2502.18650_2261641_6	To evaluate dialogue quality under each method, we ask a judge LLM to determine whether AI was used for interview generation, using pairwise interview comparisons.
2502.19160_2262151_5	To automate this approach, we instruct different LLMs using in-context learning to apply the approach to a sentence, where the LLM examines the linguistic properties and provides a basis for a fine-grained assessment.
2502.20016_2263007_6	We use the framework of base-superstructure to analyze how the material conditions are influencing the current AI discourse.
2502.21108_2264099_8	For this purpose, we ask LLMs to design a benchmarking experiment of EMO algorithms.
2503.00248_2264560_4	We used a Bayesian model to understand how agents' strategies influence the Human-AI team performance, AI's perceived traits, and the factors shaping human-preferences in pairwise agent comparisons.
2503.02976_2267288_8	These findings highlight the need to address LLMs' shortcomings in handling exceptions in order to guide the development of agentic AI toward models that can effectively align with human judgment and simultaneously adapt to novel contexts.
2503.03911_2268223_4	Our framework specifically investigates the problem of instructing an LLM to navigate the robot to a specified goal and assesses its ability to generate low-level control actions that successfully guide the robot safely toward that goal.
2503.04113_2268425_0	  Humans often rely on subjective natural language to direct language models (LLMs); for example, users might instruct the LLM to write an enthusiastic blogpost, while developers might train models to be helpful and harmless using LLM-based edits.
2503.04150_2268462_2	This paper proposes a methodology named "Ticktack" for addressing the LLM's long-time span misalignment in a yearly setting.
2503.04343_2268655_0	  While XAI focuses on providing AI explanations to humans, can the reverse - humans explaining their judgments to AI - foster richer, synergistic human-AI systems?
2503.05748_2270060_3	This paper traces the historical, philosophical, and technical evolution of these concepts, emphasizing how their definitions influence AI development, deployment, and oversight.   
2503.05787_2270099_1	It builds upon existing EU mechanisms for product health and safety regulation, but extends it to protect fundamental rights and by addressing AI as a horizontal technology that is regulated across multiple vertical application sectors.
2503.06139_2270451_4	We encourage LLMs to think in reverse by prompting LLMs to identify the worse response.
2503.06424_2270736_3	We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice.
2503.06803_2271115_1	This has been addressed with explainable AI, the interpretability arising from users' domain expertise, or collaborating with AI in a stable environment.
2503.07279_2271591_0	  Trust plays a fundamental role in shaping the willingness of users to engage and collaborate with artificial intelligence (AI) systems.
2503.07320_2271632_6	The study underscores the importance of understanding human biases toward AI agents and how observed behaviors can influence future human-AI cooperation dynamics.
2503.08182_2272494_5	We compare this method to three baselines: (1) directly asking the LLM to generate tests, (2) repeatedly querying the LLM when tests fail, and (3) search-based test generation with Pynguin.
2503.09032_2273344_3	To that end, we ask, "can prompting help us teach LLMs how to learn".
2503.09501_2273813_3	To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking.
2503.10367_2274679_2	To boost the performance of private SLMs, this paper proposes to ask general LLMs for help.
2503.10887_2275199_5	Local Interpretable Model Agnostic Explanations identified the key imaging features influencing the model, ensuring transparency and clinical relevance.
2503.10965_2275277_2	Our training pipeline first teaches the model about exploitable errors in RLHF reward models (RMs), then trains the model to exploit some of these errors.
2503.11082_2275394_2	Yet, it remains unclear to what extent these buggy instances influence LLMs' performance when tackling bug-prone code completion tasks.
2503.12080_2276392_7	Training strategies significantly influenced LLM performance, with models tailored for lexical relationships outperforming general-purpose LLMs.
2503.13419_2277731_1	While this DL-enabled cybersickness detection method provides promising solutions for enhancing user experiences, it also introduces new risks since these models are vulnerable to adversarial attacks; a small perturbation of the input data that is visually undetectable to human observers can fool the cybersickness detection model and trigger unexpected mitigation, thus disrupting user immersive experiences (UIX) and even posing safety risks.
2503.13510_2277822_5	Our findings reveal that prompt sentiment significantly influences model responses, with negative prompts often reducing factual accuracy and amplifying bias, while positive prompts tend to increase verbosity and sentiment propagation.
2503.14749_2279061_5	We propose a simple procedure, uncertainty distillation, to teach an LLM to verbalize calibrated semantic confidences.
2503.15678_2279990_7	Existing legal frameworks are evolving to address AI in cybercrime, but quantum threats require new initiatives.
2503.15772_2280084_3	In this work, we employ a straightforward approach to identify LLM-generated reviews - doing an indirect prompt injection via the paper PDF to ask the LLM to embed a watermark.
2503.17040_2281352_5	We advocate for instructors to encourage AI-assisted CPS to foster critical thinking and enhance student engagement with real-world scenarios.
2503.17662_2281974_4	Specifically, we first design a role chain method to encourage the model to self-question based on the role characteristics and dialogue context to adjust personality consistency.
2503.17994_2282306_6	Instead of directly generate architectures with LLM, We inspire the LLM's capability with a multi-level enhancement mechanism.
2503.17994_2282306_7	Specifically, on the step-level, we decompose the generation task into decision steps with powerful prompt engineering and inspire LLM to serve as instructor for architecture search based on its internal knowledge.
2503.18238_2282550_3	Analysis of fine-grained communication, collaboration, and workflow logs revealed that collaborating with AI agents increased communication by 137% and allowed humans to focus 23% more on text and image content generation messaging and 20% less on direct text editing.
2503.18809_2283121_4	For a given planning domain, we ask an LLM to generate several domain-dependent heuristic functions in the form of Python code, evaluate them on a set of training tasks within a greedy best-first search, and choose the strongest one.
2503.19092_2283404_2	This paper synthesizes existing research and presents novel experiment designs that explore how LLM-based rankers and assistants influence LLM-based judges.
2503.20797_2285109_5	Finally, we show how providing the source for political and non-political content influences the LLM's classification.
2503.21115_2285427_6	In addition, we design prompts that instruct LLMs to leverage these tools for assessing the feasibility of hub selection by evaluating various risk types and levels.
2503.21419_2285731_4	In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances.
2503.22736_2287048_4	With this in mind, we propose a model distillation pipeline in which a large generative model, a Teacher, teaches a much smaller model, a Student.
2503.22853_2287165_7	This research contributes to the broader understanding of teaching LLMs and has applications for educators, students, and developers of AI music tools alike.
2503.23084_2287396_3	These features not only distinguish reasoning tasks from memory-intensive ones but can also be manipulated to causally influence model performance on reasoning tasks.
2503.23688_2288000_7	Furthermore, the research highlights specific prompt structures and linguistic variations that can strategically trigger distinct responses from models, revealing methods for effectively navigating and influencing LLM outputs.
2504.01404_2290107_11	In rank-based identification, we ask the LLM to select buggy statements from the bug-fixing commit and rank them based on their relevance to the root cause.
2504.02406_2291109_3	Trustworthy AI is vital, especially for critical infrastructures like 6G. This paper introduces the REASON approach for holistically addressing AI's native integration and trustworthiness in future 6G networks.
2504.03708_2292411_2	Telecommunications operators, historically adept at solving content latency challenges through partnerships with providers like Google and Facebook, now have a unique opportunity to address similar AI latency concerns.
2504.04262_2292965_5	The proposed CatBoost model has used a nature inspired algorithm such as Simulated Annealing to select the most important features, Cuckoo Search to adjust outliers and grid search to fine tune its settings in such a way to achieve improved prediction accuracy.
2504.04372_2293075_10	We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization.
2504.04994_2293697_5	Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making.
2504.05804_2294507_2	Unlike existing methods that often introduce detectable anomalies, StealthRank employs an energy-based optimization framework combined with Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text sequences embedded within product descriptions that subtly yet effectively influence LLM ranking mechanisms.
2504.05857_2294560_5	Our results address human-AI interaction challenges not covered in previous WoZ research, including recording and resubmitting signs, unpredictable outputs, system latency, and privacy concerns.
2504.06017_2294720_4	Based on our results, we argue against LLM-vendor claims about limited security capabilities.
2504.06435_2295138_11	These findings suggest directions for GenAI design to safely and productively address the AI "trust gap."
2504.07135_2295838_4	In this paper, we propose Similarizing the predictive Influence of Nodes with Contrastive Learning (SINCon), a defense mechanism that encourages the model to learn graph representations where nodes with varying importance have a more uniform influence on predictions.
2504.09309_2298012_3	We reframe the multi-label classification task as a structured generation problem, instructing the LLM to directly output the relevant legal categories for a given document.
2504.09647_2298350_2	This paper systematically identifies and categorizes critical attributes influencing AI service orchestration in 6G networks and introduces an open-source, LLM-assisted toolchain that automates service packaging, deployment, and runtime profiling.
2504.11536_2300239_1	To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback.
2504.12523_2301226_5	Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference.
2504.12654_2301357_0	  This perspective paper examines a fundamental paradox in the relationship between professional expertise and artificial intelligence: as domain experts increasingly collaborate with AI systems by externalizing their implicit knowledge, they potentially accelerate the automation of their own expertise.
2504.13052_2301755_3	We demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87% against leading commercial LLMs.
2504.13500_2302203_5	Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs.
2504.13534_2302237_1	To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor.
2504.13700_2302403_2	However, instructing LLMs using natural language is limited in precision and expressiveness for conveying visualization intent, leading to misinterpretation and time-consuming iterations.
2504.13969_2302672_4	This work highlights the potential of combining physical and digital elements in AI literacy, offering a safe and engaging way for children to learn how to effectively collaborate with AI.
2504.14112_2302815_7	Lastly, the AU group expressed higher comfort in seeking personal help, managing stress, obtaining social support, and talking about health with AI, indicating potential for broader emotional support while highlighting the need for safeguards against problematic usage.
2504.14150_2302853_5	Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model.
2504.14150_2302853_10	On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.
2504.14367_2303070_3	By systematically mapping the phenotypic space, we reveal how structural variations influence LLM performance, offering actionable insights for task-specific and adaptable prompt design.
2504.15415_2304118_4	Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution.
2504.16047_2304750_6	The findings highlight that pre-training methodology significantly influences model performance on specific downstream tasks.
2504.17426_2306129_3	Our method consists in applying topic modeling on the descriptions obtained by asking an LLM to summarize the code.
2504.19005_2307708_5	Using the data, the researcher went through prompt engineering to instruct gpt-4.1-mini to automatically analyze the remaining 1,000 images.
2504.19076_2307779_3	This paper examines scenarios where LLM-evaluators may falsely indicate success, particularly when LLM-based judgments influence both system development and evaluation.
2504.19445_2308148_2	In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments.
2504.20951_2309654_3	This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity.
2504.21589_2310292_1	Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records.
2504.21770_2310473_6	In-context learning and asking the model to 'think again' improves LASHED's precision.
2505.00626_2311186_2	Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers.
2505.00626_2311186_3	In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens.
2505.02252_2312812_6	Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area.
2505.02313_2312873_3	Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering.
2505.02865_2313425_3	Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts.
2505.03818_2314378_1	Finding training examples to teach LLMs to solve these tasks can be challenging.   
2505.05115_2315675_1	(2025), I show that within their suite of research-engineering tasks the performance of AI agents on longer-duration tasks can be explained by an extremely simple mathematical model -- a constant rate of failing during each minute a human would take to do the task.
2505.05441_2316001_2	To address this, we introduce GesPrompt, a multimodal XR interface that combines co-speech gestures with speech, allowing end-users to communicate more naturally and accurately with LLM-based copilots in XR environments.
astro-ph/0101048_2323998_2	The range of kHz frequencies and the observed quality factors (Qs) are also explained by this simple dynamical model.
astro-ph/0207575_2334530_5	The spectrum of the accretion disk emission is significantly softer and in the 3--20 keV range is reasonably well described by a relativistic disk model with a mass accretion rate consistent with the value inferred from the observed X-ray flux.
astro-ph/0301443_2338226_3	We find that both spectra are described by a two-component model consisting of emission from a cool accretion disk plus a Comptonized blackbody with kTbb ~ 1.5 keV in a low opacity plasma.
astro-ph/0301443_2338226_5	The spectrum of X1556-605 can also be described by a model consisting of a blackbody plus an unsaturated Comptonization with electron energy kTe ~ 4 keV.
astro-ph/0402179_2346505_0	  We describe kHz QPOs from the hydrodynamical model of accretion disks for LMXB systems.
astro-ph/0608191_2367880_3	The source spectrum is best explained by a power-law model with a photon index of 2.2 (1.9-2.7) and an absorption-corrected luminosity of 1.0 x 10^34 ergs s^-1 in the 0.5--10 keV band for a distance of 50 kpc.
astro-ph/9611086_2379692_4	It is found that the entire variety of young LMC globular clusters may be explained in a model where they form from a fairly uniform population of roughly spherical, relaxed proto-cluster clouds very similar to Giant Molecular Clouds in the Galaxy, with star formation efficiencies between 25% and 60%.
astro-ph/9802342_2384325_1	All the remnants were well described by the model, allowing us to derive accurate values for their ages, densities, initial explosion energies, and metal abundances.
hep-th/0508177_2587329_3	Their low energy world-volume theory, truncated to the 1/2 BPS sector, is shown to be described by a Chern-Simons finite-matrix model.
1101.3316_238729_1	We train a model that separates QSOs from variable stars, non-variable stars and microlensing events using 58 known QSOs, 1,629 variable stars and 4,288 non-variables using the MAssive Compact Halo Object (MACHO) database as a training set.
1110.5632_296938_4	We then trained a one-class SVM (Support Vector Machine) model using the diagnostics features of the confirmed 58 MACHO QSOs.
1404.1521_514574_5	We evaluate the performance of training the model on the GPU and present optimizations that boost the performance on the GPU.One of the key optimizations, we propose increases the performance of a function involved in calculating and updating the gradient by approximately 50 times on the GPU for sufficiently large batch sizes.
1404.1521_514574_9	We conclude by presenting a thorough evaluation of the applicability of GP-GPU's for this task and highlight the factors limiting the performance of training a Polyglot model on the GPU.
1405.3515_524423_1	We train the model on the Google Books Ngram corpus to obtain word vector representations specific to each year, and identify words that have changed significantly from 1900 to 2009.
1412.6650_584628_3	Instead of training a completely new model or relying on mixture approaches, we propose two new methods: continued training on resampled data or insertion of adaptation layers.
1511.01574_674754_5	We find that fairly small amounts of held-out data (on the order of 30-70 thousand words) are sufficient for training the adjustment model.   
1604.00942_720109_9	Additionally, we find that training a model on the sentiment of the review text provides a competitive alternative when no five star rating information is available.
1608.03990_760452_4	When these model forms are reconstructed using neural networks (NN) and embedded within a standard solver, we show that much improved predictions in lift can be obtained for geometries and flow conditions that were not used to train the model.
1608.04077_760539_0	  In this paper, we propose a generative knowledge transfer technique that trains an RNN based language model (student network) using text and output probabilities generated from a previously trained RNN (teacher network).
1610.00494_775841_2	The tuning method that we propose enables dealing with errors without the need to re-train the system.
1701.03578_808986_0	  In this paper, we propose an efficient transfer leaning methods for training a personalized language model using a recurrent neural network with long short-term memory architecture.
1702.00137_814731_4	* How could we teach AI topics at an early undergraduate or a secondary school level?   
1707.07240_872272_7	A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs.
1710.10296_905679_7	We design the DRNN language model with Python and Theano, train the model on a CPU platform, and deploy the model on a PYNQ board to validate the model with Jupyter notebook.
1711.07908_914918_4	Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to "pretrain" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model.
1712.01996_920595_0	  Attention-based sequence-to-sequence models for automatic speech recognition jointly train an acoustic model, language model, and alignment mechanism.
1802.08925_948465_1	We used images from optical coherence tomography angiography (OCTA), a relatively new imaging modality that measures perfusion of the retinal vasculature, to train an AI algorithm to generate vasculature maps from standard structural optical coherence tomography (OCT) images of the same retinae, both exceeding the ability and bypassing the need for expert labeling.
1803.03665_953798_2	We present a simple but highly effective approach for training neural LMs using both lexical and syntactic information, and a novel approach for applying such LMs to unparsed text using sequential Monte Carlo sampling.
1803.03879_954012_2	Previous methods address this deficiency by training a grounding system via learning to reconstruct language information contained in input queries from predicted proposals.
1803.05457_955590_0	  We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering.
1803.11096_961229_0	  Group zero-attracting LMS and its reweighted form have been proposed for addressing system identification problems with structural group sparsity in the parameters to estimate.
1805.05062_978107_3	To overcome these limitations we build upon the recent reward augmented maximum likelihood approach \ie sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric.
1806.00801_986441_2	Although huge efforts have been made in building large-scale datasets very recently, e.g., the Aerial Image Dataset (AID) which contains 10,000 image samples, they are still far from sufficient to fully train a high-capacity deep CNN model.
1808.01423_1010569_4	The language model is used in a bootstrapping fashion to refine predictions in the target language for use as ground truth in training the model.
1809.00125_1020141_2	To achieve that, we train the translation model to predict the residual probability of the training data added to the prediction of the LM.
1809.10036_1030052_1	Regulatory restrictions inhibit sharing of da-ta across different agencies, which could be a significant impediment to training AI models.
1811.02162_1046795_0	  In this paper, we explore several new schemes to train a seq2seq model to integrate a pre-trained LM.
1811.08674_1053307_7	Subgraphs are obtained by training a GNN-based graph refinement model to directly predict edge probabilities.
1812.04647_1062221_3	In this paper, we propose a solution to (a) estimate n-gram counts directly from the hand-written grammar for training LMs and (b) use constrained optimization to optimize the system parameters for future use cases, while not degrading the performance on past usage.
1812.11467_1069041_5	After training the language model, we show that the perplexity metric calculated for runtime data has a strong negative correlation with the correction of the erroneous NGS reads.
1812.11750_1069324_2	This stimulates a nascent field termed as federated learning for training a machine learning model on computation, storage, energy and bandwidth limited mobile devices in a distributed manner.
1904.08796_1113836_10	Open-access data sets and software implementations could alleviate these issues, and encourage further AI applications to pediatric ophthalmology.   
1904.09324_1114364_1	We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation.
1905.10863_1129299_2	Training this model on 9x9 Go produces a superhuman Go player, thus proving that it is stable and robust.
1907.03187_1147723_3	A large Twitter based corpus allowed us to train a language model from scratch focused on Spanish and transfer that knowledge to our competition model.
1907.04446_1148982_8	We present experiments utilizing crowdworkers to help address an important real-world AI safety problem in the domain of education.
1907.08377_1152913_2	The system maintains an append-only decentralized ledger to keep the log of critical information, including who has trained the model and improved its accuracy, when it has been improved, by how much it has improved, and where to find the newly updated model.
1907.09177_1153713_2	To perform such attacks, it is necessary for experts to train a tailored LM for a specific topic.
1909.00556_1170594_1	A common solution is to train a class-based n-gram language model and then expand the classes into specific words or phrases.
1909.02060_1172098_2	To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions.
1909.08053_1178091_7	To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT.
1909.09993_1180031_7	We also investigate the impact of the vocabulary size of A2W models and the data size for training LMs.
1910.03137_1187056_0	  In machine learning Trojan attacks, an adversary trains a corrupted model that obtains good performance on normal data but behaves maliciously on data samples with certain trigger patterns.
1910.03137_1187056_4	To train the meta-model without knowledge of the attack strategy, we introduce a technique called jumbo learning that samples a set of Trojaned models following a general distribution.
1910.03432_1187351_5	We propose to train a recurrent neural network language model using the decentralized FederatedAveraging algorithm and to approximate this federated model server-side with an n-gram model that can be deployed to devices for fast inference.
1910.06294_1190213_4	In this work-in-progress we combined the effectiveness of transfer learning provided by pre-trained masked language models with a semi-supervised approach to train a fast and compact model using labeled and unlabeled examples.
1910.07360_1191279_4	Using a robust deep learning pipeline, a convolutional neural network is trained and implemented to detect rhinos and cars (considered an important tool in poaching for fast access and artefact transportation in natural habitats) in the study, that are found within live video streamed from drones Transfer learning with the Faster RCNN Resnet 101 is performed to train a custom model with 350 images of rhinos and 350 images of cars.
1911.04263_1202857_1	Several AI techniques including supervised learning and deep reinforcement learning (DRL) are adopted and improved to train effective AI agents for achieving the desired performance.
1912.03363_1215259_6	The proposed model results in 8% improvement in word error rate even when the amount of training data is a fraction of data used for training the first-pass system.
1912.08964_1220860_1	The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance.
1912.10308_1222204_2	The main challenge faced when training a language model is to deal with the language model corpus which is usually different to the one used for training the handwritten word recognition system.
2001.05295_1230708_2	This process is often constrained by having a relatively small number of patient records for training the model.
2001.05295_1230708_4	Such patient representation schemes enable a 3.5% mean improvement in AUROC on five prediction tasks compared to standard baselines, with the average improvement rising to 19% when only a small number of patient records are available for training the clinical prediction model.
2001.05838_1231251_6	The annotation scheme uses a K-means clustering algorithm along with merging conditions to achieve initial labelling information for training the U-Net model.
2001.06286_1231699_3	Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks.
2001.06286_1231699_4	While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT.
2001.09244_1234657_3	In this paper, we describe a novel 'proof of useful work' (PoUW) protocol based on training a machine learning model on the blockchain.
2001.10717_1236130_5	In this early work, a comparative evaluation of our MRE data driven simulation and the traditional method shows clinically significant differences in accuracy during landmark placement and motivates further animal model trials.
2002.12804_1250227_4	In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively.
2003.02089_1252444_0	  Federated learning (FL) is a promising technique that enables many edge devices to train a machine learning model collaboratively in wireless networks.
2003.08052_1258407_8	Finally, we trained a deep learning model that can explicitly predict and explain high level fashion concepts in a product image with its low level and domain specific fashion features.
2003.13003_1263358_4	It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.
2004.01388_1266158_5	Twenty-seven patients were randomly selected as independent test samples, and the remaining patients were used in a 5-fold cross validation experiment to confirm the hyperparameters, select optimal handcrafted features and train the model.
2004.01525_1266295_2	This paper proposes a Variational Autoencoder\cite{Kingma2014}(VAE)-based rhythm generation system, in which musicians can train a deep learning model only by selecting target MIDI files, then generate various rhythms with the model.
2004.03497_1268267_2	We train a 1.2B-parameter language model, ProGen, on ~280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component.
2004.04487_1269257_4	We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 $\rightarrow$ 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.
2004.04645_1269415_5	Using this we train a transformer-based neural model to perform extractive summarization conditioned on potential diagnoses.
2004.07159_1271929_3	This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context.
2004.14601_1279371_4	Surprisingly, training a model on either of these artificial languages leads to the same substantial gains when testing on natural language.
2004.14614_1279384_5	First, we train a large-scale language model and query it as textual knowledge.
2005.03848_1283643_2	As an alternative, we propose a new method for BERT distillation, i.e., asking the teacher to generate smoothed word ids, rather than labels, for teaching the student model in knowledge distillation.
2005.04726_1284521_1	Background knowledge provides complementary, real world factual information that can augment the limited labeled data to train a machine learning algorithm.
2005.14165_1293960_5	Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.
2006.02431_1296945_2	As a contribution to that effort, we are aggregating numerous small molecules from a variety of sources, using high-performance computing (HPC) to computer diverse properties of those molecules, using the computed properties to train ML/AI models, and then using the resulting models for screening.
2006.04229_1298743_7	We describe our methodology for collecting the data, preparing the corpus, and pre-training the model.
2006.05509_1300023_2	We evaluated five AI software products for screening and triaging TB using a large dataset that had not been used to train any commercial AI products.
2006.05676_1300190_0	  Masked language modeling (MLM) pre-training models such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens.
2006.10304_1304818_7	An additional 35% relative improvement in WER is achieved on one test set when training a TDNNF system with byte-pair encoding.
2006.11194_1305708_0	  Explainable AI provides insight into the "why" for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect.
2006.14779_1309293_0	  Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations.
2006.15437_1309951_2	One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels.
2007.05609_1317123_6	In this paper, we propose to train a context aware E2E model and allow the beam search to traverse into the context FST during inference.
2007.06162_1317676_0	  It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.
2007.07250_1318764_6	We extend that concept to address system-level autonomy capabilities of AI-enabled cyber-physical systems.
2007.11621_1323135_2	However, training an AI model on sensitive data raises concerns about the privacy of individual participants.
2007.12912_1324426_3	Motivated by these issues, this paper addresses a drone-enabled intelligent vehicular system, which is secure, easy to deploy and reliable in quality.
2007.13054_1324568_2	In the AGIFL, leveraging the flexible on-demand 3D deployment of aerial nodes such as unmanned aerial vehicles (UAVs), all the nodes can collaboratively train an effective learning model by FL.
2007.14686_1326200_8	Three of the databases (n=125, p=2,513) were used for training a machine learning model in recognizing AF events from beat-to-beat interval time series.
2008.03979_1331701_3	In this paper, we trained a Korean-specific model KR-BERT, utilizing a smaller vocabulary and dataset.
2008.07326_1335048_1	However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI.
2008.08113_1335835_5	In addition, we propose to train a parallel Bi-LRNN model based on the decoding lattices from both language models, and examine various ways of implementation.
2009.00694_1342197_1	In this paper, we present a deep learning approach to automatically assign protocols to computer tomography examinations, by pre-training a domain-specific BERT model ($BERT_{rad}$).
2009.04984_1346487_3	As the foundation for model pre-training, we synthesize a new dialogue corpus and build our training set with two unsupervised methods: 1) coherence-oriented context corruption, including utterance ordering, insertion, and replacement, to help the model capture the coherence inside the dialogue contexts; and 2) specificity-oriented automatic rescoring, which encourages the model to measure the quality of the synthesized data for dialogue-adaptive pre-training by considering specificity and informativeness.
2009.05886_1347389_1	When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected.
2009.07185_1348688_3	Significant transfer learning effects can be observed: Training a model on three simple core schemes allows it to accurately complete conclusions of different, and more complex types of arguments, too.
2009.10228_1351731_1	However, support for designing tools and curriculum to teach K-12 AI literacy is still limited.
2009.10228_1351731_2	There is a need for additional interdisciplinary human-computer interaction and education research investigating (1) how general AI literacy is currently implemented in learning experiences and (2) what additional guidelines are required to teach AI literacy in specifically K-12 learning contexts.
2009.11538_1353041_5	However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model.
2009.12787_1354290_1	In FL, a set of edge devices train a model using their local data, while repeatedly exchanging their trained updates with a central server.
2009.14109_1355612_1	How should we train a language model in this scenario?
2010.01413_1357743_4	The data is used to train a regression model and voting was used to show the highest correlation between travels made between cities and new cases of COVID-19.
2010.04887_1361217_3	We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information.
2010.07245_1363575_4	Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training.
2010.11506_1367836_4	(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.
2010.11639_1367969_2	In this paper, we consider the question of whether it is possible to pre-train a bilingual model for two remotely related languages without compromising performance at either language.
2010.12091_1368421_5	We believe that the migration dataset would be useful for training future migratable AI systems.
2010.12813_1369143_4	We train our model on subtrees sampled from WordNet, and test on non-overlapping WordNet subtrees.
2010.15437_1371767_1	While paired data are basically required to train the seq2seq model, the external LM can be trained with only unpaired data.
2011.01403_1374151_0	  State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss.
2011.02323_1375071_5	Here, we compare the efficacy of fine-tuning model parameters of pre-trained models against that of training a language model from scratch.
2011.02323_1375071_6	Moreover, we empirically argue against the strict dependency between the dataset size and model performance, but rather encourage task-specific model and method selection.
2011.07347_1380095_2	Past approach relies on either modifying the original LM architecture, re-training the LM on corpora with attribute labels, or having separately trained `guidance models' to guide text generation in decoding.
2011.07371_1380119_1	We demonstrate preliminary results by training a surrogate machine-learning model on real accelerator data to emulate the Booster environment, and using this surrogate model in turn to train the neural network for its regulation task.
2011.07713_1380461_3	However, the various environmental, diver and sensing uncertainties present underwater makes it challenging to train a robust and reliable diver action recognition system.
2011.10144_1382892_1	In this work, we estimate pollution reduction over the lockdown period by using the measurements from ground air pollution monitoring stations, training a long-term prediction model and comparing its predictions to measured values over the lockdown month.
2011.10737_1383485_2	Recently, substantial research efforts in learning-based methods for optimal control problems have been progressing significantly in addressing unknown system models, particularly when the system has complex interactions with the environment.
2011.11736_1384484_6	We first preprocessed the images to eliminate the batch effects of different devices, and then adopted a weakly supervised method to train the model without having any tags for the infected parts.
2012.12543_1400421_0	  Training a code-switching (CS) language model using only monolingual data is still an ongoing research problem.
2012.14682_1402560_5	We further devise a difficulty-aware objective, encouraging the model to output the class probability that reflects the real difficulty of each instance for a more reliable cascading mechanism.
2012.15832_1403710_2	First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity.
2101.00406_1404148_1	First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships.
2101.00529_1404271_4	In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \oscar \cite{li2020oscar}, and utilize an improved approach \short\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks.
2101.05967_1409709_1	Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more.
2101.09635_1413377_1	However, for a relatively low-resource language such as Thai, the choices of models are limited to training a BERT-based model based on a much smaller dataset or finetuning multi-lingual models, both of which yield suboptimal downstream performance.
2101.12051_1415793_0	  Edge federated learning (FL) is an emerging paradigm that trains a global parametric model from distributed datasets based on wireless communications.
2102.01380_1417869_3	ILMT encourages the E2E model to form a standalone LM inside its existing components, without sacrificing ASR accuracy.
2102.01671_1418160_8	Furthermore, it enables training a machine learning (ML) model to search for \textit{good} sets of projections in the sense of minimizing the decoding error rate.
2102.01671_1418160_9	Training our ML model enables achieving very close to the performance of full-projection decoding with a significantly reduced number of projections.
2102.01671_1418160_10	For instance, our simulation results on a (64,14) RM subcode show almost identical performance for full-projection decoding and pruned-projection decoding with 15 projections picked via training our ML model.
2102.08015_1424504_0	  In the domain of air traffic control (ATC) systems, efforts to train a practical automatic speech recognition (ASR) model always faces the problem of small training samples since the collection and annotation of speech samples are expert- and domain-dependent task.
2102.10407_1426896_5	We train the proposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual Captions training data.
2102.11258_1427747_1	Zero-shot AEG is when we train a system to grade essays written to a new prompt which was not present in our training data.
2102.12162_1428651_5	We first tune the PhoBERT on our dataset by re-training the model on the Masked Language Model task; then, we employ its encoder for text classification.
2103.02410_1432555_2	To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date.
2103.03809_1433954_5	In this paper, we propose to pre-train an assembly language model called PalmTree for generating general-purpose instruction embeddings by conducting self-supervised training on large-scale unlabeled binary corpora.
2103.06518_1436663_2	Agents are connected to a cloud system able to train AI models to improve overall energy efficiency of an AI application executed on an edge platform.
2103.10685_1440830_6	Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering.
2103.10873_1441018_4	Addressing the DNN model design, from training and dataset augmentation to 8-bit quantization and deployment, we demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for the real-time execution (up to 135 frame/s) of our novel DNN, called PULP-Frontnet.
2103.11790_1441935_5	Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the "moral direction" for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.
2103.15004_1445149_7	It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces.
2104.03150_1450568_8	Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model.
2104.05544_1452962_6	We also investigate other methods to suppress the ILM mainly by decreasing the capacity of the AED model, limiting the label context, and also by training the AED model together with a pre-existing LM.
2104.10344_1457762_7	We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and applies a text-entity fusion encoding to aggregate entity representation.
2104.10661_1458079_1	Through training the model upon a mix of the Cornell Movie Dialogue Corpus for language understanding and an open-source, anonymized, and public licensed psychotherapeutic dataset, the model achieved statistically significant performance in published, standardized qualitative benchmarks against human-written validation data - meeting or exceeding human-written responses' performance in 59.7% and 67.1% of the test set for two independent test methods respectively.
2104.11390_1458808_1	However,training large language model needs massive computing resource, as more and more open source pre-training models are available, it is worthy to study how to take full advantage of available model.
2104.11390_1458808_5	When we continue training the target model, the training loss can start from a smaller value.
2105.00395_1462954_2	In AirMixML, multiple workers transmit analog-modulated signals of their private data samples to an edge server who trains an ML model using the received noisy-and superpositioned samples.
2105.05012_1467571_4	We use the collected English-learning data to train a predictive regression model based on students' monthly examination scores.
2105.11321_1473880_2	For each architecture, we trained a model instance using the whole dataset.
2105.12655_1475214_4	In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code.
2105.12833_1475392_3	It is desirable to train a model for launch prediction on a robot, as deep neural networks can account for immeasurable dynamics.
2106.01023_1478787_4	In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs.
2106.07340_1485104_2	In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content.
2107.05383_1499395_1	We asked the world's best language model, GPT-3, fifteen difficult questions about the nature, value, and future of library and information science (LIS), topics that receive perennial attention from LIS scholars.
2107.07691_1501703_5	To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.
2107.08909_1502921_7	In this method, an adversary uses the explanations to train the generative model and reduces the number of queries to steal the model.
2107.09051_1503063_6	Lastly, open issues and opportunities address future AI-empowered finance and finance-motivated AI research.
2107.11275_1505287_1	One way to understand and improve robustness of these models is an exploration of an adversarial attack scenario: check if a small perturbation of an input can fool a model.   
2107.11913_1505925_4	We do so by training a model to classify publications related to ethical issues and concerns.
2107.12168_1506180_7	One is to pre-train a language model based on RNN, and the other is to pre-train a sequence autoencoder.
2108.03555_1512370_4	SRH images were then used to train a convolutional neural network (CNN) model using three representation learning strategies: cross-entropy, self-supervised contrastive learning, and supervised contrastive learning.
2108.05002_1513817_5	We trained the proposed language model using a corpus of approximately 70,000 LaTeX sequences provided in CROHME 2016.
2108.06209_1515024_1	w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens.
2108.08295_1517110_4	Within the purview of these case studies, we show that it is possible to capture the design space and train a model to "generalize" prediction the optimal design and mapping parameters when queried with workload and design constraints.
2108.09105_1517920_8	Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.
2108.11193_1520008_4	Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.
2108.12175_1520990_9	Finally, we show that training an acoustic model for ASR tasks separately (i.e., separate models for ATCOs and pilots) or using a multitask approach is well suited for the noisy data and outperforms the traditional ASR system where all data is pooled together.
2109.02040_1524861_6	When pre-training the LXMERT model, our alternative masking strategies consistently improve over the original masking strategy on three downstream tasks, especially in low resource settings.
2109.03570_1526391_1	Interestingly, in the absence of enough clinical data to train a model from scratch, we applied mixed-domain pretraining and cross-domain transfer approaches to generate a performant bio-clinical model suitable for real-world clinical data.
2109.05522_1528343_3	This work proposes a Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the mentioned constraints without training a complete Transformer model.
2109.06515_1529336_4	Then, we post-train the translation model with different levels of data at each training stages.
2109.06822_1529643_0	  Training a model for grammatical error correction (GEC) requires a set of labeled ungrammatical / grammatical sentence pairs, but manually annotating such pairs can be expensive.
2109.07140_1529961_0	  Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors dominate the landscape of Natural Language Processing due to their ability to scale across multiple tasks rapidly by pre-training a single model, followed by task-specific fine-tuning.
2109.07971_1530792_2	In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome?
2109.08249_1531070_2	We investigate whether we can improve the $k$NN-LM performance by instead training a LM with the knowledge that we will be using a $k$NN post-hoc.
2109.09105_1531926_6	Additionally, pre-training the LM on spoken transcripts restrain its linguistic understanding.
2109.10836_1533657_4	Taking this wide perspective, this year there will be no single theme to lead the symposium and we encourage AI-HRI submissions from across disciplines and research interests.
2109.12346_1535167_5	To address this issue, we collected more than one million Algerian tweets, and pre-trained the first Algerian language model: DziriBERT.
2109.12346_1535167_7	The obtained results show that pre-training a dedicated model on a small dataset (150 MB) can outperform existing models that have been trained on much more data (hundreds of GB).
2109.14686_1537507_6	With experiments we demonstrate that using the image indeed helps beam tracking especially when the user is in serious NLOS, and the solution relies on carefully-designed dataset for training a model.
2109.14686_1537507_7	Generally speaking, including NLOS-like data for training a model does not benefit beam tracking of the user in LOS, but including light NLOS-like data for training a model benefits beam tracking of the user in serious NLOS.
2110.00942_1539085_1	The reason for this surge in research activities in this direction are mainly due to advent of robust AI algorithms (deep learning), availability of hardware that can train those robust and complex AI algorithms and accessibility of large enough dataset required for training AI algorithms.
2110.03215_1541358_6	By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs.
2110.04725_1542868_2	However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers.
2110.05354_1543497_5	To make ILMA effective, it is essential to train the E2E model with an internal LM loss besides the standard E2E loss.
2110.06674_1544817_7	Our initial proposals for these areas include: (1) a standard of avoiding "negligent falsehoods" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction.   
2110.06968_1545111_3	We harnessed the ThetaGPU supercomputer at the Argonne Leadership Computing Facility to train our AI model using a training set of 1.5 million waveforms.
2110.06968_1545111_4	We used 16 NVIDIA DGX A100 nodes, each consisting of 8 NVIDIA A100 Tensor Core GPUs and 2 AMD Rome CPUs, to fully train our model within 3.5 hours.
2110.07814_1545957_1	To tackle this problem in NLP, we propose $\textit{in-context tuning}$, which recasts adaptation and prediction as a simple sequence prediction problem: to form the input sequence, we concatenate the task instruction, the labeled examples, and the target input to predict; to meta-train the model to learn from in-context examples, we fine-tune a pre-trained language model (LM) to predict the target label from the input sequences on a collection of tasks.   
2110.08151_1546294_4	We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks.
2110.08460_1546603_6	2) we pre-train a compressed GPT-2 model using layer truncation and compare it against the distillation-based method (DistilGPT2).
2110.10261_1548404_4	We then train LMs on the 1-best and N-best translations and study ways to improve on such a baseline LM.
2111.00610_1554713_7	Through our experiments, we also highlight some well known, but poorly documented challenges in training generative speech LMs, including the mismatch between the supervised learning objective with which these models are trained such as Mean Squared Error (MSE), and the true objective, which is speech quality.
2111.00826_1554929_0	  In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility.
2111.00826_1554929_4	We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs.
2111.01677_1555780_3	In the pretrain phase, we train the model with three tasks, (1) Video Tag Classification (VTC), (2) Mask Language Modeling (MLM) and (3) Mask Frame
2111.01677_1555780_5	In the finetune phase, we train the model with video similarity based on rank normalized human labels.
2111.09461_1563564_1	However, concerns surrounding security and trustworthiness impede the collection of large-scale representative medical data, posing a considerable challenge for training a well-generalised model in clinical practices.
2111.10267_1564370_7	Finally, we propose a heuristic for selecting the optimal number of retransmissions, which can be calculated before training the ML model.
2111.10962_1565065_2	They often train LMs with KGs in indirect ways, relying on extra entity/relation embeddings to facilitate knowledge injection.
2111.11856_1565959_4	Spatio-temporal split learning is applied to this scenario to preserve privacy and globally train a fire classification model.
2111.14347_1568450_0	  As a promising distributed machine learning paradigm, Federated Learning (FL) trains a central model with decentralized data without compromising user privacy, which has made it widely used by Artificial Intelligence Internet of Things (AIoT) applications.
2112.00567_1570343_1	The standard way to train a deep language model is to employ unsupervised learning from scratch on a large unlabeled corpus.
2112.00590_1570366_2	At ADS, we are applying modern machine learning and natural language processing techniques to our dataset of recent astronomy publications to train astroBERT, a deeply contextual language model based on research at Google.
2112.02870_1572646_1	An emerging paradigm in ML is a federated approach where the learning model is delivered to a group of heterogeneous agents partially, allowing agents to train the model locally with their own data.
2112.06905_1576681_5	It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.
2112.07219_1576995_2	Our work overcomes existing data limitations for training AI models by curating, from YouTube, the largest dataset of open surgical videos to date: 1997 videos from 23 surgical procedures uploaded from 50 countries.
2112.07571_1577347_5	Training this complex model with a previously prohibitively large dataset was made possible for the first time by a partnership with Cerebras Systems, whose CS-1 system powered all pre-training experiments.
2112.07669_1577445_1	We trained AI models using 14 million waveforms, produced with the surrogate model NRHybSur3dq8, that include modes up to $\ell \leq 4$ and $(5,5)$, except for $(4,0)$ and $(4,1)$, that describe binaries with mass-ratios $q\leq8$, individual spins $s^z_{\{1,2\}}\in[-0.8, 0.8]$, and inclination angle $\theta\in[0,\pi]$.Our probabilistic AI surrogates can accurately constrain the mass-ratio, individual spins, effective spin, and inclination angle of numerical relativity waveforms that describe such signal manifold.
2112.11438_1581214_7	In order to overcome the difficulty in using gradient descent methods to directly estimate discrete quantized weights, alternating direction methods of multipliers (ADMM) are used to efficiently train quantized LMs.
2112.11668_1581444_1	Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model.
2112.11668_1581444_4	In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization.
2201.01604_1586976_3	Here, we aim to train a machine learning model to separate these objects from the foreground stars and background galaxies using the multi-wavelength imaging data of the Fornax galaxy cluster in 6 filters, namely u, g, r, i, J and Ks.
2201.05955_1591327_2	Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns.
2201.05955_1591327_5	Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI.
2201.06009_1591381_5	On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3.
2201.06952_1592324_7	To substantiate the proposed framework, we iteratively train a model on biased and unbiased data using multiple datasets and check that the Fairness Score and the proposed process correctly identify the biases and judge the fairness.
2201.09534_1594906_12	This is the first effort to train a DL model on multiple tasks in parallel.
2201.10707_1596079_1	In this paper, we propose a generic and language-independent strategy for multilingual GEC, which can train a GEC system effectively for a new non-English language with only two easy-to-access resources: 1) a pretrained cross-lingual language model (PXLM) and 2) parallel translation data between English and the language.
2201.11990_1597362_3	In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron.
2202.01281_1600105_3	In this paper, we present an experience report of teaching an AI course to business executives in the United Arab Emirates (UAE).
2202.01281_1600105_4	Rather than focusing only on theoretical and technical aspects, we developed a course that teaches AI with a view to enabling students to understand how to incorporate it into existing business processes.
2202.02294_1601118_5	Method: We train a Transformer model from scratch and fine-tune two PTMs to evaluate the generated responses, which are compared to RRGEN, a current app response model.
2202.05983_1604807_4	We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions.
2202.10848_1609672_8	This paper addresses the AI community in this regard and stresses the influence AI systems can have on either increasing or reducing the violence that is inflicted on animals, and especially on farmed animals.
2202.11558_1610382_1	We fine-tune a collection of popular small, base, and large pretrained transformer-based language models, and train one feature-base model on the dataset with the aim of testing ensembles of these models.
2202.13610_1612434_2	We annotate over 1.5 k papers from NLP and ML to train a SciBERT-based model to automatically predict the stance of a paper based on its title and abstract.
2203.01008_1613870_0	  Decentralized learning empowers wireless network devices to collaboratively train a machine learning (ML) model relying solely on device-to-device (D2D) communication.
2203.03204_1616066_1	Considering the scarcity of funding and the little to none availability of specialised professionals to teach AI and robotics in developing countries, we present resources based on free open-source hardware and software, open educational resources, and alternative education programs.
2203.03204_1616066_2	That said, the contribution of this work is the pilot workshop of four lessons that promote diversity and inclusion on teaching AI and Robotics for children to a small gender-balanced sample of 14 children of an average age of 7.64 years old.
2203.04472_1617334_4	BinMLM trains the RNN language model on consecutive opcode traces extracted from the control-flow-graph (CFG) to characterize the candidate developers' programming styles.
2203.05936_1618798_2	The approach relies first on transforming the audio into a sequence of discrete units (or pseudo-text) and then training a language model directly on such pseudo-text.
2203.05936_1618798_7	On the basis of this study, we train a language model on the discrete units of the HuBERT features, reaching new state-of-the-art results in the lexical, syntactic and semantic metrics of the Zero Resource Speech Challenge 2021 (Track 1 - Speech Only).
2203.06112_1618974_0	  Training an AI/ML system on simulated data while using that system to infer on data from real detectors introduces a systematic error which is difficult to estimate and in many analyses is simply not confronted.
2203.06211_1619073_0	  The current standard approach to scaling transformer language models trains each model size from a different random initialization.
2203.08774_1621636_1	Our approach, contextual universal embeddings (CUE), trains LMs on one set of context, such as date and author, and adapts to novel metadata types, such as article title, or previous sentence.
2203.08774_1621636_7	Training the model initially with proxy context retains 67% of the perplexity gain after adapting to real context.
2203.09904_1622766_3	Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes.
2203.10692_1623554_3	We map words that have a common WordNet hypernym to the same class and train large neural LMs by gradually annealing from predicting the class to token prediction during training.
2203.10960_1623822_4	This practice-based research seeks to address these challenges with the use of Transformer construct to train a new model with only normal log entries.
2203.12788_1625650_5	Training LMs on generations from these artificial languages, we compare the sequence-level probability estimates given by LMs to the true probabilities in the target language.
2203.15556_1628418_0	  We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget.
2203.15556_1628418_3	We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data.
2203.15996_1628858_4	Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.
2204.04179_1634317_2	However, it is resource-intensive to train a PLM-based CCF model in an end-to-end (E2E) manner, since optimization involves back-propagating through every content encoding within a given user interaction sequence.
2204.10357_1640495_0	  Machine Teaching (MT) is an interactive process where humans train a machine learning model by playing the role of a teacher.
2204.10464_1640602_6	Our results contribute to the design of interfaces to allow end-users to be involved in judging and addressing AI fairness through a human-in-the-loop approach.
2204.14211_1644349_4	We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.
2205.01772_1646184_6	Furthermore, it shows how it is possible to fool a biometric system through a well-known presentation attack approach in the literature called morphing.
2205.02949_1647361_0	  Federated learning (FL) is a promising solution to enable many AI applications, where sensitive datasets from distributed clients are needed for collaboratively training a global model.
2205.03946_1648358_3	Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives.   
2205.07303_1651715_4	Then, we train the Tibetan monolingual pre-trained language model named TiBERT on the data and vocabulary.
2205.08084_1652496_2	In this paper, we explore the possibility of developing a unified foundation model to support \emph{open-ended domains and tasks} in an industrial recommender system, which may reduce the demand on downstream settings' data and can minimize the carbon footprint by avoiding training a separate model from scratch for every task.
2205.08514_1652926_0	  Federated learning allows distributed users to collaboratively train a model while keeping each user's data private.
2205.09646_1654058_6	For example, power-capping, which limits the maximum power a GPU can consume, can enable a 15\% decrease in energy usage with marginal increase in overall computation time when training a transformer-based language model.
2205.10364_1654776_2	NNReverse trains a representation model to represent the semantics of binary code for DNN layers.
2205.10747_1655159_5	We then instruct a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content.
2205.10981_1655393_2	To address this issue, this study teaches GPT-3 to classify whether a question is related to data science by augmenting a small training set with additional examples generated by GPT-3 itself.
2205.11482_1655894_2	In this paper, we propose the problem of fact tracing: identifying which training examples taught an LM to generate a particular factual assertion.
2205.12206_1656618_3	Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus.
2205.12445_1656857_3	In this paper, we develop a novel unsupervised over-the-air (OTA) algorithm that utilizes noisy received pilot measurements to train a deep generative model to output beamspace MIMO channel realizations.
2205.12674_1657086_3	In this work, we present TRIME, a novel yet simple training approach designed for training LMs with memory augmentation.
2206.00621_1661041_2	To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning.
2206.02043_1662463_1	In this setting, spatially distributed devices belonging to each community collaboratively contribute towards training their community model via wireless links provided by the UAV.
2206.03354_1663774_1	We propose a pipeline that utilizes English-only vision-language models to train a monolingual model for a target language.
2206.03354_1663774_3	We propose a novel approach to knowledge distillation to train the model in other languages using parallel sentences.
2206.04793_1665213_1	AI's extraordinary potential is being held back by challenges such as a lack of medical datasets for training AI models, adversarial attacks, and a lack of trust due to its black box working style.
2206.05182_1665602_0	  Machine Teaching (MT) is an interactive process where a human and a machine interact with the goal of training a machine learning model (ML) for a specified task.
2206.05885_1666305_0	  Federated learning (FL) represents a promising distributed machine learning paradigm that allows smart devices to collaboratively train a shared model via providing local data sets.
2206.06994_1667414_5	Models trained using only RGB images on ProcTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks for navigation, rearrangement, and arm manipulation, including the presently running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges.
2206.07430_1667850_6	To stably train the residual LM, we propose smoothing the estimated internal LM and optimizing it with a combination of cross-entropy and mean-squared-error losses, which consider the statistical behaviors of the internal LM in the target domain data.
2206.07948_1668368_3	In this work, we propose an approach that trains a classification model to complement the capabilities of multiple human experts.
2206.09304_1669724_4	We then train a ML model and achieve >=99% classification accuracy from cellularly-pure samples, and >=87% accuracy from cellularly-mixed samples.
2206.12839_1673259_6	Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines.
2206.14623_1675043_4	In this work, we propose a contextual density ratio approach for both training a contextual aware E2E model and adapting the language model to named entities.
2207.00804_1676702_2	Specifically, CASAS dataset is employed to train a Random Forest (RF) model for activity inference.
2207.01893_1677791_2	From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch.
2207.04901_1680799_4	We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization.
2207.06220_1682118_7	We train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors.
2207.07033_1682931_3	Several projects supported by the DAF-MIT AI Accelerator are developing public challenge problems that address numerous Federal AI research priorities.
2207.07951_1683849_1	This study adopts a recent physics-based uncertainty quantification (UQ) approach to address such model form uncertainty in Reynolds-averaged Naiver- Stokes (RANS) simulations.
2207.08057_1683955_0	  Over-the-air federated learning (AirFL) allows devices to train a learning model in parallel and synchronize their local models using over-the-air computation.
2207.08988_1684886_2	Our goal is to train a large neural network language model (NNLM) on compute-constrained devices while preserving privacy using FL and DP.
2207.09374_1685272_5	By doing so, the user directly sees which characteristics of the input data can change arbitrarily without influencing the AI's decision.
2207.13921_1689819_5	Our proposed method, HelixFold-Single, first pre-trains a large-scale protein language model (PLM) with thousands of millions of primary sequences utilizing the self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the co-evolution information.
2207.14393_1690291_1	LAD is a paradigm for creating diverse and accurate synthetic data which conveys the necessary structural constraints and can be used to train a downstream neural dialog model.
2208.03008_1693720_6	In addition, we propose a separate-joint training approach to train the model, and extensive experiments are conducted to show that the proposed method is superior to its counterparts.
2208.04714_1695426_2	We find that researchers addressing AI rights have often seemed to be unaware of the work of colleagues whose interests overlap with their own.
2208.05643_1696355_1	In Air-FEEL, distributed edge devices use their local data to collaboratively train AI models while preserving data privacy, in which the over-the-air model/gradient aggregation is exploited for enhancing the learning efficiency.
2208.05969_1696681_4	In this paper, we try to address the safe model compression problem from a safety-performance co-optimization perspective.
2208.08198_1698910_10	Moreover, we check to which extent our proposal is in line with the European AI Act proposal and current safety standardization initiatives addressing AI and Autonomous Systems
2208.09982_1700694_4	To this end, GRETEL encourages the model to efficiently extract salient sentences that are topically related to the gold summary, rather than redundant sentences that cover sub-optimal topics.
2208.12816_1703528_7	We follow a procedure that directly trains the pruned model and avoids the computationally complex ranking and fine-tuning steps.
2208.14141_1704853_4	We propose synthesising airways by style transfer using perceptual losses to train our model, Airway Transfer Network (ATN).
2208.14493_1705205_4	To demonstrate the effectiveness of your approach, we create a custom dataset which we use to train a medical NER model for German texts, GPTNERMED, yet our method remains language-independent in principle.
2209.03431_1709146_4	Then, it proceeds with physics-informed adversarial training to teach the model the system-related physics domain foreknowledge through iteratively reducing the unwanted output deviations on the previously-uncovered counterexamples.
2209.06049_1711764_2	With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well.
2209.06049_1711764_4	We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text.
2209.06317_1712032_9	This paper explores these issues, focusing on the opportunities, challenges, and potential impacts of such an approach, and discussing how it might influence AI regulations.
2209.11624_1717339_1	However, when devices in a relatively large area cooperatively train a machine learning model, the attendant straggler issues will significantly reduce the learning performance.
2209.15271_1720986_4	Among them, an open-source dataset is constructed to train a multi-form human detection model that distinguishes a human being's whole body, upper body or part body, and the followed action classification model is adopted to recognize such action as falling, sleeping or on-duty, etc.
2210.01504_1722859_1	Previous work addressing privacy issues for language models has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM.
2210.02498_1723853_5	To train a system to produce markup-and-mask rationales without annotations, we leverage in-context learning.
2210.02969_1724324_2	In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label.
2210.04621_1725976_0	  AI tools can be useful to address model deficits in the design of communication systems.
2210.05487_1726842_2	We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES.
2210.05549_1726904_3	This paper proposes the problem of continually extending an LM by incrementally post-train the LM with a sequence of unlabeled domain corpora to expand its knowledge without forgetting its previous skills.
2210.05883_1727238_3	Motivated by this observation, we propose Attribution-Driven Dropout (AD-DROP), which randomly discards some high-attribution positions to encourage the model to make predictions by relying more on low-attribution positions to reduce overfitting.
2210.06525_1727880_5	We train our model on the 4 Nguni languages of South Africa.
2210.06525_1727880_9	We also train our model as a word-level sequence model, resulting in an unsupervised morphological segmenter that outperforms existing methods by a large margin for all 4 languages.
2210.07109_1728464_4	We train a large language model (LM) to generate the next game turn, conditioning it on different information.
2210.07144_1728499_4	In our work we address this challenge by leveraging Model Reprogramming (MR), which repurposes pretrained models on a source language to adapt to the tasks that are in a different language and have scarce data - where it may be difficult to train a high-performing model from scratch or effectively fine-tune an existing pre-trained model on the specific task.
2210.08984_1730339_7	Therefore, in our journey towards an AI-enabled sustainable future, we need to address AI ethics and governance as a priority.
2210.10332_1731687_2	Addressing such incorrect model behavior via parameter adjustments is very costly.
2210.10626_1731981_3	However, Self-Supervised Learning (SSL) is a promising way to solve this problem by pre-training a DNN model utilizing unlabeled samples followed by a fine-tuned downstream task involving very limited labels.
2210.10626_1731981_4	Hence, this work proposes a hard-negative sample aware self-supervised contrastive learning method to pre-train the model for semantic segmentation.
2210.10659_1732014_4	The article integrates and consolidates the findings from existing literature and advances the AutoAI design into (1) using new and emerging sources of data for teaching and training AI algorithms and (2) enabling AI algorithms to use automated tools for training new and improved algorithms.
2210.12022_1733377_4	Our findings suggest that even though fine-tuning and prompting work well to train large LMs on large train sets, there are more efficient alternatives that can reduce compute or data cost.
2210.12378_1733733_5	With this data, we train a more robust fact-correction model to post-edit the summaries to improve factual consistency.
2210.12530_1733885_3	Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.
2210.13918_1735273_4	In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it.
2210.14845_1736200_2	This result also implies that manual efforts for developing per-voxel annotation of tumors (which took years to create) can be considerably reduced for training AI models in the future.
2210.16663_1738018_3	This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency.
2210.17236_1738591_6	For APICoder, we can directly use off-the-shelf language models, or continually pre-train the base model on a code corpus containing API information.
2210.17469_1738824_0	  Federated Edge Learning (FEEL) is a distributed machine learning technique where each device contributes to training a global inference model by independently performing local computations with their data.
2211.00083_1738989_1	Previous research on financial language models usually employs a generic training scheme to train standard model architectures, without completely leveraging the richness of the financial data.
2211.00286_1739192_0	  End-to-end (E2E) artificial intelligence (AI) pipelines are composed of several stages including data preprocessing, data ingestion, defining and training the model, hyperparameter optimization, deployment, inference, postprocessing, followed by downstream analyses.
2211.00780_1739686_6	This dataset is used to train a multi-modal ML model, Air Quality Network (AQNet) capable of fusing these various types of data sources to output predictions of various pollutants.
2211.01223_1740129_2	Next, we train a transformer-based causal language model using these representations.
2211.03363_1742269_0	  Privacy and bandwidth constraints have led to the use of federated learning (FL) in wireless systems, where training a machine learning (ML) model is accomplished collaboratively without sharing raw data.
2211.05110_1744016_4	This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining.
2211.06648_1745554_4	More specifically, after pre-training one of state-of-the-art vision-based models as our backbone network, we re-train our augmented model, consisting of the vision-based model and the multilayer perceptron (MLP) architecture.
2211.08769_1747675_8	The two decoding losses are added up to train a unified encoding model.
2211.11363_1750269_4	Furthermore, we train a domain-specific language model named AF Adapter based RoBERTa for the Chinese biomedical domain.
2211.13606_1752512_4	Training a single AI model utilizing all these data is not feasible with conventional federated learning (FL).
2212.00616_1756793_1	X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words.
2212.00616_1756793_2	Registering new imaginary words allows us to instruct the LLM to comprehend concepts that are difficult to describe with NL words, thereby making a prompt more descriptive.
2212.01215_1757392_1	However, training AI models centrally with the assistance of SAGIN faces the challenges of highly constrained network topology, inefficient data transmission, and privacy issues.
2212.01779_1757956_4	To solve the problem of scarcity of datasets on minority languages and verify the effectiveness of the MiLMo model, this paper constructs a minority multilingual text classification dataset named MiTC, and trains a word2vec model for each language.
2212.02924_1759101_2	Then we also investigate the feasibility of steering the output of this extended soft prompted T5 model at decoder level and finally analyse the utility of generated text to be used in AI related tasks such as training AI models with an interpretability analysis of the classifier trained with synthetic text, as there is a lack of proper analysis of methodologies in generating properly labelled data to be utilized in AI tasks.
2212.04940_1761117_4	More importantly, our method can reconstruct a class of similar states simultaneously, in comparison with the existing neural network methods that need to train a model for each unknown state.
2212.05956_1762133_1	However, such methods impose the additional burden of training a separate teacher model for every new dataset.
2212.07542_1763719_4	The primary concern of this paper is the creation of an interface for students to learn the principles of artificial intelligence by using a natural language pipeline to train a customized model to answer questions based on their own school curriculums.
2212.07588_1763765_9	To train the model, we devise the techniques for preparing training data as well as data augmentation.
2212.07617_1763794_1	In this paper, we propose a novel concept-based curriculum masking (CCM) method to efficiently pre-train a language model.
2212.08073_1764250_1	We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs.
2212.08073_1764250_5	In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences.
2212.08073_1764250_7	As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them.
2212.09251_1765428_3	We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering.
2212.09282_1765459_4	We use two self-supervised loss functions: a modified masked language modeling loss where only specific parts-of-speech words, that would likely require more reasoning than basic language understanding, are masked, and a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences.
2212.09849_1766026_7	Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.
2212.10561_1766738_8	Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers.
2212.11123_1767300_8	In THMA, we train AI models directly from massive HD map datasets via supervised, self-supervised, and weakly supervised learning to achieve high accuracy and efficiency required by downstream users.
2212.11311_1767488_3	Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production.
2301.02111_1773175_1	Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work.
2301.03052_1774116_6	We start by introducing some highlighted robustness challenges in the AI lifecycle and motivating AI maintenance by making analogies to car maintenance.
2301.03728_1774792_5	Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models.
2301.05804_1776868_4	Next, we use a custom salience loss function, Salience-Sensitive Focal Loss, to train a Deformable DETR object detection model in order to emphasize stronger performance on salient signs.
2301.06251_1777315_9	Building upon the soft-subRPA algorithm, we then provide a framework for training a machine learning (ML) model to search for \textit{good} sets of projections that minimize the decoding error rate.
2301.06251_1777315_10	Training our ML model enables achieving very close to the performance of full-projection decoding with a significantly smaller number of projections.
2301.06859_1777923_3	In particular, we focus on methods for collecting reliable human feedback on summaries to train a reward model which in turn improves the summarization model.
2301.08745_1779809_5	Further, we explore an interesting strategy named $\mathbf{pivot~prompting}$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably.
2301.08986_1780050_0	  Domain-adaptive pre-training (or DA-training for short), also known as post-training, aims to train a pre-trained general-purpose language model (LM) using an unlabeled corpus of a particular domain to adapt the LM so that end-tasks in the domain can give improved performances.
2301.09626_1780690_6	Instead of training a model from scratch, we exploit a smaller model that is in the target language but requires much fewer resources.
2301.12004_1783068_6	The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured.
2301.12031_1783095_7	We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks.
2301.12243_1783307_5	Our findings revealed that practitioners use the guidebook not only for addressing AI's design challenges, but also for education, cross-functional communication, and for developing internal resources.
2302.00093_1785027_5	We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.
2302.00444_1785378_2	The problem is to make full use of them to train the student model.
2302.01526_1786460_1	One of the expected roles of XAI methods is verifying whether inferences of a trained machine learning model are valid for an application, and it is an important factor that what datasets are used for training the model as well as the model architecture.
2302.03241_1788175_2	Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain.
2302.03930_1788864_5	The pre-processed result was used as input features in training a Bi-LSTM model in making future forecasts of the values of the particulate matter Pm2.5, and Pm10.
2302.05206_1790140_3	In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner.
2302.05406_1790340_10	This combination allows us to train a single model to perform joint inference with multiple knowledge graphs.
2302.08917_1793851_1	In this work, we propose to train a single multilingual language model (LM) for shallow fusion in multiple languages.
2302.12509_1797443_1	Under such a setting, multiple clients collaboratively train a global generic model under the coordination of an edge server.
2302.13681_1798615_4	We argue why the use of copyleft code to train LLMs is a legal and ethical dilemma.
2302.13817_1798751_5	We also ask ChatGPT to provide its point of view and present its responses to several questions we attempt to answer.
2302.13939_1798873_5	We train the proposed model on two model variants: 45M and 216M parameters.
2302.14389_1799323_2	To address this question, we trained a lexical language model, Glove, and a supra-lexical language model, GPT-2, on a text corpus from which we selectively removed either syntactic or semantic information.
2303.01903_1801700_5	Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge.
2303.03012_1802809_2	However, training a well-performing LLM demands a substantial workforce for data collection and annotation.
2303.03012_1802809_9	Our results show promising outcomes, demonstrating that with a reasonable number of queries, attackers can train a medium-sized backbone model to replicate specialized code behaviors similar to the target LLMs.
2303.03926_1803723_1	Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts.
2303.03956_1803753_0	  In this paper, we present a pilot study aiming to investigate the challenges of teaching AI and Robotics to children in low- and middle-income countries.
2303.03956_1803753_1	Challenges such as the little to none experts and the limited resources in a Mexican town to teach AI and Robotics were addressed with the creation of inclusive learning activities with Montessori method and open-source educational robots.
2303.04910_1804707_1	Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time, and using that model to search through the space of possible proofs.
2303.06430_1806227_3	We encourage the generative AI and HCI research communities to focus on the more complex and interdependent tasks, which require greater levels of human involvement.
2303.09461_1809258_0	  We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''.
2303.10845_1810642_1	In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}.
2303.10845_1810642_2	With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS).
2303.12135_1811932_3	Specifically, we first develop the Legal-BERT-HSLN model that considers the comprehensive context information in both intra- and inter-sentence levels to predict rhetorical roles (subtask A) and then train a Legal-LUKE model, which is legal-contextualized and entity-aware, to recognize legal entities (subtask B).
2303.12231_1812028_0	  We train a model atom to recognize hand-written digits between 0 and 9, employing intense light--matter interaction as a computational resource.
2303.12984_1812781_2	LMCodec trains a Transformer language model to predict the fine tokens from the coarse ones in a generative fashion, allowing for the transmission of fewer codes.
2303.14177_1813974_3	Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference.
2303.14956_1814753_3	In this work, we propose a simple and efficient approach to instruct large language model (LLM) to extract a variety of structures from texts.
2304.00385_1818431_9	For earlier patches that passed all the tests, we further ask the LLM to generate alternative variations of the original plausible patches.
2304.03208_1821254_2	We train Cerebras-GPT models on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules for efficient pre-training (highest accuracy for a given compute budget).
2304.03245_1821291_2	We show through a rigorous human evaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English).
2304.03262_1821308_5	Hence, it is plausible that ChatGPT has already been trained on these tasks with CoT and thus memorized the instruction so it implicitly follows such an instruction when applied to the same queries, even without CoT. Our analysis reflects a potential risk of overfitting/bias toward instructions introduced in IFT, which becomes more common in training LLMs.
2304.03262_1821308_6	In addition, it indicates possible leakage of the pretraining recipe, e.g., one can verify whether a dataset and instruction were used in training ChatGPT.
2304.03271_1821317_2	For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.
2304.03893_1821939_3	The prompts encourage ChatGPT to output a sequence of predefined robot actions, represent the operating environment in a formalized style, and infer the updated state of the operating environment.
2304.05128_1823174_2	In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations.
2304.05128_1823174_3	In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language.
2304.05510_1823556_5	We present our conversational AI prototype, available at www.chatclimate.ai and demonstrate its ability to answer challenging questions accurately in three different QA scenarios: asking from 1) GPT-4, 2) chatClimate, and 3) hybrid chatClimate.
2304.05511_1823557_4	However, sparsity introduces new challenges in training the sparse model to the same quality as the dense counterparts.
2304.05511_1823557_8	We show that we can successfully train GPT 13B to the same quality as the dense GPT 13B model, while achieving an end-end speedup of 4.5x over dense A100 baseline.
2304.06794_1824840_3	In our study, we asked GPT to identify the ten most significant subdisciplines within the field of environmental science.
2304.07061_1825107_2	It works by translating the app GUI state information and the available actions on the smartphone screen to natural language prompts and asking the LLM to make a choice of actions.
2304.08442_1826488_2	To this end, we present The MiniPile Challenge, where one pre-trains a language model on a diverse text corpus containing at most 1M documents.
2304.08442_1826488_5	To verify MiniPile's suitability for language model pre-training, we use it to pre-train a BERT and T5 model, yielding a performance drop of only $1.9\%$/$2.5\%$ on the GLUE and SNI benchmarks compared to the original pre-trained checkpoints trained on $2.6$x/$745$x the amount of data.
2304.09248_1827294_6	The optimal hyperparameters for training the model are determined using genetic algorithms.
2304.09286_1827332_6	This paper introduces a scalable learning pipeline to train AI-based agent models toward automated endovascular predictive device controls.
2304.09542_1827588_4	Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks.
2304.09655_1827701_5	Specifically, we ask ChatGPT to generate a number of program and evaluate the security of the resulting source code.
2304.09667_1827713_2	In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions.
2304.11111_1829157_6	Anxiety-induction not only influences LLMs' scores on an anxiety questionnaire but also influences their behavior in a previously-established benchmark measuring biases such as racism and ageism.
2304.11123_1829169_2	Given AI's massive potential, as well as the fierce geopolitical tensions between China and the U.S., several recent policies have been put in place to discourage AI scientists from migrating to, or collaborating with, the other nation.
2304.11872_1829918_2	To overcome these limitations, we introduce a novel method, namely GenCo, which leverages the strong generative power of LLMs to assist in training a smaller and more adaptable language model.
2304.12198_1830244_4	Our paper also explores future research directions, emphasizing the importance of addressing AI challenges in education, enhancing accessibility and inclusion for diverse student populations, and developing AI-resistant exam questions to maintain examination integrity.
2304.12898_1830944_1	ChatGPT consistent avoidance of passing the test is here overcome by asking ChatGPT to apply the Turing test to itself.
2304.13013_1831059_5	As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.
2305.00875_1833931_9	Through concept analysis, we explore the traceability and distribution of human-recognizable concepts within latent code representations which could be used to influence model predictions.
2305.01937_1834993_4	We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.
2305.03148_1836204_1	However, training AI on resource-limited devices poses significant challenges due to the demanding computing workload and the substantial memory consumption and data access required by deep neural networks (DNNs).
2305.03212_1836268_4	We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss.
2305.03653_1836709_4	We find that CoT prompts are especially useful for query expansion as these prompts instruct the model to break queries down step-by-step and can provide a large number of terms related to the original query.
2305.03701_1836757_0	  Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4, is resource-intensive.
2305.04790_1837846_5	To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly.
2305.04812_1837868_3	However, the extent to which external information influences LLMs' cognition and behaviors remains unclear.
2305.04812_1837868_4	This study investigates how external statements and opinions influence LLMs' thoughts and behaviors from a social cognitive perspective.
2305.05973_1839029_2	To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system.
2305.07429_1840485_4	The key idea is to train a deep learning model on a medical image dataset to extract four types of information: the type of image scan, the body part, the test image, and the results.
2305.07912_1840968_5	We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models.
2305.08391_1841447_2	To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input.
2305.09067_1842123_3	Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data.
2305.09434_1842490_4	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2305.10142_1843198_2	We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively.
2305.10429_1843485_1	In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks.
2305.10429_1843485_2	We then resample a dataset with these domain weights and train a larger, full-sized model.
2305.10429_1843485_3	In our experiments, we use DoReMi on a 280M-parameter proxy model to set the domain weights for training an 8B-parameter model (30x larger) more efficiently.
2305.10649_1843705_1	The core idea of ZeroPrompt is to append zeroed content to each chunk during inference, which acts like a prompt to encourage the model to predict future tokens even before they were spoken.
2305.11169_1844225_1	Specifically, we train a Transformer model on a synthetic corpus of programs written in a domain-specific language for navigating 2D grid world environments.
2305.11169_1844225_6	In summary, this paper does not propose any new techniques for training LMs of code, but develops an experimental framework for and provides insights into the acquisition and representation of formal semantics in statistical models of code.
2305.11189_1844245_5	It considers cloud operations and security in a holistic framework to collect the metrics required to assess the security threats and train the AI models to take immediate actions.
2305.12865_1845921_7	Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set.
2305.13172_1846228_0	  Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive.
2305.13252_1846308_4	Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
2305.13661_1846717_5	Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.
2305.13724_1846780_4	Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features.
2305.13733_1846789_6	Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.
2305.13917_1846973_6	We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort.
2305.14688_1847744_2	We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background.
2305.14705_1847761_1	Instruction tuning is a technique for training LLMs to follow instructions.
2305.14802_1847858_3	To perform ICL accuracy estimation, we propose a method that trains a meta-model using LLM confidence scores as features.
2305.14930_1847986_2	We ask LLMs to assume different personas before solving vision and language tasks.
2305.15075_1848131_4	To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion.
2305.15076_1848132_6	We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step.
2305.15541_1848597_4	This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model.   
2305.16765_1849821_4	We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer.
2305.16986_1850042_1	Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent.
2305.17116_1850172_2	Training LLMs on focused corpora poses computational challenges.
2305.17333_1850389_3	For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget.
2305.17718_1850774_8	Subsequently, this data is utilized to train a captioning generation BLIP-based model.
2305.18098_1851154_5	Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages.
2305.18098_1851154_6	Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model.
2305.19118_1852174_1	Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively.
2305.19339_1852395_3	We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen.
2305.19352_1852408_2	We train the LLM-BRAIn on 8,5k instruction-following demonstrations, generated in the style of self-instruct using text-davinchi-003.
2305.19512_1852568_2	In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers.
2306.00622_1853770_4	Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers.
2306.00745_1853893_5	We further implement a two-step table annotation pipeline which first determines the class of the entities described in the table and depending on this class asks ChatGPT to annotate columns using only the relevant subset of the overall vocabulary.
2306.01272_1854420_3	Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection.
2306.01311_1854459_4	Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder.
2306.01684_1854832_3	However, generating private synthetic data is much harder than training a private model.
2306.01771_1854919_4	ProcessGPT can be designed by training a generative pre-trained transformer model on a large dataset of business process data.
2306.02029_1855177_4	Each UAV agent trains a local QMIX model in its simulated environment and continuously consolidates it through federated learning with other agents, accelerating the learning process.
2306.02210_1855358_2	These generated data are used to train a downstream model on the server, which is then fine-tuned with private client data under the standard FL framework.
2306.02857_1856005_2	Methods: Topological data analysis (TDA) is applied to characterize BPV from the intrinsically nonstationary airflow signal, where the extracted features are used to train an automatic sleep stage scoring model using the XGBoost learner.
2306.02907_1856055_5	After that, \autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code.
2306.02920_1856068_2	Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives.
2306.03586_1856734_4	For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks.
2306.04803_1857951_1	We propose and investigate a simple approach of treating each row in a table as a sentence and training a language model with differential privacy.
2306.05064_1858212_6	Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model.
2306.06503_1859651_9	Therefore, we ardently advocate for the adoption of DP in training diagnostic medical AI models, given its minimal impact on performance.
2306.06615_1859763_3	Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges.
2306.10765_1863913_2	However, training cross-domain LLMs in the medical field poses significant challenges primarily attributed to the requirement of collecting data from diverse domains.
2306.10850_1863998_4	The methodology is tested using artificial and realistic Ozone concentration profiles to train a Gated Recurrent Unit (GRU) model.
2306.10900_1864048_3	Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer.
2306.11296_1864444_5	This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%.
2306.12213_1865361_0	  With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks.
2306.13421_1866569_2	In this work, we propose the Retrieval-Pretrained Transformer (RPT), an architecture and training procedure for jointly training a retrieval-augmented LM from scratch and apply it to the task of modeling long texts.
2306.13649_1866797_0	  Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model.
2306.13723_1866871_2	The interaction between users and AI results in a potentially endless feedback loop, wherein users' choices generate data to train AI models, which, in turn, shape subsequent user preferences.
2306.14824_1867972_2	Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model.
2306.14838_1867986_0	  We train a generative language model on the randomized local measurement data collected from Schr\"odinger's cat quantum state.
2306.15728_1868876_2	We first trained these spatiotemporal-graph AI models using synthetic noise, using 1.2 million modeled waveforms to densely sample this signal manifold, within 1.7 hours using 256 A100 GPUs in the Polaris supercomputer at the ALCF.
2306.15903_1869051_0	  Training AI with strong and rich strategies in multi-agent environments remains an important research topic in Deep Reinforcement Learning (DRL).
2306.15903_1869051_1	The AI's strength is closely related to its diversity of strategies, and this relationship can guide us to train AI with both strong and rich strategies.
2306.15912_1869060_5	We trained the CLAPE-DB model on the protein-DNA binding sites dataset and evaluated the model performance and generalization ability through various experiments.
2306.16092_1869240_2	By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model.
2306.17649_1870797_5	Surprisingly, we find that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance.
2307.00108_1871104_2	To handle these issues, we introduce Ticket- BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets.
2307.00461_1871457_2	This work aims to adapt these architectures in a causal setup for training LLMs.
2307.01139_1872135_3	To test our methodology, we use a human-generated scientific instruction tuning dataset and train a large multimodal model LLaMA-SciTune that connects a vision encoder and LLM for science-focused visual and language understanding.
2307.02157_1873153_5	We initially employ a Supervised Fine-Tuning (SFT) strategy to instruct the LLM-based generator in crafting suitable Job Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.
2307.02157_1873153_6	Moreover, we propose to train a model which can evaluate the matching degree between CVs and JDs as a reward model, and we use Proximal Policy Optimization (PPO)-based Reinforcement Learning (RL) method to further fine-tine the generator.
2307.02192_1873188_8	We make the source code available for the 112, 000 programs, accompanied by a separate file containing the vulnerabilities detected in each program, making the dataset ideal for training LLMs and machine learning algorithms.
2307.02499_1873495_5	Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy.
2307.02736_1873731_4	Approach: To address this need, we propose a parametric map refinement approach for learning-based $T_1\rho$ mapping and train the model in a probabilistic way to model the uncertainty.
2307.03838_1874833_3	While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning.
2307.03917_1874912_5	In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone.
2307.04114_1875109_7	For better transferability, we let the metric module adapt to different few-shot tasks and adopt MAML to train the model via bi-level optimization.
2307.04408_1875403_4	To address this issue, we propose a novel framework using examples in comparison to teach LLMs to learn translation.
2307.04599_1875594_8	Results:The study's findings show that language workbenches are of paramount importance in dealing with all aspects of modeling language development and are leveraged to define DSL explicitly addressing AI concerns.
2307.04827_1875822_3	We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model.
2307.05494_1876489_2	This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact.
2307.05494_1876489_3	Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions.
2307.06616_1877611_6	To achieve the best performance, we trained our model using two datasets, namely the FormAI dataset and the FalconVulnDB.
2307.06834_1877829_3	The proposed approach, coined as Radar-aided Dynamic blockage Recognition (RaDaR), leverages radar measurements and federated learning (FL) to train a dual-output neural network (NN) model capable of simultaneously predicting blockage status and time.
2307.07164_1878159_3	Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever.
2307.07171_1878166_8	Different from previous works like denoised smoothing, which requires training a separate model to robustify LLM, our method enjoys far better efficiency and flexibility.
2307.08487_1879482_5	Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions.
2307.08674_1879669_6	By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions.
2307.09964_1880959_1	However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand.
2307.10198_1881193_7	Our analysis shows that by 2018, the time lag between China and the USA in addressing AI research topics had evaporated.
2307.10315_1881310_1	This paper argues that training AI systems with absolute constraints -- which forbid certain acts irrespective of the amount of value they might produce -- may make considerable progress on many AI safety problems in principle.
2307.10490_1881485_2	When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction.
2307.10700_1881695_0	  Large language models (LLMs) are dramatically influencing AI research, spurring discussions on what has changed so far and how to shape the field's future.
2307.12981_1883976_5	To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images.
2307.12981_1883976_6	Then, we use 2D VLMs as our backbones to train our 3D-LLMs.
2307.14430_1885425_9	We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.
2307.16118_1887113_4	We initially train a single-task RL expert model, sample expert data in the environment, and subsequently utilize a mixed multi-task dataset for offline GPT training.
2307.16368_1887363_7	It first recognizes the actions already performed in the observed videos and then asks an LLM to predict the future actions via conditioned generation, or to infer the goal and plan the whole procedure by chain-of-thought prompting.
2307.16372_1887367_6	In addition, we trained a transformer-based music captioning model with the dataset and evaluated it under zero-shot and transfer-learning settings.
2308.00624_1888516_6	We have gathered a substantial amount of Chinese corpus to train the model and have also optimized its structure.
2308.01154_1889046_3	We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing.
2308.01240_1889132_0	  In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks.
2308.01414_1889306_3	Meanwhile, there has not been any dataset of renewable energy for training LLMs.
2308.01430_1889322_2	To train FinVis-GPT, a financial task oriented dataset was generated for pre-training alignment and instruction tuning, comprising various types of financial charts and their corresponding descriptions.
2308.01497_1889389_4	Given the enormous and non curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high quality metaphors that are unlikely to have been included in the training data.
2308.01727_1889619_4	They trained different model architectures, including LLaMA, BERT and LongFormer and evaluated their performance.
2308.03314_1891206_7	To enhance accuracy, GPTScan further instructs GPT to intelligently recognize key variables and statements, which are then validated by static confirmation.
2308.03740_1891632_3	In addition, these results suggest that nation-states -- even those conducting many large-scale influence operations per year -- are unlikely to benefit economically from training custom LLMs specifically for use in influence operations.
2308.04386_1892278_5	Experimental results on the SummEval benchmark demonstrate that CSEM can effectively train an evaluation model without human-labeled data.
2308.04430_1892322_3	SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference.
2308.04515_1892407_1	One way to increase generalization to new scenes is to automatically label target data, which can then be used for training a detector model.
2308.04711_1892603_3	The first method ($\textit{RR}$) involves training a Rationale Ranking model to score both generated rationales and retrieved contexts with respect to relevance and truthfulness.
2308.04711_1892603_5	For the second method ($\textit{RATD}$) we utilise retrieval-augmented training datasets developed by Hartill et al. 2023 to train a smaller Reasoning model such that it becomes proficient at utilising relevant information from longer text sequences that may be only partially evidential and frequently contain many irrelevant sentences.
2308.05061_1892953_4	Also, we introduce a novel approach to train a language-model-like architecture, or directly fine-tune existing language models, for in-context operator learning.
2308.06212_1894104_9	LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow.
2308.06966_1894858_6	We developed EcomGPT with different parameter scales by training the backbone model BLOOMZ with the EcomInstruct.
2308.07758_1895650_3	Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided.
2308.08239_1896131_3	The instructions are reconstructed from a collection of public datasets to teach the LLMs to memorize and retrieve past dialogues with structured memos, leading to enhanced consistency when participating in future conversations.
2308.08241_1896133_0	  This work summarizes two ways to accomplish Time-Series (TS) tasks in today's Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data.
2308.08493_1896385_3	To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it.
2308.08625_1896517_3	This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion.
2308.09308_1897200_1	However, existing methods that separately or asynchronously train the retriever and downstream model mainly due to the non-differentiability between the two parts, usually lead to degraded performance compared to end-to-end joint training.
2308.09454_1897346_2	To accomplish this, we train a high-capacity transformer model on a vast collection of highly-structured Irish folk melodies and analyze the musical qualities of the samples generated using distribution truncation sampling techniques.
2308.09954_1897846_7	Under our framework, we first ask the LLM to perform knowledge editing using raw documents, which provides a more convenient and universal approach compared to using factual triplets.
2308.10252_1898144_2	To address this, we present "LMTuner", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input.
2308.10335_1898227_7	Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help.
2308.10792_1898684_1	Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions.
2308.10873_1898765_3	In this paper, we demonstrate a framework that leverages the average spiking rate of neurons at equilibrium to train a neuromorphic spiking LM using implicit differentiation technique, thereby overcoming the non-differentiability problem of spiking neural network (SNN) based algorithms without using any type of surrogate gradient.
2308.11042_1898934_8	To evaluate our proposed technique, we trained the HS-BERT model using sentences from RISC-V, OpenRISC, MIPS, OpenSPARC, and OpenTitan SoC documentation.
2308.11534_1899426_5	Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.
2308.12060_1899952_4	This synthetic dataset facilitates training a specialized lightweight model for the KB.
2308.13911_1901803_0	  With the rise of foundation models, a new artificial intelligence paradigm has emerged, by simply using general purpose foundation models with prompting to solve problems instead of training a separate machine learning model for each problem.
2308.14328_1902220_1	The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution.
2308.14731_1902623_6	In this paper, we present an alternative: we train an open source model using sample output generated by GPT-3.5 in a process related to knowledge distillation.
2309.00237_1905041_2	We then use these synthetic notes to train our specialized clinical large language model, Asclepius.
2309.00240_1905044_6	Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately.
2309.00267_1905071_1	RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM.
2309.01576_1906380_3	Firstly, we trained a prosody prediction model using 15 different PLMs.
2309.01940_1906744_5	The code correction task asks LLMs to fix real-world erroneous code segments with different error messages.
2309.02033_1906837_1	A data recipe is a mixture of data from different sources for training LLMs, which plays a vital role in LLMs' performance.
2309.03118_1907922_4	In this paper, we propose a paradigm, termed Knowledge Solver (KSL), to teach LLMs to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability.
2309.03852_1908656_3	However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored.
2309.03876_1908680_7	To train the underlying model, we identified 11 different biases (political, geographic, gender, age) and derived an instruction-tuning corpus in which each answer was written by members of one of these demographics.
2309.03876_1908680_8	This paper presents OpinionGPT, illustrates how we trained the bias-aware model and showcases the web application (available at https://opiniongpt.informatik.hu-berlin.de).
2309.04550_1909354_3	Our method entails tasking an LLM to infer whether a patient has, or is at risk of, a particular condition on the basis of associated notes; if so, we ask the model to summarize the supporting evidence.
2309.05660_1910464_5	To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset.
2309.05689_1910493_2	Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement.
2309.05950_1910754_4	Specifically, we adopt an automatic hill-climbing procedure that converges to an effective prompt by evaluating the performance of current prompts and asking LLMs to refine them based on textual feedback, all within a conversational process without human-in-the-loop.
2309.06384_1911188_4	First, we build a dataset to train a critic model capable of evaluating the citation, correctness, and fluency of responses generated by LLMs in QA systems.
2309.07062_1911866_3	Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself.
2309.07623_1912427_5	We specifically employ a minimal dataset to instruct LLMs to recognize the intended output modality as directed by the instructions.
2309.08902_1913706_5	We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attribute to complete an evaluative statement about a person described as a member of a specific social group.
2309.09380_1914184_4	We first train a teacher model with hard labels to determine each sample's degree of relying on shortcuts.
2309.09380_1914184_6	This new ground truth label is used to train a more robust student model.
2309.09400_1914204_6	Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages.
2309.09582_1914386_3	Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model.
2309.10524_1915328_3	Specifically, we instruct an LLM to correct grammatical errors in an ASR hypothesis and use the LLM-derived representations to refine the output further.
2309.11696_1916500_4	While one can fully train an LLM for this objective, the resource consumption is unaffordable.
2309.11696_1916500_6	We contend that a mere memory module is inadequate and fully training an LLM can be excessively costly.
2309.12307_1917111_1	Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.
2309.12321_1917125_3	This paper makes a case that effective legal systems are the best way to address AI safety.
2309.12767_1917571_7	1) Furthest reasoning operates by masking previous reasoning path and generated queries for LLM, encouraging LLM generating chain of thought from scratch in each iteration.
2309.13550_1918354_7	To train our I-AI model, we utilize an eye gaze dataset to extract anatomical gaze information and generate ground truth heatmaps.
2309.13638_1918442_3	This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input.
2309.13734_1918538_1	Current stance detection methods rely predominantly on manual annotation of sentences, followed by training a supervised machine learning model.
2309.14459_1919263_4	We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM's output in meeting the goal.
2309.14482_1919286_3	However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection.
2309.14530_1919334_4	By categorizing and elucidating these genres, the study aims to facilitate the development of empirical qualitative and quantitative research, fostering evidence-based approaches to address AI-related risks in healthcare effectively.
2309.15223_1920027_2	Here we present a method based on low-rank decomposition to train a rescoring BERT model and adapt it to new domains using only a fraction (0.08%) of the pretrained parameters.
2309.16082_1920886_3	Re-training the underlying model every time individuals would like to practice their rights to be forgotten is computationally expensive.
2309.16167_1920971_1	However, few studies have addressed the LLM threat and vulnerability from an ideology perspective, especially when they are increasingly being deployed in sensitive domains, e.g., elections and education.
2309.16697_1921501_1	Students can ask ChatGPT to complete a programming task, generating a solution from other people's work without proper acknowledgment of the source(s).
2309.17147_1921951_5	Therefore, given that some high quality annotations are necessary in order to asses whether an LLM introduces bias, we argue that it is probably preferable to train a bespoke model on these annotations than it is to use an LLM for annotation.
2310.00035_1922292_1	One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations.
2310.00052_1922309_1	We trained these AI classifiers with 2.4 million IMRPhenomXPHM waveforms that describe quasi-circular, spinning, non-precessing binary black hole mergers with component masses $m_{\{1,2\}}\in[3M_\odot, 50 M_\odot]$, and individual spins $s^z_{\{1,2\}}\in[-0.9, 0.9]$; and which include the $(\ell, |m|) = \{(2, 2), (2, 1), (3, 3), (3, 2), (4, 4)\}$ modes, and mode mixing effects in the $\ell = 3, |m| = 2$ harmonics.
2310.00052_1922309_2	We trained these AI classifiers within 22 hours using distributed training over 96 NVIDIA V100 GPUs in the Summit supercomputer.
2310.00525_1922782_5	Through a feedback mechanism, the user interacts with the algorithm, correcting the algorithm output to their preferences.
2310.00603_1922860_2	In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation.
2310.00646_1922903_1	In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs.
2310.00836_1923093_7	Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5.
2310.01558_1923815_9	We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.
2310.02407_1924664_9	To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect).
2310.02439_1924696_3	We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question.
2310.02527_1924784_2	In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs.
2310.03030_1925287_3	A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules.
2310.03051_1925308_5	Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions.
2310.03214_1925471_9	Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers.
2310.03266_1925523_4	Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately.
2310.03328_1925585_2	A pressing challenge is that it's not plausible to continue training LLMs of such scale on in-domain data.   
2310.04407_1926664_1	Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals.
2310.04680_1926937_1	We study two natural scaling techniques -- weight pruning and simply training a smaller or larger model, which we refer to as dense scaling -- and their effects on two core capabilities of LLMs: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference.
2310.04782_1927039_5	Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior.
2310.05657_1927914_5	Last, we reveal that asking the LLM to explain its own ratings consistently improves the correlation between the ChatGPT and human ratings and pushes state-of-the-art (SoTA) correlations on two meta-evaluation datasets.
2310.05782_1928039_2	However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance.
2310.05782_1928039_5	To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM.
2310.05782_1928039_6	Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model.
2310.05824_1928081_2	We annotate random source words with pseudo-terminology translations obtained from word alignment to first train a terminology-aware model.
2310.05976_1928233_4	The population is evolved according to selection based on average payoff and mutation of genes by asking LLM to slightly modify the parent gene toward cooperative or selfish.
2310.06450_1928707_6	By training our model with this diversified feedback, we achieve enhanced alignment performance while using less training data.
2310.06552_1928809_4	Unsupervised pre-training alone does not guarantee precise knowledge of the ICD ontology and specialist clinical coding task, therefore we frame the task as information extraction, providing a description of each coded concept and asking the model to retrieve related mentions.
2310.07088_1929345_1	Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance.
2310.07554_1929811_7	Training such a unified model is non-trivial, as various retrieval tasks aim to capture distinct semantic relationships, often subject to mutual interference.
2310.07570_1929827_6	Our strategy outlines a productive process wherein a mathematician trains ChatGPT on pure mathematical concepts, steers ChatGPT towards generating computational topology code, and subsequently validates the generated code using established examples.
2310.07815_1930072_6	In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective.
2310.08433_1930690_2	We ask several LLMs and humans to write such a story and conduct a human evalution involving various criteria such as fluency, coherence, originality, humor, and style.
2310.08669_1930926_6	We train our model using human demonstrations and collision signals from the Habitat-Matterport 3D Dataset (HM3D).
2310.08754_1931011_1	Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations.
2310.08922_1931179_3	In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment.
2310.09478_1931735_4	We propose using unique identifiers for different tasks when training the model.
2310.09755_1932012_5	By training the large language model with our approach, the necessity for generating binary segmentation masks, as suggested in the LISA paper arXiv:2308.00692, is effectively eliminated.
2310.09810_1932067_1	In this paper, we undertake a comprehensive study by instructing ChatGPT for four prevalent vulnerability tasks: function and line-level vulnerability prediction, vulnerability classification, severity estimation, and vulnerability repair.
2310.10035_1932292_4	Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool.
2310.10089_1932346_0	  Federated learning (FL), as an emerging distributed machine learning paradigm, allows a mass of edge devices to collaboratively train a global model while preserving privacy.
2310.10158_1932415_2	Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API.
2310.10158_1932415_3	In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc.
2310.10505_1932762_6	ReMax can save about 46% GPU memory than PPO when training a 7B model and enables training on A800-80GB GPUs without the memory-saving offloading technique needed by PPO.
2310.10962_1933219_3	Building upon this premise, we propose MultiCSR, a multi-level contrastive sentence representation learning framework that decomposes the process of prompting LLMs to generate a corpus for training base sentence embedding models into three stages (i.e., sentence generation, sentence pair construction, in-batch training) and refines the generated content at these three distinct stages, ensuring only high-quality sentence pairs are utilized to train a base contrastive learning model.
2310.11324_1933581_1	Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model.
2310.11998_1934255_1	This paper studies distributed learning in wireless data center networks, which contain a central edge server and multiple edge workers to collaboratively train a shared global model and benefit from parallel computing.
2310.12303_1934560_7	However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system.
2310.12523_1934780_3	To demonstrate its applicability, we show how a private mechanism could be integrated into the existing model for training LLMs to protect user privacy; specifically, we employed differential privacy and private training using Reinforcement Learning (RL).
2310.12558_1934815_7	To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users.
2310.13011_1935268_3	Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment.
2310.13098_1935355_1	The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures.
2310.13312_1935569_4	To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets.
2310.13395_1935652_4	We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side.
2310.13522_1935779_4	We then replay this experience to train the small model.
2310.13548_1935805_1	But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy.
2310.13625_1935882_7	While the scheme will not address all AI risks, it complements proposed solutions by allowing for a more precise and flexible approach to controlling the development of frontier AI models and unwanted AI proliferation.
2310.13671_1935928_1	*Data Synthesis* is a promising way to train a small model with very little labeled data.
2310.13714_1935971_1	We have developed a framework where we train a machine learning-based model using the neural contextual representations of the comments and their corresponding codes to predict the usefulness of code-comments pair and performance analysis with LLM-generated data with base data.
2310.14122_1936379_1	Existing prompts for pointwise LLM rankers mostly ask the model to choose from binary relevance labels like "Yes" and "No".
2310.15747_1938004_4	To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\langle$V, Q, A$\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively.
2310.15780_1938037_3	We propose GPTDroid, asking LLM to chat with the mobile apps by passing the GUI page information to LLM to elicit testing scripts, and executing them to keep passing the app feedback to LLM, iterating the whole process.
2310.15851_1938108_3	Safety training focuses on further training LLM to enhance its safety.
2310.15851_1938108_9	In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses.
2310.16088_1938345_3	Using these data, we train a simple and easily-interpretable machine learning model to regress effective temperatures and luminosities with high accuracy and precision comparable to the training data.
2310.16240_1938497_2	To reduce the number of parameters, we first train the model for a fixed small number of steps before pruning the experts based on their importance scores.
2310.16535_1938792_2	Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction.
2310.16727_1938984_5	A key challenge is to systematically and transparently identify and address AI risks' root causes - also called AI hazards.
2310.16789_1939046_7	Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data.
2310.17567_1939824_4	Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills.
2310.17591_1939848_6	Training performant LLMs on small amounts of data is a difficult but potentially informative task.
2310.18313_1940570_2	Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs.
2310.18343_1940600_4	We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period.
2310.18357_1940614_4	We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms.
2310.18358_1940615_2	Traditional supervised learning usually requires training a model based on labeled data and then making predictions.
2310.18360_1940617_3	Using GPT4 as the editor, we find it can successfully edit trigger shortcut in samples that fool LLMs.
2310.19046_1941303_5	Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions.
2310.19204_1941461_3	We ask ChatGPT to generate candidates of metamorphic relations (MRs), which are basically necessary properties of the object program and which traditionally require human intelligence to identify.
2310.20440_1942697_5	We evaluate the utility of the dataset to train AI models using named-entity recognition, segmentation of figure captions into their constituent panels, and a novel context-dependent semantic task assessing whether an entity is a controlled intervention target or a measurement object.
2310.20444_1942701_1	Thus, it is of major importance to know which stakeholders influence AI research.
2310.20487_1942744_5	Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence.
2310.20563_1942820_4	Third, we describe three policy proposals that would meaningfully address the threats from advanced AI: (1) establishing a Multinational AGI Consortium to enable democratic oversight of advanced AI (MAGIC), (2) implementing a global cap on the amount of computing power used to train an AI system (global compute cap), and (3) requiring affirmative safety evaluations to ensure that risks are kept below acceptable levels (gating critical experiments).
2311.00257_1943224_6	Evaluations demonstrate up to 52\% Model FLOPs Utilization (MFU) when training the LLaMA-based model on 1024 GPUs, resulting in a 1.56 times improvement in training throughput compared to newly proposed systems like MiCS and ZeRO++.
2311.00522_1943489_3	This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model.
2311.01041_1944008_4	Instead of attempting to answer all questions, we explore a refusal mechanism that instructs LLMs to refuse to answer challenging questions in order to avoid errors.
2311.01469_1944436_3	In this study, we introduce a novel preliminary methodology to train a language model on generated labels for greenwashing risk.
2311.01918_1944885_6	To address LLMs' limitations regarding personalization and complex clinical reasoning, the paper explores the emerging development of LLM-powered autonomous agents for healthcare.
2311.01981_1944948_4	In this paper, focusing on easing the prompt forgetting during generation, we proposed an architecture to teach the model memorizing prompt during generation by synthetic gradient.
2311.02105_1945072_4	Can we train LLMs on harmful data without learning harmful behaviors?
2311.02433_1945400_6	To provide some first evidence on this hypothesis, we ask ChatGPT to annotate 106 C programs with loop invariants.
2311.03920_1946887_2	Our system integrates six diverse sensors to gather measurement parameters, which subsequently train a 1D CNN model for activity recognition.
2311.04547_1947514_5	This suggests that modelling processing effort and linguistic competence may require an approach different from training GPT-like LMs on a developmentally plausible corpus.
2311.06180_1949147_3	In stage one, we used a small portion (n=20) of the student responses on one conceptual question to iteratively train GPT.
2311.06377_1949344_4	Our emulation strategy involved using the initial five words of each PubMed abstract as a prompt and instructing the model to expand the content up to the original abstract's length.
2311.06985_1949952_9	For example, Teach-Back enables a 7B model to teach the much larger GPT-3.5 in context, surpassing human teachers by around 5% in test accuracy on medical question answering.
2311.07014_1949981_5	We achieve this via an audio-language knowledge distillation framework, where we transfer acoustic and paralinguistic information from a pre-trained speech embedding (OpenAI Whisper) teacher model to help train a student language model on an audio-text dataset.
2311.07594_1950561_6	The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converter, which transforms data into a format that LLMs can understand; (2) Multimodal Perceiver, which improves how LLMs percieve different types of data; (3) Tool Learning, which leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Method, which teaches LLMs to understand specific data types within datasets.
2311.08105_1951072_1	However, standard approaches to training LLM require a large number of tightly interconnected accelerators, with devices exchanging gradients and other intermediate states at each optimization step.
2311.08147_1951114_3	However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response.
2311.08213_1951180_5	The first stage pre-trains the student model on a large number of filtered multi-modal datasets.
2311.08369_1951336_2	When users instruct LLMs to generate texts, the instruction can include different constraints depending on the user's need.
2311.08662_1951629_7	We employ the Tabular-NLI task to showcase how our proposed strategies adeptly train a robust model, enabling it to address diverse perturbations while maintaining accuracy on the original dataset.
2311.08844_1951811_6	To train our model, we introduce a new method for automatically acquiring data from available English datasets.
2311.08877_1951844_3	We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement.
2311.09136_1952103_4	This method trains the model to prioritize the best responses from a pool of candidates created for a particular task.
2311.09613_1952580_2	Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data.
2311.09632_1952599_8	We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train LMs in a continually evolving environment.
2311.09718_1952685_1	To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions.
2311.10844_1953811_4	Open datasets were employed to train AI models, assess their performance, and analyze their capabilities and limitations in addressing the specific challenges associated with fetal brain fMRI segmentation.
2311.10934_1953901_3	We present a process to assemble such a case repository by: 1) gathering a set of ``seed'' cases -- questions one may ask an AI system -- in a particular domain, 2) eliciting domain-specific key dimensions for cases through workshops with domain experts, 3) using LLMs to generate variations of cases not seen in the wild, and 4) engaging with the public to judge and improve cases.
2311.10947_1953914_4	The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models.
2311.11045_1954012_2	Research on training small LMs has often relied on imitation learning to replicate the output of more capable models.
2311.11045_1954012_4	We seek to teach small LMs to employ different solution strategies for different tasks, potentially different from the one used by the larger model.
2311.11202_1954169_3	This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that can be used for training a harmless language model.
2311.11628_1954595_0	  We present a method to integrate Large Language Models (LLMs) and traditional tabular data classification techniques, addressing LLMs challenges like data serialization sensitivity and biases.
2311.11981_1954948_3	It is unclear whether these labels can be used for training a local model without expensive annotation checking by in-house experts.
2311.12188_1955155_4	In particular, we ask ChatGPT to give examples of how to use Bayes rule for medical diagnosis.
2311.13240_1956207_6	Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.
2311.13387_1956354_0	  As training artificial intelligence (AI) models is a lengthy and hence costly process, leakage of such a model's internal parameters is highly undesirable.
2311.13721_1956688_2	To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization.
2311.14126_1957093_4	Our experiments show that training the model in a multi-class setting can outperform the one-vs-all binary counterpart.
2311.14342_1957309_8	After training the AI models, we confirmed the model's learning effectiveness by observing changes in loss and reward values.
2311.14703_1957670_8	Finally, we find that through asking ChatGPT 3.5 to explain its reasoning prior to providing an answer, we are able to improve clinical accuracy and mitigate instances of gender and racial biases.
2311.15377_1958344_0	  Training advanced AI models requires large investments in computational resources, or compute.
2311.16441_1959408_2	This is mainly due to their distinct representation in a semantic space that is different from the natural language (NL) typically used to train LLMs.
2311.16479_1959446_5	These conversations pay more attention on detailed facts in the image, encouraging the model to answer questions based on multi-modal contexts.
2311.16494_1959461_7	3) We propose negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features.
2311.16639_1959606_1	We ask an LLM where a tweet or a sentence of a political text stands on the focal dimension and take the average of the LLM responses to position political actors such as US Senators, or longer texts such as UK party manifestos or EU policy speeches given in 10 different languages.
2311.16673_1959640_4	Furthermore, the survey presents a comprehensive collection of datasets employed to train LLMs, offering insights into the diverse data available to achieve high performance in various pre-training and downstream tasks of LLMs.
2311.16684_1959651_4	We employ a Time-to-Digital Converter to capture power fluctuations and train a supervised machine learning model to identify various types of threats.
2311.17429_1960396_2	Most of the existing attack methods focus on inserting manually predefined templates as triggers in the pre-training phase to train the victim model and utilize the same triggers in the downstream task to perform inference, which tends to ignore the transferability and stealthiness of the templates.
2311.18063_1961030_4	We trained our model using the same approach for RoBERTa model and evaluated on two text classification tasks: Sentiment Classification and Hate Speech Detection.
2311.18232_1961199_4	However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs.
2311.18609_1961576_4	We propose to train LLM to generate a postfix expression related to the arithmetic problem and incorporate it with small pretrained models.
2311.18702_1961669_0	  Since the natural language processing (NLP) community started to make large language models (LLMs) act as a critic to evaluate the quality of generated texts, most of the existing works train a critique generation model on the evaluation data labeled by GPT-4's direct prompting.
2311.18751_1961718_5	By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%).
2312.00053_1961861_6	We have created a labeled data set in Spanish, since the majority of studies focus on English, to train our system, which offers a very good performance after the validation experiments.
2312.00575_1962383_1	However, no studies have shown that instruction-tuning actually teaches LLMs to process language in a similar manner as humans.
2312.00818_1962626_2	Can we use AI itself to bridge the lack of data in the sciences in order to then train an AI?
2312.00819_1962627_4	Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results.
2312.02147_1963955_3	Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens.
2312.02406_1964214_6	Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining.
2312.02873_1964681_6	We train our autocorrection model on a synthetic dataset in a supervised manner.
2312.04412_1966220_1	In this paper, we successfully developed three elementary FL algorithms using the following three steps process: (i) specify context, (ii) ask ChatGPT to complete server and clients' callback functions, and (iii) verify the generated code.
2312.04469_1966277_5	To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking.
2312.04474_1966282_4	The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator").
2312.04828_1966636_4	The necessity is validated by continuing to train an LLM with an extra term to drive away the model parameters' direction and the model becomes damaged.
2312.05320_1967128_2	The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations.
2312.05356_1967164_9	\textsc{MINT} is effective, efficient, and reliable, capable of correcting a neural model by patching a minimum number of neurons (usually one or two neurons).
2312.05571_1967379_7	We adopt policy-gradient reinforcement learning to train the adapted LM, informed by the non-differentiable symbolic solver.
2312.06942_1968750_8	This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code.
2312.06942_1968750_12	This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding.
2312.08680_1970488_5	By iteratively asking GPT-4 with the prompts, GHGNAS continually validates the accuracy of the generated HGNNs and uses the feedback to further optimize the prompts.
2312.09203_1971011_3	A key aspect of our approach is that we elicit such scores directly, instructing the LLM to furnish numeric scores itself.
2312.09300_1971108_4	We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly.
2312.09792_1971600_9	Finally, we show that synthetic data effectively trains AI models.
2312.09971_1971779_5	Additionally, we show experimentally that the structural information can be kept unmodified when re-training the AI system with new samples while still achieving a validation accuracy similar to that obtained when re-training a neural network with similar size.
2312.10104_1971912_5	Then a dataset with effective ICD sequences is constructed to train Lever-LM.
2312.10321_1972129_5	The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample exists by modifying the database.
2312.10321_1972129_6	The latter technique is used to evaluate the relaxed equivalence in which it asks LLMs to explain the queries and then compare if they contain significant logical differences.
2312.10603_1972411_10	This progress indicates that addressing the current model's limitations could yield an AI capable of passing even the most rigorous professional certifications.
2312.11681_1973489_2	Chains address LLM errors analogously to the way crowdsourcing workflows address human error.
2312.12391_1974199_0	  As large language models (LLMs) become widespread in various application domains, a critical challenge the AI community is facing is how to train these large AI models in a cost-effective manner.
2312.12705_1974513_2	Nevertheless, training LLMs with billions of parameters poses significant challenges and requires considerable computational resources.
2312.12705_1974513_3	For example, training a one trillion parameter GPT-style model on 20 trillion tokens requires a staggering 120 million exaflops of computation.
2312.12705_1974513_5	We enable and investigate various model and data parallel training techniques, such as tensor parallelism, pipeline parallelism, and sharded data parallelism, to facilitate training a trillion-parameter model on Frontier.
2312.12705_1974513_8	We have identified efficient strategies for training large LLMs of varying sizes through empirical analysis and hyperparameter tuning.
2312.12868_1974676_2	Specifically, leveraging reinforcement learning (RL) to train our AI agents, we systematically investigate learning trust under various parameterizations of this task.
2312.13334_1975142_7	FL enables financial institutions to collaboratively train a model to detect fraudulent transactions without directly sharing customer data, thereby preserving data privacy and confidentiality.
2312.14219_1976027_0	  Federated learning is a decentralized learning paradigm wherein a central server trains a global model iteratively by utilizing clients who possess a certain amount of private datasets.
2312.14628_1976436_1	This decentralized approach involves multiple clients or silos, collaboratively training a global model under the coordination of a central server while utilizing their private local data.
2312.14856_1976664_3	Thus, from a single question template, it is possible to ask an LLM a $\textit{neighbourhood}$ of very similar programming questions, and assess the correctness of the result returned for each question.
2312.14950_1976758_3	That is, instead of asking an LLM to write a program (robotic plan) in the popular but verbose Python, ChatFly gets it to do it in MiniSpec specially designed for token efficiency and stream interpretation.
2312.15514_1977322_6	Our method does not require training the model from scratch and can be attached to the classifier simply.
2312.15696_1977504_2	Considering the exorbitant cost of training LLMs from scratch and the scarcity of annotated data within particular domains, in this work, we focus on domain-specific continual pre-training of LLMs using E-commerce domain as an exemplar.
2312.15842_1977650_2	Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model.
2312.16044_1977852_3	Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions.
2312.16211_1978019_5	We ask ChatGPT to reflect on various aspects of each causal link and we then produce visualizations that summarize these viewpoints for the human analyst to direct the edge, gather more data, or test further hypotheses.
2312.16257_1978065_5	Our casual intervention experiments showed that the spatial representations influenced the model's performance on next word prediction and a downstream task that relies on geospatial information.
2312.17235_1979043_7	Furthermore, we show that a specialized prompt that asks the LLM first to summarize the noisy short-term visual captions and then answer a given input question leads to a significant LVQA performance boost.
2401.00139_1979695_6	This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.
2401.00434_1979990_4	We try to specialize an LLM into geoscience, by further pre-training the model with a vast amount of texts in geoscience, as well as supervised fine-tuning (SFT) the resulting model with our custom collected instruction tuning dataset.
2401.00698_1980254_4	The novel ideas explored are: 1) Decaying auxiliary loss (with residual) - where we train the model on an auxiliary task of Coarse-Grained NER and include this task as a part of the loss function 2) Triplet token blending - where we explore ways of blending the embeddings of neighboring tokens in the final NER layer prior to prediction 3) Task-optimal heads - where we explore a variety of custom heads and learning rates for the final layer of the LLM.
2401.00996_1980552_4	In this article, we aim to address the safe model compression problem from the perspective of safety-performance co-optimization.
2401.03676_1983232_6	From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs.
2401.03729_1983285_1	By simply asking the LLM for an answer, or ``prompting,'' practitioners are able to use LLMs to quickly get a response for an arbitrary task.
2401.03851_1983407_4	Based on this paradigm, we trained an encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).
2401.04092_1983648_7	We further design a method instructing GPT-4V to compare two 3D assets according to user-defined criteria.
2401.05612_1985168_6	Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making.
2401.05695_1985251_3	PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process.
2401.06059_1985615_2	There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks.
2401.06072_1985628_6	Additionally, we execute a substantial range of ablation experiments and draw comparisons with several advanced commercial LLMs, to investigate the crucial factors influencing LLMs' performance in structured temporal knowledge inference tasks.
2401.06088_1985644_5	In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.
2401.06373_1985929_3	Specifically, we study how to persuade LLMs to jailbreak them.
2401.06774_1986330_3	We train a system to detect AD-related signs and symptoms from EHRs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method.
2401.06853_1986409_7	On top of that, we teach LLM to perform deliberate reasoning over the TGs via Chain-of-Thought (CoT) bootstrapping and graph data augmentation.
2401.06951_1986507_0	  Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources.
2401.06954_1986510_4	We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM.
2401.07324_1986880_2	While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models.
2401.07657_1987213_2	In this work, we pre-train a transformer model on chemical language and fine-tune it toward drug design objectives, and investigate the correspondence between high-frequency SMILES substrings and molecular fragments.
2401.08089_1987645_5	Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance.
2401.08183_1987739_0	  Wirelessly connected devices can collaborately train a machine learning model using federated learning, where the aggregation of model updates occurs using over-the-air computation.
2401.08273_1987829_1	Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task.
2401.08491_1988047_3	To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation.
2401.08711_1988267_1	Given the pressing need to teach "critical AI literacy", discussion of metaphor provides an opportunity for inquiry and dialogue with space for nuance, playfulness, and critique.
2401.09074_1988630_5	We propose a novel off-the-shelf prompting method, Chain of Simulation (CoSm), which instructs LLMs to simulate code execution line by line/follow the computation pattern of compilers.
2401.09566_1989122_6	We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions.
2401.09796_1989350_0	  The distributed (federated) LLM is an important method for co-training the domain-specific LLM using siloed data.
2401.10446_1990000_3	In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM.
2401.10657_1990211_2	We undermine model robustness by deploying an attack that focuses on input transformation while mimicking the real data and confusing the model decision-making, ultimately yielding a pronounced deterioration in model performance.
2401.10745_1990299_3	Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet.
2401.10745_1990299_5	This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.
2401.11011_1990565_1	Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields.
2401.12246_1991800_1	We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages.
2401.12474_1992028_3	Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension.
2401.13218_1992772_6	We introduce LEAFER to address the challenge LLMs face in locating the exact boundary of an argument.
2401.15241_1994795_0	  Identifying the training datasets that influence a language model's outputs is essential for minimizing the generation of harmful content and enhancing its performance.
2401.15497_1995051_2	Without explicit consent from artists, Generative AI creators scrape artists' digital work to train Generative AI models and produce art-like outputs at scale.
2401.15641_1995195_1	Besides how to construct and train LLMs, how to effectively evaluate and compare the capacity of LLMs has also been well recognized as an important yet difficult problem.
2401.16731_1996285_2	Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model.
2401.16791_1996345_0	  Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions.
2401.16795_1996349_2	Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game.
2401.17043_1996597_1	This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content.
2401.17267_1996821_3	Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA).
2401.17390_1996944_5	Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid.
2402.00282_1997921_5	Contrary to other "reference-free" metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores.
2402.00786_1998425_1	To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets.
2402.00905_1998544_5	Fine-tuning involves training the model on a specific code review dataset, while prompting involves providing explicit instructions to guide the model's generation process without requiring a specific code review dataset.
2402.00969_1998608_5	Our approach focuses on assuming a correct entity linking on the natural language questions and training a GPT model to create SPARQL queries from them.
2402.01093_1998732_4	In the first scenario, we propose an effective solution based on importance sampling: we resample the pretraining set to imitate the specialization data and train a small model on it.
2402.01691_1999330_1	Responsible AI (RAI) governance approaches at organizations have emerged as important mechanisms to address potential AI risks and harms.
2402.01732_1999371_3	To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related.
2402.01766_1999405_3	We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes.
2402.01867_1999506_2	In this work, we ask the LLM how similar are these prompted LFs.
2402.02030_1999669_3	Panacea trains a single model capable of adapting online and Pareto-optimally to diverse sets of preferences without the need for further tuning.
2402.02167_1999806_2	At the same time, several pitfalls, like the multiple ways of instructing an LLM to generate the desired result, the different perspectives leading the generation (code-based, image-based, grammar-based), and the presence of hallucinations even for the visualization generation task, make their usage less affordable than expected.
2402.02456_2000095_4	The proposed framework is an elaborate prompting pipeline that instruct LLMs to generate new TN-SS algorithms through iterative refinement and enhancement.
2402.02963_2000602_4	We demonstrated this principle by training the algorithm with data collected at different outdoors temperature, which lead to the detection of thermal bridges.
2402.03182_2000821_1	However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training.
2402.03407_2001046_3	Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model.
2402.03469_2001108_3	We aim to replicate the ground truth (gold) reward signal by achieving a monotonic relationship between the proxy and gold reward signals after training the model using the proxy reward in reinforcement learning (RL).
2402.03575_2001214_5	Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior.
2402.03659_2001298_6	The reflective agent learns how to explain past stock movements through self-reasoning, while the PPO trainer trains the model to generate the most likely explanations from input texts.
2402.03776_2001415_7	To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with instructor-provided correct answers; Zero-shot-CoT in conjunction with both instructor-formulated answers and rubrics; and Zero-shot-CoT with instructor-offered correct answers and LLM-generated rubrics.
2402.03916_2001555_2	Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden.
2402.04315_2001954_3	In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses.
2402.04400_2002039_4	In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences that can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format.
2402.04497_2002136_1	To train an LLM, one needs to alternatingly run `forward' computations and `backward' computations.
2402.04568_2002207_0	  Prompt design plays a crucial role in shaping the efficacy of ChatGPT, influencing the model's ability to extract contextually accurate responses.
2402.04617_2002256_7	Without any training, InfLLM enables LLMs that are pre-trained on sequences consisting of a few thousand tokens to achieve comparable performance with competitive baselines that continually train these LLMs on long sequences.
2402.05000_2002639_2	We term the objective of training LLMs to emulate effective teaching strategies as `pedagogical alignment.'
2402.06954_2004593_2	In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data.
2402.06954_2004593_6	Through extensive experiments, we observe that all FL algorithms outperform local training on training LLMs, demonstrating a clear performance improvement across a variety of settings.
2402.07645_2005284_2	In this work, we use LLM-generated synthetic data (GPT3.5) and a Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction model.
2402.07645_2005284_4	We show it is possible to obtain good overall performance (0.70 F1 across polarity) on real clinical data on a set of as many as 20 different factors, and high performance (0.85 F1 with 0.95 precision) on a subset of important DTD factors such as history of abuse, family history of affective disorder, illness severity and suicidality by training the model exclusively on synthetic data.
2402.07945_2005584_7	Finally, we trained a model, ScreenAgent, which achieved computer control capabilities comparable to GPT-4V and demonstrated more precise UI positioning capabilities.
2402.08078_2005717_4	Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.
2402.08157_2005796_2	In addition to potentially providing insights into the complex bushfire behaviour, direct numerical simulation (DNS) can generate synthetic remote sensing data to train AI algorithms such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which can process large amounts of remotely sensed data associated with bushfire.
2402.08208_2005847_7	To mitigate these risks, methods for training AI models that help maintain performance without overconfidence are proposed.
2402.08680_2006319_2	However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation.
2402.08699_2006338_4	RTC rests on the idea that we can ask a model to make a prediction (e.g., describe some code using natural language), feed that prediction back (e.g., synthesize code from the predicted description), and check if this round-trip leads to code that is semantically equivalent to the original input.
2402.08806_2006445_2	Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   
2402.09299_2006938_8	In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM.
2402.09360_2006999_1	On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency.
2402.09363_2007002_5	We carefully design a randomized controlled experimental setup, inserting traps into original content (books) and train a 1.3B LLM from scratch.
2402.09671_2007310_0	  This investigation reveals a novel exploit derived from PNG image file formats, specifically their alpha transparency layer, and its potential to fool multiple AI vision systems.
2402.09739_2007378_3	We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria.
2402.09939_2007578_3	This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents.
2402.10151_2007790_4	We introduce ControlLM, which leverages differential activation patterns, derived from contrasting behavioral prompts in the model's latent space, to influence the model's personality traits at inference.
2402.10239_2007878_2	However, the current practice is to design and train one deep learning model for one task with supervised learning techniques.
2402.10239_2007878_5	In this paper, we present a tokenized detector representation that allows us to train a BERT model for particle tracking.
2402.11142_2008781_4	To accurately and explicitly describe relation semantics while minimizing annotation demands, we explore the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model.
2402.11176_2008815_3	We devise a fine-grained knowledge augmentation stage to train LLMs to identify difficult fine-grained knowledge in answers.
2402.11176_2008815_4	We also propose a coarse-grained knowledge comparison stage to train LLMs to distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality.
2402.11245_2008884_2	To address the AI model placement problem under uncertainties, this paper presents a novel approach employing a sequence-to-sequence (S2S) neural network which considers uncertainty estimations.
2402.11253_2008892_2	To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model to act as both a policy and a judge.
2402.11359_2008998_1	To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications.
2402.11450_2009089_3	Our key observation is that when human-robot interactions are viewed as a partially observable Markov decision process (in which human language inputs are observations, and robot code outputs are actions), then training an LLM to complete previous interactions is training a transition dynamics model -- that can be combined with classic robotics techniques such as model predictive control (MPC) to discover shorter paths to success.
2402.11485_2009124_3	This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling.
2402.11532_2009171_3	Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached.
2402.11633_2009272_6	The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to autonomously adapt its prompt instruction when generating complex multi-intent utterances.
2402.11651_2009290_5	By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks.
2402.11801_2009440_5	Regarding emotional understanding, HEF implements a two-stage emotion prediction strategy, encouraging LLMs to prioritize primary emotions emphasized by SEMs, followed by other categories, substantially alleviates the difficulties for LLMs in fine-grained emotion detection.
2402.11890_2009529_0	  Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model.
2402.12010_2009649_1	On the one hand, data-centric approaches show great potential towards training energy-efficient AI models.
2402.12010_2009649_2	On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation.
2402.12026_2009665_1	Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios.
2402.12026_2009665_5	Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning.
2402.12621_2010260_2	However, only a few works attempted to directly train the LMs within interactive decision-making environments.
2402.12786_2010425_4	Our goal is to teach the LLM that "even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different".
2402.12786_2010425_6	To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles.
2402.12786_2010425_7	We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles.
2402.12907_2010546_1	While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts.
2402.12914_2010553_5	We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment.
2402.13210_2010849_4	To address these challenges, we propose to train a Bayesian reward model, which signals higher uncertainty further from the training data distribution.
2402.13414_2011053_4	Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels.
2402.14245_2011884_3	In this study, we train a multimodal LLM, termed CriticGPT, capable of understanding trajectory videos in robot manipulation tasks, serving as a critic to offer analysis and preference feedback.
2402.14846_2012485_5	We consider two settings (with and without instructing LLMs to simulate particular personas), two simulated populations, and three downstream tasks.
2402.14850_2012489_3	Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations.
2402.14878_2012517_3	In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM approaches.
2402.14878_2012517_7	Our projections suggest that the energy-dissipation lower-bound to train a brain scale AI system (comprising of $10^{15}$ parameters) using LIM is $10^8 \sim 10^9$ Joules, which is on the same magnitude the Landauer's adiabatic lower-bound and
2402.14904_2012543_0	  We investigate the radioactivity of text generated by large language models (LLM), i.e. whether it is possible to detect that such synthetic input was used to train a subsequent LLM.
2402.15301_2012940_6	Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets.
2402.15302_2012941_6	Overall, we observe that asking LLMs to produce instruction-centric responses enhances the unethical response generation by ~2-38% across the models.
2402.15302_2012941_8	In particular, asking edited LLMs to generate instruction-centric responses further increases the unethical response generation by ~3-16% across the different models.
2402.15627_2013266_1	Training LLMs at this scale brings unprecedented challenges to training efficiency and stability.
2402.15627_2013266_6	MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM.
2402.16065_2013704_0	  We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script.
2402.16352_2013991_3	We augment the ground-truth solutions of our seed data and train a back-translation model to translate the augmented solutions back into new questions.
2402.16786_2014425_3	Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions.
2402.16827_2014466_1	However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary.
2402.16929_2014568_1	Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts.
2402.17124_2014763_2	In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved.
2402.17124_2014763_6	And then it asks the model to "reflect" over them to generate the final answer.
2402.17385_2015024_7	Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.
2402.17801_2015440_1	Amidst the immense excitement for this new technology, its future development and applications in the creative industry hinge crucially upon two copyright issues: 1) the compensation to creators whose content has been used to train generative AI models (the fair use standard); and 2) the eligibility of AI-generated content for copyright protection (AI-copyrightability).
2402.17812_2015451_1	However, training these LLMs typically involves substantial memory and computational costs during both forward and backward propagation.
2402.17887_2015526_3	Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase.
2402.18284_2015923_3	Our method begins with probabilistic sampling to encourage a language model to generate diverse responses for each input.
2402.18571_2016210_5	Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2.
2402.19327_2016965_2	The dataset used to train the model includes 37.8 million single-point energies, 11.7 billion force pairs, and 340.2 million stresses.
2402.19423_2017061_12	Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance.
2402.19434_2017072_4	The proposed digital twin approach generates site-specific synthetic CSI data from the EM 3D model and ray tracing, which can then be used to train the DL model without real-world data collection.
2403.00952_2018071_1	Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges.
2403.01069_2018188_6	The results reveal the fine-grained effects of incorporating criteria and demonstrations and provide valuable insights on how to teach LLMs to use criteria more effectively.
2403.01209_2018328_2	Through asking LLM by well-designed questions, we acquire comprehensive knowledge about characteristics and contexts of objects, which provides valuable text descriptions for learning prompts.
2403.01570_2018689_6	Specifically, SERSAL utilizes the LLM's prior outcomes as original soft noisy annotations, which are dynamically leveraged to teach a better small student model.
2403.01570_2018689_7	Reversely, the outcomes from the trained small model are used to teach the LLM to further refine its real capability.
2403.01632_2018751_3	Due to the hallucinations and unreliability of LLMs, instructing LLMs to adhere to specified syntax becomes an increasingly important challenge.   
2403.02130_2019249_3	We experiment with different zero-shot and few-shot prompt templates for instructing LLMs to extract and normalize attribute-value pairs.
2403.02419_2019538_1	However, there is little understanding of how the number of LM calls - e.g., when asking the LM to answer each question multiple times and taking a majority vote - affects such a compound system's performance.
2403.02553_2019672_3	We then use this database to train a machine learning model that can efficiently predict magnetocaloric properties of materials based on their chemical composition.
2403.02610_2019729_9	Additionally, we perform an ablation study to select a function signature to instruct ChatGPT for level generation.
2403.02694_2019813_7	MeanCache leverages Federated Learning (FL) to collaboratively train a query similarity model without violating user privacy.
2403.02715_2019834_6	Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets.
2403.02781_2019900_4	In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels.
2403.03507_2020626_6	Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.
2403.04283_2021402_3	We start with a novel Markov Decision Process (MDP) designed for the alignment process and employ Reinforcement Learning (RL) to train a streamlined proxy model that oversees the token generation of the LLM, without altering the LLM itself.
2403.04769_2021888_0	  Large language models (LLMs) are initially trained on vast amounts of data, then fine-tuned using reinforcement learning from human feedback (RLHF); this also serves to teach the LLM to provide appropriate and safe responses.
2403.04769_2021888_2	Unlike other jailbreaks (for example, the popular "Do Anything Now" (DAN) ), our method does not rely on instructing the LLM to override its RLHF policy; hence, simply modifying the RLHF process is unlikely to address it.
2403.04784_2021903_2	To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives.
2403.04818_2021937_4	We trained our proposed ML model on a dataset of 61 historical storms in the coastal regions of the U.S. and we tested its performance in bias correcting modeled water level data predictions from hurricane Ian (2022).
2403.05217_2022336_4	Since LLMs exhibit their excellent capabilities to accomplish various tasks, we instruct LLMs to play multiple roles as generators, rerankers, and evaluators within our framework, integrating them to collaborate in the ODQA process.
2403.05434_2022553_2	Owing to the prohibitive costs of training LLMs, they are usually used as a network service, with the client charged by the count of input and output tokens.
2403.05572_2022691_5	Additionally, instructing ChatGPT to incorporate a clear understanding of empathy in its responses makes the responses align approximately 5 times more closely with the expectations of individuals possessing a high degree of empathy, compared to human responses.
2403.05583_2022702_1	We introduce Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal alignment through novel loss functions--cross-contrast (crossCon) and supervised temporal contrast (supTcon)--to train a multimodal model with a shared latent representation.
2403.05612_2022731_3	This suggests that by modifying how unfamiliar finetuning examples are supervised, we can influence a model's responses to unfamiliar queries (e.g., say ``I don't know'').
2403.05973_2023092_2	We propose APRICOT (auxiliary prediction of confidence targets): A method to set confidence targets and train an additional model that predicts an LLM's confidence based on its textual input and output alone.
2403.06354_2023473_4	We employ methods previously used for training LLMs on other languages with data scarcity, and use open source translation models to perform data augmentation and grow our dataset from millions of tokens to billions.
2403.06512_2023631_2	While conventional threat modeling methods and tools did not address AI-related threats, research on this amalgamation still lacks solutions capable of guiding and automating the process, as well as providing evidence that the methods hold up in practice.
2403.06664_2023783_7	In addition, we propose an efficient data transfer handler structure to address the system integration issues for Smart-Infinity.
2403.07118_2024237_5	Results further suggest that users of generative AI can deploy future applications faster since similar performances are obtained when training a model with only a few examples as compared to fine-tuning via a large curated dataset.
2403.07747_2024866_6	These two factors significantly influence the model results and our understanding of their mathematical reasoning capabilities.
2403.07969_2025088_1	KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.
2403.08100_2025219_0	  Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices.
2403.08229_2025348_6	Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance.
2403.08281_2025400_6	To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content.
2403.08693_2025812_4	Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks.
2403.08693_2025812_7	We conclude that, in our experiments, the quality of the web-crawled corpora does not seem to play a significant role when training LMs.
2403.09522_2026641_5	Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student.
2403.09972_2027091_4	Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation.
2403.10131_2027250_4	In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents.
2403.10433_2027552_6	We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence.
2403.11103_2028222_3	Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data.
2403.12744_2029863_3	In this paper, we propose a novel approach named I$^3$C that instructs LLMs to identify and ignore irrelevant conditions.
2403.12744_2029863_6	Lastly it instructs the LLMs with the verification on relevant and irrelevant conditions to avoid confusion and improve reasoning paths.
0803.0785_52545_5	The fusion rules in the extended picture are deduced from the known fusion rules for the Virasoro representations of LM(1,p) and are found to be in agreement with previous works.
1006.0386_193661_2	The public key cryptosystem based on rank codes was presented in 1991 by Gabidulin -Paramonov-Trejtakov(GPT).
1008.3328_208085_3	In Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS), two new versions of LMS algorithms, same formulations are performed as in the LMS algorithm with the exception that filter tap weights are now expressed using trigonometric and hyperbolic formulations, in cases for TLMS and HLMS respectively.
1104.1698_255252_2	The corresponding algorithm for the sequential determination of the generalized LM-inverse is established in the present paper.
1303.2255_413912_3	Secondly, two modifications to the ZA-LMS algorithm have been made.
1303.2255_413912_7	In addition, the mean square performance of DWZA-LMS algorithm is analyzed.
1303.3263_414920_3	The error estimation in Polynomial method is carried out by LMS Filter.
1404.6813_519866_4	This brings up the issue of studying the performance of the diffusion LMS algorithm when it is run, either intentionally or unintentionally, in a multitask environment.
1611.00196_785643_2	This paper proposes a novel distributed vector representation of a document: a simple recurrent-neural-network language model (RNN-LM) or a long short-term memory RNN language model (LSTM-LM) is first created from all documents in a task; some of the LM parameters are then adapted by each document, and the adapted parameters are vectorized to represent the document.
1703.10724_834228_6	Building LSTM $n$-gram LMs may be appealing for some practical situations: the state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes storing the identity of the words in the context and batches of $n$-gram contexts can be processed in parallel.
1710.04381_899764_4	A rigorous mean and mean square performance evaluation is conducted to justify the performance advantages of the proposed scheme over the conventional augmented LMS solution.
1806.01782_987422_3	Convergence analysis of the LMS algorithm in the case of coloured input signal, i.e., correlated input signal is demonstrated on adaptive FIR filter via power spectral density of the input signals and Fourier transform of the autocorrelation matrix of the input signal.
1806.01782_987422_4	Simulations have been carried out on adaptive filtering of FIR and IIR filters and tested on white and coloured input signals to validate the powerfulness of the genetic-based LMS algorithm.
1807.07395_1004603_5	Convergence of the proposed method is investigated and it is proved that the rate of convergence of the introduced method is equal to that of LMS algorithm in the expected value sense, provided that the distribution of the added noise is uniform.
1903.02852_1095238_2	While AL is known to be beneficial to AM training, we show that it also carries out substantial improvements to the LM when combined with SST.
1904.06234_1111274_7	We apply two different sub-approaches in the LM Based approach and the combined result of these two approaches is considered as the final output of the system.
2002.11268_1248691_4	The Density Ratio method was found to consistently outperform the dominant approach to LM and end-to-end ASR integration, Shallow Fusion.
2008.09036_1336758_1	However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies.
2010.06189_1362519_1	However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English.
2010.10657_1366987_2	In the recent decades, concerns have been raised on studying improper signals and providing an accurate model of the LMS algorithm for both proper and improper signals.
2010.10657_1366987_3	Other models for the LMS algorithm for improper signals available in the scientific literature either make use of the independence assumptions regarding the desired signal and the input signal vector, or are exclusive to proper signals; it is shown that by not considering these assumptions a more general model can be derived.
2010.11349_1367679_0	  LSTM language models (LSTM-LMs) have been proven to be powerful and yielded significant performance improvements over count based n-gram LMs in modern speech recognition systems.
2010.15577_1371907_4	The formats for submitting questions, examples of its designing and developed questions were demonstrated in view mode in Moodle LMS.
2012.05628_1393506_5	This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2.
2104.04466_1451884_3	We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level.
2104.08826_1456244_0	  Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts.
2105.13818_1476377_2	By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.
2109.04314_1527135_2	Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner.
2110.08743_1546886_6	\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM
2202.01771_1600595_3	In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action.
2202.04173_1602997_5	We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before.
2202.13169_1611993_5	We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine.
2203.02155_1615017_7	In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.
2203.10692_1623554_0	  Class-based language models (LMs) have been long devised to address context sparsity in $n$-gram LMs.
2204.06201_1636339_4	We find that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.
2204.06201_1636339_5	Moreover, we show that a complete constituency tree can be linearly separated from LM representations.
2207.09638_1685536_3	Motivated by this phenomenon, we for the first time posit that domain-general parameters can underpin a domain-general LM that can be derived from the original LM.
2207.14382_1690280_7	What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test.
2208.06946_1697658_5	We conducted a pilot experiment in which individuals are asked to distinguish between authentic passwords and honeywords when the username is provided for GPT-3 and a tweaking technique.
2208.07601_1698313_3	Noting that the computation of the LM rate can also be formulated as an entropy-based optimization problem with constraints, in this work, we transform the task into an optimal transport (OT) problem with an extra constraint.
2210.01293_1722648_7	Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs.
2210.10585_1731940_1	Through surveys of human subjects enrolled in the crowdsourcing platform Prolific.co and queries submitted to the OpenAI's language model GPT-3, I test whether the numerical response for what wage is deemed fair for a particular job description changes when respondents and GPT-3 are prompted with additional information that includes a numerical minimum wage, whether realistic or unrealistic, relative to a control where no minimum wage is stated.
2210.12353_1733708_1	MCQA tasks have traditionally been presented to LLMs like cloze tasks.
2210.12770_1734125_2	We compare Transformer models that are trained from scratch to fine-tuned BERT-based LLMs namely BERT, BioBERT, and ClinicalBERT.
2211.11483_1750389_1	Such claims have been made concerning the LaMDA model and also concerning the current wave of LLM-powered chatbots, such as ChatGPT.
2211.15006_1753912_7	The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step.
2212.05058_1761235_1	We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which AI behaviour, including its productions of bias and harm, can be analysed.
2212.08104_1764281_5	Note from the human-authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors in writing review articles.
2212.09292_1765469_4	Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools.
2212.14882_1771059_6	While further studies are needed, the initial insights of this study indicate a great potential in using large language models like ChatGPT to improve patient-centered care in radiology and other medical domains.
2301.02828_1773892_3	In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on.
2301.05272_1776336_1	The impressive scalability of LLMs due to the advent of deep learning can be seen as a continuation of empiricist lingusitic methods, as opposed to rule-based linguistic methods that are grounded in a nativist perspective.
2301.05517_1776581_1	How can we ensure that AI systems, including ChatGPT, are developed and adopted in a responsible way?
2301.12867_1783931_3	Large-scale benchmarks for accountable LLMs should consequently be developed.
2301.13852_1784916_3	In this paper, we study whether a machine learning model can be effectively trained to accurately distinguish between original human and seemingly human (that is, ChatGPT-generated) text, especially when this text is short.
2302.07136_1792070_0	  This study aims to understand the perceptions and opinions of academicians towards ChatGPT-3 by collecting and analyzing social media comments, and a survey was conducted with library and information science professionals.
2302.08579_1793513_2	To alleviate this problem, this paper designs a replaceable internal language model (RILM) method, which makes it feasible to directly replace the internal language model (LM) of E2E ASR models with a target-domain LM in the decoding stage when a domain shift is encountered.
2302.12246_1797180_1	It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers.
2302.13439_1798373_0	  The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs.
2302.14229_1799163_2	However, it is not yet known the performance of LLMs on CLS.
2303.05349_1805146_1	However, while the debate on ChatGPT in academia is making waves, more understanding is needed among lecturers and teachers on how students use and perceive ChatGPT.
2303.06074_1805871_5	The same pattern of effects was found for LLM-simulated participants.
2303.07529_1807326_1	In 2023, OpenAI responded to criticism that Kenyan workers were paid less than $2 per hour to filter traumatic content from its ChatGPT model by stating in part that it had outsourced the work to a subcontractor, who managed workers' payment and mental health concerns.
2303.08769_1808566_4	An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.
2303.09461_1809258_5	At the same time, the questions in our exam are structurally similar to those of other exams, solved homework problems, and teaching materials that can be found online and might have been part of ChatGPT's training data.
2303.11158_1810955_15	However, currently, a great amount of attention is needed before a user can utilize materials from ChatGPT
2303.11436_1811233_3	Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required.
2303.12712_1812509_8	Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.
2303.17003_1816800_3	This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed.
2304.01933_1819979_0	  The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca).
2304.01964_1820010_7	PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs.
2304.02468_1820514_4	A strategy for validating the arguments and results of ChatGPT is presented summarily as an example of safe, large-scale adoption of LLMs.
2304.02796_1820842_5	A case study is conducted to validate the advantages and drawbacks of ChatGPT, showing that designers can acquire targeted knowledge from various domains, but the quality of the acquired knowledge is highly dependent on the prompt.
2304.04370_1822416_5	Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models.
2304.05436_1823482_6	Consequently, in the second phase, bibliometric analysis has been carried out on ChatGPT publications, and 45 published studies have been analyzed thoroughly based on their methods, novelty, and conclusions.
2304.09823_1827869_7	In particular, industries related to technology, products, and operations are expected to have higher proficiency requirements for ChatGPT-related skills, while the manufacturing, services, education, and health science related industries will have lower requirements for ChatGPT-related skills.
2304.10436_1828482_0	  With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns.
2304.10778_1828824_6	The average technical debt, considering code smells, was found to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes for Amazon CodeWhisperer.   
2304.12191_1830237_6	We hypothesize that Zipf's law will hold for genlangs because (1) genlangs created by ChatGPT fundamentally operate in the same way as human language with respect to the semantic usefulness of certain tokens, and (2) ChatGPT has been trained on a corpora of text that includes many different languages, all of which exhibit Zipf's law to varying degrees.
2304.12244_1830290_8	By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT.
2304.13712_1831758_9	A curated list of practical guide resources of LLMs, regularly updated, can be found at \url{https://github.com/Mooler0410/LLMsPracticalGuide}.
2304.14178_1832224_4	In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text.
2304.14354_1832400_3	The performance of ChatGPT in solving complex problems in oil and gas engineering is discussed and the areas where LLMs are most effective are presented.
2304.14399_1832445_5	We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset.
2304.14415_1832461_5	A survey was created to measure the effects of ChatGPT on students, faculty, and staff.
2305.00118_1833174_0	  In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query.
2305.01550_1834606_7	Furthermore, our framework is robust in mitigating approximate memorization across various circumstances, including longer context, which is known to increase memorization in LLMs.
2305.02220_1835276_4	Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.
2305.02320_1835376_8	Further work is needed to determine the effect of factually wrong information in the generated responses and test our findings' generalizability with open-source LLMs.
2305.03851_1836907_5	Further research is needed to fully understand the potential applications of LLMs in sports science and medicine and to ensure that their use is ethical and beneficial to athletes, clients, patients, practitioners, and the general public.
2305.04134_1837190_5	Additionally, we introduce a new variant of the classical method in order to avoid that the solutions are found in the LLM training data (dataleakeage).
2305.04812_1837868_5	Three experiments were conducted to explore the effects of external information on LLMs' memories, opinions, and social media behavioral decisions.
2305.07716_1840772_2	Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark.
2305.09612_1842668_8	The code for this work can be found at \url{https://github.com/Ziems/llm-url}.
2305.09955_1843011_4	Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM.
2305.10383_1843439_6	A prompt for GPT-4 is developed which includes definitions, guidelines, examples, and rationales for text classification.
2305.10646_1843702_5	These commandment examples are expected to motivate the ethical use of ChatGPT.
2305.11000_1844056_0	  Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT.
2305.11169_1844225_4	We also develop a novel interventional baseline that enables us to disambiguate what is represented by the LM as opposed to learned by the probe.
2305.11391_1844447_3	Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications.
2305.11391_1844447_5	In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.
2305.12392_1845448_3	We show how a small language model could be trained to act as a verifier module for the output of an LLM~(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions.
2305.12477_1845533_4	While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD.
2305.14045_1847101_0	  Language models (LMs) with less than 100B parameters are known to perform poorly on chain-of-thought (CoT) reasoning in contrast to large LMs when solving unseen tasks.
2305.14283_1847339_5	A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader.
2305.14283_1847339_6	The rewriter is trained using the feedback of the LLM reader by reinforcement learning.
2305.14283_1847339_8	Experiments results show consistent performance improvement, indicating that our framework is proven effective and scalable, and brings a new framework for retrieval-augmented LLM.
2305.14307_1847363_2	For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed?
2305.14752_1847808_4	This combined information is then fed into an LLM, which is instructed to attempt to fix the code.
2305.14771_1847827_1	While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies of diffusion LMs have been conducted on a smaller scale.
2305.14864_1847920_1	A growing assortment of methods for compression promises to reduce the computational burden of LLMs in deployment, but so far, only quantization approaches have been demonstrated to be effective for LLM compression while maintaining zero-shot performance.
2305.15020_1848076_6	The evaluation is performed over four NLP tasks (two generative and two classification tasks) among four widely used multilingual LMs in seven languages.
2305.15717_1848773_6	However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data.
2305.15929_1848985_5	Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT
2305.17359_1850415_6	We conducted extensive experiments on the most advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier, which is trained on millions of text.
2305.17608_1850664_0	  The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts.
2305.18086_1851142_6	Studies including secondary data related to the application of ChatGPT were considered.
2305.18086_1851142_9	After multi-step screening process, 11 reviews were selected, consisting of 9 reviews specifically focused on ChatGPT and 2 reviews on broader AI topics that also included discussions on ChatGPT.
2305.18449_1851505_2	Then, we characterize ``meaningful data'' on which large language models (LLMs) are ostensibly trained, and ``well-trained LLMs'' through conditions that are largely met by today's LLMs.
2305.18616_1851672_3	Weekly surveys were conducted on collaborative interdisciplinary problem-solving, physical and cognitive engagement, and individual reflections on ChatGPT use.
2305.18752_1851808_6	Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways.
2305.19187_1852243_6	Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes).
2306.00597_1853745_4	The paper also suggests that the usage of LLMs and ChatGPT is expected to increase in the future as they offer unparalleled benefits to the programming community.
2306.01248_1854396_3	Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization.
2306.01499_1854647_3	Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting.
2306.02096_1855244_0	  With the launch of ChatGPT, serious concerns have reasonably been raised of its ill-effect on the integrity of remote take-home exams.
2306.03097_1856245_0	  Large generative AI models (GMs) like GPT and DALL-E are trained to generate content for general, wide-ranging purposes.
2306.03241_1856389_7	Additionally, we present results for publicly available Pythia LLMs, ranging from 1B to 12B, which were trained on the PILE-deduped dataset containing 207B tokens.
2306.03503_1856651_2	By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified.
2306.04610_1857758_2	Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training.
2306.05087_1858235_3	In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs.
2306.05827_1858975_3	In this paper, we present our work on a cooperative-legal question-answering LLM-based chatbot, where we developed a set of legal questions about Palestinian cooperatives, associated with their regulations and compared the auto-generated answers by the chatbot to their correspondences that are designed by a legal expert.
2306.06815_1859963_1	However, the security implications of LLMs, particularly in relation to adversarial and Trojan attacks, remain insufficiently examined.
2306.07032_1860180_3	We aim an critical issue in the emerging topic of LLM based causal structure learning, to tackle erroneous prior causal statements from LLM, which is seldom considered in the current context of expert dominating prior resources.
2306.08178_1861326_6	Source code can be found at: https://github.com/DrCintas/ASCON-with-ChatGPT.
2306.09339_1862487_4	Models for generating natural language will be considered as well as models, such as GPT-3 Codex, which complete program code or generate code from natural language instructions.
2306.10765_1863913_5	With an increasing number of domain-specific professional multimodal LLMs in the medical field being developed, MedAGI is designed to automatically select appropriate medical models by analyzing users' questions with our novel adaptive expert selection algorithm.
2306.13421_1866569_1	However, typically the retriever is not trained jointly as a native component of the LM, but added post-hoc to an already-pretrained LM, which limits the ability of the LM and the retriever to adapt to one another.
2306.14905_1868053_3	By finetuning LLMs on domain-specific academic papers that have been selected as a result of a rigorous SLR process, the proposed PRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer the potential to achieve greater efficiency, reusability and scalability, while also opening the potential for conducting incremental living systematic reviews with the aid of LLMs.
2306.17176_1870324_1	A total of 100 fact-checked news items, all sourced from independent fact-checking agencies, were presented to each of these LLMs under controlled conditions.
2306.17176_1870324_4	The effectiveness of the LLMs was gauged based on the accuracy of their classifications against the verified facts provided by the independent agencies.
2307.00012_1871008_9	Based on the execution and analysis of a sample of GPT-repaired flaky tests, we estimate that a large percentage of such repairs (roughly between 51% and 83%) can be expected to pass.
2307.02006_1873002_2	The proposed framework relies on domain-specific pre-training, to produce a specialised language model which is trained on task-specific natural data augmented by synthetic data generated by a black-box LLM.
2307.03196_1874191_4	Based on a preliminary survey on ChatGPT's quality in answering questions in Geography and GIScience, we demonstrate that this assumption might be fairly naive, and effective control in assessments and supervision is required.
2307.03744_1874739_5	Participants in our experiments were asked to solve a series of decision tasks that involved researching and comparing different products, and were randomly assigned to do so with either an LLM-based search tool or a traditional search engine.
2307.03972_1874967_6	Our findings demonstrates that further investigation is required for the application of LLMs on Chinese GEC task.
2307.07171_1878166_3	This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input.
2307.08974_1879969_3	The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked ideas, findings, and reporting standards in scholarly research, and to formulate guidelines for its use and disclosure, b) a bibliometric analysis of existing author guidelines in journals that mention GAI/GPT/LLM, with the goal of evaluating existing guidelines, analyzing the disparity in their recommendations, and identifying common rules that can be brought into the Delphi consensus process, c) a Delphi survey to establish agreement on the items for the guidelines, ensuring principled GAI/GPT/LLM use, disclosure, and reporting in academia, and d) the subsequent development and dissemination of the finalized guidelines and their supplementary explanation and elaboration documents.
2307.09009_1880004_6	This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting.
2307.10811_1881806_1	Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting.
2307.11316_1882311_12	The code will be made public at \url{https://github.com/Yangyi-Chen/LM-TOAST}.
2307.16180_1887175_4	Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality.
2307.16883_1887878_4	HAGRID is constructed based on human and LLM collaboration.
2308.00121_1888013_3	For the latter, we implemented a closed-feedback loop between LLM-generated low-level actions with a vulnerable virtual machine (connected through SSH) and allowed the LLM to analyze the machine state for vulnerabilities and suggest concrete attack vectors which were automatically executed within the virtual machine.
2308.00225_1888117_3	In this work, we investigate the effect of IT and RLHF on decision making and reasoning in LMs, focusing on three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning.
2308.01990_1889882_1	Internally, aided by an LLM-integration middleware such as Langchain, user prompts are translated into SQL queries used by the LLM to provide meaningful responses to users.
2308.02312_1890204_2	Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT's answers to programming questions.
2308.02955_1890847_2	Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models.
2308.03301_1891193_5	It can be shown that all references provided by ChatGPT that were found to be genuine have also been cited on Wikipedia pages.
2308.03313_1891205_1	As people interact with large language models (LLMs) in the opinion shaping process different from traditional media, the impacts of LLMs are increasingly recognized and being concerned.
2308.03313_1891205_9	The optimal diversity of opinion was found when the fractions of people who do not use, partially rely on, and fully rely on LLMs reached roughly 4:12:1.
2308.03333_1891225_3	In addition, by combining heterogeneous knowledge and recommendation tasks, instruction tuning is performed on LLM for personalized recommendations.
2308.03929_1891821_3	We aim to achieve fact-checking of the knowledge embedded in biological graphs that were contrived from ChatGPT contents at the aggregate level.
2308.04709_1892601_8	The study was conducted to evaluate the ability of LLM models to provide correct answers to nephSAP (Nephrology Self-Assessment Program) multiple-choice questions.
2308.05201_1893093_0	  Large Language Model (LLM) based generative AI, such as ChatGPT, is considered the first generation of Artificial General Intelligence (AGI), exhibiting zero-shot learning abilities for a wide variety of downstream tasks.
2308.05374_1893266_7	Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs.
2308.05596_1893488_3	Currently, we are witnessing a shift in the approach to tackling societal issues online, particularly leveraging large language models (LLMs) like GPT-3 or T5 that are trained on vast corpora and have strong generalizability.
2308.06373_1894265_0	  The fundamental principles, potential applications, and ethical concerns of ChatGPT are analyzed and discussed in this study.
2308.06463_1894355_2	In this study, we discover that chat in cipher can bypass the safety alignment techniques of LLMs, which are mainly conducted in natural languages.
2308.07317_1895209_2	Specifically, the Platypus family achieves strong performance in quantitative LLM metrics across model sizes, topping the global Open LLM leaderboard while using just a fraction of the fine-tuning data and overall compute that are required for other state-of-the-art fine-tuned LLMs.
2308.07462_1895354_9	Therefore, further research is needed to understand how the use of ChatGPT and more broadly generative AI tools will affect the vocabulary and lexical richness in different types of text and languages.
2308.09067_1896959_6	The sexist bias prevalent in human text is also expressed by LLMs, and even magnified in all of them but one.
2308.09267_1897159_5	We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths.
2308.09936_1897828_3	These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs.
2308.11526_1899418_6	Motivated by the success of LLMs in specific domains like science and biology, this paper introduces a LLM for log data which is trained on public and proprietary log data.
2308.12415_1900307_6	The results of the case study demonstrate the positive causal influence of prompt semantics on ChatGPT's generative performance by an average treatment effect of $\approx 3\%$. Moreover, it was found that confounders such as prompt size are highly correlated with accuracy metrics ($\approx 0.412\%$).
2308.14120_1902012_2	Real-world clinical datasets and study details from large trials across various medical specialties were presented to ChatGPT ADA without specific guidance.
2308.14337_1902229_4	Specifically, we show that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent.
2308.15276_1903168_10	Further research is needed to fully harness the potential of LLMs like ChatGPT for practical fault localisation applications.
2309.00237_1905041_7	This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals.
2309.01522_1906326_4	Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users.
2309.02077_1906881_5	To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets.
2309.02077_1906881_6	A medical consultation training set is further constructed to improve the consultation ability of LLMs.
2309.02726_1907530_4	A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation.
2309.03079_1907883_7	A Machine Learning model is then trained with LLM outputs as features.
2309.03087_1907891_6	Our results showed that nearly half of the solutions provided with the support of ChatGPT were mistakenly assumed to be correct by the students, indicating that they overly trusted ChatGPT even in their field of expertise.
2309.03914_1908718_2	This comprehensive dataset is derived from shared ChatGPT conversations collected from GitHub and Hacker News, providing a rich resource for understanding the dynamics of developer interactions with ChatGPT, the nature of their inquiries, and the impact of these interactions on their work.
2309.04198_1909002_3	Our study focuses on how the dual logic ability of LLMs is affected during the privatization process in the medical domain.
2309.04316_1909120_6	The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement.
2309.05454_1910258_2	Our extensive findings provide empirical proof of how globally recognized models like ChatGPT may be considered less effective and may require more refined prompts for these generative tasks compared to other open-sourced models such as BLOOMZ and FlanT5--which have shown promising results.
2309.05918_1910722_0	  In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic na-ture, whatever 'knowledge' these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambi-guities, intensional contexts.
2309.06687_1911491_6	Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process.
2309.06794_1911598_4	Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future.
2309.07689_1912493_4	We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights
2309.08491_1913295_4	These results demonstrate that the knowledge of LLMs varies significantly depending on the domain and that further experimentation is required to determine the circumstances under which LLMs can be used for automatic Knowledge Base (e.g., Wikidata) completion and correction.
2309.08859_1913663_3	Existing learning rate policies are primarily designed for training traditional deep neural networks (DNNs), which may not work well for LLM fine-tuning.
2309.09120_1913924_1	In this paper, we conducted a content analysis of social media discussions to gauge public perceptions of gender bias in LLMs which are trained in different cultural contexts, i.e., ChatGPT, a US-based LLM, or Ernie, a China-based LLM.
2309.09120_1913924_3	A difference between the two LLMs was seen -- ChatGPT was more often found to carry implicit gender bias, e.g., associating men and women with different profession titles, while explicit gender bias was found in Ernie's responses, e.g., overly promoting women's pursuit of marriage over career.
2309.10066_1914870_9	Based on these metrics, the fine-tuned PEGASUS model was selected as the top LLM.
2309.10092_1914896_6	To address it, we propose HERACLEs, a hierarchical neuro-symbolic planner that relies on a novel integration of (i) existing symbolic planners generating high-level task plans determining the order at which the NL sub-tasks should be accomplished; (ii) pre-trained Large Language Models (LLMs) to design sequences of robot actions based on these task plans; and (iii) conformal prediction acting as a formal interface between (i) and (ii) and managing uncertainties due to LLM imperfections.
2309.10254_1915058_1	While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted.
2309.10371_1915175_0	  A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are.
2309.10371_1915175_2	It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources.
2309.10371_1915175_4	Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.
2309.11166_1915970_1	In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs.
2309.11231_1916035_3	In this study, the features and capabilities of ChatGPT-4 are analyzed in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish.
2309.11231_1916035_4	Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors.
2309.11674_1916478_3	In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on.
2309.11805_1916609_6	Inspired by the superior performance of LLMs, we leverage their capability to understand natural language for capturing the information that was previously getting lost during the conversion of unstructured data to structured form.
2309.12161_1916965_5	Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4.
2309.12348_1917152_2	Answers to the above questions were solicited from ChatGPT itself, the responses were collected, and then the recent literature was surveyed to determine whether or not the responses are supported.
2309.12570_1917374_3	Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators.
2309.13243_1918047_2	The students were asked to revise their essays through dialogues with ChatGPT.
2309.13963_1918767_3	Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures.
2309.14348_1919152_4	RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM.
2309.14379_1919183_6	The replications among the experiments also illustrate how tasks previously requiring protracted team effort or complex computational pipelines can now be accomplished by an LLM-assisted scholar in a fraction of the time.
2309.16134_1920938_5	Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation.
2309.16697_1921501_11	Logical and critical thinking are needed to validate the result presented by ChatGPT.
2310.00194_1922451_3	To improve planning with LLMs, we propose an agentic architecture, the Modular Agentic Planner (MAP), in which planning is accomplished via the recurrent interaction of the specialized modules mentioned above, each implemented using an LLM.
2310.00603_1922860_7	After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters.
2310.01423_1923680_3	However, more study is needed to see how effective they are for multi-domain ChatGPT material.
2310.01424_1923681_6	To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first SoK on data privacy for LLMs.
2310.01434_1923691_2	This article presents an innovative approach to LLM inference, envisioning a future where LLMs with billions of parameters can be executed directly on mobile devices without network connectivity.
2310.01783_1924040_5	However, the utility of LLM-generated feedback has not been systematically studied.
2310.01957_1924214_3	A distinct pretraining strategy is devised to align numeric vector modalities with static LLM representations using vector captioning language data.
2310.01960_1924217_3	More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements.
2310.02003_1924260_4	Each instruction in turn is executed by a separate LLM agent, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective interaction with the file store.
2310.02277_1924534_1	It has been believed that weights in LLMs contain significant redundancy, leading to the conception that a considerable chunk of the parameters can be removed by pruning without compromising performance.
2310.03473_1925730_4	The scheme is designed with a novel coverage and coherence intuitive policy, which is duly rewarded by a passively trained LLM.
2310.03691_1925948_2	This idea is exemplified in DirectGPT, a user interface layer on top of ChatGPT that works by transforming direct manipulation actions to engineered prompts.
2310.04408_1926665_5	Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.
2310.04668_1926925_6	Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes.
2310.04698_1926955_7	The code is then executed by an LLM agent in a local environment and .
2310.05216_1927473_2	In this work, we probe LLMs from a human behavioral perspective, correlating values from LLMs with eye-tracking measures, which are widely recognized as meaningful indicators of human reading patterns.
2310.06474_1928731_1	Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English.
2310.08164_1930421_6	Our probes are trained on a condensed, sparse and interpretable representation of LLM activations, making it easier to correlate features of the input with our probe's predictions.
2310.08535_1930792_4	Our declarative approach, in which the behavior is described without concern for how it should be implemented or enforced, enables rapid design, implementation, and experimentation with different LLM-based agents.
2310.08885_1931142_5	Moreover, the effectiveness of LLMs in TODS is further supported by our comprehensive evaluations on TODS subtasks: dialogue state tracking, intent classification, and response generation.
2310.08915_1931172_6	This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs.
2310.09130_1931387_5	Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters.
2310.09909_1932166_6	All images used in this report can be found in https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation.
2310.10826_1933083_5	Our design is supported by demonstrations on a publicly available LLM.
2310.11409_1933666_8	The current version of the LLM-guided privilege-escalation prototype can be found at https://github.com/ipa-labs/hackingBuddyGPT.
2310.11532_1933789_3	This correction task is formulated as a multi-step rule-based LLM reasoning process, which uses explicitly written rules in prompts to decompose the task into concrete reasoning steps.
2310.12418_1934675_5	We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research.
2310.12836_1935093_5	To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning.
2310.12973_1935230_7	This hypothesis is empirically supported by the observation that the feature activation, after training with LLM transformer blocks, exhibits a stronger focus on relevant regions.
2310.13229_1935486_1	However, details about LLM training data are often not made public, which has caused concern as to whether existing bug benchmarks are included.
2310.13549_1935806_1	Large Language Models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs.
2310.13561_1935818_1	To curtail the frequency of these calls, one can employ a smaller language model -- a student -- which is continuously trained on the responses of the LLM.
2310.14724_1936981_3	The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task.
2310.15007_1937264_10	Taken together, our results show that accurate document-level membership can be inferred for LLMs, increasing the transparency of technology poised to change our lives.
2310.15051_1937308_3	Afterwards, using the provided dataset, an evaluation is conducted to assess the capabilities of LLMs, including GPT-3.5 and GPT-4.
2310.15896_1938153_4	To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health suggestions polished by ChatGPT.
2310.15941_1938198_4	We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained.
2310.16253_1938510_7	While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods.
2310.16301_1938558_2	An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks.
2310.16789_1939046_4	In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text?
2310.18373_1940630_6	This dataset allows for the evaluation of LLMs in a new context, as they are predominantly designed and trained on data from high-income North American countries.
2310.19813_1942070_8	Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
2310.20381_1942638_1	For the evaluation, a set of prompts is designed for each task to induce the corresponding capability of GPT-4V to produce sufficiently good outputs.
2311.01740_1944707_1	To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone.
2311.02105_1945072_6	Specifically, we introduce ``security vectors'', a few new parameters that can be separated from the LLM, to ensure LLM's responses are consistent with the harmful behavior.
2311.02126_1945093_1	Recently, significant progress has been made in the development of Vision Language Models (VLMs), expanding the capabilities of LLMs and enabling them to execute more diverse instructions.
2311.03287_1946254_4	Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented.
2311.04205_1947172_3	While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped.
2311.04368_1947335_3	It was conducted by administering questionnaires in the form of test papers through the LLM network interface, with the valuable participation of volunteers.
2311.04547_1947514_4	In contrast, a negative correlation was found between LM size and reading time fit of linear mixed-effects models using LM surprisal as a predictor, with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model without surprisal.
2311.05374_1948341_9	We have made publicly available the task tree, TencentLLMEval dataset, and evaluation methodology which have been demonstrated as effective in assessing the performance of Tencent Hunyuan LLMs.
2311.06180_1949147_7	Students were asked to rate the correctness and usefulness of each feedback, and to indicate which one was generated by GPT.
2311.06427_1949394_4	The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness.
2311.06825_1949792_2	Maximum ratio transmission (MRT) and matched-filtering (MF) precoding techniques are adopted at the satellite separately for the common messages (CMs) and for the private messages (PMs), which are both implemented based on the estimated LMS channels suffering from the Shadowed-Rician fading.
2311.06981_1949948_0	  Ever since the public release of ChatGPT in November 2022, serious concerns have been raised about the impact and potentially dire consequences of the increasingly widespread use of generative AI tools for purposes of scientific writing and publishing.
2311.07418_1950385_4	Experiments were performed on SLURP to quantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and Vicuna-13B (v1.1 and v1.5) with different ASR error rates.
2311.07582_1950549_6	However, further development and validation are still required before the promise of LLMs in accelerating biological discovery can be realized.
2311.08268_1951235_0	  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses.
2311.08287_1951254_5	Furthermore, a case study on the training dynamics of the LLMs reveals that the majority of syntactic knowledge is learned during the initial stages of training, hinting that simply increasing the number of training tokens may not be the `silver bullet' for improving the comprehension ability of LLMs.
2311.09247_1952214_0	  We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts.
2311.09308_1952275_2	Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language.
2311.09651_1952618_2	However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students.
2311.09651_1952618_6	However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students.
2311.09709_1952676_2	While such modifications have been proven effective in tasks like machine translation, tailoring them to LLMs demands specific modifications given the diverse nature of LLM applications.
2311.10372_1953339_2	A considerable portion of Code LLMs is derived from general LLMs through model fine-tuning.
2311.10372_1953339_6	We aim to address three questions: (1) What LLMs are specifically designed for software engineering tasks, and what is the relationship between these Code LLMs?
2311.10961_1953928_2	One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual.
2311.11811_1954778_1	A methodology is developed to explore the potential use of LLMs for translating the explanations produced by rule-based systems, from high-level programming languages to natural language, allowing all users a fast, clear, and accessible interaction with such technologies.
2311.12188_1955155_0	  Reinforcement learning-based large language models, such as ChatGPT, are believed to have potential to aid human experts in many domains, including healthcare.
2311.13720_1956687_1	To set the stage for this union, we explore two different flavors of model space problems that have been studied in the AI planning literature and explore the effect of an LLM on those tasks.
2311.14711_1957678_1	These decisions should not be left solely in the hands of frontier LLM developers.
2311.14711_1957678_4	Though there are encouraging signs of increasing external scrutiny of frontier LLMs, its success is not assured.
2311.14722_1957689_3	The generated program is then executed by a program interpreter, thus mitigating the limitations of LLM in performing accurate arithmetic calculations.   
2311.17295_1960262_5	If the current use of Elo scores is intended to substitute the costly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible.
2311.17474_1960441_5	The insight is that language understanding and tool usage are both required for network LLMs.
2311.18021_1960988_2	While ICL has been extensively studied on LLMs, its research on MLLMs remains limited.
2311.18041_1961008_4	We show that the summaries generated by models depend on the instructions and the performance of LLMs vary with different instructions sometimes resulting steep drop in ROUGE scores if prompts are not selected carefully.
2311.18743_1961710_7	Since its release, AlignBench has been adopted by top (Chinese) LLMs for evaluating their alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi, Baichuan, and Abab.
2312.00353_1962161_2	Two research questions are formulated to investigate the accuracy of LLMs in recalling information from pre-training knowledge graphs and their ability to infer knowledge graph relations from context.
2312.00678_1962486_2	To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs.
2312.01858_1963666_1	To manage the knowledge acquired by LLMs, we need to ensure that the editing of learned facts respects internal logical constraints, which are known as dependency of knowledge.
2312.03052_1964860_5	VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one.
2312.03088_1964896_3	The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM.
2312.03815_1965623_2	Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem.
2312.04032_1965840_2	Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives.
2312.04828_1966636_9	In our black-box setting, all fingerprinting steps are internally conducted by the LLMs owners.
2312.05275_1967083_6	Also, certain limitations of ChatGPT in security-related tasks are identified, such as its constrained ability to process long code contexts.
2312.05603_1967411_2	The core idea of Sim-GPT is to generate data with STS labels using GPT-4, based on which an STS model is trained.
2312.06351_1968159_4	However, quantitative research has not been conducted on how accurately different types of LLMs can handle these problems.
2312.07592_1969400_7	Multiple experiments, exploring response hallucination and considering question complexity, were conducted on ChatGPT.
2312.07876_1969684_0	  Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted in many safety-critical applications.
2312.08027_1969835_2	To alleviate this, we use an interpretable structure to explain the prompt learning principle in LLMs, which certificates that the effectiveness of language models is determined by position changes of the task's related tokens.
2312.08055_1969863_4	The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.
2312.09007_1970815_4	Complex tasks, which may involve collaborations of multiple domain-specific AI modules and IoT devices, are executed through a control script generated by the LLM using a Language-Code transformation approach, which first converts language descriptions to an intermediate finite-state machine (FSM) before final precise transformation to code.
2312.09801_1971609_5	The students in each course were asked to prompt an LLM of their choice with one question from a set of four and required to affirm or refute statements in the LLM output by using peer-reviewed references.
2312.10055_1971863_7	This work demonstrates the potential for LLM-generated feedback, but further research is required to explore its practical implementation.
2312.10075_1971883_3	While the WVS is accepted as an explicit assessment of values, we lack methods for assessing implicit moral and cultural values in media, e.g., encountered in social media, political rhetoric, narratives, and generated by AI systems such as LLMs that are increasingly present in our daily lives.
2312.10631_1972439_8	The feasibility of LLM-Twin is demonstrated by numerical experiments and case studies.
2312.10793_1972601_1	However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood.
2312.11701_1973509_2	The wide-ranging capabilities of LLMs are examined in the context of the building energy field, including intelligent control systems, code generation, data infrastructure, knowledge extraction, and education.
2312.12683_1974491_4	In experiments across four LLMs, we find that multilingual instruction tuning with as few as two to three languages is both necessary and sufficient to elicit effective cross-lingual generalisation, with the limiting factor being the degree to which a target language is seen during pretraining.
2312.13096_1974904_9	These findings highlight the potential of LLM-based chatbots in tackling different forms of false information in online environments, but also points to the substantial variation in terms of how such potential is realized due to specific factors, such as language of the prompt or the topic.
2312.17259_1979067_4	The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links.
2312.17276_1979084_0	  The recent trend of large language models (LLMs) is to increase the scale of both model size (\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama.
2312.17485_1979293_5	In this study, various prompts are designed and compared across mainstream LLMs using two distinct datasets from human reviewers and automated checkers.
2312.17493_1979301_3	Motivated by this, we investigate how data privacy can be ensured in LLM fine-tuning through practical federated learning approaches, enabling secure contributions from multiple parties to enhance LLMs.
2401.00627_1980183_3	The student data is derived from think-aloud interviews of introductory students and the AI data comes from ChatGPT's solutions collected using Zero shot approach.
2401.02051_1981607_4	They are then translated into executable codes by LLMs.
2401.02984_1982540_6	Diverse applications of LLMs in mental health care are identified, including diagnosis, therapy, patient engagement enhancement, etc.
2401.03630_1983186_3	In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs.
2401.03729_1983285_3	In this work, we ask: do variations in the way a prompt is constructed change the ultimate decision of the LLM?
2401.03855_1983411_5	The robustness of our benchmark is demonstrated by the poor performance of existing Code-LLMs.
2401.05908_1985464_5	Our model is trained from the pre-trained LLM by fine-tuning technique using datasets from the epilepsy domain.
2401.06102_1985658_3	We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework.
2401.06640_1986196_6	However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computational principles of semantic property inference are yet to be mastered by LMs.
2401.06853_1986409_5	A synthetic dataset (TGQA), which is fully controllable and requires minimal supervision, is constructed for fine-tuning LLMs on this text-to-TG translation task.
2401.07181_1986737_3	The RL agent is then deployed in these scenarios, and a reward model is learnt through the LLM preferences and feedback.
2401.07237_1986793_4	This can be viewed as a mechanism of distilling event sequence knowledge from LLMs.
2401.09083_1988639_5	Considering that LLM is trained with natural language and is not capable of directly perceiving visual concepts as contained in remote sensing images, we designed visual cues that inject visual information into ChatGPT.
2401.10360_1989914_1	A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload.
2401.10444_1989998_0	  The paper discusses what is needed to address the limitations of current LLM-centered AI systems.
2401.12773_1992327_3	Here, we report the results of a large-scale pre-registered online experiment (N = 3,552) indicating diminished fairness, trust, trustworthiness, cooperation, and coordination by human players in economic twoplayer games, when the decision of the interaction partner is taken over by ChatGPT.
2401.13588_1993142_13	Conclusion: A comprehensive qualitative performance evaluation framework for LLMs is developed and operationalized.
2401.13641_1993195_6	Experiments are carried out in order to evaluate the performance and robustness of ChatGPT, using popular public benchmarks and comparing the results with state-of-the-art methods in the field.
2401.15269_1994823_2	To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation.
2401.16186_1995740_4	Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.   
2401.16587_1996141_3	However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues.
2401.16960_1996514_8	The final prediction of the equivalent entity is derived from the LLM's output.
2401.17390_1996944_4	The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself.
2401.17459_1997013_3	Such improvement can be made, as a first step, by engineering prompts fed to the LLM based on the responses produced, to include relevant contexts and structures so that the model provides more accurate results.
2401.17461_1997015_8	The evaluation results show an overall good quality of the dialogues, though research is still needed to improve the quality of the GPT-4 evaluation metrics.
2401.17622_1997176_6	We compare the results obtained with ChatGPT to previous automatic commit message generation methods that have been trained specifically on commit data.
2401.18028_1997582_2	However, performance evaluations of LLMs for such tasks are still needed, including examining the general quality of generated impacts but also the range of types of impacts produced and resulting biases.
2402.00024_1997663_0	  Purpose: Large Language Models (LLMs) like GPT (Generative Pre-trained Transformer) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI are increasingly recognized for their potential in the field of cheminformatics, particularly in understanding Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures.
2402.00350_1997989_5	In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024.
2402.00858_1998497_1	However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features.
2402.01158_1998797_8	Furthermore, since LLM-Detector is trained based on open-source LLMs, it is easy to customize for deployment.
2402.01684_1999323_0	  With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain.
2402.01723_1999362_4	However, the accuracy and robustness of LLMs in industrial scenarios have not been well studied.
2402.01731_1999370_1	In this context, validity examinations were conducted on data sets generated according to the Two-Parameter Logistic Model (2PLM) with algorithms written by ChatGPT 3.5 and researchers.
2402.01748_1999387_1	However, recent efforts on LLMs for wireless networks are limited to a direct application of existing language models that were designed for natural language processing (NLP) applications.
2402.01750_1999389_5	To ensure the effective utilization of LLM in communication, a knowledge base is designed to supplement the necessary knowledge, dedicated prompts are introduced to facilitate understanding of pragmatic communication scenarios and task requirements, and a chain of thought is designed to assist in making reasonable trade-offs between transmission efficiency and cost.
2402.01817_1999456_7	We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs.
2402.01874_1999513_8	We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.
2402.01881_1999520_2	To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization).
2402.02008_1999647_6	Interestingly, we find that between ~50% to 90% of LLM responses are not fully supported by the sources they provide.
2402.02136_1999775_5	Moreover, whenever the user intent is correctly recognized, while users are more satisfied with the intent-based reformulations of GPT-4 compared to GPT-3.5, they tend to be more satisfied with the models' answers to their original prompts compared to the reformulated ones.
2402.02834_2000473_8	Code and models can be found at: https://github.com/Nota-NetsPresso/shortened-llm
2402.02987_2000626_0	  Significant advancements have recently been made in large language models represented by GPT models.
2402.03009_2000648_1	Although there exist various methods devoted to enhancing the long-context processing ability of LLMs, they are developed in an isolated manner and lack systematic analysis and integration of their strengths, hindering further developments.
2402.03435_2001074_1	The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy.
2402.03719_2001358_9	The applicability of LaMAI is further evidenced by its successful integration with various LLMs, highlighting its potential for the future of interactive language models.
2402.03744_2001383_5	Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.
2402.04110_2001749_3	The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4.
2402.04110_2001749_8	This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5.
2402.05110_2002749_1	We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30).
2402.05445_2003084_0	  The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware.
2402.06360_2003999_4	In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents.
2402.07179_2004818_1	Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs.
2402.07179_2004818_2	However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied.
2402.07945_2005584_1	The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent.
2402.08113_2005752_6	Our analysis revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias.
2402.09091_2006730_3	However their explicitly mention of malicious intent will be easily recognized and defended by LLMs.
2402.09216_2006855_5	This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility of LLM-based approaches.
2402.09649_2007288_5	The protein first undergoes protein encoders and PLP-former to produce protein embeddings, which are then projected by the adapter to conform with the LLM.
2402.10083_2007722_10	Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical inaccuracies in the LLM-generated responses, which were appropriately identified by the GPT-4 evaluation.
2402.10412_2008051_4	In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), an innovative hallucination metric that is specifically designed for the scenario when gold-standard answers are absent.
2402.10767_2008406_3	Extensive experiments are conducted on Causal Question Answering (CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2).
2402.10835_2008474_7	This observation can be explained by the ability of LLMs to recognize the underlying period within datasets, which is supported by our experiments.
2402.10835_2008474_8	In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases substantially improve the predictive performance of LLMs for time series.
2402.11005_2008644_4	To further illustrate the theory, we demonstrate that concept prototypes in LLMs are affected by prescriptive norms, similar to the concept of normality in humans.
2402.11060_2008699_3	In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization.
2402.11753_2009392_2	However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics.
2402.11753_2009392_5	In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics.
2402.12212_2009851_2	The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities.
2402.12566_2010205_3	GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support.
2402.12801_2010440_2	Named Entity Recognition (NER) is a critical task in information extraction that is not covered in recent LLM benchmarks.
2402.14007_2011646_2	Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.
2402.14007_2011646_3	Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.
2402.14261_2011900_7	These success metrics are designed to evaluate the performance of LLMs within a given IDE and its respective parameter space.
2402.14296_2011935_4	Therefore, in this paper, we propose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel calibration network is devised to calibrate potential bias in the stance prediction of LLMs.
2402.14805_2012444_0	  As Large Language Models (LLMs) are integrated with human daily applications rapidly, many societal and ethical concerns are raised regarding the behavior of LLMs.
2402.14845_2012484_1	Despite substantial efforts devoted to data cleaning and curation, well-constructed LLMs have been reported to suffer from copyright infringement, data poisoning, and/or privacy violations, which would impede practical deployment of LLMs.
2402.14846_2012485_2	We argue that context-dependence (specifically, value stability) should be studied as a specific property of LLMs and used as another dimension of LLM comparison (alongside others such as cognitive abilities, knowledge, or model size).
2402.14848_2012487_1	Despite LLMs advancements in recent times, their performance consistency across different input lengths is not well understood.
2402.14903_2012542_2	With the increased use of LLMs for reasoning, various number-specific tokenization schemes have been adopted, with popular models like LLaMa and PaLM opting for single-digit tokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and 3-digit numbers.
2402.14904_2012543_2	We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM.
2402.15911_2013550_2	More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM.
2402.15929_2013568_2	However, existing evaluations of LLMs on knowledge comprehension are typically conducted on small test sets, but these datasets represent only a tiny fraction of the vast number of possible queries.
2402.16539_2014178_7	These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures.
2402.17097_2014736_4	Re-Ex revises the initial response of LLMs using 3-steps : first, external tools are used to retrieve the evidences of the factual errors in the initial LLM response; next, LLM is instructed to explain the problematic parts of the response based on the gathered evidence; finally, LLM revises the initial response using the explanations provided in the previous step.
2402.17385_2015024_7	Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes.
2402.17887_2015526_3	Unlike previous methods in RAG where the retrieval model was trained separately from the LLM, we introduce JMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning phase.
2402.18120_2015759_0	  Prior research has revealed that certain abstract concepts are linearly represented as directions in the representation space of LLMs, predominantly centered around English.
2402.18439_2016078_2	NL's status as the optimal format for LLMs, particularly in single-LLM reasoning and multi-agent communication, has not been thoroughly examined.
2402.19366_2017004_2	A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations.
2403.00858_2017977_2	However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding.
2403.00863_2017982_3	Nevertheless, varying strengths and weaknesses are exhibited by different LLMs due to the diversity in data, architectures, and hyperparameters.
2403.00952_2018071_0	  Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine).
2403.01241_2018360_3	Such outliers are found to allocate most of the attention scores on initial tokens of input, termed as pivot tokens, which are crucial to the performance of quantized LLMs.
2403.01422_2018541_5	The pipeline is carefully designed to control the style of videos by improving textual inversion technique with powerful text generation capability of GPT-4.
2403.01586_2018705_7	The function is then deduced by LLMs and zero-shot classification from a predefined catalog of IoT functions.   
2403.02454_2019573_4	Three prototype games are designed across 3 different genres: (1) a minimalist base game, (2) a game with features and game feel elements added by a human game designer, and (3) a game with features and feel elements directly implemented from prompted outputs of the LLM, ChatGPT.
2403.02502_2019621_2	This learning method is designed to enhance the performance of open LLM agents.
2403.02839_2019958_3	While the fine-tuned judge models are claimed to achieve comparable evaluation capability with GPT-4, in this work, we conduct an empirical study of judge models.
2403.04013_2021132_0	  Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models.
2403.04784_2021903_4	Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets.
2403.05266_2022385_1	Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity.
2403.06465_2023584_2	The new generation of recommender systems, empowered by LLMs, are expected to be more versatile, explainable, conversational, and controllable, paving the way for more intelligent and user-centric recommendation experiences.
2403.06586_2023705_5	In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed.
2403.06660_2023779_6	Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.
2403.07747_2024866_2	FineMath is created to cover the major key mathematical concepts taught in elementary school math, which are further divided into 17 categories of math word problems, enabling in-depth analysis of mathematical reasoning abilities of LLMs.
2403.08399_2025518_12	The code for this project can be found on the GitHub repository at https://github.com/GPT-Laboratory/SLR-automation.
2403.09162_2026281_1	However, the comprehensive effects of fine-tuning on the LLMs' generalization ability are not fully understood.
2403.10434_2027553_4	These spotted signs are then passed to an LLM, which transforms them into coherent and contextually appropriate spoken language sentences.
2403.11322_2028441_4	The transitions between states are controlled by heuristic rules or decisions made by the LLM, allowing for a dynamic and adaptive progression.
2403.11439_2028558_5	In this work, we first introduce a stylized dialogue dataset StyleEval with 38 styles by leveraging the generative power of LLMs comprehensively, which has been carefully constructed with rigorous human-led quality control.
2403.12601_2029720_5	LHMKE is designed to provide a comprehensive evaluation of the knowledge acquisition capabilities of Chinese LLMs.
2403.12958_2030077_10	Overall, our results show that knowledge cutoffs are not as simple as they have seemed and that care must be taken both by LLM dataset curators as well as practitioners who seek to use information from these models.
2403.13073_2030192_5	This paper focuses on Jewish Americans as a case study, but it is probable that other minority communities (e.g., Asian Americans, Hindu Americans) may be similarly affected and, most importantly, the results should likely be interpreted as a "canary in the coal mine" that highlights deep structural concerns about the current LLM paradigm whose harms could soon affect nearly everyone.
2403.13334_2030453_2	Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance.
2403.13381_2030500_3	The potential of the VS-LMS adaptation algorithms using a DAG is then illustrated by experimental results obtained on a relevant adaptive active noise attenuation system.
2403.14643_2031762_7	However, the use of ChatGPT has also raised several concerns, including ethical, social, and employment challenges, which must be carefully considered to ensure the responsible use of this technology.
2403.14643_2031762_11	This study is expected to contribute to a greater understanding of ChatGPT and aid in predicting the potential changes it may bring about.
2403.14938_2032057_1	Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings.
2403.15371_2032490_5	Although these findings can be interpreted positively, they suggest that external summarization -- which may not be possible in more complex settings -- is important for obtaining desirable behavior from LLM agents.
2403.15491_2032610_6	These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.
2403.15501_2032620_7	The study also recorded an average response time of 6.12 seconds for the ChatGPT API, which is considered acceptable but has room for improvement.
2403.16446_2033565_10	Extensive experiments are conducted to demonstrate the effectiveness of the proposed approach, providing more insights for LLMs' safe and reliable deployments in clinical practice.
2403.17125_2034244_8	Our results suggest that caution is needed when using ICL with larger LLMs for affect-centered tasks outside their pre-training domain and when interpreting ICL results.
2403.17160_2034279_3	The conversational agent was developed based on the GPT-3.5 turbo model.
2403.17336_2034455_4	Due to the rapid development of LLMs and their ease of access via natural languages, the frontline of jailbreak prompts is largely seen in online forums and among hobbyists.
2403.18341_2035460_6	Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM.
2403.19181_2036300_5	ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks.
2403.19790_2036909_6	Our model is able to deliver triage recommendations consistent with existing clinical practices and it's architecture was implemented on a single GPU, making it practical for implementation in resource-limited NHS environments where private implementations of LLM technology will be necessary to ensure confidential clinical data is appropriately controlled and governed.
2403.19876_2036995_5	This lack of action and reliance on workarounds was explained through the perceived lack of control and distributed responsibility in the LLM supply chain, the conditional nature of engaging with ethics, and competing priorities.
2403.20330_2037449_2	The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs.
2404.00216_2037667_2	Extensive efforts have been made to enable better outputs from LLMs by mitigating hallucinations through factuality enhancement methods.
2404.00344_2037795_4	Second, a case analysis is conducted on the LLM that showed the highest performance, focusing on the quality and accuracy of its answers through manual evaluation.
2404.01096_2038547_7	We also present a novel framework for whole-program transformations that leverages lightweight static analysis to break the transformation into smaller steps that can be carried out effectively by an LLM.
2404.01475_2038926_1	However, we possess only a limited systematic understanding of the chemical capabilities of LLMs, which would be required to improve models and mitigate potential harm.
2404.01549_2039000_6	Octopus, the fine-tuned model, is proved to have better performance than GPT-4 for the software APIs calling.
2404.01799_2039250_1	While these benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?).
2404.02151_2039602_2	In this way, we achieve 100% attack success rate -- according to GPT-4 as a judge -- on Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B, Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B, GPT-3.5, GPT-4o, and R2D2 from HarmBench that was adversarially trained against the GCG attack.
2404.02403_2039854_8	Additionally, we observe improved performance when test sets are translated to English before inputting them into GPT-3.5.
2404.02421_2039872_1	In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible.
2404.02438_2039889_6	multiPPI++ recovers ground truth estimates, regardless of which NLP model produced predictions and regardless of whether they were produced by a more accurate predictor like GPT-4-32k or a less accurate predictor like KNN.
2404.02575_2040026_9	Also, we show that compared to natural language, pseudocode can better guide the reasoning of LMs, even though they are trained to follow natural language instructions.
2404.02893_2040344_1	While many strategies and datasets to enhance LLMs' mathematics are developed, it remains a challenge to simultaneously maintain and improve both language and mathematical capabilities in deployed LLM systems.
2404.03411_2040862_4	Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models.
2404.03602_2041053_1	However, little research has been conducted on error detection of LLM responses.
2404.04286_2041737_4	We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades.
2404.04286_2041737_6	This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs.
2404.05182_2042633_3	Due to the scale of LLM, PEFT operations are usually executed in the public environment (e.g., cloud server).
2404.06209_2043660_4	We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training.
2404.07108_2044559_4	It is determined by counting the revision edits generated by LLMs.
2404.07475_2044926_3	Studies of bias in LM responses to open-ended prompts (where identity classifications are left unspecified) are lacking and have not yet been grounded in end-consumer harms.
2404.07475_2044926_7	We also document a prevalence of stereotypes (e.g. perpetual foreigner) in LM-generated outputs that are known to trigger psychological harms that disproportionately affect minoritized individuals.
2404.07499_2044950_8	Further validation results indicate that the number of user rating histories provided to LLM prompts should be carefully chosen to avoid both insufficient and excessive inputs and that the output of LLMs that show high classification performance is difficult to interpret.
2404.08627_2046078_3	We find that large language models (LLMs), represented by ChatGPT, are having an increasing impact on arXiv abstracts, especially in the field of computer science, where the fraction of LLM-style abstracts is estimated to be approximately 35%, if we take the responses of GPT-3.5 to one simple prompt, "revise the following sentences", as a baseline.
2404.08676_2046127_4	It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy.
2404.09248_2046699_3	This paper introduces a novel method, Knowledgeable Agents from Language Model Rollouts (KALM), which extracts knowledge from LLMs in the form of imaginary rollouts that can be easily learned by the agent through offline reinforcement learning methods.
2404.09329_2046780_6	In contrast with previous research, no significant difference was found in the emotional content produced by LLMs and humans.
2404.10209_2047660_3	DB-GPT is designed to understand data interaction tasks described by natural language and provide context-aware responses powered by LLMs, making it an indispensable tool for users ranging from novice to expert.
2404.10229_2047680_4	The main goal of LLM-Stega is that the secure covert communication between Alice (sender) and Bob (receiver) is conducted by using the user interfaces of LLMs.
2404.10508_2047959_8	Those who are at the intersection of gender and racial minority groups--such as Black females--are consistently described by texts with lower levels of agency, aligning with real-world social inequalities; (3) Among the 3 LLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.
2404.10922_2048373_4	Our experiments, conducted on 1900 hours of transcribed data from 139 languages, establish that a multilingual speech representation can be effectively learned and aligned with a multilingual LLM.
2404.11343_2048794_4	Our main idea is to enable an LLM to directly leverage the collaborative knowledge contained in a pre-trained state-of-the-art CF-RecSys so that the emergent ability of the LLM as well as the high-quality user/item embeddings that are already trained by the state-of-the-art CF-RecSys can be jointly exploited.
2404.11502_2048953_1	For the wide application of LLMs, the inference efficiency is an essential concern, which has been widely studied in existing work, and numerous optimization algorithms and code libraries have been proposed to improve it.
2404.12038_2049489_4	Additionally, we find that our generated attack prompts may be transferable to GPT-4, and the embedding-level attacks may also be transferred to other white-box LLMs whose parameters are known.
2404.12549_2050000_0	  Large language models (LLMs) like ChatGPT have been widely adopted in work contexts.
2404.13340_2050791_3	Current research along this line primarily focuses on enhancing code generation with assistance from test cases generated by LLMs, while the performance of LLMs in test case generation alone has not been comprehensively examined.
2404.13571_2051022_7	Given annotations from LLMs, a two-stage training strategy is designed to tailor the test-time model with the limited and noisy labels.
2404.14928_2052379_5	Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability.
2404.15406_2052857_3	Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues.
2404.15458_2052909_0	  Large language models (LLMs) such as ChatGPT, Gemini, LlaMa, and Claude are trained on massive quantities of text parsed from the internet and have shown a remarkable ability to respond to complex prompts in a manner often indistinguishable from humans.
2404.15578_2053029_5	In particular, (1) the ability of LLMs in automating the process of extracting specific information such as root cause of a case from unstructured data, as well as (2) the possibility of identifying similar or related deviations by performing semantic search on the database of historical records are examined.
2404.15667_2053118_15	To recommend the use of LLMs in the screening process of SRs, more research is needed.
2404.16118_2053569_9	Honeywords generated by GPT-3.5 were found to be less distinguishable from real passwords compared to previous methods of automated honeyword generation.
2404.16308_2053759_1	However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.
2404.16653_2054104_5	It was evidenced that even the most sophisticated models, such as ChatGPT and Gemini, exhibit errors and deficiencies in their responses, with explanations often providing inconsistent.
2404.16807_2054258_3	However, the diversity aspect in LLM outputs has not been systematically studied before.
2404.16891_2054342_1	This innovation enhances the capabilities of LLMs, but it also introduces risks, as these plugins developed by various third parties cannot be easily trusted.
2404.17140_2054591_5	Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.
2404.17790_2055241_4	Consequently, Swallow achieved superior performance compared to other LLMs that were trained from scratch in English and Japanese.
2404.18373_2055824_4	This paper also presents the design framework of a network health assessment system based on LLM and focuses on its potential application value, through the case of network health management system, it is fully demonstrated that the 6G intelligent network system based on LLM has important practical significance for the comprehensive realization of intelligence.
2404.18466_2055917_3	Inspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute for full fine-tuning (FFT), to mitigate the forgetting issues, where half of the parameters are selected to learn new tasks while the other half are frozen to remain previous knowledge.
2404.18865_2056316_4	Our experiments establish where in the LLM the probe's predictions can be described as being conditional on the preceding (related) sentences.
2404.19509_2056960_7	Human raters were asked to rate the explanation of the implicatures generated by LLMs on their reasonability, logic and fluency.
2404.19737_2057188_0	  Large language models such as GPT and Llama are trained with a next-token prediction loss.
2405.00492_2057704_1	The temperature parameter of an LLM regulates the amount of randomness, leading to more diverse outputs; therefore, it is often claimed to be the creativity parameter.
2405.00545_2057757_0	  An approach is established for maximizing the Lower bound on the Mismatch capacity (hereafter abbreviated as LM rate), a key performance bound in mismatched decoding, by optimizing the channel input probability distribution.
2405.00728_2057940_3	The potential of ChatGPT to address the triage problem in emergency departments has been examined, while few studies have explored its application in outpatient departments.
2405.01592_2058804_1	With the introduction of ChatGPT, an evaluation of its simplification performance is needed.
2405.02466_2059678_4	Current intellectual property (IP) protection schemes for LLMs are either designed for white-box settings or require additional modifications to the original model, which restricts their use in real-world settings.   
2405.05080_2062292_1	These perspectives are derived from the initial results of a sub-study employing vignettes to showcase the existence of bias within black-box LLMs and explore methods for manipulating them.
2405.05418_2062630_3	To reduce excessive safety behaviours -- which was discovered to be 26.1% of safe prompts being misclassified as dangerous and refused -- we use a combination of XSTest dataset prompts as well as interactive, contextual, and few-shot prompting to examine the decision bounds of LLMs such as Llama2, Gemma Command R+, and Phi-3.
2405.05600_2062812_5	We further find that, depending on the LLM employed, new runs will be highly favored (or penalized), and this effect is magnified proportionally to the size of the holes.
2405.05647_2062859_2	While acknowledging the potential of LLMs in generating labels for computer vision, concerns are raised about the ethical implications of using patient data without explicit approval, highlighting the necessity of stringent data protection measures under GDPR.
2405.05777_2062989_4	ULRLs are also not supported by mainstream Large Language Models (LLMs) like ChatGPT, due to which gathering artificial training data for them becomes even more challenging.
2405.06211_2063423_8	Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/
2405.06410_2063622_7	Lastly, we are surprised to discover that significant overlap in the errors is made by both LLMs and untrained humans, accounting for almost 30% of all errors.
2405.06687_2063899_3	Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering.
2405.06694_2063906_3	Through extensive evaluations, SUTRA is demonstrated to surpass existing models like GPT-3.5, Llama2 by 20-30% on leading Massive Multitask Language Understanding (MMLU) benchmarks for multilingual tasks.
2405.06713_2063925_0	  The strategic significance of Large Language Models (LLMs) in economic expansion, innovation, societal development, and national security has been increasingly recognized since the advent of ChatGPT.
2405.06808_2064020_3	A case study was conducted to assess the performance of various LLMs, demonstrating that GPT-4 outperforms other models in processing and collecting necessary information, as well as executing mathematical calculations.
2405.06931_2064143_1	Recently, numerous studies have been conducted on tasks related to relevance judgment using Large Language Models (LLMs) such as GPT-4, demonstrating significant improvements.
2405.09186_2066398_2	The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable.
2405.10616_2067828_3	Yet, its application in LLMs has not been extensively studied.
2405.11002_2068214_4	With experiments on a real network intrusion detection dataset, in-context learning proves to be highly beneficial in improving the task processing performance in a way that no further training or fine-tuning of LLMs is required.
2405.11106_2068318_1	Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent.
2405.11290_2068502_4	MBIAS is designed to significantly reduce biases and toxic elements in LLM outputs while preserving the main information.
2405.11505_2068717_3	Following this, a preliminary experiment is presented as an example to demonstrate how HCD principles can be employed to enhance user experience within GPT by using a single document input to GPT's Knowledge base as a new knowledge resource to control the interactions between GPT and users, aiming to meet the diverse needs of hypothetical software learners as much as possible.
2405.11880_2069092_1	These effects are formulated as non-linear interactions between tokens/words encoded by the LLM.
2405.12842_2070054_5	This information is then utilized by LLMs to generate a sequence of actions that are executed by a scripting engine to complete an assigned task.
2405.12939_2070151_3	We identify this as a primary factor constraining the reasoning capabilities of LLMs, a limitation that cannot be resolved solely based on the predicted answers.
2405.13004_2070216_3	Each of the subproblems is formulated as an algebraic expression whose value is evaluated by the Python code generated by the LLM for the corresponding algebraic expression.
2405.13004_2070216_7	If the final answer matches the correct answer, it is produced as output else a refinement prompt is fed to the LLM.
2405.13820_2071032_3	In specific, we introduce \textsc{SafePatching}, a novel framework for comprehensive PSA, where two distinct safety patches are developed on the harmful data to enhance safety and mitigate over-safety concerns, and then seamlessly integrated into the target LLM backbone without compromising its utility.
2405.13845_2071057_0	  With the widespread application of Large Language Models (LLMs) to various domains, concerns regarding the trustworthiness of LLMs in safety-critical scenarios have been raised, due to their unpredictable tendency to hallucinate and generate misinformation.
2405.14487_2071699_5	Fundamental concepts of the progression of LLMs from Transformers, Pre-trained Transformers, and GPT is presented.
2405.14488_2071700_2	Many defense strategies have been developed to enhance the safety of LLMs.
2405.15216_2072428_1	Error correction models are designed to fix ASR errors, however, they showed little improvement over traditional LMs mainly due to the lack of supervised training data.
2405.15523_2072735_1	Memorization in LLMs is widely assumed to only occur as a result of sequences being repeated in the training data.
2405.16310_2073522_7	These relationships were found to be central to the development and adoption of LLMs, but they can also be the terrain for uncalibrated trust and reliance on untrustworthy LLMs.
2405.16311_2073523_5	Our insights suggest that researchers and practitioners should simultaneously clarify the ``who'' in considerations of explainability and transparency, the ``what'' in the information needs, and ``why'' they are needed to ensure responsible design and development across the LLM supply chain.
2405.16363_2073575_2	The framework controls the interfacing between the LLMs and the classic recommendation models through "interest clusters", the granularity of which can be explicitly determined by algorithm designers.
2405.16450_2073662_4	We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy - an LLM is instructed to initially generate Python codes and then convert them into DSL programs.
2405.17067_2074279_2	This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs.
2405.17067_2074279_8	Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs.
2405.18241_2075453_0	  Understanding how sentences are internally represented in the human brain, as well as in large language models (LLMs) such as ChatGPT, is a major challenge for cognitive science.
2405.18433_2075645_0	  We perform a missing, reproducible evaluation of all publicly available GPT-4 family models concerning the Document Understanding field, where it is frequently required to comprehend text spacial arrangement and visual clues in addition to textual semantics.
2405.18492_2075704_10	Code can be found at https://github.com/felixbmuller/llms-memorization-copyright.
2405.18632_2075844_3	Therefore, we conducted a qualitative analysis of LLM assessment comments, showing that: 1) LLMs can match the assessment capabilities of faculty members, 2) variations in LLM assessments should be interpreted as diversity rather than confusion, and 3) assessments by humans and LLMs can differ and complement each other.
2405.18710_2075922_3	However, prior experience with FP16, which was found to be less stable than BF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be a cost-effective option for LLM training.
2405.19209_2076421_9	Moreover, on the long split of Video-MME (average 44 minutes), VideoTree achieves better performance than GPT-4V and many other MLLMs that were extensively trained on video data.
2405.19334_2076546_8	A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation
2405.19487_2076699_4	All these tasks of the LLM are carried out as next token prediction on a serialized view of the dialogue in real-time.
2405.19578_2076790_3	More importantly, the performance of these LLMs has not been compared and discussed in detail when domain-specific data analysis tasks are needed.
2405.19631_2076843_5	Further, clinical notes contain trusted health information making the use of closed-source language models from commercial vendors difficult, so the identification of open source LLMs that can be run within health organizations and exhibits high performance on SDOH tasks is an urgent problem.
2405.20099_2077311_4	Unlike previous approaches, which have often compromised the utility of the model for the sake of safety, DPP is designed to achieve a minimal Attack Success Rate (ASR) while preserving the high utility of LLMs.
2405.20770_2077982_2	In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM.
2405.20947_2078159_7	Our datasets are available at https://huggingface.co/datasets/bench-llm/or-bench and the demo can be found at https://huggingface.co/spaces/bench-llm/or-bench.
2406.00936_2079224_1	Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake.
2406.01538_2079826_9	Third, we find that brain scores of trained LLMs on this dataset can largely be explained by sentence length, position, and pronoun-dereferenced static word embeddings; a small, additional amount is explained by sense-specific embeddings and contextual representations of sentence structure.
2406.01698_2079986_10	Users can also be tried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on your web browser.
2406.03007_2081295_5	Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools.
2406.03079_2081367_4	Although ChatGPT is known for its adaptability and ethical considerations when used for harmful purposes, we highlight the deep connection that may exist between ChatGPT and fraudulent actions in the volatile cryptocurrency ecosystem.
2406.03079_2081367_8	It should be noted that our work is not intended to encourage and promote fraud, but rather to raise awareness of the risks of fraud associated with the use of ChatGPT.
2406.03085_2081373_7	Subsequently, a user retrieve-generation model is adopted to seamlessly integrate the structural information into LLM, fully harnessing its emergent inferencing ability.
2406.04640_2082928_5	This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP.
2406.04817_2083105_1	The chatbot was intentionally designed to mimic proprietary commercial chatbots such as ChatGPT where the chatbot has not been tailored for the educational context; the underlying engine was OpenAI GPT-4.
2406.04941_2083229_4	Extensive evaluation of various general LLMs and medical-domain-specific LLMs is conducted.
2406.05232_2083520_6	DALD is designed to align the surrogate model's distribution with that of unknown target LLMs, ensuring enhanced detection capability and resilience against rapid model iterations with minimal training investment.
2406.05431_2083719_5	Through comprehensive evaluations of the GPT usage cost, labeling cost, and extraction accuracy for the learning methods of zero-shot, few-shot and fine-tuning, we present a Pareto-front mapping where the few-shot learning method was found to be the most balanced solution owing to both its high extraction accuracy (total F1 score>95%) and low cost (GPT usage cost of 5.97 US dollars and labeling cost of 10 I/O paired examples).
2406.05741_2084029_3	In contrast, large language models (LLMs) represented by ChatGPT and natural language processing utilizing LLMs have been developed revolutionarily.
2406.05804_2084092_5	Resources have been made publicly available at in our GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.
2406.05900_2084188_2	Seemingly remarkable results have been reported for such LLM-based HAR systems when they are evaluated on standard benchmarks from the field.
2406.05972_2084260_2	Several empirical studies have investigated the rationality and social behavior performance of LLMs, yet their internal decision-making tendencies and capabilities remain inadequately understood.
2406.05972_2084260_6	However, there are significant variations in the degree to which these behaviors are expressed across different LLMs.
2406.07295_2085583_2	Separate preference models are trained for each principle using feedback from GPT-3.5-Turbo.
2406.07528_2085816_8	Our code can be found in https://github.com/dvlab-research/Q-LLM.
2406.07594_2085882_8	Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4.
2406.08527_2086815_4	We use decision trees to convey this reasoning information, as they can be easily represented in natural language, effectively providing knowledge from prior experiments (i.e., the impact of the generated features on performance) to the LLMs.
2406.08751_2087039_4	Experiments are conducted to evaluate the completeness and satisfaction of buildings generated via LLMs.
2406.09012_2087300_1	This raises questions about the human-likeness of LLM-derived information, alignment with human intuition, and whether LLMs could possibly be considered (parts of) explanatory models of (aspects of) human cognition or language use.
2406.09671_2087959_1	This study investigates the performance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the time the study was conducted, on the Bachelor in Computer Science section of Brazil's 2021 National Undergraduate Exam (ENADE).
2406.10300_2088588_12	LLM-integrated applications are described as combinations of their LLM components.
2406.10303_2088591_3	Most medical LLMs are developed through continued training of open-source general LLMs, which require significantly fewer computational resources than training LLMs from scratch.
2406.10450_2088738_6	Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time.
2406.10918_2089206_3	However, most prior work dealing with Multi-LLM QA has focused on scenarios where the models are asked in a zero-shot manner or are given information sources to extract the answer.
2406.11020_2089308_6	Additionally, common error types are identified through manual inspection, revealing specific challenges faced by LLMs in different reasoning contexts.
2406.11046_2089334_4	These results suggest that AI tools like ChatGPT can substantially boost developer productivity, though further analysis is needed to address potential downsides such as low quality code and privacy concerns.
2406.11116_2089404_7	Significant correlations were also found between ChatGPT and laypeople across all tasks, though the correlation strength varied by task.
2406.11131_2089419_2	In this paper, we ask if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs.
2406.11336_2089624_5	Furthermore, a data enhancement strategy is designed to mitigate the impact of LLM hallucinations on forecasting results.
2406.11341_2089629_5	Our results suggest that the behavior of pre-trained LLMs can be explained by heuristics studied in cognitive science and that both ICL and SFT improve model performance on valid inferences, although only the latter mitigates most reasoning biases without harming model consistency.
2406.11629_2089917_1	However, this kind of evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs.
2406.11670_2089958_2	Different approaches and implemented detectors for the recognition of LLM-generated text are presented.
2406.12296_2090584_7	Based on the GPT-4-generated scripts, key visuals were created for the air taxi, and the ATJ was evaluated by 72 participants.
2406.12479_2090767_4	In this paper, we designed a high-quality, diversified, and unified multimodal instruction-following dataset for RSI understanding produced by GPT-4V and existing datasets, which we called RS-GPT4V. To achieve generalization, we used a (Question, Answer) which was deduced from GPT-4V via instruction-following to unify the tasks such as captioning and localization; To achieve complex scene, we proposed a hierarchical instruction description with local strategy in which the fine-grained attributes of the objects and their spatial relationships are described and global strategy in which all the local information are integrated to yield detailed instruction descript; To achieve reasoning, we designed multiple-turn QA pair to provide the reasoning ability for a model.
2406.12775_2091063_4	By carefully analyzing the internal computations of transformer-based LLMs, we discover that the bridge entity is resolved in the early layers of the model.
2406.12935_2091223_2	Although chat templates are shown to be effective in optimizing LLM performance, their impact on safety alignment of LLMs has been less understood, which is crucial for deploying LLMs safely at scale.   
2406.13229_2091517_2	While representations of translationally equivalent sentences in different languages are known to be similar after convergence, however, it remains unclear how such cross-lingual alignment emerges during pre-training of LLMs.
2406.13261_2091549_3	Enhancing honesty in LLMs addresses critical limitations and helps uncover latent capabilities that are not readily expressed.
2406.13893_2092181_5	Leveraging continual pretraining, we adapt to Galician two existing LLMs trained on larger corpora, thus mitigating the data constraints that would arise if the training were performed from scratch.
2406.13925_2092213_1	Alignment, the process of fine-tuning LLMs to better align with desired behaviors, is recognized as an effective approach to mitigate gender biases.
2406.13993_2092281_6	Our study provides insight into how biases and stereotypes are realized within LLMs when adopting different national personas.
2406.14230_2092518_3	Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs.
2406.14903_2093191_0	  As large language models (LLMs) continue to develop and gain widespread application, the ability of LLMs to exhibit empathy towards diverse group identities and understand their perspectives is increasingly recognized as critical.
2406.14903_2093191_3	GIEBench is designed to evaluate the empathy of LLMs when presented with specific group identities such as gender, age, occupation, and race, emphasizing their ability to respond from the standpoint of the identified group.
2406.14955_2093243_1	Evaluating the ICL ability of LLMs can enhance their utilization and deepen our understanding of how this ability is acquired at the training stage.
2406.14986_2093274_5	As text completion is at the core of LLMs, these results suggest that common evaluation methods may only provide a partial picture and that more research is needed to assess the extent and nature of their capabilities.
2406.15130_2093418_2	To create these tools, examples of texts generated by LLMs are needed.
2406.15259_2093547_6	Since CoT is reported to perform poorly with small LLMs, we adopted a strategy in which a large LLM (GPT-4), acting as a Teacher, generates CoT-based instructions to fine-tune a small model, Llama-2-7B, which plays the role of a Student.
2406.15325_2093613_5	Our benchmark, Bug In The Code Stack (BICS), is designed to assess the ability of LLMs to identify simple syntax bugs within large source code.
2406.15379_2093667_0	  The recent, widespread availability of Large Language Models (LLMs) like ChatGPT and GitHub Copilot may impact introductory programming courses (CS1) both in terms of what should be taught and how to teach it.
2406.15504_2093792_3	To bridge this gap, we introduce an end-to-end modality-aligning framework for LLM-graph alignment: Dual-Residual Vector Quantized-Variational AutoEncoder, namely Dr.E. Our approach is purposefully designed to facilitate token-level alignment with LLMs, enabling an effective translation of the intrinsic `language' of graphs into comprehensible natural language.
2406.15673_2093961_6	Leveraging these factors, we demonstrate that intrinsic self-correction ability is exhibited across multiple existing LLMs.
2406.15741_2094029_4	MT-Ladder is trained on pseudo-refinement triplets which can be easily obtained from existing LLMs without additional human cost.
2406.15891_2094179_3	The same prompt is presented to LLMs and human writers, and evaluation is performed by humans using a detailed rubric including various aspects like fluency, style, originality or humor.
2406.16801_2095089_1	Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems.
2406.17588_2095876_9	For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).
2406.17781_2096069_5	Variability in GPT-4's performance across concepts could be explained by specificity of the concept's color-concept association distribution.
2406.17789_2096077_0	  The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics.
2406.17888_2096176_5	Two LM-based evaluation methods are developed to compare the actual baseline feature lists against LM-generated responses.
2406.17975_2096263_5	In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered.
2406.18510_2096798_1	Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system.
2406.18725_2097013_3	However, when using Arabic transliteration and chatspeak (or arabizi), we found that unsafe content could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3 Sonnet.
2406.19358_2097646_2	The recent emergence in Large Language Models (LLM) has significantly advanced general NLP tasks, however, the capability of such LLMs in cross-lingual sentiment analysis has not been fully studied.
2406.19840_2098128_3	The insights from this research are expected to be beneficial for enhancing the robustness of and accuracy of LLMs, particularly in the development and assessment of tokenizers.
2407.00497_2098884_6	Our code can be found at https://yingjiahao14.github.io/LLMs-as-Instructors-pages/.
2407.00869_2099256_4	Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism.
2407.01082_2099469_5	Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its significant impact on improving text generation quality.
2407.01178_2099565_4	The model is named $\text{Memory}^3$, since explicit memory is the third form of memory in LLMs after implicit memory (model parameters) and working memory (context key-values).
2407.03856_2102243_7	Thanks to the residual Q-learning framework, we can restore the customized LLM with the pre-trained LLM and the \emph{residual Q-function} without the reward function $r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function $r_2$ can be derived from the residual Q-function, enabling us to directly learn the residual Q-function from the new human preference data upon the Bradley-Terry model.
2407.04121_2102508_3	RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics.
2407.04467_2102854_4	Our structured evaluation of GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making decisions in these games, are affected by at least one of the following systematic biases: positional bias, payoff bias, or behavioural bias.
2407.04675_2103062_3	Seed-ASR is developed based on the framework of audio conditioned LLM (AcLLM), leveraging the capabilities of LLMs by inputting continuous speech representations together with contextual information into the LLM.
2407.04694_2103081_0	  AI assistants such as ChatGPT are trained to respond to users by saying, "I am a large language model".
2407.04752_2103139_6	The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations.
2407.04868_2103255_1	The majority of the studies in this direction only focus on the improvements in performance of the LMs on different benchmarks, whereas LMs are considered black boxes.
2407.05205_2103592_6	These questions are presented to ChatGPT, followed by interactions to assess its effectiveness in delivering complete and meaningful responses.
2407.05347_2103734_2	In this paper, we analyze the impact of LLM output token distribution on the inference queueing delay, where the max-token clipping and the batched inference are considered.
2407.05502_2103889_1	Although the multilingual capability of LLMs offers new opportunities to overcome the language barrier, do these capabilities translate into real-life scenarios where linguistic divide and knowledge conflicts between multilingual sources are known occurrences?
2407.05710_2104097_1	and so it is expected that, following the release of code-generative AI tools, such as ChatGPT and GitHub Copilot, developers will use these tools to perform security tasks and use security APIs.
2407.06645_2105032_9	Extensive experiments have been conducted to validate the entropy law and the superiority of ZIP across different LLM backbones and alignment stages.
2407.06908_2105295_6	Using emotion attribution, we explore how different religions are represented in LLMs.
2407.06917_2105304_2	We use GlobalBias to directly probe a suite of LMs via perplexity, which we use as a proxy to determine how certain stereotypes are represented in the model's internal representations.
2407.07321_2105708_3	While the effectiveness of LLM-based QA systems has already been established at an acceptable level in popular domains such as trivia and literature, it has not often been established in niche domains that traditionally require specialized expertise.
2407.07796_2106183_7	This study enhances our understanding of LLMs' capabilities in playing games they were not specifically trained for, helping to assess their rule comprehension and strategic thinking.
2407.08410_2106797_6	Furthermore, in a single-blind reader study two senior ophthalmologists with up to 32 years of experience found RetinaVLM's reports were found to be substantially more accurate than those by ChatGPT-4o (64.3% vs. 14.3%).
2407.08440_2106827_10	The data and code can be found at: https://anonymous.4open.science/r/llm-rule-following-B3E3/
2407.09975_2108362_0	  Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot.
2407.11654_2110041_4	Based on this analysis, a physical layer framework is developed for resilient SFL with LLMs (R-SFLLM) over wireless networks.
2407.11919_2110306_5	Our experiments show that these errors can be identified with high accuracy by an LLM.
2407.12022_2110409_1	However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire.
2407.12024_2110411_5	The advantages of the proposed model are demonstrated on a set of scenarios, as well as a comparative analysis with various LLM implementations.
2407.12145_2110532_13	The usage and perceived utility of ChatGPT were moderate, but positive correlations between student grade and ChatGPT usage were found.
2407.12854_2111241_6	Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs.
2407.13069_2111456_3	In addition, the issues of variability and reproducibility of results from each trial of LLMs have rarely been considered in existing literature.
2407.13244_2111631_5	We also conclude that while the proposed benchmark is useful for identifying LLMs that are adequate for process mining tasks, further research is needed to overcome the evaluation biases and perform a more thorough ranking of the competitive LLMs.
2407.14088_2112475_2	Despite the success of LLMs, no research has been conducted to illustrate the impact of model size on the performance of fine-tuned LLMs for D2T tasks.
2407.14246_2112633_2	Unipa-GPT relies on gpt-3.5-turbo, it was presented in the context of the European Researchers' Night (SHARPER night).
2407.14246_2112633_4	The whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned systems are compared, and a brief discussion on their performance is reported.
2407.14402_2112789_7	The code will be made available at https://aka.ms/ACV-LLM.
2407.15050_2113437_5	Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent.
2407.15677_2114064_1	Promising recent research has been conducted showing that the knowledge contained in LLMs can be utilized in making goal-driven decisions that are enactable in interactive, embodied environments.
2407.16434_2114821_5	Extensive evaluations are conducted across various model architectures and sizes (including a series of auto-regressive LLMs as well as BERT-like masking models) on a diverse set of NLP tasks (e.g., context-based question-answering, exhaustive hallucination evaluation, and passage-level dense retrieval).
2407.17390_2115777_1	However, most existing evaluation practices are not designed for LLM-generated topics and result in low inter-annotator agreement scores, hindering the reliable use of LLMs for the task.
2407.17730_2116117_4	Many concerns have been raised by mental health experts regarding the use of LLMs for therapy.
2407.17915_2116302_1	While extensive research has been conducted on the safety of LLMs in chat mode, the security implications of their function calling feature have been largely overlooked.
2407.18008_2116395_9	While our findings underscore that LLM responses can be ideologically steered with political personas, they suggest that observed changes in LLM outputs could be better described as personalization to the given context rather than sycophancy.
2407.18276_2116663_9	Tools for the Recurrent Optimization via Machine Editing (ROME) method can be found at https://github.com/ajn313/ROME-LLM
2407.18418_2116805_0	  Abstention, the refusal of large language models (LLMs) to provide an answer, is increasingly recognized for its potential to mitigate hallucinations and enhance safety in LLM systems.
2407.18607_2116994_7	Overall, our findings suggest that despite GPT-4 not being explicitly designed to reason causally, it can still be a valuable tool for causal representation, as it improves the causal discovery process of causal ML algorithms that are designed to do just that.
2407.19354_2117741_3	At the current stage, comprehensive research on the security and privacy of LLM agents is highly needed.
2407.21264_2119651_8	This method is designed to enhance the model's robustness to variations in prompts and focuses on distinguishing between different source LLMs.
2408.00724_2120905_0	  While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored.
2408.01055_2121236_7	When an unhandled runtime error occurs, Healer will be activated to generate a piece of error-handling code with the help of its internal LLM and the code will be executed inside the runtime environment owned by the framework to obtain a rectified program state from which the program should continue its execution.
2408.01935_2122116_0	  Despite their impressive performance, large language models (LLMs) such as ChatGPT are known to pose important risks.
2408.02143_2122324_10	We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.
2408.02232_2122413_1	Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent.
2408.02450_2122631_12	Based on this benchmark, extensive experiments are conducted to evaluate the abilities and limitations of seven advanced LLMs.
2408.02651_2122832_1	To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs.
2408.02811_2122992_1	REGAI uses rubrics, which can be created manually or automatically by the system, to enhance the performance of LLMs for evaluation purposes.
2408.03074_2123255_0	  In this work, we want to give an overview on which pragmatic abilities have been tested in LLMs so far and how these tests have been carried out.
2408.04666_2124847_3	Moreover, there are important explanations of LLM behavior and capabilities that are lost when we engage in this kind of reduction.
2408.04905_2125086_5	We first reveal the characteristic features induced by glitch tokens on LLMs, which are evidenced by significant deviations in the distributions of attention patterns and dynamic information from intermediate model layers.
2408.05126_2125307_1	Despite the promising capabilities of various LLMs in conducting qualitative analysis, their use in the humanities and social sciences has not been thoroughly examined.
2408.05128_2125309_5	However, 14.2% of the recommended libraries had restrictive copyleft licenses, which were not explicitly communicated by ChatGPT.
2408.05542_2125723_4	Specifically, we first propose a set of ChatGPT prompting rules that are specifically designed for source code and queries.
2408.06621_2126802_8	Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.
2408.06752_2126933_2	This article assesses which ChatGPT inputs (full text without tables, figures and references; title and abstract; title only) produce better quality score estimates, and the extent to which scores are affected by ChatGPT models and system prompts.
2408.06837_2127018_0	  Large Language Models (LLMs) have been adopted for a variety of visualizations tasks, but how far are we from perceptually aware LLMs that can predict human takeaways?
2408.07904_2128085_9	All code, dataset, and the generated responses can be found in https://github.com/tanny411/llm-reliability-and-consistency-evaluation.
2408.07982_2128163_1	In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots.
2408.08054_2128235_5	Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework.
2408.08231_2128412_1	Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment.
2408.09459_2129640_4	WPN is designed to modify the output distribution of LMs by eliminating specific harmful outputs (e.g., replacing toxic responses with neutral ones), thereby transforming the model's behavior from "harmful prompt-harmful output" to "harmful prompt-harmless response".
2408.09639_2129820_1	Conventional approaches directly compare sentence probabilities assigned by LMs, but recent large language models (LLMs) are trained to perform tasks via prompting, and thus, the raw probabilities they assign may not fully reflect their grammatical knowledge.
2408.09878_2130059_3	In this paper, we focus on whether existing mini-LLMs may be unconsciously instructed in backdoor knowledge by poisoned teacher LLMs through knowledge distillation (KD).
2408.10417_2130598_4	The system is described and the efficacy of the LLM for populating the model is analyzed herein.
2408.10682_2130863_7	It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness.
2408.11517_2131698_1	The proposed method explores the multimodal capabilities of GPT-4o to interpret visual content and create engaging stories, which are illustrated by a Stable Diffusion XL model.
2408.11862_2132043_0	  In this study, the emotion and tone of preservice teachers' reflections were analyzed using sentiment analysis with LLMs:
2408.12787_2132968_4	LLM-PBE is designed to analyze privacy across the entire lifecycle of LLMs, incorporating diverse attack and defense strategies, and handling various data types and metrics.
2408.12787_2132968_7	Aimed at enhancing the breadth of knowledge in this area, the findings, resources, and our full technical report are made available at https://llm-pbe.github.io/, providing an open platform for academic and practical advancements in LLM privacy assessment.
2408.13467_2133648_6	Extensive experiments with leading edge LLMs are conducted to demonstrate the effectiveness, adaptability, and affordability of LlamaDuo across various downstream tasks.
2408.13510_2133691_0	  Large Language Model (LLM) workloads have distinct prefill and decode phases with different compute and memory requirements which should ideally be accounted for when scheduling input queries across different LLM instances in a cluster.
2408.14007_2134188_7	Our key findings reveal that: (i) in our qualitative analyses, when the documents generated by GPT were compared with the original ones, 69.7% were considered equivalent (45.7%) or required minor changes to be equivalent (24.0%); (ii) indeed, 22.4% of the comments were rated as having superior quality than the original ones; (iii) the use of quantitative metrics is susceptible to inconsistencies, for example, comments perceived as having higher quality were unjustly penalized by the BLEU metric.
2408.14853_2135034_6	The proposed attacker is trained within a reinforcement learning scheme with the LLM outputting probability of the target answer as the reward.
2408.15409_2135590_1	For this we assess over 2,000 research works based on criteria typical of what is considered good research (e.g. presence of statistical tests and reproducibility) and cross-validate it with arguments that are at the centre of controversy (e.g., claims of emergent behaviour, the use of LLMs as evaluators).
2408.15769_2135950_2	With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions.
2408.15792_2135973_0	  In Large Language Model (LLM) inference, the output length of an LLM request is typically regarded as not known a priori.
2408.16098_2136279_4	The first is a language-based representation involving relations of sub-events that can be learned by LLMs via fine-tuning.
2408.16400_2136581_3	We evaluate the performance of six open-source models that are specifically trained for vulnerability detection against six general-purpose LLMs, three of which were further fine-tuned on a dataset that we compiled.
2408.16400_2136581_5	The findings highlight significant variations in classification accuracy across benchmarks, revealing the critical influence of fine-tuning in enhancing the detection capabilities of small LLMs over their larger counterparts, yet only in the specific scenarios in which they were trained.
2408.16779_2136960_5	While they can be complemented by formal systems, the properties delivered by LLMs regarding inductive learning, are not well understood and quantified.
2408.16978_2137159_4	For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions.
2408.17175_2137356_2	However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM.
2409.00105_2137737_7	Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found to be suffering from this syndrome.
2409.00557_2138189_1	However, the effective execution of these tools relies heavily not just on the advanced capabilities of LLMs but also on precise user instructions, which often cannot be ensured in the real world.
2409.01247_2138879_4	This raises the question: How much conversational effort is needed to elicit harmful information from LLMs?
2409.01345_2138977_4	This design is intended to make better use of the LM's instruction-following capability.
2409.01584_2139216_1	However, pre-training of Vision Encoder and the integrated training of LLMs with Vision Encoder are mainly conducted using English training data, leaving it uncertain whether LVLMs can completely handle their potential when generating explanations in languages other than English.
2409.02228_2139860_1	We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels.
2409.02228_2139860_5	Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs' initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization.
2409.02387_2140019_5	The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities.
2409.02387_2140019_6	Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition.
2409.02569_2140201_8	This bias should be considered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.
2409.03161_2140793_6	The differences and similarities in the performance of LLMs measured by the MaterialBENCH are analyzed and discussed.
2409.03291_2140923_5	A purpose-trained detector generalizing across LLMs and unseen attacks can be developed, but it fails to generalize to new human-written texts.   
2409.03701_2141333_1	Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods.
2409.04056_2141688_3	Operations on the taxonomy, such as cutting links or merging classes, are performed with the help of zero-shot prompting on an open-source LLM.
2409.04109_2141741_4	By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility.
2409.05247_2142879_3	While expanding the use of LLMs to more languages may bring many potential benefits, such as assisting cross-community communication and language preservation, great care must be taken to ensure that data collection on these languages is not extractive and that it does not reproduce exploitative practices of the past.
2409.05247_2142879_5	Furthermore, linguistic complexity and cultural nuances are often lost in LLMs.
2409.08087_2145719_3	Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises.
2409.08087_2145719_7	This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.
2409.09040_2146672_5	The outputs of the simulations are then interpreted by the LLM resulting in informative comparisons and summaries.
2409.09280_2146912_4	We replaced the disputes that were prepared by the courts with the itemized disputes that were generated by GPT-3.5 and GPT-4, and repeated the same experiments.
2409.09380_2147012_4	At the same time, an inconsistency-checking approach between the LLMs' output and the reasoning process is adopted for the allocation APIs confirmation with an off-the-shelf Natural Language Processing (NLP) tool.
2409.10245_2147877_7	Mechanistic Interpretability analysis showed that this latent behaviour of LLMs could be traced to specific neurons that became activated or amplified after PEFT.
2409.10550_2148182_7	But the evaluation result shows that only weak sign of statistical truthfulness can be produced due to limited capability of current LLMs.
2409.10576_2148208_3	An automated pipeline was developed to benchmark the performance of various LMs and RAG configurations.
2409.10955_2148587_3	We introduce a method to quantify the memory strength of LLMs by measuring the divergence in LLMs' responses to different paraphrases of the same question, which is not considered by previous works.
2409.11022_2148654_1	However, existing datasets are designed for traditional machine learning methods, inadequate for LLM-based methods in terms of corpus selection, entity categorization, and design logic.
2409.12866_2150498_1	Extensive efforts have been made to evaluate the abilities of code LLMs in various aspects, with an increasing number of benchmarks and evaluation frameworks proposed.
2409.12866_2150498_7	In particular, four specification-related tasks are designed meticulously to assess the capability of LLMs from basic to advanced levels.
2409.12866_2150498_8	Counterfactual analysis is further conducted to study the performance variance of LLMs under semantics-preserving perturbations.
2409.12917_2150549_0	  Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs.
2409.13373_2151005_4	OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM).
2409.13884_2151516_2	Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs.
2409.13897_2151529_3	A comprehensive evaluation of LLMs is conducted to assess their capabilities in these languages, revealing the challenges of multilingual and multicultural generalization.
2409.13902_2151534_8	62.5% of the top 10 documents retrieved by RAG were selected as the top references in the LLM response, with an average ranking of 4.9.
2409.14037_2151669_9	Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.
2409.14478_2152110_1	However, high-quality evidence is urgently needed on the potential and limitation of LLMs in providing accurate clinical decisions based on real-world medical data.
2409.15551_2153183_4	Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction.
2409.16732_2154364_3	These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles.
2409.16900_2154532_1	The grounding of LLMs knowledge into the empirical world has been considered a crucial pathway to exploit the efficiency of LLMs in robotics.
2409.16900_2154532_4	The roadmap for LLMs grounding is envisaged in an active bodily system as the reference point for experiencing the environment, a temporally structured experience for a coherent, self-related interaction with the external world, and social skills to acquire a common-grounded shared experience.
2409.17561_2155193_2	Despite the growing interest, limited efforts have been made to thoroughly evaluate the actual capabilities of LLMs in this task.   
2409.17698_2155330_4	The findings include: First,through several rounds of iterations the inter-reliability between GPT and human raters reached a level that is generally accepted by educators.
2409.18014_2155646_2	Moreover, a dilemma was often encountered when we tried to select the most suitable LLM from a large number of LLMs amidst explosive growth aiming for outstanding performance, affordable prices, and short response delays.
2409.18290_2155922_2	To address this, we introduce RadOnc-GPT, a specialized Large Language Model (LLM) powered by GPT-4 that has been designed with a focus on radiotherapeutic treatment of prostate cancer with advanced prompt engineering, and specifically designed to assist in generating responses.
2409.18764_2156396_3	Experiments were conducted using two leading VQA benchmark datasets, ChartQA and PlotQA, with visualizations generated by OpenAI's GPT-3.5 Turbo and Meta's Llama 3.1 70B-Instruct models.
2409.18957_2156589_4	The classification is performed by LLMs using a method similar to that used by humans who manually explore and understand the data to decide classifications.
2409.18989_2156621_5	Unlike the large models used in StarCraft LLMs such as GPT-3.5, Phi2 is trained primarily on textbook data and contains little inherent knowledge of StarCraft II beyond what is provided by our training process.
2409.18996_2156628_7	An associated GitHub repository that collects the relevant papers can be found at https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs
2409.19382_2157014_2	However, the autoregressive nature of LLMs inherently poses a challenge as errors may accumulate if mistakes are made in the intermediate reasoning steps.
2409.19450_2157082_5	Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage.
2410.00699_2158900_4	Specifically, we consider a popular tuning paradigm for downstream tasks, head tuning, where all pre-trained parameters are frozen and only individual heads are trained atop pre-trained LLMs.
2410.01306_2159507_7	Upon user queries, relevant segments are retrieved and provided as context to LLMs, enhancing their ability to generate empathetic and con-textually relevant responses.
2410.01487_2159688_2	Yet, the basic linguistic units processed in these LMs are determined by subword-based tokenization, which limits their validity as models of learning at and below the word level.
2410.02338_2160539_2	While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored.
2410.02425_2160626_1	The performance of an LLM inference service is largely determined by the hardware onto which it is deployed, but understanding of which hardware will deliver on performance requirements remains challenging.
2410.02653_2160854_6	Our findings indicate that the persuasiveness of LLMs correlates positively with model size, but smaller models can also be made to have a higher persuasiveness than much larger models.
2410.02958_2161159_3	These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs.
2410.03018_2161219_1	As GenAI technologies, such as ChatGPT, become increasingly integrated into educational settings, teachers are required to adapt to evolving classroom dynamics, where AI plays a significant role in content creation, personalized learning, and student engagement.
2410.03124_2161325_2	To fine-tune a black-box LLM, labeled data are always required to adjust the model parameters.
2410.03168_2161369_7	Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs.
2410.03255_2161456_2	To objectively assess the capabilities of existing LLMs, performance benchmarks are conducted.
2410.03769_2161970_2	Moreover, the safety mechanisms of LLMs in scientific tasks are insufficiently studied.
2410.04698_2162899_2	Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios.
2410.06992_2165193_12	In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.
2410.07739_2165940_1	Although many efforts have been made, it is still a challenge to balance the training budget, downstream performance, and the general capabilities of the LLMs in many applications.
2410.07826_2166027_1	While morally clear scenarios are more discernible to LLMs, greater difficulty is encountered in morally ambiguous contexts.
2410.08068_2166269_6	Experiments are conducted on nine benchmarks which demonstrates that our approach improves the reasoning accuracy of LLMs.
2410.08102_2166303_3	In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process.
2410.08475_2166676_1	However, no matter the size of LLMs, certain problems cannot be resolved in a single forward pass.
2410.08545_2166746_8	Additionally, we find that the personalities of LLMs are derived from their pre-trained data.
2410.09024_2167225_0	  The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots.
2410.09699_2167900_5	Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results.
2410.10456_2168657_4	These allocators are designed to be fully pluggable, making it broadly applicable across all mainstream MoE-based LLMs.
2410.10852_2169053_5	Preliminary findings are presented with the approach applied to ChatGPT-4 generated test sentences.
2410.10870_2169071_1	However, pretrained LLMs such as ChatGPT are periodically evolved, i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application.
2410.10991_2169192_1	A study based on prompt engineering was carried out to uncover how LLMs discriminate varieties of Brazilian Portuguese, specifically if sociolinguistic rules are taken into account in four LLMs: GPT 3.5, GPT-4o, Gemini, and Sabi.-2.
2410.11720_2169921_5	ATTNChecker is designed based on fault propagation patterns of LLM and incorporates performance optimization to adapt to both system reliability and model vulnerability while providing lightweight protection for fast LLM training.
2410.12126_2170327_4	A natural question arises: Is there a kind of graph representation that can be described by natural language for LLM's understanding and is also easy to require to serve as the raw input for LLMs?
2410.12298_2170499_4	This structure is designed to reflect the input question and generate more validated deductive knowledge, thereby enhancing the alignment of LLMs and KGs and ensuring more cohesive integration.
2410.13032_2171233_1	One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits.
2410.13073_2171274_3	A wide range of model explanation approaches have been developed for deep learning models, However, these local explanations are designed for single-output tasks like classification and regression,and cannot be directly applied to LLMs, which generate sequences of tokens.
2410.13213_2171414_8	After that, to prevent hallucinations in LLMs, such as sacrificing solving accuracy to avoid execution errors, the model alignment and self-correction mechanism are adopted in LLMOPT.
2410.13453_2171654_4	We introduce two approaches: (1) LLM-Guided Augmentation Policy Optimization, where augmentation policies are selected by an LLM prior to training and iteratively refined across multiple training cycles, and (2) Adaptive LLM-Guided Augmentation Policy Optimization, where policies adapt in real-time based on performance metrics.
2410.13501_2171702_1	We propose an architecture where a Reinforcement Learning (RL) Agent guides an LLM's space exploration: (1) the Agent has access to domain-specific information, and can therefore make decisions about the quality of candidate solutions based on specific and relevant metrics, which were not explicitly considered by the LLM's training objective; (2) the LLM can focus on generating immediate next steps, without the need for long-term planning.
2410.13918_2172119_6	We introduce the FTSmartAudit framework, which is designed to develop cost-effective, specialized models for smart contract auditing through the fine-tuning of LLMs.
2410.14235_2172436_5	To enhance consistency across languages, we propose novel "Compositional Representations" where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations.
2410.14975_2173176_2	Specifically, the out-of-distribution detection (OoDD) capabilities of large vision-language models (LVLMs), such as GPT-4o, which are trained on massive multi-modal data, have not been sufficiently addressed.
2410.15153_2173354_2	However, the task of unlearning a fact is much more challenging in recent large language models (LLMs), because the facts in LLMs can be deduced from each other.
2410.15288_2173489_10	Its robustness was proven through consistent performance across different LLM architectures.
2410.15884_2174085_3	Quantitative scores generated by GPT model have been analyzed using Bayesian regression to derive trend lines.
2410.16314_2174515_1	This paper explores activation engineering, where outputs of pre-trained LLMs are controlled by manipulating their activations at inference time.
2410.16443_2174644_0	  Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns.
2410.17126_2175327_7	Our findings suggest that direct RL training of LLMs may be more suitable for relatively minor changes, such as alignment, than for learning new tasks altogether, even if an informative reward signal can be expressed programmatically.
2410.17406_2175607_3	Over 25,000 vulnerabilities have been identified so far in 2024, which are introduced after popular LLMs' (e.g., GPT-4) training data cutoff.
2410.17482_2175683_1	We study a range of LLMs and find that the largest models we tested are able to draw human-like inferences when the inference is determined by context and can generalize to unseen adjective-noun combinations.
2410.17558_2175759_8	Extensive experiments are conducted with 40 LLMs over 1,018 discipline-specific questions.
2410.18824_2177025_0	  Privacy vulnerabilities in LLMs, such as leakage from memorization, have been constantly identified, and various mitigation proposals have been proposed.
2410.18824_2177025_5	The experiments are executed on three different LLM architectures fine-tuned on three datasets with LoRA.
2410.18906_2177107_1	While various methods have been proposed to elicit the preferences of such models, countermeasures have been taken by LLM trainers, such that LLMs hide, obfuscate or point blank refuse to disclosure their positions on certain subjects.
2410.19811_2178012_9	The effectiveness of ControlAgent is demonstrated via extensive comparative evaluations between LLM-based and traditional human-involved toolbox-based baselines.
2410.19848_2178049_1	Further progress has been made in multimodal LLMs, with many datasets created to evaluate LLMs with vision abilities.
2410.20037_2178238_4	Furthermore, it is argued that existing dual-process computational cognitive architectures (models of the human cognitive/psychological architecture) provide usable frameworks for fundamentally enhancing LLMs by introducing dual processes (both implicit and explicit) and, in the meantime, can also be enhanced by LLMs.
2410.20749_2178950_4	Matryoshika is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance.
2410.20783_2178984_1	Recent advancements in Large Language Models (LLMs) have significantly improved text generation capabilities, but these systems are still known to hallucinate, and granular uncertainty estimation for long-form LLM generations remains challenging.
2410.21008_2179209_3	The responses were analyzed by computing averages, standard deviations, and performing significance tests to investigate differences between GPT-3.5 and GPT-4.
2410.21071_2179272_1	Artifacts generated by state of the art LLM technology are expected to be useful in the sense that a user will be able to use the LLM generated artifact after a small number of easy modifications.
2410.21218_2179419_1	In recent years, a great surge has been witnessed in the introduction of both commercial and open-source LLMs.
2410.21411_2179612_6	The manual prompt design process for LLMs at the reasoning phase is tedious and an automated prompt optimization method is desired.
2410.21750_2179951_2	We investigate this question by injecting facts into LMs from a new probing dataset, "Outlandish", which is designed to permit the testing of a spectrum of different fact types.
2410.21779_2179980_1	Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved.
2410.22839_2181040_3	This limited-size benchmark was found to produce a robust ranking that correlates to human feedback at $\rho \sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings.
2410.24190_2182391_6	The experiment results show a shift in voter choices towards the Democratic nominee following LLM interaction, widening the voting margin from 0.7% to 4.6%, even though LLMs were not asked to persuade users to support the Democratic nominee during the discourse.
2411.00154_2182581_5	In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing.
2411.00331_2182758_1	Despite efforts to improve the accuracy of LLM-based recommendation models, relatively little attention is paid to beyond-utility dimensions.
2411.00849_2183276_7	Statistical analysis was used to compare scores between feedback and non-feedback questions, and the effect of ChatGPT feedback on eye-tracking data was examined.
2411.00860_2183287_1	Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology.
2411.01245_2183672_0	  Reinforcement Learning from Human Feedback (RLHF) has been proven to be an effective method for preference alignment of large language models (LLMs) and is widely used in the post-training process of LLMs.
2411.01471_2183898_6	The framework is designed to integrate seamlessly with existing LLM systems, as it does not require modifications to the underlying architectures.
2411.01595_2184022_5	The Instruction Router is designed to generate specific prompts tailored for each corresponding LLM, guiding them to focus on distinct aspects of the RSIC task.
2411.01610_2184037_2	To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM.
2411.02523_2184950_7	Lab tests, including liver function, metabolic/toxicology panels, and serology/immune tests, were generally interpreted correctly by LLMs for differential diagnosis.
2411.02528_2184955_2	We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency.
2411.02816_2185243_9	The results and analysis of these surveys are presented to highlight the impact of ChatGPT in this context.
2411.03349_2185776_4	The resulting logic rules are translated into natural language, allowing targeted knowledge injection and seamless integration into LLM prompts for LLM's downstream task reasoning.
2411.04847_2187274_1	However, they sometimes generate responses that are logically coherent but factually incorrect or misleading, which is known as LLM hallucinations.
2411.05232_2187659_5	Additionally, the zero-shot benchmark results indicate that aggregated high-quality human reviews are overwhelmingly preferred over LLM-generated responses, even for the most capable models like GPT-4o.
2411.06145_2188572_8	Finally, 1,243 failure cases made by the best-performing LLM under test are analyzed and categorized in this paper for practical guidance and future enlightenment.
2411.06254_2188681_2	Furthermore, the internal mechanisms of LLMs during ranking are still not fully understood.
2411.06272_2188699_7	The source code for Golden Touchstone and model weight of Touchstone-GPT have been made publicly available at \url{https://github.com/IDEA-FinAI/Golden-Touchstone}, contributing to the ongoing evolution of FinLLMs and fostering further research in this critical area.
2411.06713_2189140_4	Statistically significant differences (p < 0.05) were found between Sporo and the other models, with post-hoc tests showing significant improvements over GPT-3.5, Gemma-9B, and Llama 3.2-3B. While Sporo outperformed GPT-4o by up to 10%, the difference was not statistically significant (p = 0.25).
2411.06877_2189304_5	Thus a complete replacement with LLMs is argued to be too risky and not fully reliable.
2411.06899_2189326_2	While safety alignment in short context has been widely studied, the safety concerns of long-context LLMs have not been adequately addressed.
2411.07071_2189498_0	  While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive.
2411.07091_2189518_3	Based on more than 587 patch reviews provided by RevMate, we observed that 8.1% and 7.2%, respectively, of LLM-generated comments were accepted by reviewers in each organization, while 14.6% and 20.5% other comments were still marked as valuable as review or development tips.
2411.07336_2189763_3	Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs' algorithmic abilities under simple lexical or semantic variations.
2411.07360_2189787_1	Although hallucinations in ChatGPT are studied for textual responses, it is unknown how ChatGPT hallucinates for technical texts that contain both textual and technical terms.
2411.07360_2189787_8	In a user study, we find that the improved responses with CHIME are considered more useful than those generated from ChatGPT without CHIME.
2411.07656_2190083_1	This paper addresses the specific problem of biased pronoun usage in LLM outputs, particularly the inappropriate use of traditionally gendered pronouns ("he," "she") when inclusive language is needed to accurately represent all identities.
2411.07990_2190417_3	As of yet, it is not known whether linguistic generalization in LLMs could equally well be explained as the result of analogical processes, which can be formalized as similarity operations on stored exemplars.
2411.08028_2190455_1	However, the large size and high computation demands of LLMs limit their practicality in many applications, especially when further fine-tuning is required.
2411.08534_2190961_3	In LLM-ITL, global topics and document representations are learned through the NTM.
2411.08724_2191151_6	The chunks with the highest score are selected and input into the LLMs to generate responses.
2411.09266_2191693_4	Extensive experiments are conducted on videos from a benchmark multimodal deepfake dataset to evaluate the detection performance of ChatGPT and compare it with the detection capabilities of state-of-the-art multimodal forensic models and humans.
2411.10145_2192572_6	When numerical calculations are required, we use code generated by LLMs to avoid the disadvantage of LLM not being good at calculations.
2411.10163_2192590_3	This benchmark is derived from existing QA datasets, annotated with proprietary LLMs and verified by humans for accuracy.
2411.10213_2192640_5	Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.
2411.10583_2193010_2	However, readability evaluation differs among developers, so personalization of the evaluation by LLM is needed.
2411.13560_2195987_9	Simulation results of the netlist are fed back to the LLM for further topology refinement, ensuring the circuit design specifications are met.
2411.14033_2196460_3	The tools can be regarded as a predefined operational process with private or real-time knowledge that does not exist in the parameters of LLMs.
2411.14708_2197135_2	This regression performance can be explained in part due to LLM embeddings over numeric data inherently preserving Lipschitz continuity over the feature space.
2411.15715_2198142_4	Comprehensive experiments were conducted with various LLMs such as Mixtral, LLaMA-2, Qwen, and PhiMoE across three test environments featuring different CPUs and GPUs.
2411.16337_2198764_3	A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring capabilities.
2411.16594_2199021_7	Paper list and more resources about LLM-as-a-judge can be found at https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge and https://llm-as-a-judge.github.io.
2411.16818_2199245_4	Clinical notes were concatenated chronologically for each patient and transformed into expert summaries using Med42-v2 70B. A multi-representational learning framework was developed to integrate these data sources, leveraging LLMs to enhance textual data while mitigating direct reliance on LLM predictions, which can introduce challenges in uncertainty quantification and interpretability.
2411.16985_2199412_3	Similar to a general-purpose LLM, we assume that our much smaller Reasoning Models may be asked arbitrary questions from unknown distributions, so we focus on evaluation in an unseen setting.
2411.17525_2199952_6	Further, we show that our method can be efficiently supported in terms of GPU kernels at various batch sizes, advancing both data-free and non-uniform quantization for LLMs.
2411.17569_2199996_4	Here, attackers inject malicious code for the training data, which can be carried over into the HDL code generated by LLMs.
2411.17855_2200282_0	  The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in computer science (CS) education is expected to be profound.
2412.00251_2202629_12	This study establishes that CBT specific fine-tuning can effectively encode therapeutic competencies in small LLMs, though significant technical and ethical considerations must be resolved prior to clinical deployment.
2412.00543_2202921_1	While their correlation against human annotators has been widely studied, consistency as evaluators is still understudied, raising concerns about the reliability of LLM evaluators.
2412.00546_2202924_1	In this paper, we consider the application of LLMs on symmetric tasks where a query is asked on an (unordered) bag of elements.
2412.00726_2203104_3	With the recent advancements in LLM technology, some open-source applications have been developed to address this problem.
2412.00804_2203182_1	As the problem has not been thoroughly examined yet, this study examines identity consistency across nine LLMs.
2412.01069_2203447_4	This underperformance can be traced to GPT's distinct textual and quantitative approaches: its textual processing follows a consistent, generalized pattern across firms, highlighting its strengths in language tasks.
2412.01617_2203995_2	However, we argue that the use of widespread LLMs like ChatGPT is more prevalent--and riskier, as they are not designed for this purpose.
2412.01955_2204333_6	The findings demonstrate the potential of LLMs "out-of-the-box" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.
2412.02466_2204844_3	These 30 oaths were first translated via ChatGPT and then analyzed and compared to the human translation in terms of types of gaps left unfulfilled by ChatGPT.
2412.02466_2204844_5	It concludes that ChatGPT translation of oaths is still much unsatisfactory, unveiling the need of further developments of ChatGPT, and the inclusion of Arabic data on which ChatGPT should be trained including oath expressions, oath nuances, rituals, and practices.
2412.03123_2205501_1	We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder.
2412.03856_2206234_6	However, several issues related to potential errors arising from LLMs were identified, which can potentially mislead students.
2412.04057_2206435_2	We use an evolutionary hill-climbing algorithm, where the mutations and seeds of the initial programs are controlled by LLMs.
2412.04503_2206881_2	It is intended to be useful to those in academia and industry who are interested in gaining an understanding of the key LLM concepts and technologies, and in utilising this knowledge in both day to day tasks and in more complex scenarios where this technology can enhance current practices and processes.
2412.04922_2207300_5	The best results are produced by the Mistral7-Base LLM after fine-tuning and DPO.
2412.05563_2207941_1	However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence.
2412.06564_2208942_2	A systematic mapping study was conducted, analyzing 21 relevant studies to explore reported uses of LLMs for qualitative analysis.
2412.06603_2208981_4	Our case study characterizes the impact of an LLM-powered assistant on developers' perceptions of productivity and it shows that although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.
2412.07355_2209733_3	The few preliminary studies investigating the possible combination of LLM with BCI spellers to efficiently support fast communication and control are then described.
2412.08054_2210432_7	In our design, knowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums Generation (KCG) module are transmitted between clients and the server instead of model parameters in previous FL methods.
2412.08599_2210977_1	The aim of the experiments is to gain empirically and statistically based insights into the lexicographical text type,dictionary article, in the responses of ChatGPT 3.5, as well as into the lexicographical data on which this chatbot was trained.
2412.09237_2211615_8	Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations.
2412.09416_2211794_2	To bridge this gap, we propose a unified evaluation taxonomy with eight pedagogical dimensions based on key learning sciences principles, which is designed to assess the pedagogical value of LLM-powered AI tutor responses grounded in student mistakes or confusions in the mathematical domain.
2412.10139_2212517_0	  The capacity of LLMs to carry out automated qualitative analysis has been questioned by corpus linguists, and it has been argued that corpus-based discourse analysis incorporating LLMs is hindered by issues of unsatisfying performance, hallucination, and irreproducibility.
2412.10198_2212576_1	However, this integration also introduces new security vulnerabilities, particularly in the tool scheduling mechanisms of LLM, which have not been extensively studied.
2412.10400_2212778_7	Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.
2412.10535_2212913_7	Further research is needed to evaluate these interactions across larger models and varied architectures, offering a pathway to more reliable and generalizable LLMs.
2412.10849_2213227_10	New robust benchmarks and scalable evaluation of LLM capabilities compared to human physicians are needed along with trials evaluating AI in real clinical settings.
2412.11053_2213431_4	The dynamic nature of autoregressive token generation in LLMs is therefore not supported out of the box.
2412.11328_2213706_7	Our evaluation, which encompasses over 3,000 GUI annotations from over 100 crowd-workers with UI/UX experience, shows that SCGG, in contrast to PDGG and RAGG, can lead to more effective GUI generation, and provides valuable insights into the defects that are produced by the LLMs in the generated GUI prototypes.
2412.11908_2214286_4	Our variations preserve the mathematical core and are designed to measure the generalisability of LLM problem-solving abilities, while also increasing confidence that problems are submitted to LLMs in forms that have not been seen as training instances.
2412.13233_2215611_2	To overcome this limitation and achieve integration of LLMs and Artificial Intelligence (AI) into real-world applications, customized AI agents are being constructed.
2412.13666_2216044_3	However, a combination of personalization and disinformation abilities of LLMs has not been comprehensively studied yet.
2412.13705_2216083_2	A gradient-based defensive suffix generation algorithm is designed to bolster the robustness of LLMs.
2412.13765_2216143_5	Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement.
2412.13845_2216223_10	Our paper's GitHub repository can be found at https://github.com/Darcyddx/Video-LLM.
2412.14471_2216849_8	In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \textit{Japanese abilities} for LLMs.
2412.14501_2216879_0	  The philosophy of language, which has historically been developed through an anthropocentric lens, is now being forced to move towards post-anthropocentrism due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude (Anthropic), which are considered to possess linguistic abilities comparable to those of humans.
2412.15574_2217952_8	Further advances in deep-sea species-specific LLMs are therefore required.
2412.15584_2217962_2	Users may over-rely on LLM advice that is confidently stated but wrong, or under-rely due to mistrust.
2412.15584_2217962_3	Reliance interventions have been developed to help users of LLMs, but they lack rigorous evaluation for appropriate reliance.
2412.15594_2217972_0	  Solving tabular math word problems (TMWPs) has become a critical role in evaluating the mathematical reasoning ability of large language models (LLMs), where large-scale TMWP samples are commonly required for LLM fine-tuning.
2412.16158_2218536_8	It is first trained to distill visual features from a pre-trained vision encoder and text embeddings from the LLM, enabling large-scale training with unpaired random images and text tokens.
2412.16216_2218594_0	  Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that has been widely adopted in various downstream applications of LLMs.
2412.16216_2218594_3	Lack of communication and collaboration among experts exacerbate the instability of LLMs due to the imbalance load problem of MoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning framework GraphLoRA, in which a graph router function is designed to capture the collaboration signals among experts by graph neural networks (GNNs).
2412.16783_2219161_2	Given that the latest generations of LLMs have digested and encoded extensive knowledge about different human subpopulations and individuals, the hope is that these models can be trained, tuned or prompted to align with a wide range of different human perspectives.
2412.16814_2219192_4	LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts of text, and can create new content, including text, images, or video.
2412.17019_2219397_1	While this mechanism has been extensively studied in explainability research, particularly through the attention values obtained during the forward pass of LMs, the backward pass of attention has been largely overlooked.
2412.17156_2219534_6	Theoretical challenges -- such as the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance -- must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.
2412.17200_2219578_7	Next, a series of experiments were designed and conducted on 40 students' UML modeling reports to explore the performance of ChatGPT in evaluating and grading these UML diagrams.
2412.17686_2220064_5	This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society.
2412.18544_2220922_5	We then build a standard, proper-scoring-rule forecasting benchmark, and show that our (instantaneous) consistency metrics correlate with LLM forecasters' ground truth Brier scores (which are only known in the future).
2412.18617_2220995_2	In this paper, we present a custom GPT (IBL Educator GPT) that is designed and developed based on Inquiry-based Learning and offers physics teachers a framework in which they can interact with ChatGPT and design educational strategies.
2412.18719_2221097_3	An experiment was conducted using GPT-4 to determine if machine learning methods based on LLMs can match or exceed the reliability of instructor grading in evaluating short writing assignments on topics in astronomy.
2412.18835_2221213_3	Fine-tuning open-source LLMs like the Llama series is often preferred by enterprises over using commercial ones like the GPT series due to considerations including privacy, security, openness, performance, etc.
2412.19726_2222104_2	We expect that humans will engage in a consistent reasoning process across various questions about a situation, but this is known to not be the case for current LLMs.
2412.20087_2222465_4	Scores were determined by averaging the values assessed by three distinct LLMs.
2412.21016_2223394_4	However, to the best of our knowledge, no automated robustness testing methods have been specifically designed to evaluate the overall inputs of LLM-based NLP software.
2412.21102_2223480_0	  Controlling diversity in LLM-agent world simulations is essential for maintaining stability in structured tasks while enabling variation where creativity is needed.
2501.01426_2225010_1	However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM.
2501.02178_2225762_6	This can also be presented as a study that shows the potential of LLMs for changing user experiences and making innovation possible in industries.
2501.02266_2225850_5	Also, the correlation between LLMs and humans at model accuracy and exam pass rate levels is examined.
2501.02346_2225930_2	An attempt to assess GPT-4's performance in radiation oncology was made via a dedicated 100-question examination on the highly specialized topic of radiation oncology physics, revealing GPT-4's superiority over other LLMs.
2501.02406_2225990_3	We answer the following question: Given a piece of text, can we identify whether it was produced by LLM $A$ or $B$ (where $B$ can be a human)?
2501.02531_2226115_4	Seven LLMs, including GPT-4 and Bard, were analyzed and compared against sentiment data from three independent human sample populations.
2501.03838_2227422_7	To evaluate the feasibility of LM-Net, extensive experiments have been conducted on three publicly available datasets with different modalities.
2501.03904_2227488_4	A comparative analysis of different ChatGPT models was conducted to assess their ability to understand transportation information, retrieve relevant data, and provide comprehensive responses.
2501.04277_2227861_6	These findings provide a baseline for the raw capabilities of LLMs on Q&A tasks applied to materials science, and emphasize the substantial improvement that could be brought to open-source models via prompt engineering and fine-tuning strategies.
2501.04848_2228432_5	Built on GPT-4o-mini model, \msp is designed to augment malware analysis for Android through a hierarchical-tiered summarization chain and strategic prompt engineering.
2501.05647_2229231_9	The device determines whether a new candidate list is needed by comparing the consistency of the LLM's and SRM's sorted lists.
2501.06137_2229721_6	We validate our simulation results with several real-world datasets, including one with over a million ChatGPT interactions, of which more than 150,000 conversations were identified as risky.
2501.07641_2231225_0	  Large Language Models (LLMs), such as GPT, are considered to learn the latent distributions within large-scale web-crawl datasets and accomplish natural language processing (NLP) tasks by predicting the next token.
2501.07824_2231408_2	To address this, while many previous work has focused on identifying errors in their generation and further refining them, they are slow in deployment since they are designed to verify the response from LLMs only after their entire generation (from the first to last tokens) is done.
2501.07824_2231408_5	Specifically, the proposed Streaming-VR enables on-the-fly verification and correction of tokens as they are being generated, similar to a streaming process, ensuring that each subset of tokens is checked and refined in real-time by another LLM as the LLM constructs its response.
2501.07837_2231421_4	Initially, domain-fine-tuning of the LLM is performed using a constructed railway knowledge question-and-answer dataset to improve answer accuracy in railway-related questions.
2501.07837_2231421_9	Finally, the fault handling capability of IDAS-LLM is demonstrated through simulations of real operational scenarios, proving that the proposed framework has practical application prospects.
2501.07857_2231441_4	First, smaller code units such as functions and variables are identified using syntax analysis and summarized with local LLMs.
2501.10134_2233718_3	In this article, thematic analysis has been performed on seven essays obtained from professionals in the education sector to understand the advantages and pitfalls of using GenAI models such as ChatGPT and Bard in education.
2501.10300_2233884_5	The ontology was developed utilizing suggestions from ChatGPT-3.5-010422 and validated using peer-reviewed research articles.
2501.10326_2233910_2	In academic publications, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts.
2501.10685_2234269_3	For instance, the ethical aspects of AI with respect to data privacy, transparency, and mitigation of bias are also covered, with the goal of promoting responsible use of the technology through best practices and the use of new technologies businesses can tap into the LLM potential, which help growth and stay one step ahead in the turmoil of digital marketing.
2501.11114_2234698_5	Our results are promising with regard to the incorporation of LLMs for simple cohort selection tasks, but also highlight the difficulties encountered by these models as soon as fine-grained knowledge and reasoning are required.
2501.11935_2235519_0	  LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts.
2501.11979_2235563_3	We iteratively refine the prompt by treating the deviation between the LLM output and the desired result as an error term until the output criteria are met.
2501.12332_2235916_5	To address this, we propose Retrieval Augmented Classification (RAC) for which LLM performs inferences for one label at a time using corresponding label schema; we start with the most related label and iterates until a label is chosen by the LLM.
2501.13381_2236965_8	Several interesting findings regarding LLMs' conformity are derived from empirical results and case studies.
2501.15875_2239459_2	In response, benchmarks focusing on the controllability of LLMs have been developed, but several issues remain: (1) They primarily cover major languages like English and Chinese, neglecting low-resource languages like Japanese; (2) Current benchmarks employ task-specific evaluation metrics, lacking a unified framework for selecting models based on controllability across different use cases.
2501.16149_2239733_7	These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs.
2501.16466_2240050_4	Rather than LLMs issuing low-level command-line instructions, which can lead to incorrect implementations, Incalmo allows LLMs to specify high-level tasks (e.g., infect a host, scan a network), which are then carried out by Incalmo.
2501.17024_2240608_5	Our findings underscore the potential of LLMs to achieve tasks where, in the past, implementing recommenders based on complex code analyses was required.
2501.18816_2242400_3	Furthermore, these approaches depend on the LLM to output solutions in an intermediate language, which must be translated into the representation language of the planning task.
2502.00340_2243331_8	Collider is designed for easy integration into existing LLM training frameworks, allowing systems already using token filtering to accelerate training with just one line of code.
2502.00808_2243799_4	To this end, we take the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data.
2502.00873_2243864_2	We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic.
2502.00903_2243894_2	By assessing each model's alignment with ideological perspectives, we explore how partisan selective processing could be identified in LLM-Assisted Content Analysis (LACA).
2502.01083_2244074_0	  Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs.
2502.01220_2244211_2	The accuracy of LMs is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context.
2502.01344_2244335_4	As the whole procedure conducts within LLMs, supporting and persuasive references are hard to acquire, while the absence of specific steps towards refining hidden mistakes persists even when errors are acknowledged.
2502.01436_2244427_2	These tailored models are increasingly made available through dedicated marketplaces, such as OpenAI's GPT Store.
2502.02201_2245192_6	These findings are believed to contribute to design implications for future LLM-based object manipulation interfaces, highlighting the potential for more intuitive and efficient user interactions in VR environments.
2502.02743_2245734_4	Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring adaptability to new models as they emerge.
2502.04358_2247349_4	This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them.
2502.05310_2248301_5	These choice points are resolved at runtime by LLMs, which generalize from user-provided examples of correct and incorrect decisions.
2502.07813_2250804_0	  The compositional reasoning capacity has long been regarded as critical to the generalization and intelligence emergence of large language models LLMs.
2502.07813_2250804_1	However, despite numerous reasoning-related benchmarks, the compositional reasoning capacity of LLMs is rarely studied or quantified in the existing benchmarks.
2502.08109_2251100_3	For example, the phenomenon of hallucination is known to compromise the reliability of LLMs, especially in fields that demand high factual precision.
2502.08213_2251204_1	In the proposed scheme, the Qwen2-1.5B model is frozen and its representations are passed through specially designed attention layers to the GPT-Neo-125M model, which is trained on limited computational resources.
2502.08301_2251292_10	Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.
2502.08648_2251639_4	The emerging area of generative AI was identified, linked to new AI models, such as ChatGPT, designed to generate content in the form of written text, audio, images or videos.
2502.09003_2251994_1	Quantization has been recently studied as a post-training technique for efficient LLM deployment.
2502.09606_2252597_4	Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency due to LLMs' disfavor.
2502.09690_2252681_1	While expectations for LLMs to assist systems engineering (SE) tasks are paramount; the interdisciplinary and complex nature of systems, along with the need to synthesize deep-domain knowledge and operational context, raise questions regarding the efficacy of LLMs to generate SE artifacts, particularly given that they are trained using data that is broadly available on the internet.
2502.09690_2252681_2	To that end, we present results from an empirical exploration, where a human expert-generated SE artifact was taken as a benchmark, parsed, and fed into various LLMs through prompt engineering to generate segments of typical SE artifacts.
2502.10673_2253664_2	To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed.
2502.10673_2253664_6	During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark.
2502.10709_2253700_0	  As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators.
2502.11122_2254113_6	The replay video can be viewed on https://www.bilibili.com/video/BV1uz42187EF and https://youtu.be/dO3PshWLV5M, and our codes have been open-sourced on https://github.com/luchang1113/HEP-LLM-play-StarCraftII.
2502.11133_2254124_0	  Multi-agent systems (MAS) powered by Large Language Models (LLMs) have been demonstrated to push the boundaries of LLM capabilities, yet they often incur significant costs and face challenges in dynamic LLM selection.
2502.11555_2254546_2	Typically, the safety alignment of LLM is trained on data with safety-related categories.
2502.11932_2254923_3	Our experiments with linear classifiers and cluster separability tests demonstrate that simple arithmetic equations and general language input are encoded in completely separated regions in LLMs' internal representation space across all the layers, which is also supported with more controlled stimuli (e.g., spelled-out equations).
0704.1113_1112_2	Stellar parameters are determined using an automated fitting method, combining the stellar atmosphere code FASTWIND with the genetic-algorithm optimisation routine PIKAIA.
0705.1536_5538_3	Some preliminary results on the stability of this system and the measurements of the refractive index of air with this apparatus are presented.
0707.3154_16324_4	Our observations may represent partial transitions to the hard state; complete transitions have been seen in this system by Wilms et al.
0709.0625_22890_3	The algorithm uses an exact or approximative low-rank representation of the identity-by-descent matrix, which combined with the Woodbury formula for matrix inversion results in that the computations in the AI-REML iteration body can be performed more efficiently.
0801.4200_46527_2	In a previous model calculation, the pressure and temperature dependences of the fluorescence yield have been studied on the basis of kinetic gas theory, assuming temperature-independent molecular collision cross-sections.
0801.4750_47077_0	  The introduction of new technologies and concepts of operation in the air transportation system is not possible, unless they can be proven not to adversely affect the system operation under not only nominal, but also degraded conditions.
0803.0785_52545_5	The fusion rules in the extended picture are deduced from the known fusion rules for the Virasoro representations of LM(1,p) and are found to be in agreement with previous works.
0805.0905_62082_1	The new approach is based on measuring the temperature inside a pressure sensor, which is also needed in the system.
0807.3307_74300_3	The expected number of air showers at the Auger Observatory is estimated as a function of two parameters of the model.
0808.0129_76261_3	The total mass of the quadruple system was estimated at 4.56 M_sun.
0808.0129_76261_6	The wide mutual orbit and orbit of the non-eclipsing pair are found to be close to coplanarity, preventing any changes of the inclination angle of the non-eclipsing orbit and excluding occurrence of the second system of eclipses in future.
0808.0129_76261_8	Nearby star, HD95606, was found to form loose binary with quadruple system VW LMi.
0811.1711_93063_6	Each of the AI methods considered was simulated in Matlab.
0901.1328_102570_8	YSO counterparts are found in four ultracompact HII regions and their stellar masses determined from SED model fits agree well with those estimated from the ionization requirements of the HII regions.
0903.0404_111523_7	The reduction of the model dependence is demonstrated for one of the measurement methods.
0904.3781_120447_2	Several numerical examples of the spherically confined model systems are presented as the test cases.
0906.3666_130217_2	For any initial configuration, it is proved that Dyson's model with $\beta=2$ and $N$ particles, $\X(t)=(X_1(t), ..., X_N(t)), t \in
0910.0334_148398_3	Then, we propose a mathematical kinetic interpretation of this system to finally construct a two-layer kinetic scheme in which a special treatment for the "missing" boundary condition is performed.
1003.4222_180029_1	The eigenvalue distribution in this model is expressed in terms of Laguerre polynomials in the complex plane.
1006.0386_193661_2	The public key cryptosystem based on rank codes was presented in 1991 by Gabidulin -Paramonov-Trejtakov(GPT).
1008.2866_207625_0	  We intend to show that the vacuum manifold inherent in the Minkowskian non-Abelian model involving Higgs and Yang-Mills BPS vacuum modes and herewith quantized by Dirac can be described with the help of the superselection rules if and only if the "discrete" geometry for this vacuum manifold is assumed (it is just a necessary thing in order justify the Dirac fundamental quantization scheme applied to the mentioned model) and only in the infinitely narrow spatial region of the cylindrical shape where topologically nontrivial vortices are located inside this discrete vacuum manifold.
1008.3328_208085_3	In Trigonometric LMS (TLMS) and Hyperbolic LMS (HLMS), two new versions of LMS algorithms, same formulations are performed as in the LMS algorithm with the exception that filter tap weights are now expressed using trigonometric and hyperbolic formulations, in cases for TLMS and HLMS respectively.
1010.4310_220698_6	It is found that the results of our ad hoc modifications are, to a large extend, independent of the choice of the underlying hadronic interaction model.
1011.0187_222879_12	In the article four levels of AI are developed.
1101.1039_236452_0	  New polynomials associated with a special Lipkin-Meshkov-Glick (LMG) model corresponding to the standard two-site Bose-Hubbard model are derived based on the Stieltjes correspondence.
1101.1039_236452_2	A one-dimensional classical electrostatic analogue corresponding to the special LMG model is established according to Stieltjes early work.
1101.1039_236452_4	Some relations of sums of powers and inverse powers of zeros of the new polynomials related to the eigenenergies of the LMG model are derived.
1101.1189_236602_1	The simulation is performed with a 3D hybrid model that traces the single electrons and photons in the low density region, while modeling the streamer interior as a fluid.
1101.3698_239111_5	Since the conventional form of the LLL algorithm is not immediately suitable for parallel processing, two modified LLL algorithms are considered here for the systolic array.
1101.4249_239662_5	The interpretation is supported by linear analysis and by MHD model of KH instability.
1101.4957_240370_9	Time-varying flow characteristics, such as geometrical configuration, speed, and probability density function of aircraft spatial distribution within the flow, are determined from archived Enhanced Traffic Management System data, using a tailored clustering algorithm.
1103.0871_248139_1	They were often interpreted as a fast-mode magnetoacoustic wave in the corona, despite various discrepancies between the fast-mode wave model and observations.
1103.3007_250275_3	In this model, water and air boundary layers are simultaneously disturbed due to the change in ice shape, and the effect of the interaction between air and water flows on the growth condition of the ice-water interface disturbance is taken into account.
1104.1698_255252_2	The corresponding algorithm for the sequential determination of the generalized LM-inverse is established in the present paper.
1104.3345_256899_0	  The mathematical formalism of quantum mechanics has been successfully employed in the last years to model situations in which the use of classical structures gives rise to problematical situations, and where typically quantum effects, such as 'contextuality' and 'entanglement', have been recognized.
1105.0285_259550_8	The effectiveness of the proposed algorithm is illustrated by numerical experiments.
1105.0491_259756_0	  Computer-aided surgery intensively uses the concept of navigation: after having collected CT data from a patient and transferred them to the operating room coordinate system, the surgical instrument (a puncture needle for instance) is localized and its position is visualized with respect to the patient organs which are not directly visible.
1106.5917_271552_10	Factors affecting intuition are analyzed and interpreted through our model.
1107.1476_273468_13	Complex dissipative systems whose behavior cannot be understood completely in principle will be the basis of AI.
1108.1958_279996_0	  The Kontsevich-Penner model, an Airy matrix model with a logarithmic potential, may be derived from a simple Gaussian two-matrix model through a duality.
1109.6838_291206_4	The detailed design of the agent system, which will be easily interfacable with the existing environment, is described.
1111.5202_303438_6	Some of these phenomena can be equally explained by either a wave or non-wave model alone.
1112.3902_309457_5	The timing resolution of the system has been determined and the angular resolution has been studied using Monte Carlo simulations and is compared to data.
1201.0686_312673_7	Both MSE and BER simulations using specifications of the DTMB system are carried out to prove the effectiveness of the proposed algorithm even in very harsh channel conditions such as in the single frequency network (SFN) case.
1203.0305_325664_9	The existence of such a star in the system adds a constrain to the age of the cluster, which is well covered by our age and age spread determinations.
1203.2386_327745_12	Flight testing of the system will be conducted at the next available opportunity.
1205.4724_343725_3	The presented models of the Magellanic System were constructed without prior consideration of the microlensing implications.
1205.6021_345022_3	The peak 0.1-7.0 keV X-ray flux is derived from the two-temperature APEC model to be ~(8.4 +/- 0.6) x
1205.6986_345987_4	At the same time, confounding influences such as population structure cause spurious association signals that result in false positive findings if they are not accounted for in the model.
1206.5361_351454_5	For these three regions, three linear controllers are designed with closed-loop system having small rise time, settling time and overshoot.
1206.6859_352952_8	Even if all known causal factors could be accounted for, macro-level national airspace system (NAS) delays could not be predicted with certainty from micro-level aircraft information.
1207.6180_359394_0	  Observability is a key aspect of the state estimation problem of SLAM, However, the dimension and variables of SLAM system might be changed with new features, to which little attention is paid in the previous work.
1208.2199_362771_3	In order to increase the speed of convergence, modifications have been made in the algorithm where the weights get updated depending on the severity of disturbance.
1211.0572_382818_1	The setup has been designed for the detection of the electric field strength of air showers initiated by ultra-high energy cosmic rays, without using an auxiliary trigger from another detection system.
1212.3924_393540_2	Some numerical problems tied to the resolving of the non-linear system established are also covered.
1212.3924_393540_3	Part of a detailled simulation software (CODYRUN), the numerical implementation of this airflow model is explained, insisting on data organization and processing allowing the calculation of the airflows.
1212.3924_393540_4	Comparisons are then made between the model results and in one hand analytical expressions and in another and experimental measurements in case of a collective dwelling.
1212.5665_395281_4	On one side,they are just supposed to supply the demand of cooling loads with an ideal control loop (no delay between the sollicitations and the time response of the system), The available outputs are initially the hourly cooling and heating consumptions without integrating the real caracteristics of the HVAC system This paper is also following the same multiple model approach than for the building modelling by defining different modelling levels for the air conditionning systems, from a very simplified one to a detailled one.
1301.1064_397654_1	A fixed length of the lines is considered, and the aim of the control system is to obtain figure-eight crosswind trajectories.
1303.2255_413912_3	Secondly, two modifications to the ZA-LMS algorithm have been made.
1303.2255_413912_7	In addition, the mean square performance of DWZA-LMS algorithm is analyzed.
1303.2593_414250_5	ICA-type de-convolution algorithm that is FASTICA is considered for mixed signals de-convolution and considered convenient depending upon the nature of the source and noise model.
1303.3263_414920_3	The error estimation in Polynomial method is carried out by LMS Filter.
1305.1169_428437_5	The comparative experiments on a recently proposed benchmark set that are reported here demonstrate the usefulness of going Pareto-based in AI Planning.
1307.0671_442399_5	Finally, it is demonstrated that the RB scattering data in combination with the Tenti S6 model can be used to retrieve the actual gas temperatures.
1308.4936_455111_1	Except for the 171 and 193 \ang\ wavelengths, which are affected by EUV dimming caused by coronal mass ejections (CMEs), we find near-identical size distributions of geometric (lengths $L$, flare areas $A$, volumes $V$, fractal dimension $D_2$), temporal (flare durations $T$), and spatio-temporal parameters (diffusion coefficient $\kappa$, spreading exponent $\beta$, and maximum expansion velocities $v_{max}$) in different wavelengths, which are consistent with the universal predictions of the fractal-diffusive avalanche model of a slowly-driven self-organized criticality (FD-SOC) system, i.e., $N(L)
1308.5213_455388_4	The response of the system e.g. deformation time history due to the impact of the projectile has been studied where the Newmark method for the dynamic problem has been implemented.
1309.1075_458083_3	The impact energy stored in the splash structures was estimated via a theoretical model and several morphological parameters obtained from instantaneous images of the splash.
1311.0642_474303_7	The discrepancy between discrete model predictions and Monte Carlo predictions for the depth of maximum can therefore not be explained from the number of generations that is taken into consideration.
1311.7183_480844_5	For practical applications, a reduced-dimension version of the proposed KA-STAP algorithm is also developed.
1401.3566_492797_4	An expression for the excess mean square error (MSE) of the algorithm is also derived which suggests that under the right conditions, the reweighted l1-norm penalized LMS algorithm outperforms the standard LMS, which is expected.
1403.5933_510832_3	The proposed classifier is named as AIS-INMACA introduces a novel concept to combine CA with artificial immune system to produce a better classifier which can address major problems in bioinformatics.
1404.1605_514658_3	{Associating the position of the streamer head with the maximum value of the self-enhanced electric field, a delay of 160$\,$ps was experimentally found for the peak emission of the first negative system of N$_2^+$.} {For the first time, a delay dilatation was observed experimentally on early-stage streamers and clarified theoretically.}
1404.6813_519866_4	This brings up the issue of studying the performance of the diffusion LMS algorithm when it is run, either intentionally or unintentionally, in a multitask environment.
1404.7824_520877_9	Very similar results are found for the energy dissipation in a 2-D "hybrid" shell model of loop magneto-turbulence, based on reduced magnetohydrodynamics, which is compatible with nanoflare statistics.
1406.0975_529858_1	In this paper the novelty of information system is presented, in a dynamic, easily accessible and user-friendly manner.
1406.1213_530096_2	The underlying network stack is based on a communication system that was originally designed for robust underwater communication.
1408.6109_551415_2	Extensive Monte Carlo experiments have been carried out to validate the efficiency of the developed analytical model and to investigate the realistic performance of NCCARQ.
1409.3083_555508_5	An extended overview on the control system approach, as well as details of each element of the flight controller, are presented.
1411.0237_570209_4	While it is known that software can intentionally create radio emissions from a video display unit, this is the first time that mobile phones are considered in an attack model as the intended receivers of maliciously crafted radio signals.
1411.1629_571601_1	The primary problem is that these tests are designed to test aspects of knowledge and ability that are challenging for people; the aspects that are challenging for AI systems are very different.
1411.1629_571601_2	In particular, standardized tests do not test knowledge that is obvious for people; none of this knowledge can be assumed in AI systems.
1412.6223_584201_4	Tractable density evolution (DE) equations are derived to analyze the convergence property of iterative receivers in the large-system limit, via a tool developed in statistical physics---replica method.
1412.7239_585217_1	A systematic analysis of the experimental angular distributions was performed using the coupled channels method with an extended double folding potential derived from realistic wave functions for $^{12}$C and $^{16}$O calculated with a microscopic $\alpha$ cluster model and a finite-range density-dependent nucleon-nucleon force.
1501.04448_591196_6	Model parameters are estimated through the expectation-maximization algorithm, based on the forward-backward recursions, which is implemented in the main functions of the package.
1501.04859_591607_5	Finally, to demonstrate the effectiveness of the proposed algorithm and compare with similar earlier researches, a numerical example on a multi-agent system consisting of single link flexible manipulators is carried out.
1502.05698_600358_3	The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human.
1502.06431_601091_6	Using method of multiple scales and spectral analysis of steady-state CFD solutions, frequencies and damping terms in the van der Pol oscillator model are estimated.
1502.06918_601578_0	  We consider a simplified physics of the could interface where condensation, evaporation and radiation are neglected and momentum, thermal energy and water vapor transport is represented in terms of the Boussinesq model coupled to a passive scalar transport equation for the vapor.
1503.00586_603300_3	The influence of reproduction method and array size on performance measures of representative multi-microphone hearing aid algorithm classes with spatially distributed microphones and a representative single channel noise-reduction algorithm was analyzed.
1503.06171_608885_0	  The short-term forecasting of real-time locational marginal price (LMP) and network congestion is considered from a system operator perspective.
1504.04611_616516_6	For the first time precise parameters are derived for both stars in this system.
1505.00422_620694_2	The management of this system is carried out using specialized equipment and software.
1507.08932_646593_2	For a complete understanding of this phenomenon and the underlying physical processes, an absolute calibration of the detecting antenna system is needed.
1508.05494_652150_8	Based on the developed model, the setup of the optimal control problem (OCP) is described in detail along with its numerical solution based on the direct multiple shooting method in the CasADi optimization environment.
1509.00671_655310_0	  The Airy structure in $^{16}$O+$^{14}$C rainbow scattering is studied with an extended double folding (EDF) model that describes all the diagonal and off-diagonal coupling potentials derived from the microscopic realistic wave functions for $^{16}$O using a density-dependent nucleon-nucleon force.
1509.06739_661378_2	We show that the precession period in the time interval 1991--2015 is near its equilibrium value $P_{sup} = 30.370$ days, while the observed historical changes in the phase of this variability can be interpreted in terms of the "red noise" model.
1509.08378_663017_3	Experimental results are presented for silica aerogel arranged in parallel with either 1 or 2 acoustic ports, and are in very good agreement with the theoretical model.
1510.01562_665519_5	Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems, thanks to the distributed representation of words they are based upon, such models could not be used readily in IR, where the estimation of one language model per document (or query) is required.
1602.02367_702646_4	Moreover, the theoretical properties of the algorithm are studied and it is proved that under certain assumptions the algorithm suffers a no regret bound.
1602.06913_707192_3	Specifically, the transmit power minimization problem and the max-min fairness energy harvesting problem are formulated for both the bounded CSI error model and the probabilistic CSI error model.
1603.01658_711079_1	We investigate the simulation cell size dependence of the surface tension of water from force field molecular dynamics (MD) simulations, which show that the calculated surface tension increases with increasing simulation cell size, thereby illustrating that a correction for finite size effects is required for the small system used in the AIMD simulation.
1603.06194_715615_5	Since there is a large uncertainty in the range, we cannot concretely rule out any of the geometries or spectral models, but the analysis suggests that a boundary layer type geometry with a 'cold' seed spectral model is favoured over an accretion corona model.
1603.07276_716697_6	Visualization and illustration of the proposed data-driven approach are performed on a 3-bus system as well as the IEEE 118-bus system.
1603.09022_718443_0	  A new Lp-norm constraint least mean square (Lp-LMS) algorithm with new strategy of varying p is presented, which is applied to system identification in this letter.
1604.08796_727963_4	The Ar 4p population, determined through absolute optical emission spectroscopy, is seen to decrease with separation distance, whereas a rise in emission from the N2(C--B) system is measured.
1605.02797_730912_3	Machine learning tools are thought to predict within a fraction of a millisecond if a model is excluded or not directly from the model parameters.
1605.07059_735174_7	The radii are found to be $R_1=1.835\pm0.014$ R$_{\odot}$, $R_2=2.912\pm0.014$ R$_{\odot}$ and the masses $M_1=1.1973\pm0.0037$ M$_{\odot}$, $M_2=1.2473\pm0.0039$ M$_{\odot}$. From the best-fit stellar models we infer a mixing length of 1.78, a helium abundance of $Y_{AI}=0.26^{+0.02}_{-0.01}$ and an age of $4.39\pm0.32$ Gyr.
1606.01352_739259_5	Specific approximations and simplifications are performed to enable the implementation of the algorithm using the Airbus graphical symbol library for industrial validation and verification.
1606.04766_742673_8	Validation of this system using phantom and ex vivo experiments has been demonstrated.
1606.08810_746717_1	The energy and primary mass evolution of the number of muons is studied based on the Heitler-Matthews model and Monte Carlo simulation of the air shower.
1606.08810_746717_3	An analysis approach based on the comparison between this model's predictions and data to discriminate among a set of composition scenarios is presented and tested with simulations.
1606.09524_747431_6	The discharges effects (radical and excited species creation and thermal relaxation) are involved through chemical and energy source terms calculated from a complete 2Drz streamer model already developed.
1607.01486_749037_6	Experimental data from a real-time implementation of the system on a 50 gram embedded computer are presented in addition to the simulations to demonstrate the efficacy of the proposed system.
1607.05306_752857_0	  An optical model for a solar power tower with an open volumetric air receiver (OVAR) was developed in this paper, and the optical performance and characteristics of the OVAR were studied based on the model.
1607.06532_754083_2	Their pros and cons have been extensively analyzed and evaluated in recent studies, but there is relatively less work continuing the line of research to develop an enhanced learning method that brings together the advantages of the two model families.
1608.03990_760452_3	Model augmentations are developed for the Spalart Allmaras (SA) model using adjoint-based full field inference on experimentally measured lift coefficient data.
1608.07046_763508_4	In this work, a detailed analysis in the mean and mean-square sense is carried out in order to examine the behavior of the algorithm.
1608.08644_765106_3	A measurement campaign was conducted to compare the performance of the beam-space MIMO system with a conventional 2-by-?2 MIMO system under realistic propagation conditions.
1609.01235_766713_2	However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model.
1609.01462_766940_3	Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set.
1609.07049_772527_2	These masks are designed for an average facial model and are often difficult to adjust due to poor fit to the actual patient.
1610.00058_775405_6	An analysis of the computational complexity of the proposed algorithms, the delay and a study of the greedy algorithm are then carried out.
1610.06108_781455_3	We show that also in this case the universal limit can be expressed as a matrix integral ("matrix Bessel function") known in the literature as generalized Kontsevich matrix model.
1701.05677_811085_2	For the evaluation purpose, the problems of system identification and channel equalization are considered.
1701.07338_812746_3	These offsets were found to be proportional to the geometric distance to $X_{\rm max}$. We have compared the results to a simple model based on the Cherenkov angle.
1702.00178_814772_2	Due to their ability to learn longer-term dependencies, these models are supposed to learn and to apply musical knowledge, instead of just smoothing the output of the acoustic model.
1702.04670_819264_4	The simulations are performed on FIB-SEM 3D reconstructions of an Ag model electrode for predefined saturation of the pore space with the liquid phase.
1702.07024_821618_0	  We prove the existence of Noise Induced Order in the Matsumoto-Tsuda model, where it was originally discovered in 1983 by numerical simulations.
1702.07024_821618_4	This is realized by explicit functional analytic estimates working together with an efficient algorithm.
1703.10724_834228_6	Building LSTM $n$-gram LMs may be appealing for some practical situations: the state in a $n$-gram LM can be succinctly represented with $(n-1)*4$ bytes storing the identity of the words in the context and batches of $n$-gram contexts can be processed in parallel.
1704.00717_835229_7	We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model.
1706.02565_857300_0	  In this paper, investigations are conducted using Reynolds-averaged Navier-Stokes (RANS) turbulence models to investigate the importance of turbulence modelling for nasal inspiration at a constant flow rate of 250 ml/s. Four different, standard turbulence models are tested in a model geometry based on pre-operative CT images of a selected obstructive sleep-apnea syndrome (OSAS) patient.
1707.04959_869991_4	In the paper, conditions for the complete controllability for the case of a magnetically controlled satellite with passive air drag panels are developed, and simulation case studies with the LQR and MPC control designs applied in combination with a nonlinear time-varying input transformation are presented to demonstrate the ability of the closed-loop system to satisfy mission objectives despite disturbance torques.
1707.07240_872272_7	A number of technical contributions, including employing deep convolutional neural networks (CNNs) to define the potentials and incorporating the joint stochastic approximation (JSA) strategy in the training algorithm, are developed in this work, which enable us to successfully train neural TRF LMs.
1707.09095_874127_1	After many hype cycles and lessons from AI history, it is clear that a big conceptual leap is needed for crossing the starting line to kick-start mainstream AGI research.
1707.09865_874897_5	According to our occlusion model, a point density of ~170 pt/m-sqr is needed to segment understory trees as accurately as overstory trees.
1708.02553_877565_5	On the other hand, we identify computer algebra systems (CASs) as being primitive examples of domain-specific oracles for mathematics and argue that efforts to integrate computer algebra systems with theorem provers, systems which have largely been developed independent of one another, provide a concrete set of problems related to the notion of provable safety that has emerged in the AI safety community.
1708.02553_877565_6	We review approaches to interfacing CASs with theorem provers, describe well-defined architectural deficiencies that have been identified with CASs, and suggest possible lines of research and practical software projects for scientists interested in AI safety.
1708.03091_878103_0	  A perturbation series solution is constructed in terms of Airy functions for a nonlinear two-point boundary-value problem arising in an established model of steady electrodiffusion in one dimension, for two ionic species carrying equal and opposite charges.
1708.06746_881758_0	  Air traffic is widely known as a complex, task-critical techno-social system, with numerous interactions between airspace, procedures, aircraft and air traffic controllers.
1708.06746_881758_7	Finally, at the system level, chaos is identified in conflict system and human behavioral system when traffic switch to the semi-stable or congested phase.
1709.01443_886309_4	Finally, simulation experiments in a lagoon are presented, demonstrating the fidelity of the proposed modelling system to optimize water quality control through regular in field measurements and simulations.
1709.02528_887394_4	More specifically, a closed-form lower bound is firstly derived to quantify the achievable SE of the proposed system.
1709.05067_889933_4	Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail.
1709.08272_893138_6	Thereafter, the heat transfer between the air in the cavern and the cavern wall is considered and integrated into the cavern model.
1710.00194_895577_5	Namely, the learning phase of the algorithm is composed of the following steps: (i) given a sequence of COD images, the snapshots of the optical flow are estimated from two consecutive COD images; (ii) these snapshots are then assimilated into a Navier-Stokes Equation (NSE), i.e. an initial velocity field for NSE is selected so that the corresponding NSE' solution is as close as possible to the optical flow snapshots.
1710.04381_899764_4	A rigorous mean and mean square performance evaluation is conducted to justify the performance advantages of the proposed scheme over the conventional augmented LMS solution.
1710.07375_902758_1	A simulation using SPICE (Simulation Program with Integrated Circuit Emphasis) is carried out based on a conventional CMOS transistor model.
1710.10636_906019_3	Uncertain dynamic system with different working condition has been considered for the vehicle air suspension system.
1711.01134_908144_6	Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation, and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely.
1711.05905_912915_3	Compared with existing mathematical frameworks and testing method on AI ethics technology, the advantages of the proposed approach are analyzed.
1711.10089_917099_9	The paper is concluded with a discussion of challenges and benefits of adopting big data analytics and artificial intelligence in the next-generation communication system.
1711.10339_917349_6	DCGAN is used as sample generation and feature learning model, and SVM is adopted as the classifier for predicting candidate's labels in the inference stage.
1712.02221_920820_5	Surprisingly, we observed that local air injection is not necessary to sustain the air cavities; as long as air is present in the system it is found to be captured in the cavity.
1801.00619_929550_1	It means that the distributions do not depend on the primary particle energy or mass (thus, neither on the interaction model), shower zenith angle or shower to shower fluctuations, if they are taken at the same shower age.
1801.01704_930635_2	The use of AI-based techniques is first studied in applications related to optical transmission, ranging from the characterization and operation of network components to performance monitoring, mitigation of nonlinearities, and quality of transmission estimation.
1801.02485_931416_5	The models are trained and evaluated using real market data in the Midcontinent Independent System Operator (MISO) region.
1801.02782_931713_1	In this system, the propulsion energy consumption at the UAV is taken into account so that the UAV's velocity and acceleration should not exceed a certain threshold.
1801.07844_936775_12	Our scheme is proven secure in the standard model (in a selective manner), based on the hardness of the Learning With Errors (LWE) problem.
1802.02172_941712_5	The mathematical foundations of AI non-destructive correction are presented and a series of new stochastic separation theorems is proven.
1802.04451_943991_7	The alliance of AI and blockchain is expected to create numerous possibilities.
1802.06588_946128_7	Different predictive models based on multinomial logistic regression and decision trees are formulated and calibrated with historical traffic data, and a critical evaluation of each model is conducted.
1802.07218_946758_0	  Predicting the outcomes of integrating Unmanned Aerial Systems (UAS) into the National Airspace System (NAS) is a complex problem which is required to be addressed by simulation studies before allowing the routine access of UAS into the NAS.
1802.07494_947034_5	The spectral analysis is performed with Potsdam Wolf-Rayet (PoWR) model atmospheres.
1802.09124_948664_4	Structural properties of the model allow it to be decomposed into a finite set of linear programs (LP) and a computationally tractable algorithm for its solution is described.
1803.07233_957366_1	However, the use of the scientific method - specifically hypothesis testing - in AI is typically conducted in service of engineering objectives.
1803.08198_958331_2	We analyze our method under the general asynchronous model of computation, in which each function is selected infinitely often with possibly unbounded (but sublinear) delay.
1804.03877_965570_1	The number of muons observed at the ground from UHECR-induced air showers is expected to depend upon the hadronic interaction model.
1804.07827_969520_6	In model training, LMs are learned with layer-wise dropouts for better robustness.
1805.02386_975431_1	It is shown that all these correlations exhibit the quantum phase transitions at $\Delta=-1$. However, only information deficit and local quantum uncertainty can demonstrate quantum phase transitions at $\Delta=1$. The analytical and numerical behaviors of the quantum correlations for the $XXZ$ system are presented.
1805.06217_979262_4	Experimental evaluation is also performed in an enterprise environment to demonstrate the competence of the proposed AI-framework in perceiving such a dense scenario and reason the extender deployment that achieves user quality of service (QoS).
1805.06247_979292_2	Therefore, Artificial Intelligence (AI) is adopted to support network autonomy and to capture insights on system and environment evolution.
1806.01782_987422_3	Convergence analysis of the LMS algorithm in the case of coloured input signal, i.e., correlated input signal is demonstrated on adaptive FIR filter via power spectral density of the input signals and Fourier transform of the autocorrelation matrix of the input signal.
1806.01782_987422_4	Simulations have been carried out on adaptive filtering of FIR and IIR filters and tested on white and coloured input signals to validate the powerfulness of the genetic-based LMS algorithm.
1806.02615_988255_7	As a proof-of-concept case study, a rich longitudinal dataset is selected to evaluate the effectiveness of the proposed approach versus a standard regression model.
1806.03777_989417_4	Practicability and repeatability of the developed programme has been established for monophasic system using backscattered imaging mode of SEM.
1807.06695_1003903_6	Monte-Carlo simulation results are presented for validating our theoretical analysis as well as for investigating the impact of the key system parameters.
1807.07157_1004365_2	Science observations are taken simultaneously with environmental data revealing information about the turbulence in the telescope environment as well as limitations of GPI's AO system.
1807.07395_1004603_5	Convergence of the proposed method is investigated and it is proved that the rate of convergence of the introduced method is equal to that of LMS algorithm in the expected value sense, provided that the distribution of the added noise is uniform.
1808.08079_1017225_1	We show that `diagnostic classifiers', trained to predict number from the internal states of a language model, provide a detailed understanding of how, when, and where this information is represented.
1808.08360_1017506_5	At the receiver, channel estimation is performed based on a threshold method and the estimated channel information is used for data detection via a message passing (MP) algorithm.
1809.00125_1020141_1	We investigate an alternative simple method to use monolingual data for NMT training: We combine the scores of a pre-trained and fixed language model (LM) with the scores of a translation model (TM) while the TM is trained from scratch.
1809.01022_1021038_1	Taking the cross-entropy as loss function, a feed forward network is trained by backpropagation algorithm to output the condition probability through the softmax activation function, thereby assisting in a modified log-likelihood ratio (LLR) improvement.
1809.04258_1024274_0	  In this work, an ontology-based model for AI-assisted medicine side-effect (SE) prediction is developed, where three main components, including the drug model, the treatment model, and the AI-assisted prediction model, of proposed model are presented.
1809.04258_1024274_3	The results preliminarily reveal that it is a relationship between the ontology-based attributions and the corresponding predicted indicator that can be learnt by AI for predicting the SE, which suggests the proposed model has a potential in AI-assisted SE prediction.
1809.05673_1025689_5	Considering the revolutionary impact of AI technologies on the data, transmission and protocol architecture of wireless communication systems, the future challenges of AI wireless communication systems are analyzed.
1809.07027_1027043_2	In this study, the solution is seen on philosophical conceptualization as a framework to form practical implementation model for ethics of AI.
1809.07027_1027043_4	A keyword based Systematic Mapping Study (SMS) on the keywords used in AI and ethics was conducted to help in identifying, defying and comparing main concepts used in current AI ethics discourse.
1809.07672_1027688_3	The limitation to the measurement system was found to be from the external, room temperature electronics.
1809.08267_1028283_0	  The present paper surveys neural approaches to conversational AI that have been developed in the last few years.
1809.08272_1028288_2	The simple prototype of such a system was created in the Laboratory of Applied Mathematics of Ariel University (under the supervision of Prof. Domoshnitsky Alexander) in collaboration with company TRANSIST VIDEO LLC (Skolkovo, Moscow).
1809.08748_1028764_9	Finally, the strengths of AI based techniques are determined and open research challenges deduced from the recent research are presented.
1809.09046_1029062_9	No significant variability between the different specimens of the same chamber model studied was found.
1810.03070_1034257_2	Thanks to a dedicated offine selection algorithm, 564 air shower candidates were indeed selected out of $7\cdot10^8$ transient radio signals recorded during the 314 live days of data taken during the first two years of operation of this setup (2011 and 2012).
1810.04053_1035240_1	Vast sums of money have been thrown at AI start-ups.
1810.08514_1039701_2	As the major component, the optimization problem of our system is studied in detail.
1810.09654_1040841_4	Based on a quantitative analysis of the model performances, it was found that the most appropriate model to use for predicting laminar flame properties of n-hexane-air mixtures is JetSurF.
1810.11116_1042303_0	  "Value alignment" (VA) is considered as one of the top priorities in AI research.
1811.05498_1050131_5	For this novel Fog-RAN model, the fundamental tradeoff among (a) the amount of cache memory at the SBSs, (b) the load on the downlink (from MBS to directly served users and SBSs), and (c) the aggregate load on the sidelink is studied, under the standard worst-case demand scenario.
1811.08762_1053395_2	This dissertation is a contribution to the design of an Onboard Context-Sensitive Information System (OCSIS), which was developed on a tablet.
1811.09181_1053814_2	It is then shown that the system model can be regarded as a special case of a multidimensional random-walk phase-noise model.
1811.11839_1056472_1	Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems.
1811.12454_1057087_3	In order to help them in this complex task, and additionally, provide an auxiliary tool for less experienced oncologists, it is presented a project conception of a software system that will make use of a hybrid data-oriented approach.
1812.02588_1060162_4	The proposed $q$-least mean fourth ($q$-LMF) is an extension of least mean fourth (LMF) algorithm and it is based on the $q$-calculus which is also known as Jackson derivative.
1812.07519_1065093_11	However, the NeurIPS conference is recognised to be amongst the top tier concerning ML/AI studies, so it is reasonable to consider its corpus to be representative of high-quality research.   
1812.11142_1068716_2	DIAL (The Diagrammatic AI Language) has been created with the aspiration of being an "engineering schematic" for AI Systems.
1812.11142_1068716_3	It is presented here as a starting point for a community dialogue towards a common diagrammatic language for AI Systems.
1812.11670_1069244_2	We then model the track points on trajectories as conditional Gaussian mixtures with parameters to be learned from our proposed deep generative model, which is an end-to-end convolutional recurrent neural network that consists of a long short-term memory (LSTM) encoder network and a mixture density LSTM decoder network.
1901.02661_1072208_3	The examples of past research and its implications are grouped and mapped onto the media layers of ISO OSI model to show just how ubiquitous dynamical systems theory can be and to trace the paths that may be taken now.
1901.11184_1080731_1	Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans.
1902.02905_1083989_8	Accuracy in the form of sensitivity and specificity of the AI mobile App was determined  comparing its assessments to those of the retina specialists.
1902.03875_1084959_4	Based on a simple model for the atmosphere and well-known first principles on the development of extensive air-showers, the zenith angle dependence is expressed analytically.
1902.06824_1087908_5	Multiple fare classes, concurrent continuous arrival of passengers of different fare classes, overbooking and random cancellations that are independent of class have been considered in the model.
1902.07778_1088862_2	Specifically, we study the stability of the closed-loop system when the predictor feedback is designed based on the knowledge of the nominal value of the time-varying delay.
1902.08238_1089322_0	  Inversions of airborne EM data are often an iterative process, not only requiring that the researcher be able to explore the impact of changing components such as the choice of regularization functional or model parameterization, but also often requiring that forward simulations be run and fields and fluxes visualized in order to build an understanding of the physical processes governing what we observe in the data.
1903.02534_1094920_1	Local and uniform stability of the fractional order model is studied.
1903.02852_1095238_2	While AL is known to be beneficial to AM training, we show that it also carries out substantial improvements to the LM when combined with SST.
1903.03171_1095557_6	Each of these factors is seen to present a 'moving target' for discussion, which poses a challenge for both technical specialists and non-practitioners of AI systems development (e.g., philosophers and theologians) to speak meaningfully given that the corpus of AI structures and capabilities evolves at a rapid pace.
1903.04361_1096747_4	Thus, the framework distinguishes between the different questions that might be asked about an opaque computing system, and specifies the general way in which these questions should be answered.
1903.05456_1097842_8	A validation case is considered to evaluate the efficiency of the model for a nonlinear problem.
1903.06914_1099300_7	Thereafter, the heat transfer between the air in the cavern and the cavern wall is considered and integrated into the cavern model.
1903.08238_1100624_4	The effectiveness of the system is established with a real-time system under general room conditions.
1903.09475_1101861_7	A recent addition to Z3 allowed us to describe sequences of transitions as a recursive function, thus we can check if a solution can be found in the defined model.
1904.01717_1106757_0	  In last passage percolation models lying in the Kardar-Parisi-Zhang universality class, maximizing paths that travel over distances of order $n$ accrue energy that fluctuates on scale $n^{1/3}$; and these paths deviate from the linear interpolation of their endpoints on scale $n^{2/3}$. These maximizing paths and their energies may be viewed via a coordinate system that respects these scalings.
1904.06234_1111274_7	We apply two different sub-approaches in the LM Based approach and the combined result of these two approaches is considered as the final output of the system.
1904.08796_1113836_0	  PURPOSE OF REVIEW: Despite the impressive results of recent artificial intelligence (AI) applications to general ophthalmology, comparatively less progress has been made toward solving problems in pediatric ophthalmology using similar techniques.
1904.09251_1114291_4	We show that the error dynamics follows a log-linear autonomous differential equation with several important consequences: (a) the observable state variables can be rendered convergent with a domain of attraction that is independent of the system's trajectory; (b) unlike the standard EKF, neither the linearized error dynamics nor the linearized observation model depend on the current state estimate, which (c) leads to improved convergence properties and (d) a local observability matrix that is consistent with the underlying nonlinear system.
1904.09521_1114561_3	The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge.
1905.01023_1119459_7	This review article is meant to supplement the previously presented efforts to bridge the gap between AI and physics, and take a serious step forward to filter out the "Babelian" clashes brought about from such gabs.
1905.04224_1122660_3	However, application of AI for differentiation and classification of multiple eye diseases is not yet established.
1905.06876_1125312_6	The focus is exclusively on Machine Learning, but it is hoped that the results of this research may be easily applicable to other branches of AI.
1905.07857_1126293_3	A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming.
1905.09130_1127566_3	AI-CARGO also includes a data cleaning component to deal with the heterogeneous forms in which booking data is transmitted to the airline cargo system.
1905.10985_1129421_11	Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.
1905.11474_1129910_3	Top Bankruptcy Prediction Models are A.I.-based and are in need of better explainability -the extent to which the internal working mechanisms of an AI system can be explained in human terms.
1906.02040_1134226_6	Experiments are performed on three classification databases: Hemorrhage, BraTS18 and Cervix to validate the universality of our innovative model.
1906.04006_1136192_3	This proposed A/C thermal management strategy is developed and evaluated based on a physics-based A/C system model (ACSim) from Ford Motor Company for the vehicles with electrified powertrains.
1906.04013_1136199_2	Channel statistics for different propagation scenarios were obtained, and the Saleh-Valenzuela (SV) model was found to provide a good fit for the statistical channel model.
1906.11668_1143854_0	  In the last five years, private companies, research institutions as well as public sector organisations have issued principles and guidelines for ethical AI, yet there is debate about both what constitutes "ethical AI" and which ethical requirements, technical standards and best practices are needed for its realization.
1907.00430_1144966_0	  Being a complex subject of major importance in AI Safety research, value alignment has been studied from various perspectives in the last years.
1907.01297_1145833_3	Owing to wide-spread use of neural networks in all areas of AI, this problem is seen as particularly acute and pervasive.   
1907.02227_1146763_5	We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.
1907.03064_1147600_4	Three semi-supervised training passes were performed, where the decoded output from each pass was used for acoustic model training in the subsequent pass.
1907.04944_1149480_2	For this to be possible, it would need to be the case that for any target sentence of interest, there is some continuous representation that can be passed to the language model to cause it to reproduce that sentence.
1907.05505_1150041_3	Here we consider how these automation mechanisms can be leveraged to offer AI-aaS. Use cases for AI-aaS are easily found in addressing smart applications in sectors such as transportation, manufacturing, energy, water, air quality, and emissions.
1907.06266_1150802_3	Firstly, an Extended Kalman Filter is designed as a model-based approach.
1907.12239_1156775_3	The model was constructed using different numbers of POD or DMD modes for order reduction of the fluid data and different methods of estimating the linear coefficients, and the effects of these conditions on the model performance were quantitatively evaluated.
1907.12393_1156929_10	When defining codes of conduct and regulatory policies for AI, a clear understanding about the time-scale of the race is required.
1908.01024_1159194_1	Yet at the same time, analyzing the ethical implications of AI for disabled people solely through the lens of a singular idea of "fairness" risks reinforcing existing power dynamics, either through reinforcing the position of existing medical gatekeepers, or promoting tools and techniques that benefit otherwise-privileged disabled people while harming those who are rendered outliers in multiple ways.
1908.02150_1160320_4	It is believed that this research will work as a guideline and roadmap for researchers and industries towards the real-world implementation of Industrial AI.
1908.02624_1160794_2	The deployment of AI systems has not only created a trillion-dollar industry that is projected to quadruple in three years, but has also exposed the need to make AI systems fair, explainable, trustworthy, and secure.
1908.05059_1163229_1	In this paper, we argue that Explainable Planning can be designed as a service -- that is, as a wrapper around an existing planning system that utilises the existing planner to assist in answering contrastive questions.
1908.08939_1167109_3	Considering the needs of users with disabilities can help technologists identify high-impact challenges whose solutions can advance the state of AI for all users; however, ethical challenges such as inclusivity, bias, privacy, error, expectation setting, simulated data, and social acceptability must be considered.
1908.09523_1167693_4	The focus of this paper is to illustrate exemplaryapplications of AI techniques at the Physical Layer (PHY) offuture wireless systems and therfore they can be seen as candidatetechnologies for e.g. 6G systems.
1908.10300_1168470_1	At the same time, it is expected that there will be increasing instances of AI failure as it operates on imperfect data.
1908.10322_1168492_1	It has been assumed that injecting the prior knowledge of a tokenizer into the model is essential to achieving competitive results.
1908.10749_1168919_1	The theoretical analysis is presented from a matrix optical model for an Airy beam.
1909.01683_1171721_3	Furthermore, in this algorithm, an adaptive early termination algorithm and a modified searching process are performed based on the predicted approximation error, which is determined from the FS-Net-based initial solution, so that the optimal solution can be reached earlier.
1909.03143_1173181_1	Also, a unified framework basis for the design/analysis of vectorial BSMC, as well as sliding mode control (SMC) and backstepping control (BS) for a system in lower triangular block form is derived.
1909.03373_1173411_3	The proposed approach includes a traditional AGV scheduling algorithm, which aims at solving deadlock problems, and an artificial neural network based component, which predict future tasks of the AGV system, and make decisions on whether to send an AGV to the predicted starting location of the upcoming task,so as to save the time of waiting for an AGV to go to there first when the upcoming task is created.
1909.03801_1173839_3	Even if it is not the intended use of a developed prediction model, informal decision rules can often be found in practice.
1909.05713_1175751_5	The effective exposure, regarding the role of the clouds in particular, was estimated based on the air shower and detector simulations together with a numerical weather forecast model.
1909.06695_1176733_1	Remarkable performance has been demonstrated recently across many NLP domains via a Transformer-based language model with over a billion parameters, verifying the benefits of model size.
1909.06842_1176880_2	Nowadays, many-core AI accelerators (e.g., GPUs and TPUs) are designed to improve the performance of AI training.
1909.09418_1179456_5	The advantage of the formal representation is that the functional safety of the system can be analytically inferred.
1909.09598_1179636_6	Additionally, the model is trained as an image classifier, allowing for a faster and more accurate model.
1909.10327_1180365_3	Recently, it has been shown how compression errors can be compensated for in the optimization algorithm to improve the solution accuracy.
1909.11503_1181541_5	The statement level attention mechanism is also adopted in the model.
1909.11637_1181675_4	Therefore, a comparison analysis for AI models is required to guide practitioners to the appropriate model.
1909.13519_1183557_5	The effectiveness of the proposed ATM system is demonstrated in a simulation using actual air traffic data.
1910.06266_1190185_6	In this paper, we will describe the overall design of this system, the major use-cases that have been identified for it, and the lessons learnt when deploying this system for some of those use-cases
1910.08111_1192030_8	The age of the system was estimated to from 11.7 to 13.8 Myr, depending on the model.
1910.10045_1193964_2	Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models.
1910.10799_1194718_5	We show how the Airy stress function depends on cell shape when a standard energy functional is adopted, and discuss implications for computational implementations of the model.
1910.13434_1197353_1	Such phenomenon can be quantitatively evaluated with the irreversible entropy generation (IEG), which was recently found to follow a $1/\tau$ scaling for the system under a long contact time $\tau$ with the thermal bath.
1910.14150_1198069_3	The performance of the AA-BUD algorithm has been demonstrated via extensive simulations, and it is superior to two benchmark algorithms with up to 45.8% throughput improvement.
1911.01063_1199657_3	A multivariable static output feedback (SOF) controller is designed for the linear state space model formulated in the IGC framework.
1911.01063_1199657_6	Extensive non-linear simulations are performed on high fidelity 150 mm wingspan MAV model to demonstrate the potential advantages of the proposed waypoint navigation algorithm.
1911.01704_1200298_1	To satisfy the needs of high throughput and low latency, belief propagation (BP) is chosen as the decoding algorithm.
1911.04469_1203063_8	Then, a fusion step between the two streams is performed, and the results are fed into the proposed action tracking model.
1911.08448_1207042_5	Its discretization results in some tables of bids, which are basically expected returns for main investment horizons, the key in our trading system.
1911.09264_1207858_3	During training stage, patch-based anatomical features were extracted from registered MRI-CT training images, and the most informative features were identified to train a series of classification forests with auto-context model.
1911.12886_1211480_10	Next, various training strategies are presented to meet multiple desiderata, such as model performance, generalization ability, data privacy protection, and learning with sparse annotations.
1911.13136_1211730_3	The models and Bayesian inference are illustrated on a sample of 10-year data from four major airlines within the US air transportation system.
1912.01629_1213525_5	Several experiments were carried out to verify the performance of the model in different emergency scenarios.
1912.03213_1215109_1	It is expected to be the main part of the next-generation aviation communication system to support fixed and mobile services for manned and unmanned applications.
1912.03652_1215548_5	These objectives result in trade-offs that are analyzed for a deep learning system classifying handwritten digits.
1912.06817_1218713_3	We draw on the state of the art to provide factual arguments for a discussion on well-established AI in cybersecurity issues, including the current scope of AI and its application to cybersecurity, the impact of privacy concerns on the cybersecurity data that can be collected and shared externally to the organization, how an AI decision can be explained to the person running the operations center, and the implications of the adversarial nature of cybersecurity in the learning techniques.
1912.06860_1218756_4	Specifically, we formalize the problem as a multiagent Markov Decision Process (MA-MDP) and we show that it can be considered as a Markov game in which interacting agents need to reach an equilibrium: What makes the problem more interesting is the dynamic setting in which agents operate, which is also due to the unforeseen, emergent effects of their decisions in the whole system.
1912.09318_1221214_2	Using a unique corpus of 283,676 application essays submitted to a large, selective, state university system between 2015 and 2016, we assess the extent to which applicant demographic characteristics can be inferred from application essays.
1912.10163_1222059_0	  Deep learning methods that extract answers for non-factoid questions from QA sites are seen as critical since they can assist users in reaching their next decisions through conversations with AI systems.
1912.11435_1223331_4	The results of some laboratory and field tests of the optical system are presented.
2001.00463_1225876_6	While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research.
2001.07038_1232451_0	  The lack of diversity of the Artificial Intelligence (AI) field is nowadays a concern, and several initiatives such as funding schemes and mentoring programs have been designed to overcome it.
2001.07038_1232451_3	These indicators are designed to quantify the diversity of the AI field and monitor its evolution.
2001.07054_1232467_3	Specifically, the transmit power minimization problems are formulated subject to the worst-case rate constraints under the bounded CSI error model and the rate outage probability constraints under the statistical CSI error model, respectively.
2001.08308_1233721_7	Accordingly, we advocate that our approach could be adopted more generally in model-based geostatistical design.
2001.09038_1234451_3	The next stage is design, where the goal is to assemble or tune material structures so that they can achieve user-demanded target property values based on a prediction model that is trained in the modeling stage.
2001.09335_1234748_0	  With the advent of millimeter wave (mmWave) communications, the combination of a detailed 5G network simulator with an accurate antenna radiation model is required to analyze the realistic performance of complex cellular scenarios.
2001.09464_1234877_4	However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI.
2001.09768_1235181_8	The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.
2001.09778_1235191_1	This paper provides an overview of the current and near-future applications of Artificial Intelligence (AI) in Medicine and Health Care and presents a classification according to their ethical and societal aspects, potential benefits and pitfalls, and issues that can be considered controversial and are not deeply discussed in the literature.   
2002.00357_1237780_4	The relationship of this model class with log-linear models and quasi log-linear models is studied in detail in terms of both statistics and algebraic geometry.
2002.01726_1239149_3	However, the impact of the different social media networks on machine learning model performance has not been studied comprehensively yet.
2002.05486_1242909_3	Based on this model, performance metrics including the achievable data rate and coverage probability are derived for two types of aUEs: {\it i)} the general aUE (i.e., an aUE having distinct distances from its serving aBSs) and {\it ii)} the worst-case aUE (i.e., an aUE having equal distances from its serving aBSs).

2002.08303_1245726_4	It is well known that network delay in message propagation is hard to prevent, which could potentially degrade the performance of the system or even create safety issues for vehicles at intersections.   
2002.11268_1248691_4	The Density Ratio method was found to consistently outperform the dominant approach to LM and end-to-end ASR integration, Shallow Fusion.
2002.11954_1249377_1	Specifically, random arrival traffic is considered and an adaptive modulation and coding (AMC) scheme is adopted in the cooperative transmission system to improve the system performance.
2002.11954_1249377_3	With the packet dropping rate and stationary distribution of the system state, the closed-form expression of the delay is derived.
2002.11954_1249377_5	In this context, an energy efficiency optimization problem is formulated to determine the optimum strategy of power and time allocation for the relay-assisted cooperative system.
2003.06507_1256862_1	This paper explores the proposal to extend legal personhood to AI and robots, which had not yet been examined through the lens of the general public.
2003.06700_1257055_3	CoCoPIE is a software framework that holds numerous records on mobile AI: the first framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer, language models, and so on; the fastest DNN pruning and acceleration framework, up to 180X faster compared with current DNN pruning on other frameworks such as TensorFlow-Lite; making many representative AI applications able to run in real-time on off-the-shelf mobile devices that have been previously regarded possible only with special hardware support; making off-the-shelf mobile devices outperform a number of representative ASIC and FPGA solutions in terms of energy efficiency and/or performance.
2003.07107_1257462_6	Moreover, the analytical bit-error-rate expression of the proposed system is derived over multipath Rayleigh fading channels.
2003.07107_1257462_7	Furthermore, the spectral efficiency and energy efficiency of the proposed system are analyzed.
2003.07630_1257985_2	The weighted federated learning concept formalized in this paper differs from that presented in McMahan et al.'s paper since both addition and multiplication operations are executed over ciphers in our model while these operations are executed over plaintexts in McMahan et al.'s model.   
2003.10303_1260658_5	Our framework highlights the risks, objectives and key results which are typically required to proceed through a three-phase process to the market launch of a validated medical AI product.
2003.10303_1260658_9	Our framework should be seen as a template for innovation frameworks, which can be used to coordinate team communications and responsibilities towards a reasonable product development roadmap, thus unlocking the potential of AI in medicine.
2003.11157_1261512_9	To this end, we suggest criteria for assessing whether an AI system is sufficiently transparent about conflicts of interest, and acting in a manner that is loyal to the user, and argue that AI loyalty should be considered during the technological design process alongside other important values in AI ethics such as fairness, accountability privacy, and equity.
2003.12141_1262496_0	  IBM Research Castor, a cloud-native system for managing and deploying large numbers of AI time-series models in IoT applications, is described.
2004.00392_1265162_2	A robust dynamic output feedback controller is designed that asymptotically stabilizes the interval FO-LTI system, where no limiting constraint is assumed on the state space matrices of the uncertain system.
2004.02687_1267457_5	The representation on the latent two-dimensional coordinate system can be seen as an additional metadata of the real-world data that disentangles important distribution characteristics, such as shape of the CDF, classification probabilities of underlying theoretical distributions and their parameters, information entropy, and skewness.
2004.03786_1268556_5	In this paper, a new network architecture with a special loss function is designed to serve as a downstream model of PLMs for supervised relation extraction.
2004.04479_1269249_9	We also show how to construct stealth attacks on high-dimensional AI systems that are hard to spot unless the validation set is made exponentially large.
2004.08522_1273292_1	Active contour model, colloquially known as snake model, has been studied to extract buildings from aerial and satellite imagery.
2004.09059_1273829_6	Numerical results indicate that the introduction of the IRS brings about considerable reductions in transmit power, even with moderate IRS sizes, which can be translated to range increases over the non-IRS-assisted BackCom system.
2004.11543_1276313_8	The approach is demonstrated first in a high-fidelity robotic-operating-system (ROS)-based simulation environment, then with physical UGVs and a UAV in an in-door testing facility.
2004.11793_1276563_8	The BSN was implemented under the Robot Operating System (ROS) architecture, and concerns about the system dependability are taken as adaptation goals.
2004.13351_1278121_0	  Given the fast growth of intelligent devices, it is expected that a large number of high-stake artificial intelligence (AI) applications, e.g., drones, autonomous cars, tactile robots, will be deployed at the edge of wireless networks in the near future.
2004.13563_1278333_0	  With 5G cellular systems being actively deployed worldwide, the research community has started to explore novel technological advances for the subsequent generation, i.e., 6G. It is commonly believed that 6G will be built on a new vision of ubiquitous AI, an hyper-flexible architecture that brings human-like intelligence into every aspect of networking systems.
2004.13563_1278333_1	Despite its great promise, there are several novel challenges expected to arise in ubiquitous AI-based 6G. Although numerous attempts have been made to apply AI to wireless networks, these attempts have not yet seen any large-scale implementation in practical systems.
2004.14129_1278899_1	Given a language model pre-trained on massive unlabeled text corpora, only very light supervised fine-tuning is needed to learn a task: the number of fine-tuning steps is typically five orders of magnitude lower than the total parameter count.
2004.14253_1279023_6	Human evaluation is performed over a sentence completion task, where GePpeTto's output is judged as natural more often than not, and much closer to the original human texts than to a simpler language model which we take as baseline.
2005.01101_1280896_3	In the present paper, a temporal study of the domestic U.S. ATN was performed based on annual flight data from 1996 to 2016 and network analytics were used to examine the effects of restructuring that followed the 9/11 events along with the current state of the system.
2005.02600_1282395_6	In order to demonstrate the functionality of the system, 15 different dummy landmines were buried in a sandbox.
2005.05842_1285637_4	These advantages are needed not only in game AI design, but also in robotics, as is evident from the research being done.
2005.07002_1286797_6	In particular, useful insights are drawn to characterize the effect of IRS reflection amplitude control (with/without the conventional phase shift) on the system performance under imperfect CSI.
2005.07990_1287785_5	Numerical simulations are conducted in order to verify properties of a closed-loop system for a generic model of the airship.   
2005.07990_1287785_6	Performance of the control system is examined via experiments in various scenarios using a prototype airship.
2005.08076_1287871_4	The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI based IoT application.
2005.10049_1289844_5	Log-linear model combination of acoustic and language model is performed with a per-token renormalization.
2006.03413_1297927_0	  There exists a broad class of sequencing problems, for example, in proteins and polymers that can be formulated as a heuristic search algorithm that involve decision making akin to a computer game.
2006.05203_1299717_1	These are supposed to guide the socially-responsible development of AI for the common good.
2006.09428_1303942_3	MAIEI provides 15 recommendations in relation to the sections outlined above, including: 1) focus efforts on the research and innovation community, member states, and the private sector; 2) create alignment between trading partners' policies and EU policies; 3) analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy AI; 4) focus on coordination and policy alignment; 5) focus on mechanisms that promote private and secure sharing of data; 6) create a network of AI research excellence centres to strengthen the research and innovation community; 7) promote knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8) add nuance to the discussion regarding the opacity of AI systems; 9) create a process for individuals to appeal an AI system's decision or output; 10) implement new rules and strengthen existing regulations; 11) ban the use of facial recognition technology; 12) hold all AI systems to similar standards and compulsory requirements; 13) ensure biometric identification systems fulfill the purpose for which they are implemented; 14) implement a voluntary labelling system for systems that are not considered high-risk; 15) appoint individuals to the oversight process who understand AI systems well and are able to communicate potential risks.
2006.10228_1304742_4	Moreover, the computational agent with the Deep Neural Network (DNN) model is performed in the cloud and can communicate with the perception agent and cognition agent via the Internet.
2006.11141_1305655_0	  The control design of an airborne wind energy system with rigid aircraft, vertical take-off and landing, and pumping operation is described.
2006.11141_1305655_4	Aircraft design considerations in light of system maneuverability are presented, too, as well as three possible alternative strategies for the retraction phase of the pumping cycle.
2006.11194_1305708_6	Our results indicate that, at least in some situations, the "why" information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.
2006.12020_1306534_4	OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI.
2006.12362_1306876_3	For this reason, a legal methodology must be developed that makes it possible to link intelligent systems and services to a system of rules applicable thereto.
2006.12635_1307149_5	A stability proof is conducted to validate the stability of the system with the developed control law.
2006.12635_1307149_6	Additionally, simulations are carried out to corroborate that the control law is effective applied to the dynamic model presented.
2006.13555_1308069_1	Although an array of adversarial training and/or loss function based defense techniques have been developed and proved to be effective in computer vision, defending against adversarial attacks on medical images remains largely an uncharted territory due to the following unique challenges: 1) label scarcity in medical images significantly limits adversarial generalizability of the AI system; 2) vastly similar and dominant fore- and background in medical images make it hard samples for learning the discriminating features between different disease classes; and 3) crafted adversarial noises added to the entire medical image as opposed to the focused organ target can make clean and adversarial examples more discriminate than that between different disease classes.
2007.04484_1315998_9	Two algorithms are presented, one which assumes that protected features are accessible to the model during testing, and one which assumes protected features are not accessible during testing.   
2007.07250_1318764_3	The question we address in this paper is, what type of information should be captured in the Interface Control Document (ICD) of an AI-enabled system or component to assess its compatibility with a system for which it was not designed originally.
2007.08380_1319894_1	We aim to maximize the energy efficiency of the system, including the data rate of UE and the energy consumption of UAV via jointly optimizing the UAV's trajectory and the phase shifts of reflecting elements of IRS, when the UE moves and the selection of IRSs is considered for the energy saving purpose.
2007.10943_1322457_3	Based on the alternating direction method of multipliers algorithm, a low complexity method is designed to iteratively solve the established problem.
2007.12391_1323905_8	We therefore conclude that, in the context of creative industries, maximum benefit from AI will be derived where its focus is human centric -- where it is designed to augment, rather than replace, human creativity.
2008.00363_1328085_3	In this paper, we develop an approach to explainable AI in which the anomaly is assured to be overlapping the expected location when present.
2008.02275_1329997_5	Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.
2008.03301_1331023_3	In our new approach, however, the data-dependent hill-climbing search is replaced with a model-dependent search where a globally optimal SVM model is trained first, then the algorithm looks into support vectors as the most influential data points in the model, and induces a clause that would cover the support vector and points that are most similar to that support vector.
2008.05699_1333421_3	Through comparison with a Vicon-based motion capture system, the capability of the present vision system to measure distances in real-time within an accuracy of less than a centimeter and heading within a degree with the right visual cue, is demonstrated.
2008.05860_1333582_4	Then, a novel two-timescale nonlinear joint hybrid transceiver design algorithm is developed, which can be viewed as an extension of the BCD-based joint design algorithm for reducing both the channel state information (CSI) signalling overhead and the effects of outdated CSI.
2008.06448_1334170_0	  Logs have been widely adopted in software system development and maintenance because of the rich runtime information they record.
2008.07343_1335065_7	It is envisaged that this study will provide AI researchers and the wider community with an overview of the current status of AI applications, and motivate researchers to harness AI's potential in the fight against COVID-19.
2008.09036_1336758_1	However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies.
2008.11123_1338845_3	As a result, a prototype of the test bench for the Embedded System is presented in which the main parameters of temperature sensors and operating conditions of the dehumidifiers are checked.
2008.12615_1340337_1	It is argued in this research paper that the infusion of AI into existing and future legal activities and the judicial structure needs to be undertaken by mindfully observing an alignment with the core principles of justice.
2008.12615_1340337_3	By examining the principles of justice across the Levels of Autonomy (LoA) of AI Legal Reasoning, the case is made that there is an ongoing tension underlying the efforts to develop and deploy AI that can demonstrably determine the impacts and sway upon each core principle of justice and the collective set.
2009.00802_1342305_6	Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification.
2009.01180_1342683_6	By providing the mathematical relation of RIS design with a MIMO system, a three-stage framework is presented.
2009.01812_1343315_10	The systematic and fine-grained analysis of the AI field enabled to portrait the pace of AI innovation and demonstrated that the proposed approach can be adopted to understand other fast-growing fields such as cancer research and nano science.
2009.02098_1343601_8	The generated local explanations are also visualized and presented with relevant evaluation measures that are expected to increase the users' trust in the black-box-model.
2009.03750_1345253_1	Several design cases are carried on a specific model of Power Module (PM) core, made of distinct ferrite materials and having different kinds of air gap arrangements The correspondingly obtained design results are firstly compared with the classic approach by linearization of the magnetic curve to calculate $N$ and the use of a fringing factor to determine $g$. Next, a refined design approach of specifying the inductance roll-off at the peak current and its potential limitations are discussed with respect to our design method.
2009.04095_1345598_4	Moreover, an explainability of language models coherent with pretraining is presented which verifies the context capturing capabilities of these models through a model agnostic approach.
2009.04943_1346446_2	Many research groups state that 5G cannot meet its demands without artificial intelligence (AI) integration as 5G wireless networks are expected to generate unprecedented traffic giving wireless research designers access to big data that can help in predicting the demands and adjust cell designs to meet the users requirements.
2009.06082_1347585_5	The demonstration was presented in the form of a question-answering system that took a radiology multiple choice question and a medical image as inputs.
2009.07062_1348565_5	Its dynamical characteristics are established experimentally and compared to a finite elements model.
2009.07062_1348565_7	The measurements are interpreted thanks to a simple multi-asperity contact model.
2009.07262_1348765_1	In order to ensure that the science and technology of AI is developed in a humane manner, we must develop research publication norms that are informed by our growing understanding of AI's potential threats and use cases.
2009.08512_1350015_0	  Pilot contamination attack (PCA) in a time division duplex wireless communication system is considered, where an eavesdropper (Eve) attacks the reverse pilot transmission phase in order to wiretap the data transmitted from a transmitter, Alice, to a receiver, Bob.
2009.09070_1350573_2	Statistics might even be considered a core element of AI.
2009.09079_1350582_3	These problems are: the need to bridge the divide between symbolic and non-symbolic kinds of knowledge and processing; the tendency of deep neural networks (DNNs) to make large and unexpected errors in recognition; the need to strengthen the representation and processing of natural languages; the challenges of unsupervised learning; the need for a coherent account of generalisation; how to learn usable knowledge from a single exposure; how to achieve transfer learning; how to increase the efficiency of AI processing; the need for transparency in AI structures and processes; how to achieve varieties of probabilistic reasoning; the need for more emphasis on top-down strategies; how to minimise the risk of accidents with self-driving vehicles; the need for strong compositionality in AI knowledge; the challenges of commonsense reasoning and commonsense knowledge; establishing the importance of information compression in AI research; establishing the importance of a biological perspective in AI research; establishing whether knowledge in the brain is represented in `distributed' or `localist' form; how to bypassing the limited scope for adaptation in deep neural networks; the need to develop `broad AI'; and how to eliminate the problem of catastrophic forgetting.
2009.11722_1353225_3	Based on our proposed data driven V-Model, we introduce a simple yet elegant solution for the AI components development cycle, where prototyping takes place in the cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment and evaluation on the target ECUs (Electronic Control Units) is performed as Hardware-in-the-Loop (HiL) testing.
2009.11722_1353225_4	The effectiveness of the proposed framework is demonstrated using two real-world use-cases of AI inference engines for autonomous vehicles, that is environment perception and most probable path prediction.
2009.13574_1355077_4	Moreover, the order of the learning function can be chosen arbitrarily so that the designers have the flexibility to decide the complexity of the learning algorithm.
2009.14620_1356123_7	In this paper, a review of the literature encompassing Legal Judgment Prediction is undertaken, along with innovatively proposing that the advent of AI Legal Reasoning (AILR) will have a pronounced impact on how LJP is performed and its predictive accuracy.
2009.14620_1356123_8	Legal Judgment Prediction is particularly examined using the Levels of Autonomy (LoA) of AI Legal Reasoning, plus, other considerations are explored including LJP probabilistic tendencies, biases handling, actor predictors, transparency, judicial reliance, legal case outcomes, and other crucial elements entailing the overarching legal judicial milieu.
2010.01217_1357547_8	At each stages of development, interesting experimental results are presented to demonstrate the effectiveness of the proposed system.
2010.02726_1359056_2	Advances in Artificial Intelligence (AI) involving especially Natural Language Processing (NLP) and Machine Learning (ML) are increasingly bolstering how automation can systematically perform either or both of Sentiment Analysis and Opinion Mining, all of which is being inexorably carried over into engagement within a legal context for improving LSAOM capabilities.
2010.04551_1360881_2	Discussions : The knowledge and data in the comprehensive domain are uniformly represented by using the rich connection heterogeneous Dynamic Cognitive Network constructed by conceptualized elements; A network structure of two dimensions and multi layers is designed to achieve unified implementation of AI core processing such as combination and generalization; This paper analyzes the implementation differences of computer systems in different scenes, such as open domain, closed domain, significant probability and non-significant probability, and points out that the implementation in open domain and significant probability scene is the key of AI, and a cognitive probability model combining bidirectional conditional probability, probability passing and superposition, probability col-lapse is designed; An omnidirectional network matching-growth algorithm system driven by target and probability is designed to realize the integration of parsing, generating, reasoning, querying, learning and so on; The principle of cognitive network optimization is proposed, and the basic framework of Cognitive Network Learning algorithm (CNL) is designed that structure learning is the primary method and parameter learning is the auxiliary.
2010.04551_1360881_3	The logical similarity of implementation between DC Net model and human intelligence is analyzed in this paper.
2010.06189_1362519_1	However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English.
2010.07022_1363352_1	There is a broader context that is often not considered within AI and HRI: that the problem of trustworthiness is inherently socio-technical and ultimately involves a broad set of complex human factors and multidimensional relationships that can arise between agents, humans, organizations, and even governments and legal institutions, each with their own understanding and definitions of trust.
2010.07529_1363859_5	Based on the initial measurement, it was found that aspectization has positively improved all the three quality attributes defined in the quality model.
2010.07570_1363900_6	Results of simulations for short gaps ($\le$ 5 mm) under Standard Temperature and Pressure (STP) conditions have been presented, analyzed and compared with some classical papers to evaluate the suitability of such a model for further studies of non-thermal electrical discharges.
2010.07698_1364028_4	The resource allocation algorithm design is formulated as an optimization problem for the maximization of the weighted system sum throughput while guaranteeing the quality of service of the URLLC users.
2010.10657_1366987_2	In the recent decades, concerns have been raised on studying improper signals and providing an accurate model of the LMS algorithm for both proper and improper signals.
2010.10657_1366987_3	Other models for the LMS algorithm for improper signals available in the scientific literature either make use of the independence assumptions regarding the desired signal and the input signal vector, or are exclusive to proper signals; it is shown that by not considering these assumptions a more general model can be derived.
2010.11349_1367679_0	  LSTM language models (LSTM-LMs) have been proven to be powerful and yielded significant performance improvements over count based n-gram LMs in modern speech recognition systems.
2010.12251_1368581_1	We propose a scalable and automatic approach for improving NLU in a large-scale conversational AI system by leveraging implicit user feedback, with an insight that user interaction data and dialog context have rich information embedded from which user satisfaction and intention can be inferred.
2010.13830_1370160_5	Furthermore, it is generally believed that trust is crucial for adoption of both AI and robotics, particularly when transitioning technologies from the lab to industrial, social, and consumer applications.
2010.14617_1370947_1	The backpropagation algorithm of artificial neural networks was thought not suitable for brain cortex, and there is a lack of algorithm for memory engram.
2010.14617_1370947_4	The role of the LTP and LTD in the cerebellum is also explained in algorithm level.
2010.15577_1371907_4	The formats for submitting questions, examples of its designing and developed questions were demonstrated in view mode in Moodle LMS.
2011.00518_1373266_4	Secondly, original papers are traced for AI markers.
2011.01047_1373795_2	The optimization of chiller system power consumption had been extensively studied in the mechanical engineering and building service domains.
2011.01991_1374739_3	With ILME, the internal LM scores of an E2E model are estimated and subtracted from the log-linear interpolation between the scores of the E2E model and the external LM.
2011.01991_1374739_4	The internal LM scores are approximated as the output of an E2E model when eliminating its acoustic components.
2011.02237_1374985_7	Simulation results are presented which demonstrate the advantages and effectiveness of the proposed algorithm as compared to benchmark schemes.
2011.02272_1375020_2	Brittleness to minor adversarial changes in the input data, ability to explain the decisions, address the bias in their training data, high opacity in terms of revealing the lineage of the system, how they were trained and tested, and under which parameters and conditions they can reliably guarantee a certain level of performance, are some of the most prominent limitations.
2011.02585_1375333_3	Realistic convection heat transfer at both sides of the system are taken into account in our modeling.
2011.03615_1376363_6	The main idea of this work is to motivate the application of RL beyond the model-free perspective which was extensively adopted in recent years.
2011.04548_1377296_3	The underlying technology was developed to meet the needs for performance, transparency, user acceptance and ease of use, central aspects to the adoption of AI-based decision support systems.
2011.08512_1381260_3	A collection of intelligent system failures experienced in the real world (i.e., incidents) is needed to ensure intelligent systems benefit people and society.
2011.10367_1383115_6	The 20 most important features are selected using the Shapley values to present a full human-understandable model that reveals how the attributes of an individual are related to its model prediction.
2011.10738_1383486_5	The value of the imputed data for distribution system operation is illustrated via a matrix completion based state estimation strategy.
2011.11484_1384232_5	A close to 0.5 criteria on Shannon entropy is derived in this paper for the theoretical investigation of AI accuracy and correctness for classification problems.
2012.00676_1388554_3	A mathematical analysis of drug deposition and retention in the lungs is developed through a coupled mathematical model of aerosol transport in air as well as drug molecule transport in the mucus layer.
2012.05556_1393434_6	In the post-stall flow over the airfoil at the angle of attack of 12 degrees and Reynolds number of 63000, the D-D bulge model at 7kV has approximately the same effect of leading-edge suction enhancement and reattachment promotion as the S-H model with Dc = 0.16, which was previously proved to be sufficiently high to achieve the control performance.
2012.06058_1393936_5	A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on.
2012.06103_1393981_1	In order to improve the robustness of the mmWave system under the presence of the random blockages, multiple reconfigurable intelligent surfaces (RISs) are deployed to enhance the spatial diversity gain, and robust beamforming is then designed based on a stochastic optimization for minimizing the maximum outage probability among multiple users to ensure the fairness.
2012.06310_1394188_9	Apart from model development, data preprocessing and augmentation are also performed to cope with the challenge of insufficient data samples often encountered in medical applications.
2012.08545_1396423_2	Using this workflow, an ensemble of four openly available AI models can be run on HAL to process an entire month's worth (August 2017) of advanced Laser Interferometer Gravitational-Wave Observatory data in just seven minutes, identifying all four all four binary black hole mergers previously identified in this dataset and reporting no misclassifications.
2012.12543_1400421_3	In this work, an RNN language model is trained using alternate batches from only monolingual English and Spanish data and the perplexity of the language model is computed.
2012.12543_1400421_4	From the results, it is concluded that using alternate batches of monolingual data in training reduced the perplexity of a CS language model.
2012.12679_1400557_1	The instrument is designed following a modular approach so that each sub-system can be integrated in parallel before their assembly at system level.
2012.15035_1402913_0	  Across a growing number of domains, human experts are expected to learn from and adapt to AI with superior decision making abilities.
2101.00566_1404308_3	Furthermore, approximate spectral efficiency (SE) and area spectral efficiency (ASE) expressions are derived and quantified for diverse system parameters.
2101.00594_1404336_5	On the contrary, the flight path dynamics are perturbed by both model uncertainties and atmospheric disturbances; thus the incremental sliding mode control is adopted.
2101.02341_1406083_9	Also, we apply the Ethereum blockchain in our outsourcing algorithm to enable fair payments, which ensures that the cloud server gets paid only when he correctly accomplished the outsourced work.
2101.03204_1406946_6	In this paper, we investigate how spelling errors can be corrected in context, with a pre-trained language model BERT.
2101.06060_1409802_4	In this paper we look more closely at the question of AI value alignment and suggest that the power and autonomy of AI systems gives rise to opportunities and challenges in the domain of value that have not been encountered before.
2101.06573_1410315_4	We show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities.
2101.07073_1410815_5	The complexity of the proposed AO algorithm is analyzed theoretically.
2101.09672_1413414_6	Numerical results are presented to demonstrate the excellent performance of the proposed algorithm in terms of both estimation accuracy and overfitting avoidance.
2101.12051_1415793_7	It is found that autonomous driving tasks are more sensitive to model parameter errors than other tasks since the neural networks for autonomous driving contain sparser model parameters.
2102.00178_1416667_4	The outputs of the trained networks are adopted into a modified MCTS detection algorithm to provide useful node statistics and facilitate enhanced tree search process.
2102.00458_1416947_1	Earth's climate can be understood as a dynamical system that changes due to external forcing and internal couplings.
2102.01330_1417819_1	Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques.
2102.02842_1419331_4	Such forecasts may be considered "human-expert" forecasts and do not qualify as AI/ML approaches, although they can be used as an indicator of human expert performance.
2102.05490_1421979_4	Both the history-based supervisor and the safety advisor are designed based on an approximate probabilistic relation between the original system and its finite abstraction.
2102.05756_1422245_4	However, consistent with common assumptions about the negative implications of AI, people are evaluated more negatively if they are suspected to be using algorithmic responses.
2102.07638_1424127_4	A 1/2 criteria on Shannon entropy is derived in this paper so that error rate can approach zero or is zero for AI pattern classification problems.
2102.08220_1424709_4	Lastly, to further increase the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand.
2102.09364_1425853_7	And, if no, then how can AI ethics be made useful for AI practitioners?
2102.10787_1427276_0	  As the use of artificial intelligence (AI) in high-stakes decision-making increases, the ability to contest such decisions is being recognised in AI ethics guidelines as an important safeguard for individuals.
2102.13277_1429766_1	A two dimensional numerical study has been carried out over NACA 0021 with k-\omega SST model with non-linear correction at Re = 120,000 for various angles of attack which experiences the formation of a laminar separation bubble (LSB).
2102.13277_1429766_5	It is found that the non-linear k-\omega SST and k-kl-\omega transition model provide comparable quality of prediction in lift and drag coefficients (in spite of the fact that non-linear k-\omega SST involves solving less number of transport equation than the transition model) as observed in the experiments whereas k-\omega SST and SA models under predict the drag coefficient value at low angle of attack due to inability to capture the separation induced transition.
2103.00689_1430834_4	The color centers show a strong diameter-dependent emission intensity, which can be explained with a theoretical model for chemical reactivity taking into account strain along the tube curvature.
2103.01829_1431974_3	According to the proposed prior-aided iterative angle estimation algorithm, azimuth/elevation angles can be estimated, and these angles are adopted to achieve precise beam-alignment and refine GTTDU module for further eliminating delay-beam squint.
2103.04314_1434459_4	This paper presents the use of a machine learning expert system, which is developed with meaning-assigned nodes (facts) and correlations (rules).
2103.04860_1435005_4	In this paper, first a high frequency transformer is designed for a Dual and Triple Active Bridge converter for the More Electric Aircraft DC power system.
2103.06520_1436665_4	The TAP PM2.5 is estimated based on a two-stage machine learning model coupled with the synthetic minority oversampling technique and a tree-based gap-filling method.
2103.07155_1437300_1	The approach is studied here in the context of the base model being linear regression.
2103.07155_1437300_2	The AI-model approximates the residual error of the linear model and the explanations are formulated in terms of the change of the interpretable base model's parameters.
2103.07155_1437300_3	Criteria are formulated for the precise relation between lost accuracy of the surrogate, the accuracy of the AI-model, and the surrogate fidelity.
2103.14362_1444507_4	The feature sequence is input into DeepAR as covariant, which makes the statistical characteristics of network traffic near a period of time in the past be considered when updating parameters, and the interference of non-stationary network traffic on model training will be reduced.
2103.14988_1445133_7	In the scripting system, both Java based NMR methods and original CPython based libraries are supported.
2103.15004_1445149_10	Here the article introduces two paradigmatic implementations of the proposed XR testbed for human-AI interactions and interfaces and shows how a valid and systematic investigation can be conducted.
2103.15294_1445439_2	Many researchers argue that little substantial progress has been made for AI in recent decades.
2104.01266_1448684_2	However, successful forms of human-AI partnership have rarely been demonstrated in real-world settings.
2104.02332_1449750_3	It is trained on the word confusion network from an ASR system.
2104.03741_1451159_2	In this paper, starting from a baseline model that captures the fundamental dynamics of a race for domain supremacy using AI technology, we demonstrate how socially unwanted outcomes may be produced when sanctioning is applied unconditionally to risk-taking, i.e. potentially unsafe, behaviours.
2104.03820_1451238_5	Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system's outputs would be established, and future opportunities for generative AI in application modernization.
2104.04466_1451884_3	We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level.
2104.08477_1455895_0	  In a time-reversal invariant system, while the anomalous Hall effect identically vanishes in the linear response regime due to the constraint of time-reversal symmetry on the distribution of Berry curvature, a nonlinear Hall effect can emerge in the second-order response regime if the inversion symmetry is broken to allow a nonzero Berry curvature dipole (BCD) on the Fermi surface.
2104.08826_1456244_0	  Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts.
2104.12547_1459965_0	  This paper aims to provide an overview of the ethical concerns in artificial intelligence (AI) and the framework that is needed to mitigate those risks, and to suggest a practical path to ensure the development and use of AI at the United Nations (UN) aligns with our ethical values.
2104.12672_1460090_0	  In the field of eXplainable AI (XAI), robust "blackbox" algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance.
2104.13180_1460598_4	The controller is trained using a reduced model of the physical system, i.e, the spring-slider model, which embodies the main dynamics of the physical problem for a given earthquake magnitude.
2104.13182_1460600_7	Additionally, to investigate the impact of RISs on system coverage, the asymptotic expressions of two coverage probabilities are derived.
2104.14088_1461506_0	  The sixth generation (6G) systems are generally recognized to be established on ubiquitous Artificial Intelligence (AI) and distributed ledger such as blockchain.
2105.00333_1462892_4	Three experimental studies are presented, illustrating the ability of these AI methodologies to produce state-of-the-art performance in the whole food supply chain.
2105.00832_1463391_2	Previous studies proposed that a phase transition, which can be considered to represent the emergence of order in language, occurs in the random language model.
2105.01774_1464333_4	We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project.
2105.05012_1467571_0	  In this paper, the Robotic Assistant Agent for student and machine co-learning on AI-FML practice with AIoT application is presented.
2105.08679_1471238_2	A novel trivariate Bernoulli model is considered to incorporate these features, and the Bayesian estimation of the model parameters is suggested using data augmentation.
2105.08847_1471406_1	As artificial intelligence technologies -- in education and beyond -- may contribute to inequitable outcomes for marginalized communities, various approaches have been developed to evaluate and mitigate the harmful impacts of AI.
2105.08867_1471426_1	Building and maintaining public trust in AI has been identified as the key to successful and sustainable innovation.
2105.10356_1472915_7	Ongoing changes in AI technology suggest that AI certification regimes should be designed to emphasize governance criteria of enduring value, such as ethics training for AI developers, and to adjust technical criteria as the technology changes.
2105.12499_1475058_4	Furthermore, using a combination of AIA 171/193 and 193/211 channel ratios in combination with spectroscopic determination of the standard deviation of the loop apex temperature over several hours alongside simulations from the outlined multi-stranded loop model, it is demonstrated that both the imposed heating rate and number of strands can be realised.
2106.00326_1478090_8	We further describe how these groups are constituted in terms of sociodemographics as well as opinions on AI.
2106.00409_1478173_7	The effect of various parameters on the agreement between model and experiment is studied, such as the used transport data, the background ionization level, the photoionization rate, the gas temperature, the voltage rise time and the voltage boundary conditions.
2106.02834_1480598_1	This has prompted the creation of an ever-growing pre-trained model universe, where each model is trained on large amounts of language or domain specific data with a carefully curated, linguistically informed vocabulary.
2106.03669_1481433_4	Based on the experimental results the YOLOv5 algorithm was found to be more effective at detecting and identifying cactus disease than the Faster R-CNN algorithm.
2106.03797_1481561_6	A proof of concept is also developed, and extensive evaluation is conducted for 3D reconstruction and applications of AI for defect detection.
2106.06178_1483942_2	However, although promising results have been reported, practical design guidelines and performance guarantees of AI-based approaches are still missing.
2106.07199_1484963_3	The evaluation is carried out through an experimentation of a LTE system concretely reproduced using Software-Defined Radios.
2106.07380_1485144_8	With the use of a machine learning model to predict social trends through the reaction users have to post, a better picture of the near future can be envisioned.
2106.07483_1485247_1	In response, explainable artificial intelligence (XAI) tools that analyze the inner workings of a model have been created.
2106.08011_1485775_2	To facilitate consensus phase, we propose an AirComp-based DSGD with gradient tracking and variance reduction (DSGT-VR) algorithm, where both precoding and decoding strategies are developed for D2D communication.
2106.08258_1486022_2	In this paper, we consider an AI system from the domain practitioner's perspective and identify key roles that are involved in system deployment.
2106.10832_1488596_4	OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI.
2106.14064_1491828_1	In the case where $X$ is a cartesian product, the method produces nonseparable positive definite kernels that may be useful in multivariate interpolation.\ In addition, it can be interpreted as an abstract multivariate generalization of the well-established Gneiting's model for constructing space-time covariances commonly cited in the literature.\ Many parametric models discussed in statistics can be interpreted as particular cases of the method.
2106.14151_1491915_11	A comprehensive model considering the physical, algorithmic and hardware properties of the magnetometer of the survey system is presented.
2106.14647_1492411_11	Paper was presented at the first Conference on Deployable AI at IIT-Madras in June 2021.
2106.16122_1493886_0	  Departing from the claim that AI needs to be trustworthy, we find that ethical advice from an AI-powered algorithm is trusted even when its users know nothing about its training data and when they learn information about it that warrants distrust.
2107.00266_1494278_5	The approach is illustrated on a realistic aeroelastic aircraft model built with a coupled fluid-structure solver which order is reduced to decrease the number of optimisation variables.
2107.03641_1497653_7	CFD analysis was performed on each model and compared with a baseline no-adhesion model, comparing airflow and heat and mass transfer.
2107.03712_1497724_0	  While the original Ait-Sahalia interest rate model has been found considerable use as a model for describing time series evolution of interest rates, it may not possess adequate specifications to explain responses of interest rates to empirical phenomena such as volatility 'skews' and 'smiles', jump behaviour, market regulatory lapses, economic crisis, financial clashes, political instability, among others collectively.
2107.08909_1502921_0	  The advance of explainable artificial intelligence, which provides reasons for its predictions, is expected to accelerate the use of deep neural networks in the real world like Machine Learning as a Service (MLaaS) that returns predictions on queried data with the trained model.
2107.10043_1504055_2	However, both linearity of the underlying SS model and accurate knowledge of it are often not encountered in practice.
2107.10574_1504586_6	This problem is found to have a partial quasiconvexity that leads to the development of an efficient parameter estimation and radio map construction algorithm.
2108.00588_1509403_5	Thus, our specific objectives were: (1) to reveal whether participants with diverse problem-solving styles were left behind in a set of AI products; and (2) to relate participants' problem-solving diversity to their demographic diversity, specifically, gender and age.   
2108.01174_1509989_6	Multiple aspects of an AI system can be explained; these include biases that the data might have, lack of data points in a particular region of the example space, fairness of gathering the data, feature importances, etc.
2108.01394_1510209_7	For this model, we use YOLOv3 (You Only Look Once) a computer vision-based algorithm popularly used to detect objects which is developed based on Convolution Neural Networks (CNNs) which is a machine learning (ML) based tool.
2108.02656_1511471_1	The deep features being distinguishing in classification from the convolutional neural networks (CNN) are demonstrated in this study to provide comprehensive interpretability for the proposed CAD system using pathological knowledge.
2108.03383_1512198_6	The architecture of an AI-driven customized smart factory is presented.
2108.04666_1513481_8	The details of the model will be presented in the contribution.
2108.06159_1514974_1	Unlike classical symbolic AI systems, neural networks are trained using large data sets and their inner structure containing possibly billions of parameters does not lend itself to human interpretation.
2108.06159_1514974_4	As a step towards developing robust AI systems for such applications, this paper presents how the robustness of AI systems can be practically examined and which methods and metrics can be used to do so.
2108.07320_1516135_3	The utility of this model is demonstrated through its use in identifying near-term opportunities for improving performance at elevated power densities, and for designing a thermal management strategy that maximizes overall power output.
2108.07400_1516215_1	Industrial automation software is typically developed in a model-driven fashion where abstractions of physical processes called plant models are co-developed and iteratively refined along with the control code.
2108.07700_1516515_0	  While the role of states, corporations, and international organizations in AI governance has been extensively theorized, the role of workers has received comparatively little attention.
2108.08062_1516877_5	More importantly, the rigorous proven properties such as the convergence of CL-DCEE algorithm are established under mild assumptions on noises, and the impact of noises on the search performance is examined.
2108.13363_1522178_0	  We use an autoethnographic case study of a Latinx high school student from an agricultural community in California to highlight how AI is learned outside classrooms and how her personal background influenced her social-justice oriented applications of AI technologies.
2108.13654_1522469_0	  As a prominent attribution-based explanation algorithm, Integrated Gradients (IG) is widely adopted due to its desirable explanation axioms and the ease of gradient computation.
2109.01658_1524479_9	While initial results are promising, much work is still needed on model development, clinical testing and standardisation.
2109.02353_1525174_1	However, model communication over wireless channels, especially in uplink model uploading of FEEL, has been widely recognized as a bottleneck that critically limits the efficiency of FEEL.
2109.02944_1525765_4	We agree with Floridi that much can be learned from these registers about the role of AI systems in municipal city management.
2109.04314_1527135_2	Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner.
2109.04606_1527427_3	The probabilistic feasible region at every time step can be established through the simulation of the system state and the evaluation of convex constraints.
2109.04646_1527467_4	To further add complexity, a high degree of model accuracy is required when lives are at stake, creating a need for the deployment of highly accurate, however computationally intensive models to resource-constrained devices.
2109.05334_1528155_6	The performance of the enhanced LMMSE algorithm is demonstrated through computer simulations for narrowband MIMO systems in Rayleigh fading channels.
2109.07906_1530727_5	Transparency, privacy, accountability and fairness are identified as the most common AI ethics principles.
2109.07906_1530727_6	Similarly, lack of ethical knowledge and vague principles are reported as the significant challenges for considering ethics in AI.
2109.09586_1532407_4	We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education level of the person to whom the explication is intended for.
2109.12383_1535204_2	For instance, if the model is being asked to identify arguments for the trigger "protested", we will provide that trigger as part of the input to the language model, allowing it to produce different representations for candidate arguments than when it is asked about arguments for the trigger "arrest" elsewhere in the same sentence.
2110.00931_1539074_4	Tests of this prototype are carried out under four scenarios including sample generation, AI-based stability prediction, data-driven dynamic component modeling, and AI-aided stability control, which prove the validity, flexibility, and efficiency of the design and implementation of the AI-oriented power system dynamic simulator.
2110.01256_1539399_1	As unlabeled data carry rich task-relevant information, they are proven useful for few-shot learning of language model.
2110.01500_1539643_5	It is expected that this factorization can transfer the improvement of the standalone language model to the Transducer for speech recognition, which allows various language model adaptation techniques to be applied.
2110.04509_1542652_2	In this work, the effect of the inner air circulation on the system's heat and mass transfer performance and energy efficiency are studied theoretically and experimentally.
2110.04878_1543021_2	In this paper, an attention matrix between the sentences of the whole text is adopted as a weighted adjacent matrix of a fully connected graph of the text, which can be produced through the pre-training language model.
2110.07477_1545620_3	In the previous work, these two modules are loosely connected in the model training and are shallowly integrated during inference, where a simple switching or copy mechanism is adopted to incorporate recommended items into generated responses.
2110.08173_1546316_2	To catalyse the research in this direction, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, which is constructed based on the Unified Medical Language System (UMLS) Metathesaurus.
2110.08637_1546780_6	Even the developers of AI systems can't explain why a certain output is derived.
2110.08743_1546886_6	\footnote{The code can be found at https://github.com/ShannonAI/GNN-LM
2110.08796_1546939_0	  In keeping with the rapid development of communication technology, a new communication structure is required in a next-generation communication system.
2110.09271_1547414_0	  Trustworthy artificial intelligence (AI) has become an important topic because trust in AI systems and their creators has been lost.
2110.09748_1547891_4	An analytical comparison and some case studies that consider various points in the design parameter space have been conducted to prove the feasibility and validity of our design and evaluation system.
2110.11578_1549721_5	Meanwhile, a crypto-aided secure validation protocol is designed to verify that the contribution of model update from each client is bounded without leaking privacy.
2110.14348_1552491_2	To substantiate the physical relevance of the LES model, a set of experimental aerosol concentration measurements are carried out, and their results are used for validating the LES model results.
2110.14348_1552491_9	The LES-based results are examined in juxtaposition with the classical Wells-Riley model, which is shown to significantly underestimate the infection probability, highlighting the importance of employing accurate indoor turbulence modeling when evaluating different risk-reduction strategies.
2111.01726_1555829_10	Results will be presented on AI instruction's ability to improve human decision-making and human-AI teaming in Hanabi.
2111.02168_1556271_9	These elements are then passed to a large language model for the final nomination.
2111.04455_1558558_1	Although increasingly adaptive language learning tools are being developed with the application of AI to the Computer Assisted Language Learning field, there have been concerns regarding insufficient information and teacher preparation.
2111.04455_1558558_3	Therefore, this review synthesized information on AI tools that were developed between 2017 and 2020.
2111.04456_1558559_4	We propose that the principles composing this framework can be translated into guidelines that improve practitioner-patient relationships and, concurrently, patient agency regarding the use of AI in healthcare while broadening the discourse on this topic.
2111.05071_1559174_2	In this article, we describe and discuss the two primary enforcement mechanisms proposed in the AIA: the conformity assessments that providers of high-risk AI systems are expected to conduct, and the post-market monitoring plans that providers must establish to document the performance of high-risk AI systems throughout their lifetimes.
2111.06590_1560693_3	The closed-loop pole placement problem in the linear matrix inequality (LMI) regions is considered a classic robust control problem; however, that requires knowledge about the state and input matrices of the linear system.
2111.08222_1562325_1	The lack of interpretability inherent in many modern AI techniques is believed to be hurting their adoption, as users may not trust systems whose decision processes they do not understand.
2111.13756_1567859_6	An in-depth review of AI algorithms [1] and success stories to the proper implementations of such algorithms can be found in the aforenoted special issue and collection of papers.
2111.14125_1568228_6	The proposed system is equipped with a NodeRED dashboard which processes, visualizes, and stores the primary sensor data that are acquired through a public air quality sensor network, and further, the dashboard is integrated with a machine-learning model to obtain temporal and geo-spatial air quality predictions.
2112.01016_1570792_2	The gap is glaring: what is considered "explained" to AI-experts versus non-experts are very different in practical scenarios.
2112.01282_1571058_3	Therefore, new methodologies are needed to provide a well-vetted and real-world applicable structure and path through the checks and balances needed for ethically assessing and guiding the development of AI.
2112.01325_1571101_7	Finally, future trends of AI-driven NOMA-F-RANs, including open research issues and challenges, are identified.
2112.02583_1572359_5	Moreover, we also derive, for the first time, the Cramer-Rao lower bound (CRLB) for a MIMO system, where phase noise is estimated using only angular information.
2112.04573_1574349_5	Following the rigorous/ established selection process, a total of thirty-two articles were finally selected, reviewed and analyzed to summarize on the application of AI and ML domain and techniques which are most often used in libraries.
2112.04889_1574665_6	Among other findings, we identify metamodeling of complex security of electricity supply models using AI methods and applications of AI-based methods for forecasts of storage dispatch and (non-)availabilities as promising fields of application that have not sufficiently been covered, yet.
2112.06751_1576527_8	We therefore show that it is vital to consider how the decision to defer is communicated to a human when designing selective prediction systems, and that the composite accuracy of a human-AI team must be carefully evaluated using a human-in-the-loop framework.
2112.07467_1577243_0	  As consensus across the various published AI ethics principles is approached, a gap remains between high-level principles and practical techniques that can be readily adopted to design and develop responsible AI systems.
2112.12326_1582102_3	Furthermore, we derive closed-form expressions for the MTCD system's peak AoI, which are formulated as the optimization objective under the constraints of EH power, status update rate and stability conditions.
2112.12387_1582163_1	Although recent research showed the promise of AI in assisting with such tasks, it remains largely unknown how AI should be designed to facilitate effective collaboration between user experience (UX) evaluators and AI.
2112.14933_1584709_0	  Rhetorical Frames in AI can be thought of as expressions that describe AI development as a competition between two or more actors, such as governments or companies.
2201.00692_1586064_2	To fully realize these opportunities, existing regulatory guidance and industry best practices should be taken into consideration in order to increase the overall trustworthiness of the system and enable broader adoption.
2201.01466_1586838_10	The most common representations for AI, methods, and machine learning are covered.
2201.01466_1586838_14	Emotions are central to human intelligence, but little use has been made in AI.
2201.01466_1586838_17	Finally, a summary is made of the current state of AI and what to do in the future.
2201.02303_1587675_2	In the early stage, the mutual support between technological path and artistic ideas turned out to be a failure, while AI-driven expressive repetitions are made probable in the contemporary technological context, paving the way for the transformation of AI literature from proof for technical possibilities to self-verification of literary value.
2201.03263_1588635_0	  Outcomes of data-driven AI models cannot be assumed to be always correct.
2201.04263_1589635_8	The AI-based systems' end-users and their decision-making archetypes during collaboration with these systems should be considered during the AI risk management.
2201.04263_1589635_9	Using some real-world scenarios, it will be highlighted that decision-making archetypes of users should be considered a design principle in AI-based systems.
2201.05647_1591019_3	The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks.
2201.06317_1591689_5	Next, we introduce PVAE-BERT, which equips the model with a pretrained large-scale language model, i.e., Bidirectional Encoder Representations from Transformers (BERT), enabling the model to go beyond comprehending only the predefined descriptions that the network has been trained on; the recognition of action descriptions generalises to unconstrained natural language as the model becomes capable of understanding unlimited variations of the same descriptions.
2201.06997_1592369_11	The concept of Explainable AI is involved in this study for the better interpretation & understanding of the model behavior.
2201.07040_1592412_9	Thus, currently available AI benchmarks are improperly aligned with desired targets for AI automation in clinical settings, and novel benchmarks should be created to fill these gaps.
2201.07718_1593090_2	Root mean square error (RMSE), Average absolute error, Absolute relative error, squared correlation and model building time were determined to evaluate the performance of each algorithm.
2201.07935_1593307_1	Several reviews have been conducted regarding Artificial intelligence (AI) techniques to improve pregnancy outcomes.
2201.07935_1593307_13	More research is required to validate this technology from a physician's perspective, such as pilot studies and randomized controlled trials on AI and its applications in a hospital setting.
2201.07984_1593356_4	Specifically, we collect a sheer number of source codes (both Java and Python) from the Alipay code repository and incorporate both syntactic and semantic code knowledge into our model through the help of code parsers, in which AST information of the source codes can be interpreted and integrated.
2201.09031_1594403_0	  In this work, two problems associated with a downlink multi-user system are considered with the aid of intelligent reflecting surface (IRS): weighted sum-rate maximization and weighted minimal-rate maximization.
2201.09647_1595019_0	  The AlphaFold computer program predicted protein structures for the whole human genome, which has been considered as a remarkable breakthrough both in artificial intelligence (AI) application and structural biology.
2201.09647_1595019_5	Based on the available data, the second round of AI-powered compound generation was conducted and through which, a more potent hit molecule, ISM042-2 048, was discovered with a Kd value of 210.0 +/- 42.4 nM (n = 2), within 30 days and after synthesizing 6 compounds from the discovery of the first hit ISM042-2-001.
2201.11260_1596632_14	We also look at different sub-spaces of extrapolation for specific individuals subject to AI models and report how these extrapolations can be interpreted, not mathematically, but from a humanistic point of view.
2201.12107_1597479_3	Nevertheless, they suffer from the factor of black box systems, where an assessment can be learned and generated easily, but without any geometrical indicator about the reasons of the system's decision.
2202.01771_1600595_3	In this approach, goals and observations are represented as a sequence of embeddings, and a policy network initialized with a pre-trained LM predicts the next action.
2202.01916_1600740_0	  Many data-driven applications in material science have been made possible because of recent breakthroughs in artificial intelligence(AI).
2202.02647_1601471_3	The resulting "Neural Narrative Maps" (NNMs), are intended to provide insight into the organization of information, opinion, and belief in the model, which in turn provide means to understand intent and response in the context of physical distance.
2202.03543_1602367_2	Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective.
2202.04173_1602997_5	We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before.
2202.05347_1604171_2	In order to proper model La Rance, parametrisation methodologies were developed for simulating (i) turbines (in pumping and power generation modes), (ii) transition ramp functions (for opening and closing hydraulic structures) and (iii) equivalent lagoon wetted area.
2202.05402_1604226_7	However, in our third experiment, participants were presented with just an AI explanation but no recommendation and had to arrive at their own decision.
2202.05686_1604510_0	  We consider the problem of estimating the topology of multiple networks from nodal observations, where these networks are assumed to be drawn from the same (unknown) random graph model.
2202.05686_1604510_1	We adopt a graphon as our random graph model, which is a nonparametric model from which graphs of potentially different sizes can be drawn.
2202.06539_1605363_0	  Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set.
2202.07446_1606270_3	A relational approach, that focus on the relational nature of things, is needed to deal with the ethical, legal, societal, cultural, and environmental implications of AI.
2202.07446_1606270_4	A relational approach to AI recognises that objective and rational reasoning cannot does not always result in the 'right' way to proceed because what is 'right' depends on the dynamics of the situation in which the decision is taken, and that rather than solving ethical problems the focus of design and use of AI must be on asking the ethical question.
2202.09360_1608184_0	  The noise and drift requirements for a navigation-grade gyroscope are widely known, yet there is no simple analytic model of how the noise and drift of a gyroscope influence the fix displacement error (FDE) of an inertial navigation system (INS).
2202.10276_1609100_2	This survey is conducted with a main intention of highlighting the most relevant information related to security vulnerabilities in the context of machine learning (ML) classifiers; more specifically, directed towards training procedures against data poisoning attacks, representing a type of attack that consists of tampering the data samples fed to the model during the training phase, leading to a degradation in the models accuracy during the inference phase.
2202.11289_1610113_3	The parts were represented as a three-dimensional (3-D) Finite Element Analysis (FEA) mesh with values of each node in the x, y, z coordinate system.
2202.12803_1611627_5	In particular, we show that MPC can be successfully implemented using a rate-based prediction model with two inputs (EGR and VGT positions) identified from data with steady-state wall temperature dynamics, however, closed-loop performance can be improved if a prediction model (i) is identified from data with transient thermal dynamics, and (ii) has the fuel injection rate as extra model input.
2202.13169_1611993_5	We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine.
2203.00320_1613182_3	The convergence condition of the GNLMP algorithm is derived, and the ability of the GNLMP algorithm to process multidimensional time-varying graph signals with multiple features is demonstrated as well.
2203.01556_1614418_5	Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us.
2203.01695_1614557_1	These patterns are acquired through a system with a coherent light source that employs a diffractive optical element (DOE) to modulate the scene resulting in coded diffraction patterns at the sensor.
2203.02094_1614956_5	The search phase of our training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs.
2203.02155_1615017_7	In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.
2203.03847_1616709_5	The main constituents of human trust in AI are identified from the literature and are characterized within the AEC project types, processes, and technologies.
2203.04866_1617728_7	Moreover, based on that the self-powered IoT devices are expected to have a more considerable capability in the heavy data flow area, we also developed a placement upgrade strategy to upgrade the general homogeneous self-powered IoT system to the heterogeneous self-powered IoT system.
2203.08465_1621327_7	Even though a lot of technical publications can be found in the literature regarding the development of AI models and many consultancy companies provide corresponding services for building AI innovations, we found very few publications sharing details about what an end-to-end framework could look like.
2203.10692_1623554_0	  Class-based language models (LMs) have been long devised to address context sparsity in $n$-gram LMs.
2203.11205_1624067_5	It is created for the assessment of Breast Imaging Reporting and Data System (BI-RADS) and density at the breast level.
2203.12687_1625549_1	Given the progress in the development of AI, a correspondingly sophisticated understanding of trust in the technology is required.
2203.13948_1626810_7	Following model development, an internal validation trial was conducted within the Tempus Labs molecular sequencing laboratory.
2203.15595_1628457_3	This paper proposes a cross-media scientific research achievements retrieval method based on deep language model (CARDL).It achieves a unified cross-media semantic representation by learning the semantic association between different modal data, and is applied to the generation of text semantic vector of scientific research achievements, and then cross-media retrieval is realized through semantic similarity matching between different modal data.
2203.15781_1628643_2	In this context, the value of V2X communications for DRL-based platoon controllers is studied with an emphasis on the tradeoff between the gain of including exogenous information in the system state for reducing uncertainty and the performance erosion due to the curse-of-dimensionality.
2203.16773_1629635_3	Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task.
2203.16932_1629794_7	The effectiveness and robustness of the proposed algorithm are demonstrated using a navigation scenario involving gravitational map matching.
2204.04935_1635073_3	Participants were asked to solve word puzzles while being supported by no system or an adaptive AI interface.
2204.05151_1635289_0	  Benchmarks are seen as the cornerstone for measuring technical progress in Artificial Intelligence (AI) research and have been developed for a variety of tasks ranging from question answering to facial recognition.
2204.06201_1636339_5	Moreover, we show that a complete constituency tree can be linearly separated from LM representations.
2204.06876_1637014_10	Last, the convergence of a classic distributed optimization algorithm is analyzed.
2204.07483_1637621_4	Once learned, more specific sentiment queries can be made of the model with high levels of accuracy when compared to traditional keyword searches.
2204.08135_1638273_6	The effects of system parameters on the SOP of the considered system are demonstrated and the accuracy of the developed analytical results is verified.
2204.08832_1638970_8	The ratio of the number of vocabulary parameters to the total number of model parameters can be empirically chosen as 20% for de facto tokenizers and 40% for other tokenizers to obtain a reasonable trade-off between model size and performance.
2204.09287_1639425_4	To progress in this direction, this paper presents use cases where complex workflows are required and investigates the main issues to be addressed for the HPC/DA/AI convergence.
2204.09719_1639857_1	With the fast development of neural network-based models, a lot of neural-based conversational AI system are developed.
2204.12641_1642779_4	Finally, this framework is constructed with an interdisciplinary and holistic view of AI/ML Innovation and builds on advances in business strategy in harmony with technological progress for AI/ML.
2204.13029_1643167_11	Analytical expressions for the Signal-to-Interference and Noise Ratio (SINR) for the non-coherent RIS-empowered system are presented.
2204.13371_1643509_4	This finding was made with the help of a simulated procedural forest model which offers the consideration of more realistic occlusion properties than our previous statistical model.
2204.13828_1643966_2	We draw from communication theories and literature on trust in technologies to develop a conceptual model called MATCH, which describes how trustworthiness is communicated in AI systems through trustworthiness cues and how those cues are processed by people to make trust judgments.
2205.00584_1644996_1	In such scenarios, user requests can be issued in one shot in the form of a complex and long query, unlike conversational and exploratory search models, where require short utterances or queries are often presented to the system step by step.
2205.00965_1645377_6	Results of the survey show that progress has been made with recent SoTA conversational AI, but there are still persistent challenges that need to be solved, and the female gender is more common than the male for conversational AI.
2205.01066_1645478_5	Extensive analyses were carried out to quantify health inequalities (a) embedded in two real-world ICU datasets; (b) induced by AI models trained for two resource allocation scenarios.
2205.01070_1645482_2	We propose that there is an opportunity to improve the extent to which interventions are understood to be effective in their contribution to the change required for Responsible AI.
2205.01614_1646026_2	For full automation of dent inspection, the acquired point cloud data must be analysed via a reliable segmentation algorithm, releasing humans from the search and evaluation of damage.
2205.03327_1647739_4	With this model and a set of offline RSS measurements, the unknown parameters are estimated.
2205.04034_1648446_4	The basis of this work is the vast amount of data which is required to validate the legitimacy of the digital twins model.
2205.06680_1651092_2	Although progress has been made in developing automatic methods to detect AI-synthesized faces, there is no open platform to study the human performance of AI-synthesized faces detection.
2205.07820_1652232_5	Finally, through simulation experiments in typical scenarios, this paper studies and compares the air defense capabilities of the system in two different modes with and without coordination, and verifies the superiority of the multi-ship cooperative air defense model in reducing the probability of missile penetration; Further, the ability changes of the defense system under different parameters such as missile speed, speed, angle, ship interception rate, range, and number of fire units are studied, and the weak points of the defense formation, defense range settings, and interception settings are obtained.
2205.08123_1652535_2	In studies with a mismatch between an AI normal report and the original radiologist report, a consensus read by two board-certified radiologists was conducted to make the final diagnosis.   
2205.09696_1654108_0	  Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies.
2205.09696_1654108_6	We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague.
2205.12749_1657161_6	We demonstrate this by presenting two instantiations: (1) an assessment that measures the classification accuracy of a system with the option to incorporate label uncertainties; (2) an assessment where the usefulness of provided explanations is determined in a human-centric manner.
2205.13770_1658182_6	Extensive evaluations are conducted to validate the performance of the proposed analytical model and algorithms.
2205.14828_1659240_4	In this paper, the performance of several algorithms in AI has been presented, such as XGBoost, LightGBM, custom 1-D Dense Block Neural Networks (DBNNs), and custom 1-D Convolutional Neural Networks (CNNs) in identifying LFV signals, specifically $\tau^{-} \rightarrow \mu^{-}\mu^{-}\mu^{+}$ decay from the combined LHCb and Monte-Carlo-Simulated data that imitates the signatures of the said decay.
2206.00335_1660755_4	The framework is designed to help organizations deploying AI systems translate ethical AI principles into practice and align their AI systems and processes with the forthcoming European AI Act.
2206.02787_1663207_6	With AI, new radiomic models using the deep learning techniques will be also described.
2206.05427_1665847_7	Then, based on such a probabilistic model, a iterative detection algorithm is developed under the framework of sparse variational inference, where each update iteration is obtained in a closed-form and the number of active devices can be automatically estimated for effectively avoiding the overfitting of noise.
2206.07433_1667853_5	Our simulations are carried out in a standard pre-operational experimental set-up with the full global observing system, 52 km global resolution and $10^6$ model variables.
2206.08287_1668707_1	While there has been significant work on analysing and mitigating algorithmic bias, the broader mechanisms of how bias emerges in AI applications are not well understood, hampering efforts to address bias where it begins.
2206.08434_1668854_0	  In this work, the case is made for a wholistic top-down re-envisioning of the system stack from the programming language level down through the system architecture to bridge this complexity gap.
2206.08603_1669023_3	In this study, PID and I-PD controllers are designed for the air gap control of the new cross-type 4-pole mechanical contactless carrier system.
2206.09978_1670398_4	Our contributions are threefold: 1) we introduce a practice-based approach for understanding ethical AI; 2) we present empirical findings from our study on the operationalization of ethics in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and 3) based on our empirical findings, we suggest that ethical AI practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations.
2206.10604_1671024_3	A method has been developed for the training process of a machine learning model, which reflects the internal connections between the components of an artificial intelligence system that allow it to learn from training data.
2206.11899_1672319_2	So, this piece is about the disciplinary and conceptual questions that might be encountered, and -- in my view -- may need addressing for engagements with AI research and its affiliates.
2206.13289_1673709_2	Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation), whereas the core-linguistic concepts (e.g., morphological or syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.
2206.13950_1674370_5	We demonstrate this unification in the example of an optomechanical system, where mechanical dissipation is also considered.
2206.14305_1674725_5	The model was developed using a training set of 378 patients across our health system and tested on a separate set of 93 patients.
2206.15132_1675552_5	Then, the standardization of AI for CSI feedback enhancement in 5G-advanced is presented in detail.
2206.15132_1675552_6	First, the scope of the AI for CSI feedback enhancement in 5G-Advanced is presented and discussed.
2206.15132_1675552_7	Then, the main challenges and open problems in the standardization of AI for CSI feedback enhancement, especially focusing on performance evaluation and the design of new protocols for AI-enabled CSI feedback, are identified and discussed.
2207.00804_1676702_0	  An autonomous Artificial Internet of Things (AIoT) system for elderly dementia patients monitoring in a smart home is presented.
2207.00969_1676867_8	By using human motions recognition as a concrete AI inference task, extensive experiments are conducted to verify the performance of our derived optimal ISCC scheme.
2207.01244_1677142_6	Finally, numerical results are presented which demonstrate the performance gains of the proposed hybrid IRS architecture and its optimal design over the conventional schemes with active/passive IRS only under various practical system setups.
2207.01493_1677391_2	However, the implications of AI ethics principles and guidelines are still being debated.
2207.02543_1678441_6	At first, a reduced set of model evaluations is performed and the corresponding solutions are used to establish an approximate mapping from the problem's parametric space to its solution space using deep feedforward neural networks and convolutional autoencoders.
2207.02947_1678845_0	  This study investigates an optimal investment problem for an insurance company operating under the Cramer-Lundberg risk model, where investments are made in both a risky asset and a risk-free asset.
2207.03509_1679407_3	This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller.
2207.06372_1682270_7	The system architecture has been designed to scale up with the number of devices to be monitored and with the number of software components to be considered in the distributed logging system.
2207.09374_1685272_2	We argue that in order to fully understand a decision, not only knowledge about relevant features is needed, but that the awareness of irrelevant information also highly contributes to the creation of a user's mental model of an AI system.
2207.10245_1686143_5	Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias.
2207.11564_1687462_0	  The need for explainable AI (XAI) is well established but relatively little has been published outside of the supervised learning paradigm.
2207.12052_1687950_3	A design science method is adopted for conducting the experimental study with structured machine learning techniques which is the primary element of a comprehensive AI solution framework informed through a proposed moderation of the technology-organization-environment theory.
2207.12229_1688127_1	There have been many AI related courses in the respects of algorithms and applications, while not many courses in system level are seriously taken into considerations.
2207.14086_1689984_5	However, it can be seen that general interest in AI and a higher educational level are predictive of some engagement with AI.
2207.14086_1689984_8	These have been identified in several ethical guidelines (including the EU Commission's proposal) as key elements for the development of ethical AI.
2207.14086_1689984_10	We conclude that the low level of ethical implications may pose a serious problem for the actual implementation of ethical AI for the Common Good and emphasize that those who are presumably most affected by ethical issues of AI are especially unaware of ethical risks.
2208.00681_1691393_0	  Efforts to promote fairness, accountability, and transparency are assumed to be critical in fostering Trust in AI (TAI), but extant literature is frustratingly vague regarding this 'trust'.
2208.01740_1692452_5	In order to showcase the methodology, a tool that visualizes different outputs of the algorithm is developed.
2208.02502_1693214_1	At the same time, little focus is made on how such loss affects the dynamics of the formation as a system.
2208.06622_1697334_3	First, in order to reduce the number of RF chains and the channel estimation overhead, RF beamformers are designed based on the 3D geometry-based mmWave channel model using slow time-varying angular parameters of the channel.
2208.06742_1697454_4	In this paper, machine learning (ML) based classification approaches, namely logistic regression, neural networks, random forest and K-nearest neighbor, were studied for model reduction of SCUC.
2208.06946_1697658_5	We conducted a pilot experiment in which individuals are asked to distinguish between authentic passwords and honeywords when the username is provided for GPT-3 and a tweaking technique.
2208.07601_1698313_3	Noting that the computation of the LM rate can also be formulated as an entropy-based optimization problem with constraints, in this work, we transform the task into an optimal transport (OT) problem with an extra constraint.
2208.10087_1700799_5	This paper is meant to complement principles-based frameworks like Australia's Artificial Intelligence Ethics Framework and the EU Assessment List for Trustworthy AI.
2208.14360_1705072_8	Several experiments using different atlases are conducted to evaluate the segmentation performance of the trained model compared to the state-of-the-art, and the results show higher segmentation accuracy and robustness of the proposed model compared to the existing methods across different intra- and inter-domain datasets.
2209.03433_1709148_0	  Explainable Artificial Intelligence (XAI) methods are intended to help human users better understand the decision making of an AI agent.
2209.04963_1710678_0	  Responsible AI is widely considered as one of the greatest scientific challenges of our time and is key to increase the adoption of AI.
2209.06317_1712032_4	The best way to reduce risks is to implement comprehensive AI lifecycle governance where policies and procedures are described and enforced during the design, development, deployment, and monitoring of an AI system.
2209.07335_1713050_5	The output of the PRISMA model led to the identification of 23 related articles, and the findings of this study were presented based on the analysis of these articles.
2209.08983_1714698_7	The numerical results show that the proposed algorithms achieve more than $100 \%$ gain in terms of minimum SINR, compared to a system with random RIS phase shifts, when $40$ RIS elements, $20$ antennas at the BS and $10$ users, are considered.
2209.09204_1714919_7	In all AI false negative (FN) cases, a radiologist was found to have also made the same error when compared to final ground-truth labels.
2209.09491_1715206_4	Our algorithm was able to successfully train the agents, and its performance was preliminarily proven through the mini-competition against 10 teams wishing to take part in the AI Soccer international competition.
2209.09856_1715571_4	To test the efficacy of this algorithm, simulation results are presented along with evaluations of key system parameters, including the Rician factor and RIS location, on the achievable sum rate of the users.
2209.13273_1718988_1	Recently, its use has been extended beyond communication networks, and successful applications of the AIMD algorithm have been reported in transportation, energy, and mathematical biology.
2209.13646_1719361_2	To overcome the complication, lots of research related to vibration-based monitoring system with sensor has been devised.
2209.14195_1719910_4	An Android application has been developed to show the applicability and performance of the system.
2209.15615_1721330_6	An expectation-conditional-maximization (ECM) algorithm is developed for estimation of such a model and some properties of the algorithm are established.
2210.00439_1721794_1	The URANS simulation with the Spalart-Allmaras (SA) turbulence model is checked for the quality of the generated mesh and compared with the available wind tunnel data.
2210.00811_1722166_4	The performance of the algorithm and the sequence of improvements leading to the achieved results are presented and discussed.
2210.01478_1722833_3	A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations.
2210.03221_1724576_7	Our work establishes a theoretical foundation for a portable quantum pre-trained language model that could be trained on private data and made available for public use with privacy protection guarantees.
2210.03332_1724687_2	Numerous efforts have been made to automate the entire glaucoma classification procedure however, these existing models in general have a black box characteristics that prevents users from understanding the key reasons behind the prediction and thus medical practitioners generally can not rely on these system.
2210.03842_1725197_2	They mark a new era in human-AI interaction (HAI) that diverges from traditional human-computer interaction (HCI), where computers are commonly seen as tools instead of social actors.
2210.04742_1726097_7	In such a split ML system, the precoding and combining matrices are regarded as trainable parameters, while MIMO channel matrix is regarded as unknown (implicit) parameters.
2210.06772_1728127_3	In our method, multiple teachers are trained on disjoint training sets whose privacy one wishes to protect, and teachers' predictions supervise the training of a student model in an alternating manner at each time step.
2210.07041_1728396_3	We study this problem with a simple two-tower language model setting, where two language models with identical configurations are trained side-by-side cooperatively.
2210.08132_1729487_7	Finally, a case study is presented to demonstrate the efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly detection framework.
2210.08692_1730047_5	Different DSs are trained via RL with GUS, the classic agenda-based user simulator (ABUS) and other ablation simulators respectively, and are compared for cross-model evaluation, corpus-based evaluation and human evaluation.
2210.08984_1730339_3	Social diversity, equity and inclusion are considered key success factors of AI to mitigate risks, create values and drive social justice.
2210.10585_1731940_1	Through surveys of human subjects enrolled in the crowdsourcing platform Prolific.co and queries submitted to the OpenAI's language model GPT-3, I test whether the numerical response for what wage is deemed fair for a particular job description changes when respondents and GPT-3 are prompted with additional information that includes a numerical minimum wage, whether realistic or unrealistic, relative to a control where no minimum wage is stated.
2210.11584_1732939_0	  Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research.
2210.13569_1734924_7	Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth.
2210.13966_1735321_0	  We survey a current, heated debate in the AI research community on whether large pre-trained language models can be said to "understand" language -- and the physical and social situations language encodes -- in any important sense.
2210.14596_1735951_0	  Digital Twins are digital representations of systems in the Internet of Things (IoT) that are often based on AI models that are trained on data from those systems.
2210.15889_1737244_9	This survey is expected to help new researchers enter this rapidly evolving field and accelerate the progress towards data-and knowledge-driven AI.
2210.16147_1737502_6	These evaluations are carried out against a baseline that includes estimates of next-word predictability from a Transformer neural network language model.
2210.16539_1737894_8	Mean, standard deviation and the maximum among accuracy scores over 15 experiment runs are adopted as performance measurements for the AD detection system.
2211.01329_1740235_9	Once the process noise covariance is learned, it is fed into the model-based navigation filter.
2211.02919_1741825_1	In pursuit of the balance between energy efficiency and low-cost implementations, the cloud-management transmission protocol is adopted in the integrated multi-media system.
2211.05344_1744250_1	Most PLMs are trained with linguistic-agnostic pre-training tasks on the surface form of the text, such as the masked language model (MLM).
2211.06346_1745252_2	Differences between traditional ethics in the medical domain and emerging ethical challenges with AI-driven healthcare are presented, particularly as they relate to transparency, bias, privacy, safety, responsibility, justice, and autonomy.
2211.06890_1745796_1	By putting UAM into practical future transportation, several benefits can be realized, i.e., (i) the total travel time of passengers can be reduced compared to traditional transportation and (ii) there is no environmental pollution and no special labor costs to operate the system because electric batteries will be used in UAM system.
2211.07280_1746186_0	  While certain industrial sectors (e.g., aviation) have a long history of mandatory incident reporting complete with analytical findings, the practice of artificial intelligence (AI) safety benefits from no such mandate and thus analyses must be performed on publicly known ``open source'' AI incidents.
2211.07642_1746548_8	The extensive experiments show that training time, resource usage and accuracy of the model are found to be significantly better than five state-of-the-art competing models.
2211.07643_1746549_6	Numerical experiments and comparative analysis were carried out between our proposed system, using the most accurate random forest (RF) model, and the two most used state-of-the-art machine learning approaches, Logistic Regression (LR) and Support Vector Machine (SVM), using three real-life diabetes datasets.
2211.08244_1747150_8	Based on the X-Ray images, an algorithm was developed that classifies X-Ray images with height accuracy and power faster thanks to the architecture transformation of the model.
2211.09038_1747944_4	As a result, different strategies of AI deployment are required between these two radiotherapy modalities.
2211.09084_1747990_1	To guarantee a high product quality and make sure that functional safety standards such as ISO26262 are fulfilled, the exploitation of potentials of model-driven systems engineering in the form of automatic analyses, consistency checks, and tracing mechanisms is indispensable.
2211.09924_1748830_1	This paper proposes a new Linear Matrix Inequality (LMI) for static output feedback control assuming that a Linear Quadratic Regulator (LQR) has been previously designed for the system.
2211.10806_1749712_6	CESO has also been chosen as the prominent way to express both fragments and the final proposed scenario content by our AI-assisted Cyber Exercise Framework (AiCEF).
2211.10859_1749765_0	  Intelligent human inputs are required both in the training and operation of AI systems, and within the governance of blockchain systems and decentralized autonomous organizations (DAOs).
2211.11483_1750389_0	  This work is intended as a voice in the discussion over previous claims that a pretrained large language model (LLM) based on the Transformer model architecture can be sentient.
2211.12035_1750941_11	It is hoped that this data set will further accelerate research in data-driven urban AI, even as our baseline model facilitates quantitative comparison to future methods.
2211.12143_1751049_1	To mitigate the negative effects of false information more effectively, the development of Artificial Intelligence (AI) systems assisting fact-checkers is needed.
2211.13125_1752031_9	Additionally, to better understand the distillation approach, extensive experimentation on the independent modules of the proposed model was conducted.
2211.14352_1753258_1	In this study, the effects of these conditions on airway closure are examined in a model problem, where an elastoviscoplastic (EVP) single liquid layer lines the inner wall of a rigid pipe and surrounds the air core.
2211.15950_1754856_4	However, advanced AI-based diagnosis addressing intrinsic noise in CBCT has not been devised, discouraging the practical use of AI solutions for CBCT.
2211.16444_1755350_0	  The need for AI systems to provide explanations for their behaviour is now widely recognised as key to their adoption.
2212.00193_1756370_1	However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work.
2212.00389_1756566_8	The training based on the RCS is performed with the widely used Deep Q-network (DQN) and tested in the AI Soccer environment implemented with Webots simulation software.
2212.01768_1757945_5	In this way, the depth of every pixel can be learned via a meaningful geometry model.
2212.03601_1759778_2	Target-oriented management decisions are made, among other things, on the basis of risk management and compliance in combination with the internal control system as governance functions.
2212.04645_1760822_10	Finally, open challenges and promising future research directions have been identified and discussed in the area of AI/ML-based fog/edge computing.
2212.05058_1761235_1	We argue the intentional fictional projection of subjectivity onto LLMs can yield an alternate frame through which AI behaviour, including its productions of bias and harm, can be analysed.
2212.06576_1762753_5	In contrast to clean AI models, the poisoned AI models were trained with traffic sign images containing systematic, physically realizable, traffic sign modifications (i.e., triggers) to change a correct class label to another label in a presence of such a trigger.
2212.06828_1763005_2	Although there comes a fast increasing trend to study the quality issues of deep neural networks (DNNs) at the model level, few studies have been performed to investigate the quality of DNNs at both the unit level and the potential impacts on the system level.
2212.07996_1764173_4	OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI.

2212.08364_1764541_6	The article is intended to inform decision-makers at leading AI companies, regulators, and standard-setting bodies.
2212.09172_1765349_1	Considering the rapidly emerging new machine learning techniques, such as graph learning, federated learning, and transfer learning, a timely survey is needed to provide an overview of resource management and network slicing techniques of AI-enabled wireless networks.
2212.09292_1765469_4	Further research is needed to fully understand the implications of large language models like ChatGPT and to devise strategies for combating the risk of cheating using these tools.
2212.09542_1765719_0	  The limit free energy of an enriched mixed $p$-spin spin glass model has been identified with the solution of a Hamilton-Jacobi equation.
2212.10560_1766737_4	Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations.
2212.10788_1766965_6	Analysis of the post-learning features showed that node types that were not known to the model beforehand are distinguished through the learning process based on the graph structure.
2212.11738_1767915_4	In the second part of the study, the negative impacts of AI on the environment and the emerging technological solutions to support Green AI are examined.
2212.14177_1770354_3	AI has been shown to improve efficiency in medical-image generation, processing, and interpretation, and a variety of such AI models have been developed across research labs worldwide.
2212.14882_1771059_6	While further studies are needed, the initial insights of this study indicate a great potential in using large language models like ChatGPT to improve patient-centered care in radiology and other medical domains.
2301.01135_1772199_3	The mock-up was then experimentally tested, and the obtained data were analysed using numerical simulations with Ansys Fluent and a theoretical model based on a previously established analytical SWBLI model.
2301.01277_1772341_7	After the respecification of the initial theoretical model, a nested model, which was also based on TAM, was developed and tested.
2301.01558_1772622_7	The use of the uncertainties in the new coordinate system for subsequent high-level analyses is illustrated.
2301.02828_1773892_3	In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on.
2301.04246_1775310_4	While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.
2301.05133_1776197_3	Some artists fear if the cheaper price and conveyor-belt speed that comes with AI-produced images is seen as an improvement to the current system, it may permanently change the way society values/views art and artists.
2301.05272_1776336_1	The impressive scalability of LLMs due to the advent of deep learning can be seen as a continuation of empiricist lingusitic methods, as opposed to rule-based linguistic methods that are grounded in a nativist perspective.
2301.05517_1776581_1	How can we ensure that AI systems, including ChatGPT, are developed and adopted in a responsible way?
2301.08828_1779892_7	A case study of a middle-aged PTSD patient treated with the AI-enabled RPM system is demonstrated in this study.
2301.08865_1779929_3	Two policies are considered for the proposed system.
2301.09001_1780065_1	To ease the development process, continuous pipelines for AI have become an active research area where consolidated and in-depth analysis regarding the terminology, triggers, tasks, and challenges is required.
2301.10007_1781071_0	  In order to develop trustworthy healthcare artificial intelligence (AI) prospective and ergonomics studies that consider the complexity and reality of real-world applications of AI systems are needed.
2301.10009_1781073_9	The future directions of AI in RPM applications are analyzed based on the challenges and trends
2301.11047_1782111_12	Industrial parties are involved in Green AI studies, albeit most target academic readers.
2301.11616_1782680_2	To ensure responsible AI, the risks associated with AI systems' development and use must be identified, assessed and mitigated.
2301.12369_1783433_0	  Existing algorithms for ensuring fairness in AI use a single-shot training strategy, where an AI model is trained on an annotated training dataset with sensitive attributes and then fielded for utilization.
2301.12867_1783931_3	Large-scale benchmarks for accountable LLMs should consequently be developed.
2301.13852_1784916_3	In this paper, we study whether a machine learning model can be effectively trained to accurately distinguish between original human and seemingly human (that is, ChatGPT-generated) text, especially when this text is short.
2302.00763_1785697_6	We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.
2302.02065_1786999_0	  In this work, the uplink channel estimation problem is considered for a millimeter wave (mmWave) multi-input multi-output (MIMO) system.
2302.03774_1788708_0	  Significant enthusiasm around AI uptake has been witnessed across societies globally.
2302.04110_1789044_3	Arguments in support of the idea that regulation stifles innovation are analysed and criticised, and an alternative point of view is offered, showing how regulation and standards can foster innovation in the field of AI.
2302.04603_1789537_5	The resulting data is analyzed using reflexive thematic analysis to identify the main challenges facing the implementation of contestability in public AI.
2302.04844_1789778_4	Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment.
2302.06034_1790968_1	RE for AI practices have not been studied much and have scarce empirical studies.
2302.06476_1791410_2	However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot.
2302.06609_1791543_1	Their demands and requirements for AI explainability are not incorporated into the design and evaluation of XAI techniques, which are developed to explain the rationales of AI decisions to end-users and assist their critical decisions.
2302.06975_1791909_4	This review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of AI models.
2302.08579_1793513_2	To alleviate this problem, this paper designs a replaceable internal language model (RILM) method, which makes it feasible to directly replace the internal language model (LM) of E2E ASR models with a target-domain LM in the decoding stage when a domain shift is encountered.
2302.08807_1793741_5	We find that although the voice of AI is considered valuable, AI still plays a secondary role in the group because it cannot fully follow the dynamics of the discussion and make progressive contributions.
2302.09487_1794421_0	  The use of AI in healthcare is designed to improve care delivery and augment the decisions of providers to enhance patient outcomes.
2302.10294_1795228_6	As a baseline model, large eddy simulations (LES) are carried out using the discontinuous Galerkin flow solver FLEXI.
2302.10816_1795750_2	In this paper, we argue that RE should not only be carefully conducted but also tailored for Responsible AI.
2302.11957_1796891_2	However, it is not yet known whether LLMs can be served as a high-quality sentence simplification system.
2302.12246_1797180_1	It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers.
2302.13142_1798076_7	Control development and validations were conducted on two fuel cell system (FCS) models, a nonlinear open-source model and a proprietary Ford high-fidelity model
2302.14229_1799163_2	However, it is not yet known the performance of LLMs on CLS.
2302.14360_1799294_3	The study findings suggest that users are comfortable with AI systems if they have control and their privacy is not affected.