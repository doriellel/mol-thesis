23_33157_3	Knowledge neurons are identified by masking the o part from sentences representing relational triplets (s, r, o), having the LLM predict the masked part, and observing the LLM's activation during the prediction.
0912.2373_162124_4	A model used to describe the stopping force experienced by a projectile impacting a granular bed can be shown to predict these behaviors for our system's geometry, indicating that the same mechanics dictate both steady-state and transient drag forces in granular systems, regardless of geometry or material properties of the grains.
1103.5169_252437_4	In particular, pilots rarely perfectly execute the recommended maneuvers, despite the fact that the collision avoidance system's effectiveness relies on their doing so.
1208.6130_366702_5	We also give a heuristic condition for when an arbitrary system's nonlinear response means that its ZT ceases to indicate (even qualitatively) the lowest temperature to which the system can refrigerate.
1301.0038_396628_2	Furthermore, their model checking verification typically becomes unfeasible due to the huge state space explosion caused by the system's concurrency.
1301.1064_397654_3	Moreover, the proposed approach features few parameters, whose effects on the system's behavior are very intuitive, hence simplifying tuning procedures.
1301.3535_400125_1	Passengers' experience is becoming a key metric to evaluate the air transportation system's performance.
1302.1382_405722_2	Each clinical study's method for measuring the CAD system's improvement in test sensitivity is examined in this meta-analysis.
1303.5148_416805_2	One approach to speech language model adaptation is self-training, in which a language model's parameters are tuned based on automatically transcribed audio.
1304.0401_419534_4	Our model's accuracy in both EROS-2 and MACHO training sets is about 90% precision and 86% recall, improving the state of the art models accuracy in quasar detection.
1310.0586_465589_6	The algorithm uses only the measured traction force on the tether and it is able to adapt the system's operation to maximize the average force with uncertain and time-varying wind.
1605.04872_732987_1	This dynamic behavior not only characterizes the evolution of the system but also affect the system's functioning.
1606.02761_740668_0	  Epsilon Eridani is a nearby, young Sun-like star that hosts a ring of cool debris analogous to the solar system's Edgeworth-Kuiper belt.
1606.03175_741082_2	We provide a constant-factor approximation of the system's degrees of freedom (DoF), for arbitrary number of transmitters, number of receivers, content library size, receiver cache size, and transmitter cache size (as long as the transmitters combined can store the entire content library among them).
1606.08810_746717_3	An analysis approach based on the comparison between this model's predictions and data to discriminate among a set of composition scenarios is presented and tested with simulations.
1608.08196_764658_0	  We argue that there already exists de facto artificial intelligence policy - a patchwork of policies impacting the field of AI's development in myriad ways.
1609.04879_770357_8	This paper discusses the research leading to Extreme AI; develops the ideas found in that thesis; discusses the development of other personality engines; and provides examples of Extreme AI's use in two game demos.
1609.05918_771396_5	If this is the case, its dynamical tidal radius is only <19 pc at this distance from the LMC, and smaller than the system's extent on the sky.
1610.00494_775841_4	The added cascade modulates the AI legacy system's decisions.
1701.06356_811764_1	The web based platform provides online plotting and analysis tools which allow users to learn, evaluate, teach and see the performance of parallel algorithms from a system's viewpoint.
1704.00717_835229_5	In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs, and quirks.
1704.00717_835229_10	Surprisingly, we find that having access to the model's internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior.
1706.01580_856315_5	We quantify our system's performance on both simulated and real data sets, and demonstrate city-scale map reconstruction accurate to within 2 meters using nearly 90,000 aerial video frames - to our knowledge, the largest and fastest such reconstruction to date.
1706.05442_860177_3	We show that the random data arrival behavior at the transmitter and the channel randomness of the legitimate link can improve the system's security.
1707.01659_866691_0	  In this work, a dynamic system is controlled by multiple sensor-actuator agents, each of them commanding and observing parts of the system's input and output.
1707.04959_869991_3	Nevertheless the linearized time-varying system is completely controllable, under easily verifiable conditions, and the system's disturbance rejection capabilities can be enhanced by adding air drag panels exemplifying a beneficial interplay between hardware design and control.
1710.11009_906392_1	We propose an artificial-noise (AN) aided scheme to enhance the system's security in the presence of an eavesdropper by exploiting the decoupled nature of the power-line and wireless communication media.
1712.01078_919677_6	We compare magnetic field measurements in the inner spectrometer volume during system commissioning with corresponding simulations, which allows to verify the system's functionality in fine-tuning the magnetic field configuration.
1712.02146_920745_1	For a system identification scenario, such a derivation makes it hard to incorporate prior information on the system's impulse response.
1712.05855_924454_6	In this paper, we propose several open research directions in systems, architectures, and security that can address these challenges and help unlock AI's potential to improve lives and society.
1712.05889_924488_4	To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state.
1805.10410_983455_2	Aided inertial navigation is fundamentally a nonlinear observer design problem; thus, current solutions are based on approximations of the system dynamics, such as an Extended Kalman Filter (EKF), which uses a system's Jacobian linearization along the current best estimate of its trajectory.
1805.10410_983455_3	On the basis of the theory of invariant observer design by Barrau and Bonnabel, and in particular, the Invariant EKF (InEKF), we show that the error dynamics of the point contact-inertial system follows a log-linear autonomous differential equation; hence, the observable state variables can be rendered convergent with a domain of attraction that is independent of the system's trajectory.
1806.11432_997072_2	To do so, we define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of functions that forces the model's generated output to include a set of user-defined keywords.
1808.08079_1017225_4	Results from such an intervention reveal a large increase in the language model's accuracy.
1809.06800_1026816_2	However, visual co-articulation effects in visual speech signals damage the performance of visual speech LM's as visually, people do not utter what the language model expects.
1809.06800_1026816_3	These models are commonplace but while higher-order N-gram LM's may improve classification rates, the cost of this model is disproportionate to the common goal of developing more accurate classifiers.
1810.04053_1035240_4	Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic.
1811.10399_1055032_5	A camera, aligned with the system's predetermined orientation serves as input to the computer system, which has the object recognition Neural Network deployed to carry out real-time object detection.
1812.01184_1058758_4	For machines in general, and for AI's especially, operating over extended periods or in extreme environments will require energy usage orders of magnitudes more efficient than exists today.
1812.03631_1061205_1	A representative task is Visual Question Answering where large diagnostic datasets have been proposed to test a system's capability of answering questions about images.
1812.03980_1061554_4	These ethical principles should define the boundaries of AI's freedom and creativity.
1903.09475_1101861_3	We propose two tests: checking whether a final(goal) state exists in the model's described problem space and checking whether the transitions described can provide a path from the identified initial states to any the goal states (meaning a solution has been found).
1904.09251_1114291_4	We show that the error dynamics follows a log-linear autonomous differential equation with several important consequences: (a) the observable state variables can be rendered convergent with a domain of attraction that is independent of the system's trajectory; (b) unlike the standard EKF, neither the linearized error dynamics nor the linearized observation model depend on the current state estimate, which (c) leads to improved convergence properties and (d) a local observability matrix that is consistent with the underlying nonlinear system.
1905.04610_1123046_5	These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time.
1906.01148_1133334_1	In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences.
1906.01148_1133334_4	While updates can increase the AI's predictive performance, they may also lead to changes that are at odds with the user's prior experiences and confidence in the AI's inferences, hurting therefore the overall team performance.
1906.01895_1134081_9	And the system's computation and communication delay under the interaction scenario between tester and remote data center are analyzed.
1906.11124_1143310_4	A detailed case study is carried out with an extensive DNB-specific CHF database to demonstrate (1) the improved performance of the hybrid approach as compared to traditional domain knowledge-based models, and (2) the hybrid model's superior generalization capabilities over standalone ML methods across a wide range of flow conditions.
1907.07247_1151783_5	In order to maintain the model's accuracy with respect to some test set we propose both financial and non-financial (gamified) incentive structures for providing good data.
1907.12932_1157468_4	We demonstrate that qualitatively similar behaviour to the experiments is exhibited by a previously established, depth-averaged mathematical model; a consequence of the model's intricate solution structure.
1908.07690_1165860_3	Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context.
1908.11199_1169369_1	However, the DL model's complexity usually results in difficult model interpretation when used in clinical.
1909.04879_1174917_3	The translation model's predictions are weighted by the language model with a hand-crafted ratio in advance.
1910.04404_1188323_0	  Explanation is necessary for humans to understand and accept decisions made by an AI system when the system's goal is known.
1910.04404_1188323_2	In such situations, explanations should aim to increase user satisfaction, taking into account the system's decision, the user's and the other agents' preferences, the environment settings and properties such as fairness, envy and privacy.
1910.04404_1188323_5	We then review the state of the art and discuss research directions towards efficient methodologies and algorithms for generating explanations that will increase users' satisfaction from AI system's decisions in multi-agent environments.
1910.05749_1189668_4	The SPED model's slow speed is a bottleneck to performing a large parameter sweep.
1910.06848_1190767_1	This paper describes Facebook AI's submission to the WAT 2019 Myanmar-English translation task.
1910.14659_1198578_3	By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model's WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation.
1911.03064_1201658_5	We then propose embedding and sentiment prediction-derived regularization on the language model's latent representations.
1911.03216_1201810_1	While useful and necessary, we argue that this "agency" approach disregards more indirect and complex risks resulting from AI's interaction with the socio-economic and political context.
1911.03216_1201810_2	This paper calls for a "structural" approach to assessing AI's effects in order to understand and prevent such systemic risks where no individual can be held accountable for the broader negative impacts.
1911.08005_1206599_7	Recognizing the importance of the latter task, we leave this to further research in support of the legal system's incremental adaptation to the novel challenges of present and future AI technologies.
1911.08976_1207570_1	Red Dragon AI's entries used the language of the questions and explanation text directly, rather than a constructing a separate graph-like representation.
1911.09853_1208447_1	However, the lack of explainability and interpretability of successful AI models is a key stumbling block when trust in a model's prediction is critical.
1912.02164_1214060_4	Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation.
1912.08148_1220044_0	  In the linear minimum mean square error (LMMSE) estimation for orthogonal frequency division multiplexing (OFDM) systems, the problem about the determination of the algorithm's parameters, especially those related with channel frequency response (CFR) correlation, has not been readily solved yet.
1912.13415_1225311_5	Because the bulk of our model's parameters are pre-trained and we eschew recurrence for self-attention, our model is fast to train.
2001.02114_1227527_1	In these scenarios, full automation is often undesirable, not only due to the significance of the outcome, but also because human experts can draw on their domain knowledge complementary to the model's to ensure task success.
2001.02114_1227527_6	Through two human experiments, we show that confidence score can help calibrate people's trust in an AI model, but trust calibration alone is not sufficient to improve AI-assisted decision making, which may also depend on whether the human can bring in enough unique knowledge to complement the AI's errors.
2001.06286_1231699_6	We also evaluated the importance of language-specific tokenizers and the model's fairness.
2001.07417_1232830_1	The counterfactual approach we consider defines an explanation as a set of the system's data inputs that causally drives the decision (i.e., changing the inputs in the set changes the decision) and is irreducible (i.e., changing any subset of the inputs does not change the decision).
2001.08298_1233711_2	We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance.
2001.09766_1235179_0	  Given AI's growing role in modeling and improving decision-making, how and when to present users with feedback is an urgent topic to address.
2002.03806_1241229_3	To fill this gap, in this paper the concept of fair buffer scheduling is proposed which can potentially contribute to strategical and tactical operations synchronization that would result in ATFM delay mitigation by increasing the system's robustness.
2003.07679_1258034_2	Further, little is known about generative AI's potential for malicious misuse at large scale.
2004.00584_1265354_7	This way, Ditto is forced to learn "harder" to improve the model's matching capability.
2004.02289_1267059_2	Prior work has studied the trade-off between improving the system's accuracy following an update and the compatibility of the updated system with prior user experience.
2004.02451_1267221_3	In this paper, using English data, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model's robustness on them, with a negligible loss of perplexity.
2004.04479_1269249_6	We show that in both cases, i.e., in the case of an attack based on adversarial examples and in the case of a stealth attack, the dimensionality of the AI's decision-making space is a major contributor to the AI's susceptibility.
2004.04479_1269249_8	According to our findings, robustness to adversarial examples requires either (a) the data distributions in the AI's feature space to have concentrated probability density functions or (b) the dimensionality of the AI's decision variables to be sufficiently small.
2004.05067_1269837_2	One popular method for determining a model's ability to induce syntactic structure trains a model on strings generated according to a template then tests the model's ability to distinguish such strings from superficially similar ones with different syntax.
2004.09890_1274660_1	Occlusion is a well established method that provides explanations on discrete language data, e.g. by removing a language unit from an input and measuring the impact on a model's decision.
2004.12506_1277276_3	Our work takes a step in understanding GPT-2's outputs in terms of discourse coherence.
2004.12506_1277276_4	We perform a comprehensive study on the validity of explicit discourse relations in GPT-2's outputs under both organic generation and fine-tuned scenarios.
2004.14614_1279384_1	To reduce this homogeneity, external knowledge such as the speaker's profile and domain knowledge is applied as an additional condition to diversify a model's output.
2005.01101_1280896_4	Centrality measures were computed to assess the system's topology and its global robustness.
2005.01101_1280896_12	Findings suggest that although the system's ability to sustain its operational level under extreme circumstances has lately improved, its tolerance to targeted attacks has deteriorated.
2005.01101_1280896_13	The presented methodology may be applied on different network levels or different transportation networks, to provide a general perspective of the system's vulnerabilities.
2005.02335_1282130_0	  Explainable machine learning and artificial intelligence models have been used to justify a model's decision-making process.
2005.03094_1282889_4	In this paper we share our experience applying the AIOps approach to a production cloud object storage service to get actionable insights into system's behavior and health.
2005.11072_1290867_2	Developing regulatory frameworks is, however, challenging due to AI's global reach and the existence of widespread misconceptions about the notion of regulation.
2005.13635_1293430_2	Determining whether an AI caused a specific event and, if so, what triggered the AI's action, are key forensic questions.
2005.14165_1293960_8	At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.
2006.01912_1296426_5	In general, the better a model's next-word expectations, the better its psychometric predictive power.
2006.04707_1299221_4	Finally, to help practitioners with applying these recommendations, we review a case study of AI's use in forest ecosystem restoration, demonstrating how an impact assessment framework can translate into effective and responsible AI practices.
2006.08828_1303342_0	  The broader ambition of this article is to popularize an approach for the fair distribution of the quantity of a system's output to its subsystems, while allowing for underlying complex subsystem level interactions.
2006.09428_1303942_3	MAIEI provides 15 recommendations in relation to the sections outlined above, including: 1) focus efforts on the research and innovation community, member states, and the private sector; 2) create alignment between trading partners' policies and EU policies; 3) analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy AI; 4) focus on coordination and policy alignment; 5) focus on mechanisms that promote private and secure sharing of data; 6) create a network of AI research excellence centres to strengthen the research and innovation community; 7) promote knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8) add nuance to the discussion regarding the opacity of AI systems; 9) create a process for individuals to appeal an AI system's decision or output; 10) implement new rules and strengthen existing regulations; 11) ban the use of facial recognition technology; 12) hold all AI systems to similar standards and compulsory requirements; 13) ensure biometric identification systems fulfill the purpose for which they are implemented; 14) implement a voluntary labelling system for systems that are not considered high-risk; 15) appoint individuals to the oversight process who understand AI systems well and are able to communicate potential risks.
2006.12683_1307197_1	However, it remains challenging to incorporate such tools into pathologists' practice; one main concern is AI's insufficient workflow integration with medical decisions.
2006.12683_1307197_3	To bridge the gap between pathologists and AI, we developed a human-AI collaborative diagnosis tool -- xPath -- that shares a similar examination process to that of pathologists, which can improve AI's integration into their routine examination.
2006.12695_1307209_2	To explore this question, we first propose a series of collaborative techniques to engage human pathologists with AI given AI's capabilities and limitations, based on which we prototype Impetus - a tool where an AI takes various degrees of initiatives to provide various forms of assistance to a pathologist in detecting tumors from histological slides.
2006.14779_1309293_5	Rather, explanations increased the chance that humans will accept the AI's recommendation, regardless of its correctness.
2007.00900_1312414_8	The comparison between two VQA models shows BERT based explanations and the use of object features improve the user's prediction of the model's competencies.
2007.11820_1323334_5	We install a Trojan virus into the real Go AI that manipulates the AI's behavior.
2007.12525_1324039_7	Besides, Local Interpretable Model-agnostic Explanations (LIME) is used to explain the model's interpretability.
2008.01307_1329029_4	This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task.
2008.02275_1329997_0	  We show how to assess a language model's knowledge of basic concepts of morality.
2008.03301_1331023_5	This approach yields an algorithm that captures SVM model's underlying logic and outperforms %GG: the FOIL algorithm --> other ILP algorithms other ILP algorithms in terms of the number of induced clauses and classification evaluation metrics.
2008.07343_1335065_7	It is envisaged that this study will provide AI researchers and the wider community with an overview of the current status of AI applications, and motivate researchers to harness AI's potential in the fight against COVID-19.
2008.10530_1338252_1	The addition of control measures introduces a mixing of order and disorder in the system's evolution which fall under a different mathematical class of models that can eventually lead to critical phenomena.
2008.10530_1338252_5	It utilizes optimization to learn the best-fit values of the model's parameters from past data in each region in the world, and it updates the predicted infections curves for any future restrictions that may be added or relaxed anywhere.
2009.02645_1344148_4	The ensembled model better solves this problem, making the model's accuracy reached 95.9% on subtask A, which just worse than human's by only 3% accuracy.
2009.03954_1345457_1	This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model's ability to model reading times is a linear function of its perplexity.
2009.04095_1345598_3	We conducted experiments by finetuning these models for cross domain and disparate data and penned an in-depth analysis of model's performances.
2009.06807_1348310_2	We also show GPT-3's strength in generating text that accurately emulates interactive, informational, and influential content that could be utilized for radicalizing individuals into violent far-right extremist ideologies and behaviors.
2009.07053_1348556_2	Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task.
2009.07262_1348765_1	In order to ensure that the science and technology of AI is developed in a humane manner, we must develop research publication norms that are informed by our growing understanding of AI's potential threats and use cases.
2009.11523_1353026_1	A language model's vocabulary$-$typically selected before training and permanently fixed later$-$affects its size and is part of what makes it resistant to such adaptation.
2010.01479_1357809_2	With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities.
2010.01479_1357809_5	We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations.
2010.01985_1358315_8	We propose the future use of such a complexity metric for use in computing an AI system's intelligence.
2010.03677_1360007_6	This happens due to the dynamic non-linearity of social-systems where superposition principle cannot be applied between all of its inputs and outputs of the system as the system's own attributes get altered upon each input.
2010.07487_1363817_5	This model rests on two key properties of the vulnerability of the user and the ability to anticipate the impact of the AI model's decisions.
2010.10874_1367204_5	Finally, we explore the model's potential in not only detecting, but also projecting, turn-completions.
2010.11855_1368185_6	We find that within a model family, as the number of parameters, training epochs, and data set size increase, so does a model's ability to generalize to negative n-gram data, indicating standard self-supervision generalizes too far.
2010.11982_1368312_3	We present the Turking Test, which examines a model's ability to follow natural language instructions of varying complexity.
2010.11982_1368312_6	Analyzing the model's error patterns reveals that the model tends to ignore explicit instructions and often generates outputs that cannot be construed as an attempt to solve the task.
2010.12821_1369151_4	Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages.
2011.08298_1381046_1	This paper describes Facebook AI's submission to WMT20 shared news translation task.
2011.10208_1382956_6	Quantitative evaluation shows that our approach outperforms a baseline, and we present qualitative evaluation of our system's capabilities.
2012.00955_1388833_3	We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.
2012.05453_1393331_7	In this paper, we investigate the language model's capabilities for causal association among events expressed in natural language text using sentence context combined with event information, and by leveraging masked event context with in-domain and out-of-domain data distribution.
2012.07805_1395683_2	We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data.
2012.08713_1396591_2	On the other hand, an interpretable model reveals the reason behind a model's prediction, ensures its transparency and allows us to plan the crime prevention steps accordingly.
2012.15035_1402913_6	We discuss our measure's applications in domains other than Go, especially in domains in which AI's decision making ability will likely surpass that of human experts.
2012.15562_1403440_4	Relying on matrix factorization, our methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix.
2012.15613_1403491_5	Our results show that languages that are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts.
2101.02032_1405774_7	To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI's indifferent behavior.
2101.05405_1409147_3	We propose two metrics to quantify user-level data leakage by measuring a model's ability to produce unique sentence fragments within training data.
2101.05509_1409251_7	Then, we involve adversarial training to improve the model's robustness.
2101.10002_1413744_1	We provide a new perspective on the problem and show that the processing time (delay) at the relay can be exploited to improve the system's security.
2102.00233_1416722_4	We use patent-based indicators for the period between 1974-2018 to analyse the evolution of AI's global technological space, to identify its technological core as well as changes to its overall relatedness and knowledge complexity.
2102.03919_1420408_4	Absent intervention, participants predict that the AI's classifications will match their own, but explanations generated by Bayesian Teaching improve their ability to predict the AI's judgements by moving them away from this prior belief.
2102.07333_1423822_2	In order to circumvent these limits, we use instead the language and conceptualisation of 'Good Data', as a more expansive term to elucidate the values, rights and interests at stake when it comes to AI's development and deployment, as well as that of other digital technologies.
2102.07333_1423822_5	Overall we view AI's 'goodness' as an explicly political (economy) question of power and one which is always related to the degree which AI is created and used to increase the wellbeing of society and especially to increase the power of the most marginalized and disenfranchised.
2102.07536_1424025_8	In fact, AI's corrupting force is as strong as humans'.
2102.07638_1424127_0	  In this paper from communication channel coding perspective we are able to present both a theoretical and practical discussion of AI's uncertainty, capacity and evolution for pattern classification based on the classical Rademacher complexity and Shannon entropy.
2102.07638_1424127_3	Secondly based on the Shannon mathematical theory on communication coding, we derive several sufficient and necessary conditions for an AI's error rate approaching zero in classifications problems.
2102.09343_1425832_1	The r&d in question is overtly and avowedly logicist in form, and since we are hardly the only ones who have established a firm foundation in the attempt to imbue AI's with their own ethical sensibility, the pursuit of our proposal by those in different methodological camps should, we believe, be considered as well.
2102.09690_1426179_3	To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A".
2102.09690_1426179_5	On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.
2102.09692_1426181_0	  People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong.
2103.07155_1437300_2	The AI-model approximates the residual error of the linear model and the explanations are formulated in terms of the change of the interpretable base model's parameters.
2103.08377_1438522_5	It is then numerically optimised to replicate the detailed model's prediction in two rounds; First, to selected species (OH,H,CO and CH4) profiles in perfectly stirred reactor (PSR) simulations and then re-optimised to the detailed model's prediction of the laminar flame speed.
2103.08377_1438522_7	The MLOCK algorithm systematically perturbs all three Arrhenius parameters for selected reactions and assesses the suitability of the new parameters through an objective error function which quantifies the error in the compact model's calculation of the optimisation target.
2103.10873_1441018_5	We showcase how, scaling our model's memory and computational requirement, we can significantly improve the onboard inference (top energy efficiency of 0.43 mJ/frame) with no compromise in the quality-of-result vs. a resource-unconstrained baseline (i.e., full-precision DNN).
2103.12407_1442552_4	With few-shot learning, the model's accuracy can be as high as 85 per cent.
2104.01940_1449358_11	Our study thus reinforces recent results on the difficulty of making claims about a deep model's world knowledge or linguistic competence based on performance on specific benchmark problems.
2104.03820_1451238_5	Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system's outputs would be established, and future opportunities for generative AI in application modernization.
2104.08017_1455435_6	In this analysis, we show that the quality of the code embedding model is the bottleneck for our model's performance, and discuss future directions of study in this area.
2104.08742_1456160_0	  We introduce a technique for improving document-level language models (LM) by leveraging "ancient history": text that is outside the LM's current context window.
2104.08742_1456160_2	The selected text spans are then copied directly into the LM's context window, replacing less predictive spans.
2104.08742_1456160_3	This method can improve perplexity of pretrained LMs with no updates to the LM's own parameters.
2104.09635_1457053_3	First, evaluating the systematicity of a language model's syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly?
2104.09635_1457053_4	Second, evaluating a model's likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs?
2104.10979_1458397_3	We integrate several covariate datasets to enhance the model's ability to capture the complex spatiotemporal dynamics of NO2.
2105.02986_1465545_4	We then derive a closed-form expression for the achievable rate and use it to evaluate the system's performance supported with numerical results.
2105.02986_1465545_5	We show that the RIS provided array gain improves the system's coverage, and provides nearly a 2-fold increase in the minimum rate and a 1.5-fold increase in the per-user throughput.
2105.06635_1469194_6	Our algorithm's most extensive feature is that it can quickly replan the path to overcome the emergency on a construction site.
2105.10614_1473173_1	Most research on algorithmic decision-making solely centers on the algorithm's performance, while recent work that explores human-machine collaboration has framed the decision-making problems as classification tasks.
2105.11447_1474006_5	We find similar results even when taking into account our uncertainty in a model's true performance during selection, as well as when varying the amount of computation and number of examples used for selection.
2105.13328_1475887_6	We find that our model's response scores on various human-generated prompts collected from the Facebook Empathetic Dialogues dataset outperform baseline counterparts.
2105.13328_1475887_7	Moreover, our model improves upon various history-based conversational AI models developed recently, as our model's performance over a sustained conversation of 3 or more interactions outperform similar conversational AI models.
2106.01465_1479229_0	  Is it possible to use natural language to intervene in a model's behavior and alter its prediction in a desired way?
2106.01465_1479229_2	Specifically, we propose a new language understanding task, Linguistic Ethical Interventions (LEI), where the goal is to amend a question-answering (QA) model's unethical behavior by communicating context-specific principles of ethics and equity to it.
2106.01465_1479229_3	To this end, we build upon recent methods for quantifying a system's social stereotypes, augmenting them with different kinds of ethical interventions and the desired model behavior under such interventions.
2106.04684_1482448_5	We find that medical experts exposed to explanations generated by Bayesian Teaching successfully predict the AI's diagnostic decisions and are more likely to certify the AI for cases when the AI is correct than when it is wrong, indicating appropriate trust.
2106.07380_1485144_6	I collected Reddit post data from an online data set and analyzed the model's performance when trained on a single subreddit and a collection of subreddits.
2106.08407_1486171_5	In this work, we seek to test the interaction model's applicability to intensely turbulent flames characterized by large $Ka$. To that end, we investigate the local flame structures: thermal, chemical structure, the effect of curvature, along the direction that is normal to the chosen isothermal surfaces.
2106.09106_1486870_2	In previous work we have found that humans tend to assume that the AI's decision process mirrors their own.
2106.09140_1486904_3	The present study addresses this question by evaluating human-AI interactions using Grice's four maxims; we demonstrate that humans do, indeed, apply these maxims to interactions with AI, even making explicit references to the AI's performance through a Gricean lens.
2106.14720_1492484_4	We wanted to see if we could use GPT-3's few-shot learning capabilities to more easily develop a solution that would have better performance than our prior work.
2107.03178_1497190_5	Yet all of these factors, we propose, are essential to providing the explanatory depth that people require to accept and trust the AI's decision-making.
2107.07015_1501027_0	  In decision support applications of AI, the AI algorithm's output is framed as a suggestion to a human user.
2107.09234_1503246_0	  Saliency methods -- techniques to identify the importance of input features on a model's output -- are a common step in understanding neural network behavior.
2107.12021_1506033_5	We further introduce a zero-shot setting with VSEPs to evaluate a model's ability to associate a novel word with a novel visual category.
2107.14052_1508064_4	Our best human judgment is needed to contain AI's harmful impact.
2108.00104_1508919_4	The "Structural Scaffold" idea guides the language model's representation via additional structure loss that separately predicts the incremental constituency parse.
2108.01174_1509989_8	To understand and validate an AI system's outcomes (such as classification, recommendations, predictions), that lead to developing trust in the AI system, it is necessary to involve explicit domain knowledge that humans understand and use.
2108.01928_1510743_7	The examples are provided through the model's context and thus require neither fine-tuning nor an additional forward pass.
2108.02393_1511208_3	It employs a model-free control policy framework and a guaranteed convergent adaptive learning architecture to solve the system's Bellman optimality equation.
2108.03502_1512317_3	Additionally, we employ hyperparameter tuning so that the model's output becomes less random and more tied to the original text.
2108.03502_1512317_4	We evaluate the resulting texts with a set of metrics, showing that our solution can surpass the state-of-the-art model's performance without additional changes in architecture or loss function.
2108.03578_1512393_5	From there, we propose a practical pipeline to evaluate language models in open-ended generation task, and research on how to improve the model's performance in all dimensions by leveraging different auxiliary training objectives.
2108.07732_1516547_9	Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions.
2108.07732_1516547_10	We find that natural language feedback from a human halves the error rate compared to the model's initial prediction.
2108.07951_1516766_6	Here we present our work to build an end-to-end machine learning system along with the discussion of some of practical challenges we faced related to extreme skewness in class distribution, concept drift, estimation of the uncertainty associated with the model's prediction and the overall scalability of the system.
2108.13654_1522469_1	It measures feature importance by averaging the model's output gradient interpolated along a straight-line path in the input data space.
2109.01517_1524338_2	Yet, AI's path has never been smooth, having essentially fallen apart twice in its lifetime ('winters' of AI), both after periods of popular success ('summers' of AI).
2109.01517_1524338_3	We provide a brief rundown of AI's evolution over the course of decades, highlighting its crucial moments and major turning points from inception to the present.
2109.03433_1526254_3	Spatial heterogeneity can drive the parameter estimations varying over space; failing to consider the spatial variations may limit the model's prediction performance.
2109.03570_1526391_4	Finally, we studied the impact of the model's vocabulary on the NER performances by offering an interesting vocabulary-centric analysis.
2109.04413_1527234_4	We use this dataset in two tasks designed to test i) a language model's ability to detect idiom usage, and ii) the effectiveness of a language model in generating representations of sentences containing idioms.
2109.05014_1527835_5	Inspired by GPT-3's power in knowledge retrieval and question answering, instead of using structured KBs as in previous work, we treat GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge.
2109.07185_1530006_4	However, we observed that gradual unfreezing had no significant impact on the model's accuracy compared to standard fine-tuning.
2109.07848_1530669_2	We probe the LM's expectations by generating from it: we use stochastic decoding to derive a set of sentence completions, and estimate the probability that the LM assigns to each interpretation based on the distribution of parses across completions.
2109.08029_1530850_4	In addition, we show that increasing the language model's size improves notably its performance, yielding results comparable to the state-of-the-art with our largest model, significantly outperforming current multimodal systems, even though augmented with external knowledge.
2109.08975_1531796_4	Nevertheless, simply finetuning the model on new data is infeasible since it may cause the model's performance on previously learned data to degrade over time, which is also known as the problem of catastrophic forgetting.
2109.14989_1537810_5	More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model's internal states.
2110.01509_1539652_3	Our empirical findings vindicate the overall framework and highlight the advantages of a modular design, in particular its ability to emulate established heuristics (such as hermeneutic cycles), to explore the model's uncertainty, to cope with the plurality of correct solutions (underdetermination), and to exploit higher-order evidence.
2110.04509_1542652_2	In this work, the effect of the inner air circulation on the system's heat and mass transfer performance and energy efficiency are studied theoretically and experimentally.
2110.05367_1543510_2	Forgetting information in the original training data may damage the model's downstream performance by a large margin.
2110.05448_1543591_6	By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1.
2110.06620_1544763_2	Our work proposes adaptive early exit strategy to maximize the efficiency of the pre-training process by relieving the model's subsequent layers of the need to process latent features by leveraging earlier layer representations.
2110.07143_1545286_3	Specifically, we extend the previous function-preserving on Transformer-based language model, and further improve it by proposing advanced knowledge for large model's initialization.
2110.07178_1545321_8	In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size.
2110.07280_1545423_9	Finally, we investigate what makes P-Adapters successful and conclude that a significant factor is access to the LLM's embeddings of the original natural language prompt, particularly the subject of the entity pair being queried.
2110.07774_1545917_8	This research is conducted in three steps: (a) data preprocessing; (b) prediction by a hybrid Convolutional Neural Network and Gated Recurrent Unit (CNN-GRU) along with a 3D-CNN model; (c) The third and last step is the comparison of the model's performance with the proposed model by comparing the experimental results.
2110.07774_1545917_10	Mont-Carlo dropouts are added to the network layers to enhance the model's prediction performance by a robust approach of switching off between different neurons.
2110.08294_1546437_2	We present coherence boosting, an inference procedure that increases a LM's focus on a long context.
2110.08527_1546670_4	We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model's language modeling ability, as well as its performance on downstream NLU tasks.
2110.08743_1546886_3	This learning paradigm provides direct access to the reference contexts and helps improve a model's generalization ability.
2110.09277_1547420_1	Manual testing and simulations are often used, but are only capable of exploring a subset of the system's reachable states.
2110.09277_1547420_2	Formal methods are mathematically-based techniques for the specification and development of software, which can provide proofs of properties and exhaustive checks over a system's state space.
2110.11597_1549740_2	Unfortunately, the ground truth is unavailable for the model's decision, so evaluation is limited to qualitative assessment.
2110.11597_1549740_4	We propose to improve XAI from the vantage point of the user's trust by exploring a black-box model's latent feature space.
2110.15146_1553289_4	To boost the learning efficiency and the online adaptability of the proposed DQN-routing, we further exploit the knowledge concerning the system's dynamics by using a deep value network (DVN) conceived with a feedback mechanism.
2111.00601_1554704_6	But how may it affect the system's security and trustworthiness?
2111.01726_1555829_5	This inaccurate self-assessment presents a barrier for XAI, since explanations of an AI's strategy may not be properly understood or implemented by human recipients.
2111.04437_1558540_3	However, this raises concern over AI's influence on artistic autonomy within the process of creativity.
2111.06908_1561011_6	Second, we implement local rule extraction to show that individuals are assigned to personality classes because of their unique financial behavior, and that there exists a positive link between the model's prediction confidence and the number of features that contributed to the prediction.
2111.07627_1561730_4	Our model's predictions are compared with corresponding observations, providing an additional venue to assess the validity of the existing representations of the lung's bronchial tree.
2111.08222_1562325_0	  Despite AI's superhuman performance in a variety of domains, humans are often unwilling to adopt AI systems.
2111.09509_1563612_4	For local structure - e.g., individual dependencies - model-generated text is substantially less novel than our baseline of human-generated text from each model's test set.
2111.09509_1563612_6	We also perform extensive manual analysis showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).
2111.14168_1568271_8	Given the recent growth in the importance of artificial intelligence (AI), we suggest accounting for AI's fundamental role in Industry 4.0 and understanding the fourth industrial revolution as an AI-powered natural collaboration between humans and machines.
2111.14833_1568936_6	More specifically, we show that three algorithms inspired by human-like social intelligence are, in principle, vulnerable to attacks that exploit weaknesses introduced by cooperative AI's algorithmic improvements and report experimental findings that illustrate how these vulnerabilities can be exploited in practice.
2112.02125_1571901_6	Our experiments demonstrate that while the approach has promise (the LLMs could collectively repair 100% of our synthetically generated and hand-crafted scenarios), a qualitative evaluation of the model's performance over a corpus of historical real-world examples highlights challenges in generating functionally correct code.
2112.02870_1572646_4	The buyer can acquire the model of interest from the ML market, and interested sellers spend local computations on their data to enhance that model's quality.
2112.06751_1576527_4	In particular, we study the impact of communicating different types of information to humans about the AI system's decision to defer.
2112.08787_1578563_4	Under this framework, we design (1) a region-aware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training.
2112.08802_1578578_0	  An extractive rationale explains a language model's (LM's) prediction on a given task instance by highlighting the text inputs that most influenced the prediction.
2112.08802_1578578_1	Ideally, rationale extraction should be faithful (reflective of LM's actual behavior) and plausible (convincing to humans), without compromising the LM's (i.e., task model's) task performance.
2112.11446_1581222_4	We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity.
2112.12100_1581876_5	To evaluate our prototype we compared experts' opinion with our system's recommendations, and resulted in 89%, 79%, and 93% F1-scores when recommending skills, learning topics, and educational materials respectively.
2112.12326_1582102_3	Furthermore, we derive closed-form expressions for the MTCD system's peak AoI, which are formulated as the optimization objective under the constraints of EH power, status update rate and stability conditions.
2112.12938_1582714_4	We formulate a notion of counterfactual memorization which characterizes how a model's predictions change if a particular document is omitted during training.
2201.00971_1586343_2	This is a vulnerability that can compromise the privacy of the model's training data.
2201.02730_1588102_1	Unfortunately, AI's capabilities and vulnerabilities make it a double-edged sword that may jeopardize the security of future networks.
2201.05159_1590531_5	Cloud-based interfaces provide the AI developer greater scope for controlling how the AI system is used, and for protecting against unauthorized modifications to the system's design.
2201.06317_1591689_4	Additional experiments show that the model's channel-separated visual feature extraction module can cope with objects of different shapes.
2201.06796_1592168_3	Exemplifying this approach, we present CoAuthor, a dataset designed for revealing GPT-3's capabilities in assisting creative and argumentative writing.
2201.06796_1592168_5	We demonstrate that CoAuthor can address questions about GPT-3's language, ideation, and collaboration capabilities, and reveal its contribution as a writing "collaborator" under various definitions of good collaboration.
2201.07368_1592740_3	Masking the adipose tissue during training and inference (while retaining the pleural line and Merlin's space artifacts such as A-lines and B-lines) improved the AI model's diagnostic accuracy.
2201.08239_1593611_4	The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias.
2201.11117_1596489_3	Providing the system's users with some understanding of AI operations can support predictability, but forcing AI to explain itself risks constraining AI capabilities to only those reconcilable with human cognition.
2201.12107_1597479_3	Nevertheless, they suffer from the factor of black box systems, where an assessment can be learned and generated easily, but without any geometrical indicator about the reasons of the system's decision.
2202.00828_1599652_4	When we have full access to the prompt model's gradients but full finetuning remains prohibitively expensive (e.g., T0 (Sanh et al., 2021)), we learn a set of soft prompt continuous vectors to iteratively update the prompt model.
2202.01142_1599966_3	Alongside an examination of the model's responses in answering open-ended questions, we devise a true/false quiz framework to characterize the performance of the language model.
2202.02879_1601703_6	This snapshot of AI's impact on SDG11 is inherently partial, yet useful to advance our understanding as we move towards more mature systems and research on the impact of AI systems for social good.
2202.03286_1602110_4	We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot.
2202.04565_1603389_2	However, treating physicians may not fully entrust the AI's recommended prescriptions due to known limitations or when the AI recommendation may go beyond physicians' current knowledge.
2202.05262_1604086_1	We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions.
2202.05302_1604126_6	Evaluating an AI's contractual trustworthiness involves predicting future model behavior using behavior certificates (BCs) that aggregate behavioral evidence from diverse sources including empirical out-of-distribution and out-of-task evaluation and theoretical proofs linking model architecture to behavior.
2202.05313_1604137_3	Assurance cases are an intensively discussed option today for specifying a sound and comprehensive safety argument to demonstrate a system's safety.
2202.05946_1604770_4	The model's parameters predict whether the statistical linkage will appear, and what market structures facilitate algorithmic collusion.
2202.05983_1604807_5	This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction.
2202.08901_1607725_3	We found that the presence of the affordance of interactively manipulating the AI system's prediction parameters affected users' dwell times, and eye-fixations on AOIs, but not mental workload.
2202.08979_1607803_2	In our paradigm we confronted human users with quantitative prediction tasks: asking them for a first response, before confronting them with an AI's recommendations (and explanation), and then asking the human user to provide an updated final response.
2202.08979_1607803_3	The difference between final and first responses constitutes the shift or sway in the human decision which we use as metric of the AI's recommendation impact on the human, representing the trust they place on the AI.
2202.10459_1609283_3	AI's existing approaches are soil management, crop diseases identification, weed identification, and management in collaboration with IoT devices.
2202.11812_1610636_1	While prior work studies the effects of model accuracy on humans, we endeavour here to investigate the complex dynamics of how both a model's predictive performance and bias may transfer to humans in a recommendation-aided decision task.
2202.11812_1610636_2	We consider the domain of ML-assisted hiring, where humans -- operating in a constrained selection setting -- can choose whether they wish to utilize a trained model's inferences to help select candidates from written biographies.
2203.00789_1613651_9	Thanks to this simulation system's extensibility and repeatability, we have consolidated this unified physical threat monitoring system and verified its effectiveness and user-friendliness.
2203.07911_1620773_3	By studying the embeddings of a large corpus of garble, extant language, and pseudowords using CharacterBERT, we identify an axis in the model's high-dimensional embedding space that separates these classes of $n$-grams.
2203.08410_1621272_3	We also optimize GPT-3's performance with known techniques such as contextual calibration and dynamic in-context example retrieval.
2203.10923_1623785_4	We also discuss issues that are unique to edge applications such as protecting a model's intellectual property and verifying its integrity.
2203.11147_1624009_7	The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
2203.12574_1625436_2	Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model.
2203.13108_1625970_4	SR searches for the optimal model structure while simultaneously optimizing the model's parameters without relying on an a-priori model structure.
2203.16773_1629635_4	Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability.
2204.00212_1630350_3	We demonstrate that consistent improvement is achieved by the LLM's bidirectionality, pretraining, in-domain finetuning and context augmentation.
2204.00419_1630557_2	The review identifies the major disciplinary and methodological perspectives on AI's impact on work, and the obstacles they face in making predictions.
2204.07459_1637597_2	Our system's key contributions are as follows: 1)
2204.08377_1638515_0	  AI's rapid growth has been felt acutely by scholarly venues, leading to growing pains within the peer review process.
2204.09082_1639220_3	While several studies have started to analyze adoption criteria from a technical perspective, research providing a human-centered perspective with a focus on AI's potential for becoming a coequal team member in the decision-making process remains limited.
2204.10019_1640157_4	The main message of this paper is that current frozen-model techniques such as prompt tuning are only the tip of the iceberg, and more powerful methods for leveraging frozen LMs can do just as well as fine tuning in challenging domains without sacrificing the underlying model's versatility.
2204.10943_1641081_5	The AI smart NIC frees up the system's compute resources to perform the more compute-intensive tensor operations and increases the overall node-to-node communication efficiency.
2204.12130_1642268_2	In this work, we introduce LM-Debugger, an interactive debugger tool for transformer-based LMs, which provides a fine-grained interpretation of the model's internal prediction process, as well as a powerful framework for intervening in LM behavior.
2204.12230_1642368_5	We measured user trust as the agreement between the model's and the radiologist's diagnosis as well as the radiologists' feedback on the model explanations.
2204.14211_1644349_3	The benchmark hence allows researchers to periodically track an LM's ability to retain previous knowledge and acquire updated/new knowledge at each point in time.
2205.01070_1645482_4	In this paper we argue that insights from using this perspective demonstrate that the majority of current initiatives taken by various actors in the field, focus on low-order interventions, such as short-term fixes, tweaking algorithms and updating parameters, absent from higher-order interventions, such as redefining the system's foundational structures that govern those parameters, or challenging the underlying purpose upon which those structures are built and developed in the first place(high-leverage).
2205.01232_1645644_0	  Despite AI's significant growth, its "black box" nature creates challenges in generating adequate trust.
2205.01232_1645644_5	Simply put, TRUST XAI models the statistical behavior of the AI's outputs in an AI-based system.
2205.01232_1645644_7	We use mutual information to rank these variables and pick only the most influential ones on the AI's outputs and call them "representatives" of the classes.
2205.01411_1645823_0	  Research on human-AI teams usually provides experts with a single label, which ignores the uncertainty in a model's recommendation.
2205.01467_1645879_6	By conducting an online experiment, we demonstrate that humans can use such contextual information to adjust the AI's decision, finally resulting in CTP.
2205.04810_1649222_4	As such, we introduce three methods to improve a neural model's performance in the low-resource setting, finding that limiting the model's self-attention is the most effective one, improving on downstream tasks such as NLI and POS tagging by up to 5% for the languages we test on: English, Hindi, and Turkish.
2205.06241_1650653_3	In this paper, we present two experiment (total N = 364) exploring the effects of CF explanations of AI system's predictions on lay people's causal beliefs about the real world.
2205.06241_1650653_4	In Experiment 1 we found that providing CF explanations of an AI system's predictions does indeed (unjustifiably) affect people's causal beliefs regarding factors/features the AI uses and that people are more likely to view them as causal factors in the real world.
2205.08084_1652496_6	We demonstrate the foundation model's versatility on a wide range of tasks such as retrieval, ranking, zero-shot recommendation, explanation generation, personalized content creation, and conversational recommendation, and manage to deploy it on both cloud servers and mobile devices.
2205.09738_1654150_9	We discuss the model's capability to yield better out-of-distribution generalisation in artificial agents, thus advancing toward Artificial General Intelligence.
2205.09944_1654356_4	On the system side, we further introduce the concept of User Satisfaction Ratio (USR) to evaluate the system's overall service ability of satisfying a variety of tasks with different SRZs.
2205.10893_1655305_4	Thor increases a language model's success rate on the PISA dataset from $39\%$ to $57\%$, while solving $8.2\%$ of problems neither language models nor automated theorem provers are able to solve on their own.
2205.11055_1655467_4	Notably, on an out-of-domain evaluation, TempLM reduces a finetuned BART model's unfaithfulness rate from 83% to 0%.
2205.12302_1656714_2	In this study, we present a collection of methods to analyze the hidden states of GPT-2 and use the model's navigation of garden path sentences as a case study.
2205.12302_1656714_5	Using these methods, we find that negating tokens have minimal impacts on the model's representations for unambiguous forms of sentences with ambiguity solely over what the object of a verb is, but have a more substantial impact of representations for unambiguous sentences whose ambiguity would stem from the voice of a verb.
2205.12302_1656714_6	Further, we find that analyzing the decoder model's hidden states reveals periods of ambiguity that might conclude in a garden path effect but happen not to, whereas surprisal analyses routinely miss this detail.
2205.12558_1656970_1	In this work, we study constrained sampling from such language models: generating text that satisfies user-defined constraints, while maintaining fluency and the model's performance in a downstream task.
2205.12600_1657012_2	In this work, we want to find evidence of the model's task-specific competence from pretraining and are specifically interested in locating a very small subset of pretraining data that directly supports the model in the task.
2205.12749_1657161_4	By comparing the acceptance rates of provided solutions, we can assess how the AI system performs compared to the domain expert, and whether the AI system's explanations (if provided) are human-understandable.
2205.13583_1657995_8	We utilized a cohort that includes 1066 biopsy slides from 400 subjects to validate the system's performance and achieved a histological severity classification accuracy of 86.70%, sensitivity of 84.50%, and specificity of 90.09%.
2206.00335_1660755_6	At the AI system level, we connect governance requirements to AI system life cycles to ensure governance throughout the system's life span.
2206.00474_1660894_6	Our work contributes better designs to investigate an AI model's fairness-and move closer towards responsible AI.
2206.04793_1665213_1	AI's extraordinary potential is being held back by challenges such as a lack of medical datasets for training AI models, adversarial attacks, and a lack of trust due to its black box working style.
2206.04796_1665216_1	However, to sustain this throughput in real-world applications, AIMC tiles must be supplied with data at very high bandwidth and low latency; this poses an unprecedented pressure on the on-chip communication infrastructure, which becomes the system's performance and efficiency bottleneck.
2206.04935_1665355_2	We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM's contextualized embeddings.
2206.06632_1667052_2	Explainable AI (xAI) methods can be useful in determining a neural model's relationship with data toward making it \textit{interpretable} by establishing a quantitative and tractable relationship between the input and the model's output.
2206.08264_1668684_0	  Open AI's language model, GPT-3, has shown great potential for many NLP tasks, with applications in many different domains.
2206.08264_1668684_1	In this work we carry out a first study on GPT-3's capability to communicate musical decisions through textual explanations when prompted with a textual representation of a piece of music.
2206.08932_1669352_3	We put Open AI's generative natural language model, GPT-3, to the test.
2206.08932_1669352_5	We assessed GPT-3's creativity on Guilford's Alternative Uses Test and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question.
2206.10744_1671164_2	We can disentangle the model's embeddings and identify components encoding both types of information with probing.
2206.13475_1673895_2	One promising strategy for assigning trust involves employing explanation techniques that elucidate the rationale behind a black-box model's predictions in a manner that humans can understand.
2206.14576_1674996_1	More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature.
2206.14576_1674996_2	We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning.
2207.00112_1676010_3	In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy.
2207.00112_1676010_6	Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance.
2207.00740_1676638_0	  The explanation to an AI model's prediction used to support decision making in cyber security, is of critical importance.
2207.00740_1676638_1	It is especially so when the model's incorrect prediction can lead to severe damages or even losses to lives and critical assets.
2207.00740_1676638_4	It identifies the features that lead to the model's borderline prediction, and those with positive individual contributions are extracted.
2207.02463_1678361_5	Optimized are only the pruning scores - parameters coupled with the model's weights that act as gates.
2207.02543_1678441_7	This mapping serves a means to obtain very accurate initial predictions of the system's response to new query points at negligible computational cost.
2207.03380_1679278_4	We find that (1) untrained versions of each model already explain significant amount of signal in the brain by capturing similarity in brain responses across identical words, with the untrained LSTM outperforming the transformerbased models, being less impacted by the effect of context; (2) that training NLP models improves brain scores in the same brain regions irrespective of the model's architecture; (3) that Perplexity (test loss) is not a good predictor of brain score; (4) that training data have a strong influence on the outcome and, notably, that off-the-shelf models may lack statistical power to detect brain activations.
2207.04188_1680086_4	Thus, if desirable, resampling techniques can improve the model's recall and f1-score with a slight decline in accuracy and precision.
2207.06220_1682118_8	Using crowd-sourcing, we observe that for the top 10% most likely citations to be tagged as unverifiable by our system, humans prefer our system's suggested alternatives compared to the originally cited reference 70% of the time.
2207.08057_1683955_4	Under perfect channel state information (CSI), the new framework minimizes the aggregated model's distortion and retains the local models' recoverability by optimizing the transmit beamformers of the devices, the receive beamformers of the BS, and the RIS configuration in an alternating manner.
2207.08333_1684231_4	We aim to quantitatively measure a model's performance in understanding context.
2207.08333_1684231_5	To verify the current existing VL model's capability, we sliced the original input image into pieces and randomly placed them, distorting the global context of the image.
2207.08333_1684231_6	Our paper discusses each VL model's level of interpretation on global context and addresses how the structural characteristics influenced the results.
2207.08401_1684299_3	After reading, Marvista generates an explainable human-AI summary that combines both AI's processing of the text, the user's reading behavior, and user-generated data in the reading process.
2207.09374_1685272_1	However, all common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decision.
2207.09374_1685272_4	Our approach, which we call Alterfactual Explanations, is based on showing an alternative reality where irrelevant features of an AI's input are altered.
2207.09374_1685272_5	By doing so, the user directly sees which characteristics of the input data can change arbitrarily without influencing the AI's decision.
2207.09374_1685272_7	We show that alterfactual explanations are suited to convey an understanding of different aspects of the AI's reasoning than established counterfactual explanation methods.
2207.10245_1686143_3	With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model's internal state relates to the bias in a downstream task (semantic textual similarity).
2207.13825_1689723_2	We find that AI's impact on phishing may be overestimated but could lead to more attacks going undetected.
2207.14160_1690058_1	xAI allows for improving models beyond the accuracy metric by, e.g., debugging the learned pattern and demystifying the AI's behavior.
2207.14160_1690058_10	The last level is the aggregated comprehensibility score, which encapsulates the ease of correctly interpreting the algorithm's output in one easy to compare value.
2207.14502_1690400_4	The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter.
2208.00780_1691492_4	Our explanations help users more accurately reject AI's wrong decisions than all other tested methods.
2208.01726_1692438_4	To overcome this problem, we incorporate a jammer to increase the system's secrecy by disrupting the eavesdropper through a broadcasted jamming signal.
2208.01726_1692438_5	Leveraging the well-adopted Gamma and Exponential distributions approximations, the system's secrecy level is quantified by deriving approximate and asymptotic expressions of the secrecy intercept probability (IP) metric in terms of the main network parameters.
2208.02957_1693669_2	Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other.
2208.04690_1695402_2	This paper carries out a descriptive study to broadly explore AI's implementations in healthcare delivery with a more holistic view of the usability of various Telemedical Innovations in enhancing Virtual Diagnostic Solutions (VDS).
2208.06345_1697057_6	We compared qualitative and quantitative characteristics of the model's dynamics before and after applying control and verified the results obtained using simulation.   
2208.07960_1698672_8	In addition, we found that users' perception of the AI's performance relative on their own also had a significant impact on whether their accuracy improved when given AI recommendations.
2208.09770_1700482_2	First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks.
2208.10264_1700976_1	A TE can also reveal consistent distortions in a language model's simulation of a specific human behavior.
2208.10544_1701256_1	Recently developed synthetic face image detectors boast "better-than-human" discriminative ability, especially those guided by human perceptual intelligence during the model's training process.
2208.10544_1701256_5	All subjects first examined samples without any AI support, followed by samples given (a) the AI's decision ("synthetic" or "authentic"), (b) class activation maps illustrating where the model deems salient for its decision, or (c) both the AI's decision and AI's saliency map.
2208.10544_1701256_7	Interesting observations from this experiment include: (1) models trained with human-guidance offer better support to human examination of face images when compared to models trained traditionally using cross-entropy loss, (2) binary decisions presented to humans offers better support than saliency maps, (3) understanding the AI's accuracy helps humans to increase trust in a given model and thus increase their overall accuracy.
2208.11243_1701955_3	However, most algorithms involve tricky parameter selection and complicated procedures that make the algorithm's decision rule obscure, so it is often difficult to explain and predict the errors and uncertainties of the resulting DTM.
2209.03661_1709376_1	The model's intrinsic gender bias shows an outdated and unequal view of women in our culture and encourages discrimination.
2209.05056_1710771_5	In doing this, we performed performance-based ablation studies, explored the impact of altering the YOLOv5 model's backbone, neck, and anchor structural elements, and annotated a unique endoscope dataset.
2209.06528_1712243_1	Explainable AI (XAI) techniques are defined to unveil the reasoning behind the system's predictions and decisions, and they become even more critical when dealing with sensitive and personal health data.
2209.07417_1713132_2	These xLPLMs include Meta-AI's wmt21-dense-24-wide-en-X (2021) and NLLB (2022).
2209.09815_1715530_4	We study the effects of varying the integer bit-width on the model's metric performance.
2209.11102_1716817_11	We additionally investigate the challenges posed by different conflict types and how synthetic data improves a model's understanding of conflict-specific semantics.
2209.11234_1716949_9	Finally, AI's advantages, disadvantages, and future in ME are discussed.
2209.14238_1719953_6	We demonstrated our algorithm's performance using simulated experiments on a simple 3-D example map and on a dense 3-D map of San Francisco.
2209.15093_1720808_3	We propose conceptual consistency to measure a LLM's understanding of relevant concepts.
2209.15093_1720808_5	To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge.
2210.01461_1722816_7	Such neural interfaces would offer access to deeper brain regions and better understanding for brain's functions and working mechanism, which improves BCIs operative stability and system's efficiency.
2210.03735_1725090_3	Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI's outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers.
2210.05487_1726842_3	We find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity.
2210.06376_1727731_3	However, this approach is restricted by the LM's vocabulary available for masked predictions, and its precision is subject to the context provided by the assertion.
2210.07228_1728583_3	We argue that the misalignment between the model's likelihood and the task-specific notion of utility is the key factor to understanding the effectiveness of decoding algorithms.
2210.08962_1730317_6	The BWM evaluation revealed that predictive nature was AI's most important criterion and role, while the mass-market potential was the less important criterion.
2210.09150_1730505_4	Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains.
2210.10332_1731687_5	The specific combination of a large-scale pre-trained LM that inherently but also diffusely encodes world knowledge with a clear-structured revision engine makes it possible to update the model's knowledge with little effort and the help of user interaction.
2210.12530_1733885_3	Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task -- such as variable names and descriptions -- to encourage downstream model outputs to be consistent with the LM's common-sense reasoning based on the metadata.
2210.15138_1736493_3	As a consequence, the model gains the capability of zero-shot transfer to segment novel categories; (ii) without loss of generality, we experiment on a broad range of self-supervised models that have been pre-trained with different schemes, e.g. visual-only models (MoCo v3, DINO), language-only models (BERT), visual-language model (CLIP), and show that, the proposed fusion approach is effective to any pair of visual and language models, even those pre-trained on a corpus of uni-modal data; (iii) we conduct thorough ablation studies to analyze the critical components in our proposed Fusioner, while evaluating on standard benchmarks, e.g. PASCAL-5i and COCO-20i , it surpasses existing state-of-the-art models by a large margin, despite only being trained on frozen visual and language features; (iv) to measure the model's robustness on learning visual-language correspondence, we further evaluate on synthetic dataset, named Mosaic-4, where images are constructed by mosaicking the samples from FSS-1000.
2210.15859_1737214_1	One such approach, the $k$NN-LM, interpolates any existing LM's predictions with the output of a $k$-nearest neighbors model and requires no additional training.
2211.04509_1743415_13	Our model's interpretability also allows human experts to participate in the decision-making by reviewing the interpretation and making informed interventions.
2211.04584_1743490_4	We point out that to amplify AI's impact on carbon-neutral transition of the electric energy systems, the AI algorithms originally developed for other applications should be tailored in three layers of technology, markets, and policy.
2211.05110_1744016_2	While many downstream applications provide the model with an informational context to aid its performance on the underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored.
2211.05110_1744016_3	As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge.
2211.06753_1745659_6	We found that the Seamful XAI design process helped users foresee AI harms, identify underlying reasons (seams), locate them in the AI's lifecycle, learn how to leverage seamful information to improve XAI and user agency.
2211.07047_1745953_4	The results indicate that the language model's sensitivity score aligns better with the professionals than the xgboost classifier on tf-idf embeddings, which suggests that xgboost uses some spurious features.
2211.07642_1746548_7	We also provide offline evaluations on several publicly available datasets as well as one online dataset to demonstrate the model's efficacy.
2211.08244_1747150_4	We used X-ray images as an example to show the model's efficiency.
2211.08380_1747286_6	In addition, OREO-LM provides reasoning paths as rationales to interpret the model's decision.
2211.08411_1747317_4	In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training.
2211.08412_1747318_5	A model's factual consistency is then measured according to its accuracy, i.e.\ the proportion of documents where it assigns a higher score to the factually consistent summary.
2211.08769_1747675_10	DupMAE is simple but empirically competitive: with a small decoding cost, it substantially contributes to the model's representation capability and transferability, where remarkable improvements are achieved on MS MARCO and BEIR benchmarks.
2211.09527_1748433_3	In particular, we investigate two types of attacks -- goal hijacking and prompt leaking -- and demonstrate that even low-aptitude, but sufficiently ill-intentioned agents, can easily exploit GPT-3's stochastic nature, creating long-tail risks.
2211.10938_1749844_4	Thus, the student model not only can learn the pre-trained model's predictive probabilities but also align the distributions between the pre-trained and student models.
2211.11158_1750064_7	Ultimately, GPT-3's sentential concepts can be aligned to images using CLIP, to form a bottleneck layer.
2211.11363_1750269_1	In spite of its high efficiency, continual pretraining suffers from catastrophic forgetting, which may harm the model's performance in downstream tasks.
2211.15006_1753912_5	Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the LLM's generated candidate consensus statements for agreement and quality.
2211.15006_1753912_8	Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%).
2212.01834_1758011_2	The critical elements develop from a contrast between Stability AI's Diffusion and OpenAI's Dall-E.
2212.05058_1761235_5	These interviews probe the model's moral imperatives to be helpful, truthful and harmless by design.
2212.06823_1763000_8	Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction.
2212.08204_1764381_7	We show that this improves the model's performance on processing long passages and results in better long-range text comprehension.
2212.10029_1766206_6	We propose an extension where we add a constraint satisfaction layer on top of the LM's raw predictions to apply commonsense constraints.
2212.10029_1766206_7	As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM's pictures of everyday things can be significantly reduced.
2212.10071_1766248_6	Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample.
2212.10693_1766870_2	However, building Artificial Intelligence (AI) based software with limited or no insight into the system's inner workings poses significant new challenges to RE.
2212.10696_1766873_2	We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally figure in a model's inferences in question answering.
2212.10696_1766873_3	We then test this notion by observing a model's behavior on answering questions about a story after performing two novel semantic interventions: deletion intervention and negation intervention.
2212.11261_1767438_2	A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed.
2212.11311_1767488_6	Though production applications of our model are limited by ethical considerations, the model's competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.
2212.14402_1770579_6	While we find no benefit in fine-tuning over GPT-3.5's zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5's zero-shot performance.
2212.14402_1770579_8	GPT-3.5's ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance.
2212.14447_1770624_5	Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user).
2301.01768_1772832_5	Prompting ChatGPT with 630 political statements from two leading voting advice applications and the nation-agnostic political compass test in three pre-registered experiments, we uncover ChatGPT's pro-environmental, left-libertarian ideology.
2301.03391_1774455_11	The proposed framework explains the algorithm's process with the proper texts, graphics and tables.
2301.05578_1776642_5	In addition, we urge designers to design against potential harms that may be caused by a generative model's hazardous output, misuse, or potential for human displacement.
2301.05809_1776873_1	However, prior studies calibrated human trust only based on AI confidence indicating AI's correctness likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.
2301.06676_1777740_1	It is critical to have confidence in AI's trustworthiness.
2301.07597_1778661_6	Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs.
2301.08653_1779717_7	We find that ChatGPT's bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches.
2301.12004_1783068_8	This paper also investigates how the number of examples in the prompt and the type of example selection used affect the model's performance.
2301.12243_1783307_5	Our findings revealed that practitioners use the guidebook not only for addressing AI's design challenges, but also for education, cross-functional communication, and for developing internal resources.
2301.12564_1783628_5	GPT-3's judgments are broadly similar to human judgments and generally align with proposed constraints in the literature but, in some cases, GPT-3's judgments and human judgments diverge from the literature and from each other.
2301.12597_1783661_7	We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.
2301.12726_1783790_2	We propose model specialization, to specialize the model's ability towards a target task.
2301.13382_1784446_5	The resulting exploratory data analysis showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression.
2301.13382_1784446_6	To extend the model's testable range, the research deletes and appends random rows such that recall alone cannot explain emergent numeracy.
2301.13848_1784912_2	First, we find instruction tuning, and not model size, is the key to the LLM's zero-shot summarization capability.
2301.13852_1784916_5	The goal is to analyze model's decisions and determine if any specific patterns or characteristics can be identified.
2301.13852_1784916_11	Using explainability, we observe that ChatGPT's writing is polite, without specific details, using fancy and atypical vocabulary, impersonal, and typically it does not express feelings.
2301.13867_1784931_9	Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student.
2302.00763_1785697_6	We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.
2302.01339_1786273_6	Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an "actual human philosopher".
2302.01854_1786788_0	  Optimization of human-AI teams hinges on the AI's ability to tailor its interaction to individual human teammates.
2302.02060_1786994_2	In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\texttt{[MASK]}$ tokens.
2302.03037_1787971_4	In this work, we present an explainable artificial intelligence (XAI)-based framework, LiteVR, for cybersickness detection, explaining the model's outcome and reducing the feature dimensions and overall computational costs.
2302.03037_1787971_9	Furthermore, based on the XAI-based feature ranking and dimensionality reduction, we significantly reduce the model's size by up to 4.3x, training time by up to 5.6x, and its inference time by up to 3.8x, with higher cybersickness detection accuracy and low regression error (i.e., on Fast Motion Scale (FMS)).
2302.03258_1788192_3	By leveraging FDT, we are able to extract information encoded in a large dataset produced by Earth System Models, which includes 8250 years of internal climate fluctuations, to estimate the climate system's response to forcings.
2302.03494_1788428_3	However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study.
2302.05284_1790218_0	  As Artificial Intelligence (AI) continues to advance rapidly, it becomes increasingly important to consider AI's ethical and societal implications.
2302.05319_1790253_4	The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code.
2302.05319_1790253_6	SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights.
2302.06600_1791534_3	Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters ($\sim0.01$% of model parameters) responsible for ($>95$%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model.
2302.06706_1791640_4	Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate.
2302.06761_1791695_2	To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts.
2302.07080_1792014_2	This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model's responses.
2302.07248_1792182_5	Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer.
2302.07257_1792191_7	In the future, LLM's medical knowledge can be also used to improve the performance of vision-based medical-image CAD models.
2302.08081_1793015_6	To fill this gap, we conducted an evaluation of ChatGPT's performance on four widely used benchmark datasets, encompassing diverse summaries from Reddit posts, news articles, dialogue meetings, and stories.
2302.08081_1793015_7	Our experiments reveal that ChatGPT's performance is comparable to traditional fine-tuning methods in terms of Rouge scores.
2302.09068_1794002_4	ChatGPT's outputs on such problems generally tended to be unpredictable: even as it made irrational decisions (or employed an incorrect reasoning process) for some simpler decision-making problems, it was able to draw correct conclusions for more complex bet structures.
2302.09185_1794119_6	Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures.
2302.09643_1794577_2	Inspired by ChatGPT's abysmal failure to answer the question, I have recently come back to this problem and now have a more satisfactory answer, thanks in no small part to what I learned form a page of Wolfram's Math World, which I located by a Google search.
2302.10198_1795132_2	However, the quantitative analysis of ChatGPT's understanding ability has been given little attention.
2302.10724_1795658_3	In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection.
2302.10916_1795850_3	Specifically, we explore ChatGPT's ability to provide code, explain basic concepts, and create knowledge related to SPC practice, learning, and research.
2302.11107_1796041_1	Integrated Gradients (IG) is a popular XAI algorithm that attributes relevance scores to input features commensurate with their contribution to the model's output.
2302.11520_1796454_4	The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output.
2302.11520_1796454_7	Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models.
2302.11703_1796637_2	Based on user research, they need to contextualize the model's behavior and potential failures within their product-specific data instances and user scenarios.
2302.12019_1796953_4	We compare qualitative and quantitative characteristics of the model's dynamics before and after applying control and verify the obtained results by numerical simulation.
2302.12813_1797747_6	LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses.
2302.13793_1798727_0	  Generative pre-trained language models (GPLMs) like ChatGPT encode in the model's parameters knowledge the models observe during the pre-training phase.
2302.13793_1798727_5	In this paper we study the differences in answer correctness generated by ChatGPT when leveraging the model's knowledge alone vs. in combination with the prompt knowledge.
2302.13795_1798729_6	Our comprehensive meta-analysis of ChatGPT's current perception after 2.5 months since its release can contribute to shaping the public debate and informing its future development.
2302.13814_1798748_1	We found that ChatGPT's performance changes dramatically based on the requirement to show its work, failing 20% of the time when it provides work compared with 84% when it does not.
2302.13814_1798748_3	We also have released the dataset of ChatGPT's responses to the MWPs to support further work on the characterization of LLM performance and present baseline machine learning models to predict if ChatGPT can correctly answer an MWP.
2302.13814_1798748_4	We have released a dataset comprised of ChatGPT's responses to support further research in this area.
2302.14543_1799477_5	The proposed algorithm's performance is evaluated using numerical simulation and then compared to the well-known artificial potential field (APF) algorithm for collision avoidance.
2303.00293_1800090_5	These insights are valuable for understanding its limitations and guiding future research in addressing these challenges to enhance GPT-3.5's overall performance and generalization abilities.
2303.01248_1801045_8	Our experiments reveal ChatGPT's ability to assess human personalities, and the average results demonstrate that it can achieve more consistent and fairer assessments in spite of lower robustness against prompt biases compared with InstructGPT.
2303.01258_1801055_9	The best performing model (domain-adapted RoBERTa) achieved a five-class accuracy of 77.4%, which was better than the physician's performance (66%), the best vision model's performance (48.1), and was similar to the multimodal model's performance (77.2).
2303.01903_1801700_7	The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer.
2303.03480_1803277_1	Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM's commonsense reasoning capabilities for making sequential navigational decisions.
2303.03953_1803750_5	Even when applied on Slovenian language as an under-resourced language, ChatGPT's performance is no worse than when applied to English.
2303.03981_1803778_3	We identified concerns regarding anchoring bias and a misunderstanding of the AI's capabilities.
2303.04731_1804528_4	Finally, we survey to assess doctors' and patients' trust in XAI explanations of the model's decisions on thyroid nodule images.
2303.06628_1806425_1	Nevertheless, during the continual training of the Contrastive Language-Image Pre-training (CLIP) model, we observe that the model's zero-shot transfer ability significantly degrades due to catastrophic forgetting.
2303.07142_1806939_5	We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance.
2303.07142_1806939_7	Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate "reasoning" in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.
2303.07610_1807407_2	To further explore ChatGPT's potential in this regard, a study is conducted to assess its ability to rank content.
2303.07610_1807407_5	The results on the test set show that ChatGPT's ranking preferences are consistent with human to a certain extent.
2303.07610_1807407_6	This preliminary experimental finding implies that ChatGPT's zero-shot ranking capability could be used to reduce annotation pressure in a number of ranking tasks.
2303.07940_1807737_3	The early detection of the change (drift) is crucial for updating the model's knowledge, which is challenging especially in scenarios where the ground truth associated to the stream data is not readily available.
2303.07940_1807737_4	Among many other techniques, the estimation of the model's confidence has been timidly suggested in a few studies as a criterion for detecting drifts in unsupervised settings.
2303.07940_1807737_5	The goal of this manuscript is to confirm and expose solidly the connection between the model's confidence in its output and the presence of a concept drift, showcasing it experimentally and advocating for a major consideration of uncertainty estimation in comparative studies to be reported in the future.
2303.08774_1808571_5	This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
2303.09461_1809258_5	At the same time, the questions in our exam are structurally similar to those of other exams, solved homework problems, and teaching materials that can be found online and might have been part of ChatGPT's training data.
2303.09601_1809398_6	Our system's success in generating DIsorder-Specific Multi-Objective Policies (DISMOP) and interpretable policy dynamics demonstrates the potential of DRL in providing personalized and efficient therapeutic recommendations.
2303.09743_1809540_3	We employed AI lifecycle comicboarding, an adapted version of the comicboarding method, to elicit stakeholder feedback and design ideas across various components of an AI system's design.
2303.09743_1809540_5	Our participants shared concerns and design suggestions around the AI system's overall objective, specific model design choices, dataset selection, and use in deployment.
2303.09743_1809540_6	Our findings demonstrate that stakeholders, even without AI knowledge, can provide specific and critical feedback on an AI system's design and deployment, if empowered to do so.
2303.10338_1810135_1	However, AI model's partnership with human radiologist remains an unexplored challenge due to the lack of health information standards, contextual and workflow differences, and data labeling variations.
2303.11436_1811233_4	In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, SuperGLUE, MATH and HANS.
2303.11436_1811233_7	Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities.
2303.11508_1811305_7	By enabling HMI during an AI uses inference, we will introduce the AI-in-the-loop concept that combines AI's and humans' strengths.
2303.11717_1811514_8	After introducing the fundamental techniques, this work focuses on the technological development of various AIGC tasks based on their output type, including text, images, videos, 3D content, etc., which depicts the full potential of ChatGPT's future.
2303.12641_1812438_5	Concretely, we apply the methods of RRR, CDEP and ClArC for model correction, and (4) (re-)evaluate the model's performance and remaining sensitivity towards the artifact.
2303.12712_1812509_7	Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT.
2303.12712_1812509_8	Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.
2303.12767_1812564_1	Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF).
2303.13217_1813014_8	Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.
2303.13524_1813321_3	We randomly exposed 100 participants to each variant and found that the group of participants unaware of ChatGPT's text synthetization was more inclined to believe the responses were misinformation.
2303.13547_1813344_0	  This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability.
2303.13547_1813344_3	Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive.
2303.13712_1813509_4	We first consider fixed human decision functions which map observable features and the algorithm's recommendations to final decisions.
2303.13712_1813509_7	We then consider a decision maker who is aware of the algorithm's manipulation and responds strategically.
2303.13780_1813577_3	In this paper, we aim to further mine ChatGPT's translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose an optimal temperature setting and two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP).
2303.13780_1813577_4	We show that: 1) The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; 2) Emphasizing the task information can further improve ChatGPT's performance, particularly in complex MT tasks; 3) Introducing domain information can elicit ChatGPT's generalization ability and improve its performance in the specific domain; 4) ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community.
2303.14070_1813867_4	The fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's ability to understand patient needs and provide informed advice.
2303.14425_1814222_0	  The model's ability to understand synonymous expression is crucial in many kinds of downstream tasks.
2303.14920_1814717_2	We compare this model's performance with a DeBERTa model pre-trained on clinical texts from our institutional EHR (MeDeBERTa) and an XGBoost model.
2303.15056_1814853_3	Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks.
2303.15430_1815227_7	Our approach, TextMI, significantly reduces model complexity, adds interpretability to the model's decision, and can be applied for a diverse set of tasks while achieving superior (multimodal sarcasm detection) or near SOTA (multimodal sentiment analysis and multimodal humor detection) performance.
2303.15473_1815270_3	In this experiment, we explore CoHA with three increasingly complex versions of a simple system, using Open AI's ChatGPT service.
2303.15473_1815270_4	The quality of ChatGPT's responses were systematically assessed to determine the feasibility of CoHA given the current state of LLM technology.
2303.15621_1815418_5	In this paper, we particularly explore ChatGPT's ability to evaluate factual inconsistency under a zero-shot setting by examining it on both coarse-grained and fine-grained evaluation tasks including binary entailment inference, summary ranking, and consistency rating.
2303.15621_1815418_7	However, a closer inspection of ChatGPT's output reveals certain limitations including its preference for more lexically similar candidates, false reasoning, and inadequate understanding of instructions.
2303.15714_1815511_7	We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.
2303.16341_1816138_2	To strengthen model's understanding into such fine-grained details, we propose a simple yet effective video-language modeling framework, S-ViLM, by exploiting the intrinsic structures of these two modalities.
2303.16421_1816218_6	We conduct a series of experiments on 11 datasets to evaluate ChatGPT's commonsense abilities, including answering commonsense questions, identifying necessary knowledge, generating knowledge descriptions, and using knowledge descriptions to answer questions again.
2303.16618_1816415_6	Our results suggest that the degree to which professional translations in our domain are context-specific can be preserved to a better extent by a contextual machine translation model than a non-contextual model, which is also reflected in the contextual model's superior reference-based scores.
2303.16626_1816423_1	The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues.
2303.16626_1816423_2	Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.
2303.16972_1816769_5	Queer in AI's work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.
2303.17071_1816868_8	In a new finding, we also show that GPT-4's performance (70%) on an open-ended version of the MedQA question-answering (QA) dataset (Jin et al. 2021, USMLE) is well above the passing level (60%), with DERA showing similar performance.
2303.17557_1817354_3	In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples.
2303.17650_1817447_2	Although, many anecdotal examples across the internet have evaluated ChatGPT's strength and weakness, only a few systematic research studies exist.
2303.17807_1817604_9	The model's accuracy was lower for questions requiring TKM-specialized knowledge.
2303.17807_1817604_11	A positive correlation was observed between the consistency and accuracy of GPT-4's responses.
2303.18116_1817913_1	This includes interaction with ChatGPT in natural language and using mathematical formalism, which, under careful supervision by a human-expert, led to producing a working code in MATLAB, Python and R for sampling from a given copula model, evaluation of the model's density, performing maximum likelihood estimation, optimizing the code for parallel computing for CPUs as well as for GPUs, and visualization of the computed results.
2304.00740_1818786_3	We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system.
2304.01083_1819129_6	More crucially, those symbolic concepts can be used to explain the exact reasons accountable for the LLM's prediction errors.
2304.01746_1819792_3	Our evaluation involves assessing ChatGPT's performance on five official test sets in three different languages, along with three document-level GEC test sets in English.
2304.01752_1819798_3	To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models.
2304.01852_1819898_4	This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.
2304.02554_1820600_1	In this study, we explored ChatGPT's ability to perform human-like summarization evaluation using four human evaluation methods on five datasets.
2304.03245_1821291_3	Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system's translations are better.
2304.03262_1821308_1	For example, by simply adding CoT instruction ``Let's think step-by-step'' to each input query of MultiArith dataset, GPT-3's accuracy can be improved from 17.7\% to 78.7\%.
2304.03262_1821308_7	Our experiments report new baseline results of ChatGPT on a variety of reasoning tasks and shed novel insights into LLM's profiling, instruction memorization, and pretraining dataset leakage.
2304.03271_1821317_6	In this paper, we provide a principled methodology to estimate the water footprint of AI, and also discuss the unique spatial-temporal diversities of AI's runtime water efficiency.
2304.03893_1821939_2	The paper proposes easy-to-customize input prompts for ChatGPT that meet common requirements in practical applications, such as easy integration with robot execution systems and applicability to various environments while minimizing the impact of ChatGPT's token limit.
2304.03893_1821939_4	Experiments confirmed that the proposed prompts enable ChatGPT to act according to requirements in various environments, and users can adjust ChatGPT's output with natural language feedback for safe and robust operation.
2304.04007_1822053_6	We investigated various segmentation algorithms for sky detection and found that the Otsu algorithm reported the highest classification rate and computational efficiency, despite the algorithm's simplicity and ease of implementation.
2304.04193_1822239_2	This paper first presents a thorough evaluation of ChatGPT's performance on extractive summarization and compares it with traditional fine-tuning methods on various benchmark datasets.
2304.04193_1822239_6	These observations highlight potential directions for enhancing ChatGPT's capabilities in faithful summarization using two-stage approaches.
2304.04370_1822416_6	We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop.
2304.05077_1823123_0	  We demonstrate that if consciousness is relevant for the temporal evolution of a system's states--that is, if it is dynamically relevant--then AI systems cannot be conscious.
2304.05351_1823397_2	In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets.
2304.05351_1823397_4	Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar.
2304.05351_1823397_6	This research provides insights into ChatGPT's capabilities and serves as a foundation for future work aimed at improving financial market analysis and prediction by leveraging social media sentiment and historical stock data.
2304.05406_1823452_2	We then explore the model's responses using a multi-document context (ten distilled documents).
2304.05454_1823500_2	In this work, we investigate ChatGPT's ability on zero-shot temporal relation extraction.
2304.05454_1823500_4	Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts.
2304.06122_1824168_3	This work assesses ChatGPT's aptitude in answering quizzes, homework, exam, and laboratory questions in an introductory-level computer engineering course.
2304.06815_1824861_7	Our goal here is to investigate this question, using the code summarization task and evaluate whether automatically augmenting an LLM's prompt with semantic facts explicitly, actually helps.   
2304.07333_1825379_6	In addition, ChatGPT's Big Five personality traits were tested using the OCEAN test and its personality type was queried using the Myers-Briggs Type Indicator (MBTI) test.
2304.07842_1825888_7	In addition, we have developed a robust and modular system with optional submodules that can enhance the system's performance by incorporating real-time surveillance data, metadata related to exercises (such as sectors or runways), or even introducing a deliberate read-back error to train ATCo trainees to identify them.
2304.07880_1825926_6	By evaluating on datasets originally conceived in the target language as well as translated ones, we study the contributions of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language, and 2) enriching the model's knowledge about a domain or culture.
2304.08191_1826237_6	Therefore, in this paper, we explore ChatGPT's capability for DL program repair by asking three research questions.
2304.08191_1826237_8	(2) How can ChatGPT's repair performance be improved by prompting?
2304.08191_1826237_11	Also, we propose various prompt templates to facilitate the performance and summarize the advantages and disadvantages of ChatGPT's abilities such as detecting bad code smell, code refactoring, and detecting API misuse/deprecation.
2304.08767_1826813_4	MLMD features a plug and play usage (i.e., no need to retrain the victim model) for adversarial defense and it is agnostic to classification tasks, victim model's architectures, and to-be-defended attack methods.
2304.08979_1827025_2	ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability.
2304.08979_1827025_3	In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains.
2304.08979_1827025_4	We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions.
2304.08979_1827025_5	We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability in an imperceptible way.
2304.08979_1827025_7	We believe that our study provides valuable insights into ChatGPT's reliability and underscores the need for strengthening the reliability and security of large language models (LLMs).
2304.09138_1827184_5	We also conduct a comprehensive investigation on ChatGPT/GPT-4's reasoning ability by introducing varying levels of inference difficulty.
2304.09248_1827294_7	Additionally, data augmentation and various sampling techniques are implemented to enhance the model's performance.
2304.09542_1827588_5	Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge.
2304.09582_1827628_2	Our findings indicate that while ChatGPT's performance on emotional dialogue understanding may still lag behind that of supervised models, it exhibits promising results in generating emotional responses.
2304.09826_1827872_6	We examine current strategies for improving AI fairness, assess their limitations in terms of real-world deployment, and explore potential paths forward to ensure we reap AI's benefits without causing society's collapse.
2304.09858_1827904_4	A literature review highlights recent advancements in BMI typing, sublimation of conscious processes, and generative AI's potential in thought typing based on text prompts.
2304.10009_1828055_4	This study deals with data-driven modeling based on historical data to make predictions without prior knowledge of the system's parameter, demonstrating several merits in identifying patterns that can be difficult for human analysts to detect, high accuracy when trained on large datasets, and the potential to improve over time with new data.
2304.10149_1828195_5	Specifically, we design a set of prompts and evaluate ChatGPT's performance on five recommendation scenarios.
2304.10436_1828482_6	In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety evaluator by prompting.
2304.10513_1828559_3	To better understand the model's particular weaknesses in providing truthful answers, we embark an in-depth exploration of open-domain question answering.
2304.10513_1828559_4	Specifically, we undertake a detailed examination of ChatGPT's failures, categorized into: comprehension, factuality, specificity, and inference.
2304.10513_1828559_7	Our findings suggest that augmenting the model with granular external knowledge and cues for knowledge recall can enhance the model's factuality in answering questions.
2304.10578_1828624_2	Despite enormous effort devoted to understanding the impact of AI on labor and the economy and AI's recent successes in accelerating scientific discovery and progress, we lack a systematic understanding of how AI advances may benefit scientific research across disciplines and fields.
2304.10578_1828624_6	Lastly, we examine demographic disparities in AI's benefits across scientific disciplines and find that disciplines with a higher proportion of women or Black scientists tend to be associated with less benefit, suggesting that AI's growing impact on research may further exacerbate existing inequalities in science.
2304.10592_1828638_8	To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability.
2304.10755_1828801_3	The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals.
2304.10970_1829016_3	Rather than targeting state-of-the-art performance, our objective is to highlight GPT-4's potential to assist research on a challenging technical problem through a simple prompting scheme that requires relatively limited domain expertise\footnote{Code available at \href{https://github.com/mingkai-zheng/GENIUS}{https://github.com/mingkai-zheng/GENIUS}.}.
2304.11085_1829131_4	Therefore, this study investigates the consistency of ChatGPT's zero-shot capabilities for text annotation and classification, focusing on different model parameters, prompt variations, and repetitions of identical inputs.
2304.11085_1829131_5	Based on the real-world classification task of differentiating website texts into news and not news, results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability.
2304.11123_1829169_2	Given AI's massive potential, as well as the fierce geopolitical tensions between China and the U.S., several recent policies have been put in place to discourage AI scientists from migrating to, or collaborating with, the other nation.
2304.11158_1829204_1	In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII).
2304.11158_1829204_3	We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs.
2304.11384_1829430_6	Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.
2304.11633_1829679_2	Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts.
2304.11633_1829679_3	Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation.
2304.11686_1829732_6	Our insight is that ChatGPT's performance can be substantially enhanced when ChatGPT is guided to focus on the subtle code difference.
2304.11938_1829984_3	In this paper, we present an empirical study of ChatGPT's potential as a fully automated programming assistant, focusing on the tasks of code generation, program repair, and code summariziation.
2304.11938_1829984_4	The study investigates ChatGPT's performance on common programming problems and compares it with state-of-the-art approaches on two benchmarks.
2304.11938_1829984_9	Our findings contribute interesting insights to the development of LLMs for programming assistance, notably by demonstrating the importance of prompt engineering, and providing a better understanding of ChatGPT's practical applications for software engineering.
2304.11957_1830003_3	Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent.
2304.12087_1830133_7	The import risk model's precision increases for countries that are more connected within the WAN, and recovers a geographic distance-dependence that suggests a pull- rather than a push- dynamic of the distribution process.
2304.12198_1830244_2	This study further shows a significant improvement in the model's accuracy when answering FE exam questions through noninvasive prompt modifications, substantiating the utility of prompt modification as a viable approach to enhance AI performance in educational contexts.
2304.12203_1830249_1	This article investigates the performance of LLMs on exams and their implications for assessment, focusing on ChatGPT's abilities and limitations.
2304.12203_1830249_2	We propose guidelines for creating LLM-resistant exams, including content moderation, deliberate inaccuracies, real-world scenarios beyond the model's knowledge base, effective distractor options, evaluating soft skills, and incorporating non-textual information.
2304.12241_1830287_1	With advancements happening daily, many people are increasingly worried about AI's impact on their lives.
2304.12562_1830608_2	We empirically evaluate ChatGPT's performance on requirements information retrieval (IR) tasks to derive insights into designing or developing more effective requirements retrieval methods or tools based on generative LLMs.
2304.12562_1830608_4	Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision).
2304.12898_1830944_4	ChatGPT's self-assessment makes serious implications about our understanding of the Turing test and the nature of consciousness.
2304.13009_1831055_2	One notable model is Visual ChatGPT, which combines ChatGPT's LLM capabilities with visual computation to enable effective image analysis.
2304.13009_1831055_3	The model's ability to process images based on textual inputs can revolutionize diverse fields.
2304.13009_1831055_8	By exploring the applicability of these techniques within publicly available datasets of satellite images, we demonstrate the current model's limitations in dealing with remote sensing images, highlighting its challenges and future prospects.
2304.13734_1831780_1	In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements.
2304.14177_1832223_1	However, there has been limited research evaluating ChatGPT's keyphrase generation ability, which involves identifying informative phrases that accurately reflect a document's content.
2304.14177_1832223_2	This study seeks to address this gap by comparing ChatGPT's keyphrase generation performance with state-of-the-art models, while also testing its potential as a solution for two significant challenges in the field: domain adaptation and keyphrase generation from long documents.
2304.14827_1832873_2	Given ChatGPT's promising performance across various tasks, we proceed to carry out thorough evaluations on the whole test sets of 11 datasets, including temporal and causal relations, PDTB2.0-based, and dialogue-based discourse relations.
2304.14993_1833039_4	This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science.
2305.00948_1834004_5	We speculate that OpenAI o1's unique advantage over other models may result from the model's chain-of-thought mechanism, which mimics the structure of human reasoning used in complex cognitive tasks, such as linguistic analysis.
2305.01639_1834695_1	However, LLM's responses may leak the sensitive private information contained in in-context exemplars.
2305.01639_1834695_3	The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets.
2305.01651_1834707_6	Yet, prepending entity definitions in an LM's context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection.
2305.02160_1835216_2	To encourage fairness and transparency, there exists an urgent demand for reliable explanations that allow users to consistently understand the model's behavior.
2305.02160_1835216_4	Specifically, we propose a post-hoc interpretability method for extracting predictive high-level features (concepts) from the pretrained model's hidden layer activations.
2305.02182_1835238_2	In this study, we aim to conduct an empirical analysis of ChatGPT's recommendation ability from an Information Retrieval (IR) perspective, including point-wise, pair-wise, and list-wise ranking.
2305.02230_1835286_4	Our comprehensive evaluation provides valuable insights into ChatGPT's impact from both student and instructor perspectives.
2305.02230_1835286_6	Overall, we believe that this study significantly clarifies and advances our understanding of ChatGPT's capabilities and potential impact on computer science education.
2305.02231_1835287_0	  Trustworthy Artificial Intelligence (AI) is based on seven technical requirements sustained over three main pillars that should be met throughout the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3) robust, both from a technical and a social perspective.
2305.02231_1835287_1	However, attaining truly trustworthy AI concerns a wider vision that comprises the trustworthiness of all processes and actors that are part of the system's life cycle, and considers previous aspects from different lenses.
2305.02412_1835468_1	Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning).
2305.02564_1835620_9	DupMAE is simple but empirically competitive: it substantially improves the pre-trained model's representation capability and transferability, where superior retrieval performances can be achieved on popular benchmarks, like MS MARCO and BEIR.
2305.02748_1835804_2	Indeed, Stuart Russell proposed shifting AI's focus away from simply ``intelligence'' towards intelligence ``provably aligned with human values''.
2305.02748_1835804_3	This challenge -- the value alignment problem -- with others including an AI's learning of human values, aggregating individual values to groups, and designing computational mechanisms to reason over values, has energised a sustained research effort.
2305.02823_1835879_3	The LLM's macroeconomic expectations exhibit under-reaction commonly found in consensus SPF forecasts.
2305.02823_1835879_5	Finally, using a sample of articles outside of the LLM's training period I find that the correlation with existing survey measures persists -- indicating these results do not reflect memorization but generalization on the part of the LLM.
2305.03212_1836268_2	In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases.
2305.03429_1836485_1	Our study aims to generate text that emulates Lovecraft's unique writing style and themes, while also examining the effectiveness of prompt engineering techniques in guiding the model's output.
2305.03429_1836485_6	In addition to presenting the GPT model's capabilities, this paper provides a comprehensive description of its underlying architecture and offers a comparative analysis with related work that simulates other notable authors and philosophers, such as Dennett.
2305.03731_1836787_3	Furthermore, we investigate the impact of different instruction strategies on ChatGPT's performance and observe that the fundamental patterns of a capacity limit persist.
2305.04160_1837216_6	X-LLM's training consists of three stages: (1) Converting Multimodal Information:
2305.04207_1837263_6	In this work, we perform the first empirical study to evaluate ChatGPT's capability of unit test generation.
2305.04388_1837444_1	It is tempting to interpret these CoT explanations as the LLM's process for solving a task.
2305.04388_1837444_3	However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction.
2305.05377_1838433_10	This progress suggests that focusing on the latest model's shortcomings could lead to a highly performant AI capable of mastering the most demanding professional certifications.
2305.05668_1838724_2	Experimental data was collected and synthetically augmented to 1000 data points, enhancing the model's precision.
2305.05668_1838724_4	The model's performance was benchmarked against a Simple Artificial Neural Network (ANN) model by assessing mean squared error (MSE) and R-squared (R2) values for both training and validation datasets.
2305.06453_1839509_1	By adopting LLM as the reasoning core, we introduce Autonomous GIS as an AI-powered geographic information system (GIS) that leverages the LLM's general abilities in natural language understanding, reasoning, and coding for addressing spatial problems with automatic spatial data collection, analysis, and visualization.
2305.07375_1840431_2	In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities.
2305.07375_1840431_4	Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF.
2305.07378_1840434_0	  Ensuring that large language models (LMs) are fair, robust and useful requires an understanding of how different modifications to their inputs impact the model's behaviour.
2305.07507_1840563_6	On the other hand, downstream performance is mainly driven by the model's size and prior legal knowledge which can be estimated by upstream and probing performance.
2305.07667_1840723_11	They also show how, as LLM's are exposed to human narratives, they induce not only human knowledge but also human biases.
2305.07722_1840778_2	We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process.
2305.07759_1840815_6	This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency.   
2305.07868_1840924_5	This paper underscores the need for further research into AI's role in enriching our understanding of the past and bridging historical knowledge gaps.
2305.07969_1841025_4	Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text.
2305.08005_1841061_2	We present an empirical study examining the effectiveness of ChatGPT's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in LLMs even when protections are in place.
2305.08360_1841416_3	To answer these questions, we conducted experiments using the CodeXGlue dataset to evaluate ChatGPT's capabilities for two code generation tasks, including text-to-code and code-to-code generation.
2305.08377_1841433_4	To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generalization ability and the task-specific evidence provided by the full labeled dataset.
2305.08391_1841447_1	In this paper, we aim to systematically inspect ChatGPT's performance in two discourse analysis tasks: topic segmentation and discourse parsing, focusing on its deep semantic understanding of linear and hierarchical discourse structures underlying dialogue.
2305.08714_1841770_6	This observation underscores the fact that even the highly performance GPT-4 model encounters significant stability issues when dealing with diverse Japanese prompt templates, rendering the consistency of the model's output results questionable.
2305.09031_1842087_6	Comparing interfold sub-model disagreement against human interobserver values is an efficient way to approximate a model's epistemic uncertainty - its lack of knowledge due to insufficient relevant training data - a key functionality for adopting these applications in clinical practice.
2305.09434_1842490_5	Within it, we extract the static context of the GUI page and the dynamic context of the iterative testing process, design prompts for inputting this information to LLM, and develop a neural matching network to decode the LLM's output into actionable steps to execute the app.
2305.09781_1842837_1	The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence.
2305.10142_1843198_5	We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model's negotiation strategy iteratively.
2305.10163_1843219_9	The ChatGPT's performance surged to 70.04 and GPT-4 achieved the highest score of 82.59.
2305.10434_1843490_4	We also propose a fine-tuning strategy that adapts large vision-language models like CLIP by modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images in the document.
2305.10510_1843566_2	Specifically, we examine ChatGPT's accuracy in translating between English and languages that exclusively use gender-neutral pronouns.
2305.10519_1843575_8	Our results reveal that the knowledge in LLMs with the same backbone architecture adheres to the scaling law, while tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.
2305.10568_1843624_5	While GPT-3's performance is not perfect, it is better than that of humans -- likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012).
2305.10998_1844054_6	Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of LLM's predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentation from web.
2305.11490_1844546_5	Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation.
2305.11627_1844683_5	Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality.
2305.11652_1844708_6	AI refers to the computer-based system's ability to perform tasks with intelligence typically associated with human decision-making, they can learn from past experiences and solve problems.
2305.11882_1844938_4	Our study aimed to evaluate ChatGPT's ability to accurately identify topics in student comments based on an existing framework consisting of positive and negative comments.
2305.12001_1845057_4	Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart.
2305.12295_1845351_5	We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT.
2305.12564_1845620_2	Specifically, people perceive male gender identity (1) following demonstrations of ChatGPT's core abilities (e.g., providing information or summarizing text), (2) in the absence of such demonstrations, and (3) across different methods of eliciting perceived gender (using various scales and asking to name ChatGPT).
2305.12564_1845620_3	Moreover, we find that this seemingly default perception of ChatGPT as male can reverse when ChatGPT's feminine-coded abilities are highlighted (e.g., providing emotional support for a user).
2305.12747_1845803_4	To this end, we propose using membership inference to audit whether a code snippet used is in the PLG model's training data.
2305.12763_1845819_2	We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory.
2305.12763_1845819_3	We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature.
2305.12799_1845855_4	Inspired by LLM's powerful capability in task guidance, we propose a new paradigm of annotated data expansion named as ChatGenImage.
2305.12865_1845921_9	The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models.
2305.12870_1845926_2	Nevertheless, they overlooked the possibility of incorporating any reciprocal "feedback"--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively.
2305.12907_1845963_4	Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks.
2305.13673_1846729_4	We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm.   
2305.13724_1846780_2	We focus on ChatGPT's reading comprehension and introduce it to EDSS, a task of synthesizing speech that can empathize with the interlocutor's emotion.
2305.13733_1846789_6	Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance.
2305.13927_1846983_1	This paper focuses on unique AI applications in the military, emphasizes strategic imperatives for success, and aims to rekindle excitement about AI's role in national security.
2305.14279_1847335_2	We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps).
2305.14318_1847374_7	The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence.
2305.14386_1847442_1	Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning.
2305.14386_1847442_2	Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3.
2305.14450_1847506_5	Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.
2305.14497_1847553_3	In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable.
2305.14571_1847627_2	This fixed vocabulary limits the model's robustness to spelling errors and its capacity to adapt to new domains.
2305.14625_1847681_1	These methods, best exemplified by the KNN-LM, interpolate the LM's predicted distribution of the next word with a distribution formed from the most relevant retrievals for a given prefix.
2305.14628_1847684_8	Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output.
2305.14688_1847744_4	We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\% of the original ChatGPT's capability.
2305.14718_1847774_4	Subsequently, by using LM's value estimate, A-LoL only trains on positive advantage (leftover) data points, making it resilient to noise.
2305.14752_1847808_8	Our results demonstrate ESBMC-AI's capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy.
2305.14763_1847819_0	  The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine "intelligence".
2305.14775_1847831_3	Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal.
2305.14795_1847851_2	Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs.
2305.14975_1848031_4	For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.
2305.14976_1848032_0	  ChatGPT's emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks.
2305.14976_1848032_1	However, the model's efficacy across diverse linguistic contexts remains largely uncharted territory.
2305.14976_1848032_2	This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT's capabilities on Arabic languages and dialectal varieties.
2305.14976_1848032_4	To our knowledge, this marks the first extensive performance analysis of ChatGPT's deployment in Arabic NLP.
2305.14976_1848032_6	We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling the relative shortcomings of both models in handling Arabic dialects compared to MSA.
2305.15076_1848132_1	However, the knowledge in static language models falls out of date, limiting the model's effective "shelf life."
2305.15076_1848132_6	We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step.
2305.15334_1848390_6	To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs.
2305.15501_1848557_4	We further find that this derived model's conditionals can even occasionally outperform the original MLM's conditionals.
2305.15717_1848773_1	This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model.
2305.15717_1848773_7	We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality.
2305.16347_1849403_2	Current research addressing this limitation has largely focused on enhancing the prompts before output generation or improving the model's performance up front.
2305.16347_1849403_6	A novelty of our evolutionary algorithm is that the pre-trained generative model gives us implicit mutation operations, leveraging the model's stochastic generative capability to automate the creation of Pareto-optimized images more faithful to user preferences.
2305.16765_1849821_3	We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model's behavior in predictable ways.
2305.16765_1849821_5	On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM's word embeddings.
2305.16837_1849893_6	We juxtapose and analyze ChatGPT's answers with the respective state of the art outputs (where available) and/or human expert ground truth.
2305.16867_1849923_1	We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour.
2305.16867_1849923_6	We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a "social chain-of-thought" (SCoT) strategy.
2305.16867_1849923_8	These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.
2305.17378_1850434_1	In this study, we empirically investigate improving an LM's generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output.
2305.17705_1850761_5	We demonstrate the proposed LMD system's utility through real-world trials in a university campus with a single robotic courier.
2305.18081_1851137_0	  This study explores the robustness of university assessments against the use of Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and evaluates the ability of academic staff to detect its use when supported by the Turnitin Artificial Intelligence (AI) detection tool.
2305.18086_1851142_4	Objective: To evaluate the existing reviews and literature related to ChatGPT's applications and its potential impact on different fields by conducting a systematic review of reviews and bibliometric analysis of primary literature.
2305.18156_1851212_5	We explore the impact of task instructions and examples on GPT-3's output, focusing on controlling aspects such as minimal edits, fluency edits, and learner levels.
2305.18354_1851410_3	Our failure analysis reveals that GPT-4's capacity to identify objects and reason about them is significantly influenced by the sequential nature of the text that represents an object within a text encoding of a task.
2305.18426_1851482_2	To achieve this objective, we introduced the utilization of Explainable Artificial Intelligence (XAI) techniques for the first time, which allowed us to analyze the data and gain valuable insights into the system's behavior.
2305.18486_1851542_2	In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations.
2305.18486_1851542_8	By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.
2305.18569_1851625_3	We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare.
2305.18569_1851625_5	We also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts.
2305.19103_1852159_4	We identify two main findings: 1) Both models strongly align with human representations in non-sensorimotor domains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5; 2) GPT-4's gains are associated with its additional visual learning, which also appears to benefit related dimensions like haptics and imageability.
2305.19280_1852336_4	We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.
2305.19707_1852763_6	A data-centric framework enhanced the system's performance by improving passage retrieval and question reformulation.
2305.19713_1852769_3	We study two types of attack strategies: 1) replacing certain words in an LLM's output with their synonyms given the context; 2) automatically searching for an instructional prompt to alter the writing style of the generation.
2306.00176_1853324_1	Because these challenges will persist even as LLM technology improves, we argue that any automated annotation process using an LLM must validate the LLM's performance against labels generated by humans.
2306.00227_1853375_1	ChatGPT's generative capabilities and versatility across technical and creative domains led to its widespread adoption, marking a departure from more limited deployments of previous AI systems.
2306.00227_1853375_2	While society grapples with the emerging cultural impacts of this new societal-scale technology, critiques of ChatGPT's impact within machine learning research communities have coalesced around its performance or other conventional safety evaluations relating to bias, toxicity, and "hallucination."
2306.00227_1853375_5	By analyzing ChatGPT's social impact through a social-centered framework, we challenge individualistic approaches in AI development and contribute to ongoing debates around the ethical and responsible deployment of AI systems.
2306.00813_1853961_5	UniDiff effectively learns aligned semantics and mitigates the issue of semantic collapse during fine-tuning on small datasets by leveraging RSC on visual features from CLIP and diffusion models, without altering the pre-trained model's basic architecture.
2306.01117_1854265_2	In this paper, we study whether a model's reasoning given a specific input differs based on the first names provided.
2306.01169_1854317_9	We anticipate that our work will inform NLP researchers about the extent to which ChatGPT's capabilities for summarizing long documents overlap with practitioners' needs.
2306.01333_1854481_4	This toolkit operates on statistical theories, analyzing a large dataset to reveal a model's fairness.
2306.01497_1854645_4	We evaluate our model's performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French.
2306.01590_1854738_4	In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions.
2306.01795_1854943_3	It also considers the philosophical implications of AI and creativity, questioning whether consciousness can be researched in machines and AI's potential interests and decision-making capabilities.
2306.01795_1854943_4	Overall, we aim to stimulate a reflection on AI's use and ethical implications in creative contexts.
2306.02231_1855379_5	In addition, compared with PPO, APA offers a more stable form of control over the deviation from the model's initial policy, ensuring that the model improves its performance without collapsing to deterministic output.
2306.02272_1855420_1	To address this challenge, we introduce the outlier-aware weight quantization (OWQ) method, which aims to minimize LLM's footprint through low-precision representation.
2306.02388_1855536_3	Empirical results show that our approach consistently improves the model's performance on downstream tasks that require commonsense reasoning.
2306.02707_1855855_2	A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs.
2306.02773_1855921_1	We trace AI's evolution from early statistical methods to sophisticated machine learning, highlighting XAI's role in popular financial applications.
2306.02858_1856006_5	To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality.
2306.02914_1856062_1	While previous studies have focused on GPT's ability to generate code for visualizations, this study goes beyond code generation to evaluate GPT's abilities in various visualization tasks, such as data interpretation, visualization design, visual data exploration, and insight communication.
2306.02914_1856062_2	The evaluation utilized GPT-3.5 and GPT-4 to complete assignments of CS171, and included a quantitative assessment based on the established course rubrics, a qualitative analysis informed by the feedback of three experienced graders, and an exploratory study of GPT's capabilities in completing border visualization tasks.
2306.02914_1856062_4	The study also demonstrates GPT's potential in completing various visualization tasks, such as data cleanup, interaction with visualizations, and insight communication.
2306.03024_1856172_2	The Pok\'emon universe serves as an ideal testing ground for auditing ChatGPT's reasoning capabilities due to its closed world assumption.
2306.03024_1856172_3	After bringing ChatGPT's background knowledge (on the Pok\'emon universe) to light, we test its reasoning process when using these concepts in battle scenarios.
2306.03024_1856172_5	Our ultimate goal is to assess ChatGPT's ability to generalize, combine features, and to acquire and reason over newly introduced knowledge from human feedback.
2306.03090_1856238_6	For example, 82% of the model's suggestions point to places in the transcript where the teacher is already implementing that suggestion.
2306.03289_1856437_6	We then explored methods to modify these problems to adapt them to ChatGPT's capabilities to reduce potential misuse by students.
2306.03423_1856571_4	In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack.
2306.03423_1856571_9	With this machine-labeled data, we train a prompt classifier to predict whether ChatGPT will refuse a given question, without seeing ChatGPT's response.
2306.03763_1856911_2	In this research, we introduce a novel framework that leverages ChatGPT's graph inference capabilities to enhance Graph Neural Networks (GNN).
2306.03763_1856911_5	Furthermore, the portfolios constructed based on our model's outputs demonstrate higher annualized cumulative returns, alongside reduced volatility and maximum drawdown.
2306.04031_1857179_5	In turn, the model's choices can change the guide's state.
2306.04308_1857456_2	With that in mind, this research conducted an extensive investigation into seven LLM's, aiming to assess the temporal stability and inter-rater agreement on their responses on personality instruments in two time points.
2306.04504_1857652_5	This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain.
2306.04563_1857711_6	We put ChatGPT's sense of humor to the test.
2306.04563_1857711_7	In a series of exploratory experiments around jokes, i.e., generation, explanation, and detection, we seek to understand ChatGPT's capability to grasp and reproduce human humor.
2306.04605_1857753_1	Drawing on pertinent articles from 2016 to 2023, this systematic literature evaluation reveals generative AI's potential applications, benefits, and constraints in this area.
2306.04605_1857753_5	Ultimately, generative AI's practical application can significantly improve software product management activities, leading to more efficient use of resources, better product outcomes, and improved end-user experiences.
2306.04653_1857801_2	A case study in Aveiro City demonstrates the system's effectiveness in generating actionable insights that enhance security, energy efficiency, and sustainability, while highlighting the potential of AI and IoT-driven solutions for smart city development.
2306.05087_1858235_7	Our results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation ability and 88.28% of GPT-4's in terms of F1-score on our test dataset.
2306.05179_1858327_2	M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels.
2306.06123_1859271_2	The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery.
2306.06199_1859347_5	The model responses are inconsistent across prompts and settings, highlighting GPT-3's unreliability.
2306.06264_1859412_4	More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence.
2306.06331_1859479_0	  This study offers a complete analysis of ChatGPT's mathematics abilities in responding to multiple-choice questions for the Vietnamese National High School Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
2306.06331_1859479_2	The outcomes demonstrate that ChatGPT's performance varies depending on the difficulty level and subject.
2306.07207_1860355_5	In addition, we implement a two-phase training approach for Valley: the first phase focuses solely on training the projection module to facilitate the LLM's capacity to understand visual input, and the second phase jointly trains the projection module and the LLM to improve their instruction following ability.
2306.07377_1860525_0	  In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa, Google's PaLM) have become the dominant approach for building AI systems to analyze and generate language online.
2306.07384_1860532_5	We also observe inverse scaling for most-type quantifier understanding, which is contrary to human psycho-linguistic experiments and previous work, where the model's understanding of most-type quantifier gets worse as the model size increases.
2306.07402_1860550_4	We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product.
2306.07402_1860550_6	We find that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.
2306.07799_1860947_1	In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal).
2306.07799_1860947_2	Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts.
2306.08000_1861148_3	We hypothesize that domain pre-trained models such as CXR-BERT, BlueBERT, and ClinicalBERT offer the potential to improve the performance of CLIP-like models with specific domain knowledge by replacing BERT weights at the cost of breaking the original model's alignment.
2306.08173_1861321_0	  The surge in multimodal AI's success has sparked concerns over data privacy in vision-and-language tasks.
2306.08190_1861338_6	An advantage of this approach is that the model provided evidence for its decision, which adds transparency to the model's decision-making and offers a chance for users to verify the validity of the evidence provided.
2306.09200_1862348_7	Finally, we propose a full evaluation framework for evaluating language model's chess ability.
2306.09390_1862538_1	We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models.
2306.09390_1862538_2	Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT.
2306.09390_1862538_4	Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.
2306.09442_1862590_6	Our framework consists of three steps: 1) Exploring the model's range of behaviors in the desired context; 2) Establishing a definition and measurement for undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure to develop diverse adversarial prompts.
2306.09445_1862593_1	Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society.
2306.09541_1862689_2	As a result, developers face a new challenge: validating AI's suggestions.
2306.09914_1863062_2	Our empirical findings indicate a significant variance in ChatGPT-4's performance across these disciplines.
2306.10052_1863200_2	The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases.
2306.10052_1863200_3	These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights.
2306.10073_1863221_8	While we identified certain limitations in GPT-4's handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses.
2306.10193_1863341_2	LM responses are typically sampled from the model's predicted distribution over the large, combinatorial output space of natural language.
2306.10512_1863660_4	This involves estimating the characteristics or value of each test item in the benchmark, and tailoring each model's evaluation instead of relying on a fixed test set.
2306.10512_1863660_5	This paradigm provides robust ability estimation, uncovering the latent traits underlying a model's observed scores.
2306.10645_1863793_3	We examine ChatGPT's ability to pursue multiple interconnected learning objectives, adapt the educational activity to users' characteristics, such as culture, age, and level of education, and its ability to use diverse educational strategies and conversational styles.
2306.10702_1863850_3	Considering the growing reliance on ChatGPT for language tasks, the importance of news recommendation in addressing social issues, and the trend of using language models in recommendations, this study conducts an initial investigation of ChatGPT's performance in news recommendations, focusing on three perspectives: personalized news recommendation, news provider fairness, and fake news detection.
2306.10702_1863850_7	We also surpass fixed evaluations by developing a webpage to monitor ChatGPT's performance on weekly basis on the tasks and prompts we investigated.
2306.11296_1864444_1	This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging.
2306.11892_1865040_4	ChatGPT has shown to be a strong baseline in many NLP tasks, and we believe it has the potential to improve our model in the task of semantic matching and enhance our model's understanding of food-related concepts and relationships.
2306.11943_1865091_2	In contrast to previous research on probing models for linguistic features, we study pre-trained models in a setting that allows for objective and straightforward evaluation of a model's ability to learn semantics.
2306.11980_1865128_0	  Interactive user interfaces have increasingly explored AI's role in enhancing communication efficiency and productivity in collaborative tasks.
2306.12132_1865280_3	Our study shows that ChatGPT's evaluation aligns well with human evaluation, and we propose a ``best of three'' strategy to improve its output stability.
2306.12132_1865280_4	We also discuss the concept of trustworthiness in AI and its implications for non-experts using ChatGPT's unprocessed outputs.
2306.12198_1865346_6	Additionally, by inspecting the attention weights layer by layer, we uncover an unconventional finding that layer 10, rather than the model's final layer, is the optimal layer to unfreeze for the least parameter-intensive approach to fine-tune the model.
2306.13671_1866819_4	We examine the ethical challenges stemming from ChatGPT's deceptive human-like interactions and propose a roadmap for developing more transparent and trustworthy chatbots.
2306.13906_1867054_6	Further, we demonstrated how to analyze GPT-4's predictions to identify and mitigate deficiencies in annotation guidelines, and subsequently improve the performance of the model.
2306.14062_1867210_4	With the rise of Large Language Models (LLMs), NLP tasks have significantly improved because of the LLM's semantic understanding and scalability.
2306.14395_1867543_1	Our primary observation is that by optimizing those structural parameters (or designs) specifically to a target system's I/O characteristics (e.g., latency, bandwidth), we can offer a faster lookup compared to the ones that are not optimized.
2306.15895_1869043_3	Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance.
2306.15903_1869051_1	The AI's strength is closely related to its diversity of strategies, and this relationship can guide us to train AI with both strong and rich strategies.
2306.16282_1869430_1	Our analysis focuses on ChatGPT's capabilities in Mathematics Education, particularly in teaching basic Linear Algebra.
2306.16282_1869430_3	These occurrences raise concerns regarding the system's genuine understanding of mathematics, as it appears to rely more on visual patterns rather than true comprehension.
2306.16388_1869536_7	When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages.
2306.16639_1869787_2	Using decimal exercises and student data from a prior study of the learning game Decimal Point, with more than 5,000 open-ended self-explanation responses, we investigate ChatGPT's capability in (1) solving the in-game exercises, (2) determining the correctness of students' answers, and (3) providing meaningful feedback to incorrect answers.
2306.16639_1869787_5	We conclude with a discussion of ChatGPT's strengths and weaknesses and suggest several venues for extending its use cases in digital teaching and learning.
2306.17582_1870730_3	We explore ChatGPT's ability to use free-form dialog, parse XML tags, and to synthesize code, in addition to the use of task-specific prompting functions and closed-loop reasoning through dialogues.
2306.17842_1870990_1	SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary.
2307.00112_1871108_12	In order to improve ChatGPT's performance in the future, further research is needed to better understand how it can answer different types of questions.
2307.00457_1871453_4	GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation.
2307.01135_1872131_5	Users perceive ChatGPT's responses as having higher information quality compared to Google Search, despite displaying a similar level of trust in both tools.
2307.02483_1873479_3	Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist.
2307.02762_1873757_5	Specifically, we propose (1) the peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on the preferences of two answers.
2307.02762_1873757_8	Interestingly, PR can induce a relatively accurate self-ranking of models under the anonymous setting, where each model's name is unrevealed.
2307.02779_1873774_4	Experimental results demonstrate the system's remarkable ability to accurately comprehend user demands, efficiently execute AI models with minimal cost, and effectively create high-performance AI models at edge servers.
2307.03027_1874022_3	There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function.
2307.03196_1874191_4	Based on a preliminary survey on ChatGPT's quality in answering questions in Geography and GIScience, we demonstrate that this assumption might be fairly naive, and effective control in assessments and supervision is required.
2307.03718_1874713_2	Frontier AI models pose a distinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to robustly prevent a deployed model from being misused; and, it is difficult to stop a model's capabilities from proliferating broadly.
2307.03952_1874947_3	Concretely, we employ a variety of prompting strategies to explore ChatGPT's ability in recognizing personality from given text, especially the level-oriented prompting strategy we designed for guiding ChatGPT in analyzing given text at a specified level.
2307.03987_1874982_3	Specifically, we first identify the candidates of potential hallucination leveraging the model's logit output values, check their correctness through a validation procedure, mitigate the detected hallucinations, and then continue with the generation process.
2307.04274_1875269_7	Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.
2307.04408_1875403_5	Our approach involves presenting the model with examples of correct and incorrect translations and using a preference loss to guide the model's learning.
2307.04492_1875487_4	In this paper, we propose a neural network-based tool that instructors can use to determine the original effort (and LLM's contribution) put by students in writing source codes.
2307.04683_1875678_3	CORE-GPT's performance was evaluated on a dataset of 100 questions covering the top 20 scientific domains in CORE, resulting in 100 answers and links to 500 relevant articles.
2307.04699_1875694_1	International collaborations can unlock AI's ability to further sustainable development, and coordination of regulatory efforts can reduce obstacles to innovation and the spread of benefits.
2307.05082_1876077_1	The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
2307.05113_1876108_2	We introduces the DetectBench, a reading comprehension dataset designed to assess a model's ability to jointly ability in key information detection and multi-hop reasoning when facing complex and implicit information.
2307.05113_1876108_4	To enhance model's detective skills, we propose the Detective Thinking Framework.
2307.05337_1876332_5	The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key idea behind the solution.
2307.05360_1876355_2	In this paper, we carry out a comprehensive evaluation of ChatGPT's coding capabilities based on what is to date the largest catalog of coding challenges.
2307.05488_1876483_1	The objective of the studies is to assess ChatGPT's ability to comprehend theoretical concepts and differentiate between constructs.
2307.05488_1876483_8	These biases may impact the responses of constructs and should be considered when interpreting ChatGPT's conceptual capabilities.
2307.05494_1876489_1	While many approaches have been proposed to make AI more energy-efficient and environmentally friendly, environmental inequity -- the fact that AI's environmental footprint can be disproportionately higher in certain regions than in others -- has emerged, raising social-ecological justice concerns.
2307.05494_1876489_2	This paper takes a first step toward addressing AI's environmental inequity by balancing its regional negative environmental impact.
2307.05494_1876489_3	Concretely, we focus on the carbon and water footprints of AI model inference and propose equity-aware geographical load balancing (GLB) to explicitly address AI's environmental impacts on the most disadvantaged regions.
2307.05646_1876641_3	In this work, we propose a framework to improve an LLM's performance on CR-containing reviews by fine tuning on highly inferential tasks.
2307.05950_1876945_6	We further evaluate LLM's logging generalization capabilities using unseen data (LogBench-T) derived from code transformation techniques.
2307.06018_1877013_4	To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation.
2307.06081_1877076_4	This model's validity was subsequently tested through Partial Least Squares - Structural Equation Modeling (PLS-SEM), using data collected from 183 software professionals.
2307.06530_1877525_2	Our primary focus is to investigate the potential of using an LLM's in-context learning capabilities to enhance the performance of ASR systems, which currently face challenges such as ambient noise, speaker accents, and complex linguistic contexts.
2307.06530_1877525_4	Unfortunately, our initial experiments did not yield promising results, indicating the complexity of leveraging LLM's in-context learning for ASR applications.
2307.06569_1877564_2	Specifically, the model's predictions are treated as the truth assignment of a co-occurrence logic formula to compute the logic loss, which measures the consistency between the predictions and the logic constraints.
2307.06569_1877564_4	To further enhance the model's adaptability to novel action labels, we experiment with rules generated using GPT-3.5, which leads to a slight decrease in performance.
2307.06865_1877860_0	  The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output.
2307.06908_1877903_3	FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM's propensity to generate true facts from the corpus vs. similar but incorrect statements.
2307.07171_1878166_5	However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data.
2307.07312_1878307_2	In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read.
2307.07522_1878517_1	AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models.
2307.07522_1878517_7	These advances hold the promise to unleash AI's potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve.
2307.07696_1878691_4	It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks.
2307.07870_1878865_3	We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits.
2307.08041_1879036_2	Despite the limitations, we remain confident in its natural capacity to unify visual and textual representations, facilitating scalable multimodal training with LLM's original recipe.
2307.08191_1879186_5	This study provides a simple overview of GPT's capabilities in supporting quantum computing research while highlighting the limitations of the current GPT at the same time.
2307.08260_1879255_4	However, assessing the performance of ChatGPT's output poses a challenge, particularly in scenarios where queries lack clear objective criteria for correctness.
2307.08260_1879255_10	These findings provide a compact yet insightful glimpse into ChatGPT's capabilities and areas for improvement.
2307.08487_1879482_1	However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks.
2307.08576_1879571_7	Compare GPT's responses with expected results, assess its understanding of depressive symptoms, and performance differences under different conditions.
2307.08576_1879571_8	Results: GPT's performance in depression assessment was evaluated.
2307.08789_1879784_1	DALL.E, an advanced AI image generator, works alongside ChatGPT's language processing to transform text descriptions and image clues into realistic visual representations of the content.
2307.08941_1879936_4	By incorporating NTK into the compression process, MLP Fusion not only preserves the original model's output but also maintains its training dynamics.
2307.08985_1879980_2	However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions.
2307.08985_1879980_4	Through the iterative process, users can efficiently explore the model's capability, and clarify their intent.
2307.09009_1880004_6	This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting.
2307.09009_1880004_9	GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task.
2307.09009_1880004_11	We provide evidence that GPT-4's ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts.
2307.09018_1880013_3	HeLM encodes complex data modalities by learning an encoder that maps them into the LLM's token embedding space and for simple modalities like tabular data by serializing the data into text.
2307.09476_1880471_2	We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: "overthinking" and "false induction heads".
2307.09702_1880697_1	This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary.
2307.09753_1880748_4	Unmaking the ecosystem analyzes the values, structures, and incentives surrounding the model's production.
2307.09753_1880748_6	Unmaking the output analyzes the model's generative results, revealing its logics through prompting, reflection, and iteration.
2307.09885_1880880_2	Recent advances in Artificial Intelligence (AI) and the discipline of Natural Language Processing have prompted language test providers to explore AI's potential applicability within language testing, leading to transformative activity patterns surrounding language instruction and learning.
2307.09885_1880880_3	However, with concerns over AI's trustworthiness, it is imperative to understand the implications of integrating AI into language testing.
2307.10236_1881231_2	While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior.
2307.10250_1881245_0	  This study evaluates the GPT-4 Large Language Model's abductive reasoning in complex fields like medical diagnostics, criminology, and cosmology.
2307.11434_1882429_1	Amongst other quality indicators, it has a large degree of influence on the model's accuracy, generalisability, training times and parallelisability.
2307.11516_1882511_5	Indigo operates through an iterative feedback loop, harnessing the human expert's contextual knowledge and the AI's data-driven insights to craft and refine strategies towards a well-defined goal.
2307.11643_1882638_5	The results demonstrated its effectiveness in explaining the IE Mask R-CNN model's predictions.
2307.12169_1883164_1	We study the optimal parallelization strategy of LLMs and propose a novel datacenter network design tailored to LLM's unique communication pattern.
2307.12507_1883502_0	  In this paper, we study the problem of generating obstinate (over-stability) adversarial examples by word substitution in NLP, where input text is meaningfully changed but the model's prediction does not, even though it should.
2307.12596_1883591_7	Subsequently, we investigate ChatGPT's self-repairing ability and its interaction with static analysis tools to fix the errors uncovered in the previous step.
2307.12976_1883971_7	We evaluate prominent editing methods on RippleEdits, showing that current methods fail to introduce consistent changes in the model's knowledge.
2307.13705_1884700_3	Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
2307.13779_1884774_4	Even without the use of prompt engineering, it is shown that GPT's predictions align significantly with human-provided appraisals and emotional labels.
2307.13960_1884955_1	It directly uses the data snapshots obtained at varying flight conditions, and encodes the physical understanding of the nonlinear model's polynomial dependency on flight conditions to produce a polynomial-dependent LPV-ROM.
2307.14192_1885187_2	By exploring the upgrade path from GPT-1 to GPT-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating ChatGPT into our daily lives.
2307.14206_1885201_2	Using both general and subject specific prompts, the study assesses the accuracy, helpfulness, and reliability of ChatGPT's responses across different versions of the tool.
2307.14206_1885201_4	However, occasional inaccuracies highlight the need for users to remain critical of ChatGPT's responses.
2307.14662_1885657_6	A new framework for ergodic capacity analysis is also provided to acquire the proposed system's effective capacity.
2307.14692_1885687_3	We show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities.
2307.15343_1886338_4	Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs's problem-solving and information retrieval abilities.   
2307.15376_1886371_5	Human evaluators rated both the accuracy and fluency of translations, offering a comprehensive perspective on the language model's performance.
2307.15780_1886775_6	Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics.
2307.15983_1886978_2	Firstly, the text features remain fixed after being calculated and cannot be adjusted according to image features, which decreases the model's adaptability.
2307.15983_1886978_3	Secondly, the model's output solely depends on the similarity between the text and image features, leading to excessive reliance on LVLMs.
2307.15983_1886978_5	Specifically, one branch implements our proposed ConditionNet, which guides image features to form an adaptive textual cache that adjusts based on image features, achieving instance-wise inference and improving the model's adaptability.
2307.15983_1886978_7	The model's output is jointly determined by the two branches, thus overcoming the limitations of existing methods that rely solely on LVLMs.
2307.16139_1887134_2	This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information.
2307.16139_1887134_3	Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses.
2307.16139_1887134_4	This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score.
2307.16139_1887134_5	During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge.
2307.16139_1887134_6	We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses.
2307.16180_1887175_4	Specifically, extensive experiments will be conducted to explore: 1) the personality types of different LLMs, 2) the possibility of changing the personality types by prompt engineering, and 3) How does the training dataset affect the model's personality.
2307.16481_1887476_3	We propose an approach that allows the user to iteratively take into account multiple model's outputs as part of their sensemaking process.
2307.16806_1887801_3	We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
2307.16850_1887845_4	This comparative analysis highlights the importance of considering the language model's perspective, depth of knowledge, and currency when evaluating the usefulness of generated information in healthcare applications.
2307.16888_1887883_6	To demonstrate the threat, we propose a simple method to perform VPI by poisoning the model's instruction tuning data, which proves highly effective in steering the LLM.
2308.01284_1889176_1	Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator.
2308.01415_1889307_4	Extensive experiments have been conducted on this dataset to evaluate the model's performance by adopting an external GPT-4 as the judge.
2308.01552_1889444_4	The results highlight ChatGPT's competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.
2308.01825_1889717_2	We find that pre-training loss is a better indicator of the model's performance than the model's parameter count.
2308.01906_1889798_3	We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs.
2308.01906_1889798_4	To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs.
2308.02025_1889917_4	After building a typology of AI applications in manufacturing, we highlight the diverse possibilities for AI's implementation at different scales and application types.
2308.02025_1889917_5	We discuss the importance of considering AI's implications both for individual firms and for society at large, encompassing economic prosperity, equity, environmental health, and community safety and security.
2308.02025_1889917_6	The study finds that there is a predominantly optimistic outlook in prior literature regarding AI's impact on firms, but that there is substantial debate and contention about adverse effects and the nature of AI's societal implications.
2308.02045_1889937_5	The paper delves into challenges presented by such AI, from potential biases in analysis to ChatGPT's limited reasoning capabilities.
2308.02312_1890204_2	Despite this popularity, no comprehensive study has been conducted to evaluate the characteristics of ChatGPT's answers to programming questions.
2308.02624_1890516_5	However, an ensemble of those models exhibits substantial predictive power suggesting that competing models may capture different aspects of AI exposure that collectively account for AI's variable impact across occupations, regions, and time.
2308.02903_1890795_5	It enables our model to improve a system's capability of handling conversations with complex multilingual intent and slot values of distant languages.
2308.03161_1891053_0	  The rationale behind a deep learning model's output is often difficult to understand by humans.
2308.03279_1891171_7	With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average.
2308.03296_1891188_1	Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set?
2308.03301_1891193_2	This paper tested what archaeological literature appears to have been included in ChatGPT's training phase.
2308.03314_1891206_5	Instead of relying solely on GPT to identify vulnerabilities, which can lead to high false positives and is limited by GPT's pre-trained knowledge, we utilize GPT as a versatile code understanding tool.
2308.03762_1891654_1	However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason.
2308.03762_1891654_2	This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems.
2308.03866_1891758_4	Our hypothesis is that the level of uncertainty contained in the flow of attention is related to the quality of the model's response itself.
2308.03873_1891765_4	That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts.
2308.03929_1891821_4	We adopted a biological networks approach that enables the systematic interrogation of ChatGPT's linked entities.
2308.03992_1891884_6	By integrating cutting-edge Natural Language Processing techniques such as topic modelling and sentiment analysis, we offer an in-depth understanding of the system's impact on learner engagement, motivation, and inquiry-based learning.   
2308.04346_1892238_7	These findings emphasize the critical role of public perception in shaping AI's impact on society and the need to correct biases in AI systems.
2308.04788_1892680_1	Our proposed general knowledge transfer approach guides the LLM towards a similar and familiar API or code snippet it has encountered before, improving the model's generalization ability for unseen knowledge.
2308.04813_1892705_0	  With the continuous emergence of Chinese Large Language Models (LLMs), how to evaluate a model's capabilities has become an increasingly significant issue.
2308.04813_1892705_1	The absence of a comprehensive Chinese benchmark that thoroughly assesses a model's performance, the unstandardized and incomparable prompting procedure, and the prevalent risk of contamination pose major challenges in the current evaluation of Chinese LLMs.
2308.04823_1892715_4	Gscore uniquely automates the quality measurement of a model's text generation against reference standards, providing a detailed and nuanced assessment of model performance.
2308.04838_1892730_6	We also specifically investigate ChatGPT's ability to engage in multi-round fixing process (i.e., ChatGPT's dialog ability) of facilitating code generation.
2308.04941_1892833_6	This alignment with empirical findings regarding eye movements in dyslexic individuals highlights the model's potential to aid in understanding the cognitive processes underlying reading and eye movements, as well as how reading deficits associated with dyslexia may emerge from maladaptive predictive processing.   
2308.05061_1892953_1	However, the current model's overdependence on function data overlooks the invaluable human insight into the operator.
2308.05201_1893093_2	Leveraging an extensive dataset from a prominent online labor market, we uncover a post-ChatGPT decline in labor demand, supply, and transactions for submarkets pertaining to text-related and programming-related jobs, in comparison to those not directly exposed to ChatGPT's core functionalities.
2308.05201_1893093_5	Although the per-period job diversity freelancers apply for tends to be more limited, those who successfully navigate skill transitions from text to programming demonstrate greater resilience to ChatGPT's overall market contraction impact.
2308.05201_1893093_6	As AI becomes increasingly versatile and potent, our paper offers crucial insights into AI's influence on labor markets and individuals' reactions, underscoring the necessity for proactive interventions to address the challenges and opportunities presented by this transformative technology.
2308.05713_1893605_2	Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems.
2308.05999_1893891_6	This suite of metrics has demonstrated a better ability to assess a model's performance in real-world scientific applications, in contrast to traditional AI benchmarking methodologies.
2308.06032_1893924_2	We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying GPT-3.5's legal reasoning and ChatGPT's legal drafting capabilities.
2308.06032_1893924_6	GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations).
2308.06032_1893924_9	However, ChatGPT's drafting skills (though, perhaps, still inferior to lawyers) could assist lawyers in providing legal services.
2308.06032_1893924_10	Our research is the first to systematically study an LLM's legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.
2308.06055_1893947_1	The system's building blocks are tailored to seamlessly integrate IQA, ensuring reliable performance in disease classification.
2308.06373_1894265_3	ChatGPT's operational principles, prospective applications, and ability to advance a range of human endeavours are discussed in the paper.
2308.06920_1894812_8	This research sheds light on the collaborative synergy between human expertise and AI assistance, wherein ChatGPT's cognitive abilities enhance the design and development of potential pharmaceutical solutions.
2308.07326_1895218_1	By employing a behavioral psychology framework called OCEAN (Openness, Conscientiousness, Extroversion, Agreeableness, Neuroticism), we quantitatively gauged the model's responsiveness to tailored prompts.
2308.07326_1895218_4	Our findings underscore GPT's versatility and ability to discern and adapt to nuanced instructions.
2308.07326_1895218_5	Furthermore, historical figure simulations highlighted the LLM's capacity to internalize and project instructible personas, precisely replicating their philosophies and dialogic styles.
2308.07876_1895768_4	The experiments reveal ChatGPT's strengths and limitations, and crucially show ZSP's outperformance of dictionary-based methods and its competitive edge over some supervised models.
2308.07902_1895794_10	Finally, we give our suggestions on the future direction of LLM's evaluation.
2308.07935_1895827_5	By underlining the significance of prompt engineering, particularly in zero-shot contexts, this study spotlights ChatGPT's potential to substantially boost sentiment analysis in financial applications.
2308.08241_1896133_1	Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TS-for-LLM, where we aim to activate LLM's ability for TS data by designing a TS embedding method suitable for LLM.
2308.08241_1896133_7	By treating LLM as the pattern machine, TEST can endow LLM's ability to process TS data without compromising language ability.
2308.08407_1896299_3	Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders.
2308.08469_1896361_9	Ablation studies further validate each component's contribution to LLM4TS and underscore the essential role of utilizing LLM's pre-trained weights for optimal performance.
2308.08493_1896385_4	An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference.
2308.08998_1896890_0	  Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences.
2308.09012_1896904_4	Instead, we view this as a multimodal task, using text as auxiliary information to facilitate the visual model's understanding of the logo.
2308.09217_1897109_1	ChatGPT's output is compared to the results of the Ontology Alignment Evaluation Initiative 2022 campaign using conference track ontologies.
2308.09313_1897205_9	Furthermore, our approach operates without the requirement for direct access to the language model's parameters.
2308.09380_1897272_2	Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI systems in healthcare.
2308.09454_1897346_4	We evaluate the effect of these sampling strategies in two scenarios: optimal circumstances with a well-calibrated model and suboptimal circumstances where we systematically degrade the model's performance.
2308.09954_1897846_9	In addition to assessing the effectiveness of knowledge editing and the retention of unrelated knowledge from conventional studies, we further test the LLM's ability in two aspects: 1) Reasoning with the altered knowledge, aiming for the LLM to genuinely learn the altered knowledge instead of simply memorizing it.
2308.10144_1898036_1	While there is a growing demand to tailor LLMs for custom decision-making tasks, finetuning them for specific tasks is resource-intensive and may diminish the model's generalization capabilities.
2308.10248_1898140_1	However, these methods do not fully elicit a model's capabilities.
2308.10345_1898237_5	GPT-4's code corrections led to a 90% reduction in vulnerabilities, requiring only an 11% increase in code lines.
2308.10397_1898289_1	However, the majority of existing methods focus on measuring the model's preference towards sentences containing biases and stereotypes within datasets, which lacks interpretability and cannot detect implicit biases and stereotypes in the real world.
2308.10443_1898335_11	The paper concludes by discussing LLM's impact on CTF exercises and its implications.
2308.10454_1898346_6	The results will shed light on the design of educational system in terms of harnessing AI's potential to empower educational stakeholders.
2308.10837_1898729_1	However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem.
2308.10855_1898747_3	Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework.
